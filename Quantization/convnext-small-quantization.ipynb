{"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1663377,"sourceType":"datasetVersion","datasetId":918039},{"sourceId":2546969,"sourceType":"datasetVersion","datasetId":1544742},{"sourceId":6810129,"sourceType":"datasetVersion","datasetId":3917752}],"dockerImageVersionId":30683,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Imports here\nimport torch\nimport PIL\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nfrom torchvision import datasets, transforms, models\nfrom torch import nn\nfrom torch import optim\nfrom collections import OrderedDict\nfrom time import time\nimport copy\nfrom sklearn.metrics import confusion_matrix,classification_report\nimport numpy as np\nimport skimage.io as io\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2024-04-09T08:47:56.484514Z","iopub.execute_input":"2024-04-09T08:47:56.485531Z","iopub.status.idle":"2024-04-09T08:47:56.499597Z","shell.execute_reply.started":"2024-04-09T08:47:56.485493Z","shell.execute_reply":"2024-04-09T08:47:56.498518Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"data_dir = '/kaggle/input/eurosat-dataset'\ntrain_dir = data_dir + '/EuroSAT'\n\n# Define your transformations\ntrain_transforms = transforms.Compose([\n    #transforms.RandomRotation(30),\n    transforms.RandomResizedCrop(224),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\n# Load all the images from the train folder\nall_data = datasets.ImageFolder(train_dir, transform=train_transforms)\n\n# Calculate the sizes for train, validation, and test sets\ntotal_size = len(all_data)\ntrain_size = int(0.7 * total_size)\ntest_size = int(0.2 * total_size)\nvalid_size = total_size - train_size - test_size\n\n# Use random_split to split the dataset\ntrain_data, valid_data, test_data = torch.utils.data.random_split(all_data, [train_size, valid_size, test_size])\n\n# Create data loaders\ntrainloader = torch.utils.data.DataLoader(train_data, batch_size=50, shuffle=False)\nvalidloader = torch.utils.data.DataLoader(valid_data, batch_size=50)\ntestloader = torch.utils.data.DataLoader(test_data, batch_size=50)","metadata":{"execution":{"iopub.status.busy":"2024-04-09T08:48:00.125712Z","iopub.execute_input":"2024-04-09T08:48:00.126599Z","iopub.status.idle":"2024-04-09T08:48:25.453866Z","shell.execute_reply.started":"2024-04-09T08:48:00.126566Z","shell.execute_reply":"2024-04-09T08:48:25.452704Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Load a pre-trained network \nmodel = models.convnext_tiny(pretrained=True,weight='ConvNeXt_Tiny_Weights')\nmodel.name = \"onvnext_tiny\"\nmodel","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-04-09T09:22:09.287700Z","iopub.execute_input":"2024-04-09T09:22:09.288192Z","iopub.status.idle":"2024-04-09T09:22:10.040701Z","shell.execute_reply.started":"2024-04-09T09:22:09.288156Z","shell.execute_reply":"2024-04-09T09:22:10.039608Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":48,"outputs":[{"execution_count":48,"output_type":"execute_result","data":{"text/plain":"ConvNeXt(\n  (features): Sequential(\n    (0): Conv2dNormActivation(\n      (0): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\n      (1): LayerNorm2d((96,), eps=1e-06, elementwise_affine=True)\n    )\n    (1): Sequential(\n      (0): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n          (1): Permute()\n          (2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=96, out_features=384, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=384, out_features=96, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n      )\n      (1): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n          (1): Permute()\n          (2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=96, out_features=384, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=384, out_features=96, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.0058823529411764705, mode=row)\n      )\n      (2): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n          (1): Permute()\n          (2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=96, out_features=384, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=384, out_features=96, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.011764705882352941, mode=row)\n      )\n    )\n    (2): Sequential(\n      (0): LayerNorm2d((96,), eps=1e-06, elementwise_affine=True)\n      (1): Conv2d(96, 192, kernel_size=(2, 2), stride=(2, 2))\n    )\n    (3): Sequential(\n      (0): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n          (1): Permute()\n          (2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=192, out_features=768, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=768, out_features=192, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.017647058823529415, mode=row)\n      )\n      (1): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n          (1): Permute()\n          (2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=192, out_features=768, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=768, out_features=192, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.023529411764705882, mode=row)\n      )\n      (2): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n          (1): Permute()\n          (2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=192, out_features=768, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=768, out_features=192, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.029411764705882353, mode=row)\n      )\n    )\n    (4): Sequential(\n      (0): LayerNorm2d((192,), eps=1e-06, elementwise_affine=True)\n      (1): Conv2d(192, 384, kernel_size=(2, 2), stride=(2, 2))\n    )\n    (5): Sequential(\n      (0): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n          (1): Permute()\n          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=384, out_features=1536, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=1536, out_features=384, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.03529411764705883, mode=row)\n      )\n      (1): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n          (1): Permute()\n          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=384, out_features=1536, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=1536, out_features=384, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.0411764705882353, mode=row)\n      )\n      (2): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n          (1): Permute()\n          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=384, out_features=1536, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=1536, out_features=384, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.047058823529411764, mode=row)\n      )\n      (3): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n          (1): Permute()\n          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=384, out_features=1536, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=1536, out_features=384, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.052941176470588235, mode=row)\n      )\n      (4): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n          (1): Permute()\n          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=384, out_features=1536, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=1536, out_features=384, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.058823529411764705, mode=row)\n      )\n      (5): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n          (1): Permute()\n          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=384, out_features=1536, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=1536, out_features=384, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.06470588235294118, mode=row)\n      )\n      (6): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n          (1): Permute()\n          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=384, out_features=1536, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=1536, out_features=384, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.07058823529411766, mode=row)\n      )\n      (7): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n          (1): Permute()\n          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=384, out_features=1536, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=1536, out_features=384, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.07647058823529412, mode=row)\n      )\n      (8): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n          (1): Permute()\n          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=384, out_features=1536, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=1536, out_features=384, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.0823529411764706, mode=row)\n      )\n    )\n    (6): Sequential(\n      (0): LayerNorm2d((384,), eps=1e-06, elementwise_affine=True)\n      (1): Conv2d(384, 768, kernel_size=(2, 2), stride=(2, 2))\n    )\n    (7): Sequential(\n      (0): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n          (1): Permute()\n          (2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=768, out_features=3072, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=3072, out_features=768, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.08823529411764706, mode=row)\n      )\n      (1): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n          (1): Permute()\n          (2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=768, out_features=3072, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=3072, out_features=768, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.09411764705882353, mode=row)\n      )\n      (2): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n          (1): Permute()\n          (2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=768, out_features=3072, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=3072, out_features=768, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.1, mode=row)\n      )\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=1)\n  (classifier): Sequential(\n    (0): LayerNorm2d((768,), eps=1e-06, elementwise_affine=True)\n    (1): Flatten(start_dim=1, end_dim=-1)\n    (2): Linear(in_features=768, out_features=1000, bias=True)\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"for param in model.parameters():\n    param.requires_grad = False","metadata":{"execution":{"iopub.status.busy":"2024-04-09T09:22:18.041323Z","iopub.execute_input":"2024-04-09T09:22:18.041689Z","iopub.status.idle":"2024-04-09T09:22:18.047878Z","shell.execute_reply.started":"2024-04-09T09:22:18.041662Z","shell.execute_reply":"2024-04-09T09:22:18.046676Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"num_ftrs = model.classifier[-1].in_features\nmodel.classifier[-1] = nn.Linear(num_ftrs, 10)  ","metadata":{"execution":{"iopub.status.busy":"2024-04-09T09:22:22.208581Z","iopub.execute_input":"2024-04-09T09:22:22.208964Z","iopub.status.idle":"2024-04-09T09:22:22.215362Z","shell.execute_reply.started":"2024-04-09T09:22:22.208936Z","shell.execute_reply":"2024-04-09T09:22:22.214245Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ndevice","metadata":{"execution":{"iopub.status.busy":"2024-04-09T09:22:26.900414Z","iopub.execute_input":"2024-04-09T09:22:26.900801Z","iopub.status.idle":"2024-04-09T09:22:26.907921Z","shell.execute_reply.started":"2024-04-09T09:22:26.900769Z","shell.execute_reply":"2024-04-09T09:22:26.906864Z"},"trusted":true},"execution_count":51,"outputs":[{"execution_count":51,"output_type":"execute_result","data":{"text/plain":"device(type='cuda', index=0)"},"metadata":{}}]},{"cell_type":"code","source":"model.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-04-09T09:22:30.946913Z","iopub.execute_input":"2024-04-09T09:22:30.947308Z","iopub.status.idle":"2024-04-09T09:22:30.998888Z","shell.execute_reply.started":"2024-04-09T09:22:30.947274Z","shell.execute_reply":"2024-04-09T09:22:30.997877Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":52,"outputs":[{"execution_count":52,"output_type":"execute_result","data":{"text/plain":"ConvNeXt(\n  (features): Sequential(\n    (0): Conv2dNormActivation(\n      (0): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\n      (1): LayerNorm2d((96,), eps=1e-06, elementwise_affine=True)\n    )\n    (1): Sequential(\n      (0): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n          (1): Permute()\n          (2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=96, out_features=384, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=384, out_features=96, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n      )\n      (1): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n          (1): Permute()\n          (2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=96, out_features=384, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=384, out_features=96, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.0058823529411764705, mode=row)\n      )\n      (2): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n          (1): Permute()\n          (2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=96, out_features=384, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=384, out_features=96, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.011764705882352941, mode=row)\n      )\n    )\n    (2): Sequential(\n      (0): LayerNorm2d((96,), eps=1e-06, elementwise_affine=True)\n      (1): Conv2d(96, 192, kernel_size=(2, 2), stride=(2, 2))\n    )\n    (3): Sequential(\n      (0): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n          (1): Permute()\n          (2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=192, out_features=768, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=768, out_features=192, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.017647058823529415, mode=row)\n      )\n      (1): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n          (1): Permute()\n          (2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=192, out_features=768, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=768, out_features=192, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.023529411764705882, mode=row)\n      )\n      (2): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n          (1): Permute()\n          (2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=192, out_features=768, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=768, out_features=192, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.029411764705882353, mode=row)\n      )\n    )\n    (4): Sequential(\n      (0): LayerNorm2d((192,), eps=1e-06, elementwise_affine=True)\n      (1): Conv2d(192, 384, kernel_size=(2, 2), stride=(2, 2))\n    )\n    (5): Sequential(\n      (0): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n          (1): Permute()\n          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=384, out_features=1536, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=1536, out_features=384, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.03529411764705883, mode=row)\n      )\n      (1): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n          (1): Permute()\n          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=384, out_features=1536, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=1536, out_features=384, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.0411764705882353, mode=row)\n      )\n      (2): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n          (1): Permute()\n          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=384, out_features=1536, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=1536, out_features=384, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.047058823529411764, mode=row)\n      )\n      (3): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n          (1): Permute()\n          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=384, out_features=1536, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=1536, out_features=384, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.052941176470588235, mode=row)\n      )\n      (4): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n          (1): Permute()\n          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=384, out_features=1536, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=1536, out_features=384, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.058823529411764705, mode=row)\n      )\n      (5): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n          (1): Permute()\n          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=384, out_features=1536, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=1536, out_features=384, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.06470588235294118, mode=row)\n      )\n      (6): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n          (1): Permute()\n          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=384, out_features=1536, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=1536, out_features=384, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.07058823529411766, mode=row)\n      )\n      (7): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n          (1): Permute()\n          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=384, out_features=1536, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=1536, out_features=384, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.07647058823529412, mode=row)\n      )\n      (8): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n          (1): Permute()\n          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=384, out_features=1536, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=1536, out_features=384, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.0823529411764706, mode=row)\n      )\n    )\n    (6): Sequential(\n      (0): LayerNorm2d((384,), eps=1e-06, elementwise_affine=True)\n      (1): Conv2d(384, 768, kernel_size=(2, 2), stride=(2, 2))\n    )\n    (7): Sequential(\n      (0): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n          (1): Permute()\n          (2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=768, out_features=3072, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=3072, out_features=768, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.08823529411764706, mode=row)\n      )\n      (1): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n          (1): Permute()\n          (2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=768, out_features=3072, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=3072, out_features=768, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.09411764705882353, mode=row)\n      )\n      (2): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n          (1): Permute()\n          (2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=768, out_features=3072, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=3072, out_features=768, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.1, mode=row)\n      )\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=1)\n  (classifier): Sequential(\n    (0): LayerNorm2d((768,), eps=1e-06, elementwise_affine=True)\n    (1): Flatten(start_dim=1, end_dim=-1)\n    (2): Linear(in_features=768, out_features=10, bias=True)\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n\nepochs = 5\nprint_every = 30 # Prints every 30 images out of batch of 50 images\nsteps = 0","metadata":{"execution":{"iopub.status.busy":"2024-04-09T09:22:36.373501Z","iopub.execute_input":"2024-04-09T09:22:36.373868Z","iopub.status.idle":"2024-04-09T09:22:36.380938Z","shell.execute_reply.started":"2024-04-09T09:22:36.373841Z","shell.execute_reply":"2024-04-09T09:22:36.379686Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"def validation(model, testloader, criterion,device):\n    test_loss = 0\n    accuracy = 0\n    \n    for ii, (inputs, labels) in enumerate(testloader):\n        \n        inputs, labels = inputs.to(device), labels.to(device)\n        \n        output = model.forward(inputs)\n        test_loss += criterion(output, labels).item()\n        \n        ps = torch.exp(output)\n        equality = (labels.data == ps.max(dim=1)[1])\n        accuracy += equality.type(torch.FloatTensor).mean()\n    \n    return test_loss, accuracy","metadata":{"execution":{"iopub.status.busy":"2024-04-09T09:22:39.559760Z","iopub.execute_input":"2024-04-09T09:22:39.560671Z","iopub.status.idle":"2024-04-09T09:22:39.567615Z","shell.execute_reply.started":"2024-04-09T09:22:39.560637Z","shell.execute_reply":"2024-04-09T09:22:39.566551Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"def train(model, trainloader, validloader, criterion, optimizer, device, epochs, steps,print_every):\n    print(\"Training process initializing .....\\n\")\n\n    for e in range(epochs):\n        running_loss = 0\n        model.train() \n    \n        for ii, (inputs, labels) in enumerate(trainloader):\n            steps += 1\n        \n            inputs, labels = inputs.to(device), labels.to(device)\n        \n            optimizer.zero_grad()\n        \n            # Forward and backward passes\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n        \n            running_loss += loss.item()\n        \n            if steps % print_every == 0:\n                model.eval()\n\n                with torch.no_grad():\n                    valid_loss, accuracy = validation(model, validloader, criterion, device)\n            \n                print(\"Epoch: {}/{} | \".format(e+1, epochs),\n                      \"Training Loss: {:.4f} | \".format(running_loss/print_every),\n                      \"Validation Loss: {:.4f} | \".format(valid_loss/len(validloader)),\n                      \"Validation Accuracy: {:.4f}\".format(accuracy/len(validloader)))\n            \n                running_loss = 0\n                model.train()\n\n    print(\"\\nTraining process is now complete!!\")\n    return model\n","metadata":{"execution":{"iopub.status.busy":"2024-04-09T09:22:42.883414Z","iopub.execute_input":"2024-04-09T09:22:42.884064Z","iopub.status.idle":"2024-04-09T09:22:42.894649Z","shell.execute_reply.started":"2024-04-09T09:22:42.884030Z","shell.execute_reply":"2024-04-09T09:22:42.893374Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"model = train(model,trainloader, validloader,criterion, optimizer,device, epochs,steps, print_every)","metadata":{"execution":{"iopub.status.busy":"2024-04-09T09:22:46.637077Z","iopub.execute_input":"2024-04-09T09:22:46.638142Z","iopub.status.idle":"2024-04-09T09:51:14.005945Z","shell.execute_reply.started":"2024-04-09T09:22:46.638095Z","shell.execute_reply":"2024-04-09T09:51:14.004899Z"},"trusted":true},"execution_count":56,"outputs":[{"name":"stdout","text":"Training process initializing .....\n\nEpoch: 1/5 |  Training Loss: 1.9959 |  Validation Loss: 1.5898 |  Validation Accuracy: 0.6044\nEpoch: 1/5 |  Training Loss: 1.4024 |  Validation Loss: 1.1579 |  Validation Accuracy: 0.7319\nEpoch: 1/5 |  Training Loss: 1.0866 |  Validation Loss: 0.9677 |  Validation Accuracy: 0.7730\nEpoch: 1/5 |  Training Loss: 0.9674 |  Validation Loss: 0.8453 |  Validation Accuracy: 0.7970\nEpoch: 1/5 |  Training Loss: 0.8595 |  Validation Loss: 0.7817 |  Validation Accuracy: 0.8037\nEpoch: 1/5 |  Training Loss: 0.7645 |  Validation Loss: 0.7128 |  Validation Accuracy: 0.8230\nEpoch: 1/5 |  Training Loss: 0.7176 |  Validation Loss: 0.6710 |  Validation Accuracy: 0.8370\nEpoch: 1/5 |  Training Loss: 0.6918 |  Validation Loss: 0.6278 |  Validation Accuracy: 0.8396\nEpoch: 1/5 |  Training Loss: 0.6480 |  Validation Loss: 0.6135 |  Validation Accuracy: 0.8367\nEpoch: 1/5 |  Training Loss: 0.6191 |  Validation Loss: 0.6020 |  Validation Accuracy: 0.8422\nEpoch: 1/5 |  Training Loss: 0.6252 |  Validation Loss: 0.5857 |  Validation Accuracy: 0.8444\nEpoch: 1/5 |  Training Loss: 0.5881 |  Validation Loss: 0.5411 |  Validation Accuracy: 0.8570\nEpoch: 2/5 |  Training Loss: 0.2379 |  Validation Loss: 0.5329 |  Validation Accuracy: 0.8596\nEpoch: 2/5 |  Training Loss: 0.5352 |  Validation Loss: 0.5278 |  Validation Accuracy: 0.8519\nEpoch: 2/5 |  Training Loss: 0.5330 |  Validation Loss: 0.5269 |  Validation Accuracy: 0.8504\nEpoch: 2/5 |  Training Loss: 0.5285 |  Validation Loss: 0.4959 |  Validation Accuracy: 0.8607\nEpoch: 2/5 |  Training Loss: 0.5158 |  Validation Loss: 0.4936 |  Validation Accuracy: 0.8615\nEpoch: 2/5 |  Training Loss: 0.4952 |  Validation Loss: 0.4908 |  Validation Accuracy: 0.8574\nEpoch: 2/5 |  Training Loss: 0.4983 |  Validation Loss: 0.4705 |  Validation Accuracy: 0.8667\nEpoch: 2/5 |  Training Loss: 0.4978 |  Validation Loss: 0.4702 |  Validation Accuracy: 0.8704\nEpoch: 2/5 |  Training Loss: 0.5154 |  Validation Loss: 0.4673 |  Validation Accuracy: 0.8659\nEpoch: 2/5 |  Training Loss: 0.4760 |  Validation Loss: 0.4617 |  Validation Accuracy: 0.8685\nEpoch: 2/5 |  Training Loss: 0.4640 |  Validation Loss: 0.4419 |  Validation Accuracy: 0.8696\nEpoch: 2/5 |  Training Loss: 0.4994 |  Validation Loss: 0.4543 |  Validation Accuracy: 0.8704\nEpoch: 2/5 |  Training Loss: 0.4762 |  Validation Loss: 0.4399 |  Validation Accuracy: 0.8693\nEpoch: 3/5 |  Training Loss: 0.3699 |  Validation Loss: 0.4371 |  Validation Accuracy: 0.8767\nEpoch: 3/5 |  Training Loss: 0.4246 |  Validation Loss: 0.4266 |  Validation Accuracy: 0.8767\nEpoch: 3/5 |  Training Loss: 0.4276 |  Validation Loss: 0.4100 |  Validation Accuracy: 0.8826\nEpoch: 3/5 |  Training Loss: 0.4532 |  Validation Loss: 0.4129 |  Validation Accuracy: 0.8811\nEpoch: 3/5 |  Training Loss: 0.4464 |  Validation Loss: 0.4212 |  Validation Accuracy: 0.8770\nEpoch: 3/5 |  Training Loss: 0.4424 |  Validation Loss: 0.4049 |  Validation Accuracy: 0.8785\nEpoch: 3/5 |  Training Loss: 0.4484 |  Validation Loss: 0.4201 |  Validation Accuracy: 0.8715\nEpoch: 3/5 |  Training Loss: 0.4645 |  Validation Loss: 0.4057 |  Validation Accuracy: 0.8796\nEpoch: 3/5 |  Training Loss: 0.4184 |  Validation Loss: 0.4065 |  Validation Accuracy: 0.8781\nEpoch: 3/5 |  Training Loss: 0.4466 |  Validation Loss: 0.3963 |  Validation Accuracy: 0.8819\nEpoch: 3/5 |  Training Loss: 0.4263 |  Validation Loss: 0.3991 |  Validation Accuracy: 0.8807\nEpoch: 3/5 |  Training Loss: 0.4354 |  Validation Loss: 0.3895 |  Validation Accuracy: 0.8896\nEpoch: 4/5 |  Training Loss: 0.0868 |  Validation Loss: 0.3966 |  Validation Accuracy: 0.8826\nEpoch: 4/5 |  Training Loss: 0.4002 |  Validation Loss: 0.3886 |  Validation Accuracy: 0.8863\nEpoch: 4/5 |  Training Loss: 0.4009 |  Validation Loss: 0.3938 |  Validation Accuracy: 0.8822\nEpoch: 4/5 |  Training Loss: 0.3980 |  Validation Loss: 0.3778 |  Validation Accuracy: 0.8878\nEpoch: 4/5 |  Training Loss: 0.3857 |  Validation Loss: 0.3904 |  Validation Accuracy: 0.8807\nEpoch: 4/5 |  Training Loss: 0.4017 |  Validation Loss: 0.3707 |  Validation Accuracy: 0.8904\nEpoch: 4/5 |  Training Loss: 0.3912 |  Validation Loss: 0.3918 |  Validation Accuracy: 0.8719\nEpoch: 4/5 |  Training Loss: 0.4204 |  Validation Loss: 0.3834 |  Validation Accuracy: 0.8830\nEpoch: 4/5 |  Training Loss: 0.4125 |  Validation Loss: 0.3840 |  Validation Accuracy: 0.8907\nEpoch: 4/5 |  Training Loss: 0.4026 |  Validation Loss: 0.3643 |  Validation Accuracy: 0.8893\nEpoch: 4/5 |  Training Loss: 0.3766 |  Validation Loss: 0.3634 |  Validation Accuracy: 0.8863\nEpoch: 4/5 |  Training Loss: 0.4207 |  Validation Loss: 0.3569 |  Validation Accuracy: 0.8937\nEpoch: 4/5 |  Training Loss: 0.3928 |  Validation Loss: 0.3713 |  Validation Accuracy: 0.8867\nEpoch: 5/5 |  Training Loss: 0.2138 |  Validation Loss: 0.3679 |  Validation Accuracy: 0.8863\nEpoch: 5/5 |  Training Loss: 0.3882 |  Validation Loss: 0.3757 |  Validation Accuracy: 0.8822\nEpoch: 5/5 |  Training Loss: 0.3874 |  Validation Loss: 0.3747 |  Validation Accuracy: 0.8826\nEpoch: 5/5 |  Training Loss: 0.3732 |  Validation Loss: 0.3506 |  Validation Accuracy: 0.8915\nEpoch: 5/5 |  Training Loss: 0.3633 |  Validation Loss: 0.3544 |  Validation Accuracy: 0.8937\nEpoch: 5/5 |  Training Loss: 0.3838 |  Validation Loss: 0.3606 |  Validation Accuracy: 0.8900\nEpoch: 5/5 |  Training Loss: 0.3804 |  Validation Loss: 0.3560 |  Validation Accuracy: 0.8885\nEpoch: 5/5 |  Training Loss: 0.3766 |  Validation Loss: 0.3580 |  Validation Accuracy: 0.8937\nEpoch: 5/5 |  Training Loss: 0.3826 |  Validation Loss: 0.3647 |  Validation Accuracy: 0.8867\nEpoch: 5/5 |  Training Loss: 0.3730 |  Validation Loss: 0.3543 |  Validation Accuracy: 0.8907\nEpoch: 5/5 |  Training Loss: 0.4049 |  Validation Loss: 0.3492 |  Validation Accuracy: 0.8915\nEpoch: 5/5 |  Training Loss: 0.3730 |  Validation Loss: 0.3543 |  Validation Accuracy: 0.8915\nEpoch: 5/5 |  Training Loss: 0.3846 |  Validation Loss: 0.3629 |  Validation Accuracy: 0.8900\n\nTraining process is now complete!!\n","output_type":"stream"}]},{"cell_type":"code","source":"def pred(Model,Testloader):\n    all_labels = []\n    all_predictions = []\n    correct = 0\n    total = 0\n    start_time = time()\n    with torch.no_grad():\n        Model.eval()\n        for images, labels in Testloader:\n            all_labels.extend(labels.numpy())\n            images, labels = images.to(device), labels.to(device)\n            outputs = Model(images)\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n            predicted_tensor_cpu = predicted.to('cpu')\n            all_predictions.extend(predicted_tensor_cpu.numpy())\n    end_time = time()\n    print(\"Time: \",end_time - start_time)\n    print('Accuracy achieved by the network on test images is: %d%%' % (100 * correct / total))\n    \n    return all_labels,all_predictions","metadata":{"execution":{"iopub.status.busy":"2024-04-09T09:52:13.456790Z","iopub.execute_input":"2024-04-09T09:52:13.457838Z","iopub.status.idle":"2024-04-09T09:52:13.466471Z","shell.execute_reply.started":"2024-04-09T09:52:13.457791Z","shell.execute_reply":"2024-04-09T09:52:13.465443Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"labels_fp32,predictions_fp32 = pred(model,testloader)","metadata":{"execution":{"iopub.status.busy":"2024-04-09T09:52:16.867678Z","iopub.execute_input":"2024-04-09T09:52:16.868058Z","iopub.status.idle":"2024-04-09T09:52:54.332235Z","shell.execute_reply.started":"2024-04-09T09:52:16.868030Z","shell.execute_reply":"2024-04-09T09:52:54.331223Z"},"trusted":true},"execution_count":58,"outputs":[{"name":"stdout","text":"Time:  37.45922541618347\nAccuracy achieved by the network on test images is: 88%\n","output_type":"stream"}]},{"cell_type":"code","source":"import seaborn as sns\ndef metrics(labels,predictions):\n    classes = train_data.dataset.classes\n    cm = confusion_matrix(np.array(labels), np.array(predictions))\n    print(\"Confusion Matrix:\")\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n    plt.xlabel('Predicted')\n    plt.ylabel('Actual')\n    plt.title('Confusion Matrix')\n    plt.show()\n    print('----------------------------------------------------------------')\n    print(\"Classification Report:\")\n    report = classification_report(np.array(labels),np.array(predictions))\n    print(report)","metadata":{"execution":{"iopub.status.busy":"2024-04-09T09:53:00.571313Z","iopub.execute_input":"2024-04-09T09:53:00.572321Z","iopub.status.idle":"2024-04-09T09:53:00.580539Z","shell.execute_reply.started":"2024-04-09T09:53:00.572276Z","shell.execute_reply":"2024-04-09T09:53:00.579459Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"metrics(labels_fp32,predictions_fp32)","metadata":{"execution":{"iopub.status.busy":"2024-04-09T09:53:07.101019Z","iopub.execute_input":"2024-04-09T09:53:07.101694Z","iopub.status.idle":"2024-04-09T09:53:08.029101Z","shell.execute_reply.started":"2024-04-09T09:53:07.101663Z","shell.execute_reply":"2024-04-09T09:53:08.028034Z"},"trusted":true},"execution_count":60,"outputs":[{"name":"stdout","text":"Confusion Matrix:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 800x600 with 2 Axes>","image/png":"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"},"metadata":{"image/png":{"width":793,"height":689}}},{"name":"stdout","text":"----------------------------------------------------------------\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.89      0.87      0.88       568\n           1       0.87      0.95      0.91       610\n           2       0.88      0.88      0.88       627\n           3       0.82      0.76      0.79       457\n           4       0.93      0.94      0.93       491\n           5       0.85      0.86      0.85       417\n           6       0.86      0.84      0.85       508\n           7       0.93      0.97      0.95       620\n           8       0.81      0.78      0.80       477\n           9       0.96      0.94      0.95       625\n\n    accuracy                           0.88      5400\n   macro avg       0.88      0.88      0.88      5400\nweighted avg       0.88      0.88      0.88      5400\n\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\ndef print_model_size(mdl):\n    torch.save(mdl.state_dict(), \"tmp.pt\")\n    print(\"%.2f MB\" %(os.path.getsize(\"tmp.pt\")/1e6))\n    os.remove('tmp.pt')","metadata":{"execution":{"iopub.status.busy":"2024-04-09T09:53:15.909499Z","iopub.execute_input":"2024-04-09T09:53:15.910385Z","iopub.status.idle":"2024-04-09T09:53:15.915870Z","shell.execute_reply.started":"2024-04-09T09:53:15.910348Z","shell.execute_reply":"2024-04-09T09:53:15.914886Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"code","source":"print(\"Size of fp32 model:\",end='')\nprint_model_size(model)","metadata":{"execution":{"iopub.status.busy":"2024-04-09T09:53:19.246251Z","iopub.execute_input":"2024-04-09T09:53:19.247249Z","iopub.status.idle":"2024-04-09T09:53:19.677809Z","shell.execute_reply.started":"2024-04-09T09:53:19.247206Z","shell.execute_reply":"2024-04-09T09:53:19.676723Z"},"trusted":true},"execution_count":62,"outputs":[{"name":"stdout","text":"Size of fp32 model:111.37 MB\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Quantization","metadata":{}},{"cell_type":"markdown","source":"# FP-16","metadata":{}},{"cell_type":"code","source":"model_fp16 = copy.deepcopy(model)","metadata":{"execution":{"iopub.status.busy":"2024-04-09T09:53:23.241161Z","iopub.execute_input":"2024-04-09T09:53:23.241802Z","iopub.status.idle":"2024-04-09T09:53:23.282623Z","shell.execute_reply.started":"2024-04-09T09:53:23.241768Z","shell.execute_reply":"2024-04-09T09:53:23.281534Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"model_fp16.half()\nmodel_fp16.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-04-09T09:53:28.192760Z","iopub.execute_input":"2024-04-09T09:53:28.193185Z","iopub.status.idle":"2024-04-09T09:53:28.213058Z","shell.execute_reply.started":"2024-04-09T09:53:28.193153Z","shell.execute_reply":"2024-04-09T09:53:28.211996Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":64,"outputs":[{"execution_count":64,"output_type":"execute_result","data":{"text/plain":"ConvNeXt(\n  (features): Sequential(\n    (0): Conv2dNormActivation(\n      (0): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\n      (1): LayerNorm2d((96,), eps=1e-06, elementwise_affine=True)\n    )\n    (1): Sequential(\n      (0): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n          (1): Permute()\n          (2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=96, out_features=384, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=384, out_features=96, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n      )\n      (1): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n          (1): Permute()\n          (2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=96, out_features=384, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=384, out_features=96, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.0058823529411764705, mode=row)\n      )\n      (2): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n          (1): Permute()\n          (2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=96, out_features=384, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=384, out_features=96, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.011764705882352941, mode=row)\n      )\n    )\n    (2): Sequential(\n      (0): LayerNorm2d((96,), eps=1e-06, elementwise_affine=True)\n      (1): Conv2d(96, 192, kernel_size=(2, 2), stride=(2, 2))\n    )\n    (3): Sequential(\n      (0): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n          (1): Permute()\n          (2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=192, out_features=768, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=768, out_features=192, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.017647058823529415, mode=row)\n      )\n      (1): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n          (1): Permute()\n          (2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=192, out_features=768, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=768, out_features=192, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.023529411764705882, mode=row)\n      )\n      (2): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n          (1): Permute()\n          (2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=192, out_features=768, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=768, out_features=192, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.029411764705882353, mode=row)\n      )\n    )\n    (4): Sequential(\n      (0): LayerNorm2d((192,), eps=1e-06, elementwise_affine=True)\n      (1): Conv2d(192, 384, kernel_size=(2, 2), stride=(2, 2))\n    )\n    (5): Sequential(\n      (0): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n          (1): Permute()\n          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=384, out_features=1536, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=1536, out_features=384, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.03529411764705883, mode=row)\n      )\n      (1): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n          (1): Permute()\n          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=384, out_features=1536, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=1536, out_features=384, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.0411764705882353, mode=row)\n      )\n      (2): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n          (1): Permute()\n          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=384, out_features=1536, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=1536, out_features=384, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.047058823529411764, mode=row)\n      )\n      (3): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n          (1): Permute()\n          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=384, out_features=1536, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=1536, out_features=384, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.052941176470588235, mode=row)\n      )\n      (4): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n          (1): Permute()\n          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=384, out_features=1536, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=1536, out_features=384, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.058823529411764705, mode=row)\n      )\n      (5): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n          (1): Permute()\n          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=384, out_features=1536, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=1536, out_features=384, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.06470588235294118, mode=row)\n      )\n      (6): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n          (1): Permute()\n          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=384, out_features=1536, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=1536, out_features=384, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.07058823529411766, mode=row)\n      )\n      (7): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n          (1): Permute()\n          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=384, out_features=1536, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=1536, out_features=384, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.07647058823529412, mode=row)\n      )\n      (8): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n          (1): Permute()\n          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=384, out_features=1536, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=1536, out_features=384, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.0823529411764706, mode=row)\n      )\n    )\n    (6): Sequential(\n      (0): LayerNorm2d((384,), eps=1e-06, elementwise_affine=True)\n      (1): Conv2d(384, 768, kernel_size=(2, 2), stride=(2, 2))\n    )\n    (7): Sequential(\n      (0): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n          (1): Permute()\n          (2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=768, out_features=3072, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=3072, out_features=768, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.08823529411764706, mode=row)\n      )\n      (1): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n          (1): Permute()\n          (2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=768, out_features=3072, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=3072, out_features=768, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.09411764705882353, mode=row)\n      )\n      (2): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n          (1): Permute()\n          (2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=768, out_features=3072, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=3072, out_features=768, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.1, mode=row)\n      )\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=1)\n  (classifier): Sequential(\n    (0): LayerNorm2d((768,), eps=1e-06, elementwise_affine=True)\n    (1): Flatten(start_dim=1, end_dim=-1)\n    (2): Linear(in_features=768, out_features=10, bias=True)\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"def pred_fp16(Model,Testloader):\n    all_labels = []\n    all_predictions = []\n    correct = 0\n    total = 0\n    start_time = time()\n    with torch.no_grad():\n        Model.eval()\n        for images, labels in Testloader:\n            all_labels.extend(labels.numpy())\n            images, labels = images.to(device), labels.to(device)\n            outputs = Model(images.half())\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n            predicted_tensor_cpu = predicted.to('cpu')\n            all_predictions.extend(predicted_tensor_cpu.numpy())\n    end_time = time()\n    print(\"Time: \",end_time - start_time)\n    print('Accuracy achieved by the network on test images is: %d%%' % (100 * correct / total))\n    \n    return all_labels,all_predictions","metadata":{"execution":{"iopub.status.busy":"2024-04-09T09:53:35.249369Z","iopub.execute_input":"2024-04-09T09:53:35.249770Z","iopub.status.idle":"2024-04-09T09:53:35.258469Z","shell.execute_reply.started":"2024-04-09T09:53:35.249739Z","shell.execute_reply":"2024-04-09T09:53:35.257344Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"code","source":"labels_fp16,predictions_fp16 = pred_fp16(model_fp16,testloader)","metadata":{"execution":{"iopub.status.busy":"2024-04-09T09:53:39.192239Z","iopub.execute_input":"2024-04-09T09:53:39.192631Z","iopub.status.idle":"2024-04-09T09:54:02.313247Z","shell.execute_reply.started":"2024-04-09T09:53:39.192601Z","shell.execute_reply":"2024-04-09T09:54:02.312155Z"},"trusted":true},"execution_count":66,"outputs":[{"name":"stdout","text":"Time:  23.115638732910156\nAccuracy achieved by the network on test images is: 88%\n","output_type":"stream"}]},{"cell_type":"code","source":"metrics(labels_fp16,predictions_fp16)","metadata":{"execution":{"iopub.status.busy":"2024-04-09T09:54:02.315105Z","iopub.execute_input":"2024-04-09T09:54:02.315446Z","iopub.status.idle":"2024-04-09T09:54:03.103210Z","shell.execute_reply.started":"2024-04-09T09:54:02.315417Z","shell.execute_reply":"2024-04-09T09:54:03.102166Z"},"trusted":true},"execution_count":67,"outputs":[{"name":"stdout","text":"Confusion Matrix:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 800x600 with 2 Axes>","image/png":"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"},"metadata":{"image/png":{"width":793,"height":689}}},{"name":"stdout","text":"----------------------------------------------------------------\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.88      0.87      0.87       568\n           1       0.90      0.95      0.92       610\n           2       0.87      0.91      0.89       627\n           3       0.83      0.79      0.81       457\n           4       0.93      0.94      0.94       491\n           5       0.84      0.84      0.84       417\n           6       0.88      0.84      0.86       508\n           7       0.94      0.95      0.94       620\n           8       0.80      0.79      0.79       477\n           9       0.97      0.94      0.95       625\n\n    accuracy                           0.89      5400\n   macro avg       0.88      0.88      0.88      5400\nweighted avg       0.89      0.89      0.89      5400\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# FP-64","metadata":{}},{"cell_type":"code","source":"model_64 = copy.deepcopy(model)\nmodel_64.double()","metadata":{"execution":{"iopub.status.busy":"2024-04-09T09:54:31.450635Z","iopub.execute_input":"2024-04-09T09:54:31.451421Z","iopub.status.idle":"2024-04-09T09:54:31.501762Z","shell.execute_reply.started":"2024-04-09T09:54:31.451387Z","shell.execute_reply":"2024-04-09T09:54:31.500692Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":70,"outputs":[{"execution_count":70,"output_type":"execute_result","data":{"text/plain":"ConvNeXt(\n  (features): Sequential(\n    (0): Conv2dNormActivation(\n      (0): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\n      (1): LayerNorm2d((96,), eps=1e-06, elementwise_affine=True)\n    )\n    (1): Sequential(\n      (0): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n          (1): Permute()\n          (2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=96, out_features=384, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=384, out_features=96, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n      )\n      (1): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n          (1): Permute()\n          (2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=96, out_features=384, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=384, out_features=96, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.0058823529411764705, mode=row)\n      )\n      (2): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n          (1): Permute()\n          (2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=96, out_features=384, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=384, out_features=96, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.011764705882352941, mode=row)\n      )\n    )\n    (2): Sequential(\n      (0): LayerNorm2d((96,), eps=1e-06, elementwise_affine=True)\n      (1): Conv2d(96, 192, kernel_size=(2, 2), stride=(2, 2))\n    )\n    (3): Sequential(\n      (0): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n          (1): Permute()\n          (2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=192, out_features=768, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=768, out_features=192, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.017647058823529415, mode=row)\n      )\n      (1): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n          (1): Permute()\n          (2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=192, out_features=768, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=768, out_features=192, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.023529411764705882, mode=row)\n      )\n      (2): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n          (1): Permute()\n          (2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=192, out_features=768, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=768, out_features=192, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.029411764705882353, mode=row)\n      )\n    )\n    (4): Sequential(\n      (0): LayerNorm2d((192,), eps=1e-06, elementwise_affine=True)\n      (1): Conv2d(192, 384, kernel_size=(2, 2), stride=(2, 2))\n    )\n    (5): Sequential(\n      (0): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n          (1): Permute()\n          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=384, out_features=1536, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=1536, out_features=384, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.03529411764705883, mode=row)\n      )\n      (1): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n          (1): Permute()\n          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=384, out_features=1536, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=1536, out_features=384, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.0411764705882353, mode=row)\n      )\n      (2): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n          (1): Permute()\n          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=384, out_features=1536, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=1536, out_features=384, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.047058823529411764, mode=row)\n      )\n      (3): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n          (1): Permute()\n          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=384, out_features=1536, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=1536, out_features=384, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.052941176470588235, mode=row)\n      )\n      (4): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n          (1): Permute()\n          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=384, out_features=1536, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=1536, out_features=384, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.058823529411764705, mode=row)\n      )\n      (5): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n          (1): Permute()\n          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=384, out_features=1536, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=1536, out_features=384, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.06470588235294118, mode=row)\n      )\n      (6): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n          (1): Permute()\n          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=384, out_features=1536, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=1536, out_features=384, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.07058823529411766, mode=row)\n      )\n      (7): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n          (1): Permute()\n          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=384, out_features=1536, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=1536, out_features=384, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.07647058823529412, mode=row)\n      )\n      (8): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n          (1): Permute()\n          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=384, out_features=1536, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=1536, out_features=384, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.0823529411764706, mode=row)\n      )\n    )\n    (6): Sequential(\n      (0): LayerNorm2d((384,), eps=1e-06, elementwise_affine=True)\n      (1): Conv2d(384, 768, kernel_size=(2, 2), stride=(2, 2))\n    )\n    (7): Sequential(\n      (0): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n          (1): Permute()\n          (2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=768, out_features=3072, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=3072, out_features=768, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.08823529411764706, mode=row)\n      )\n      (1): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n          (1): Permute()\n          (2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=768, out_features=3072, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=3072, out_features=768, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.09411764705882353, mode=row)\n      )\n      (2): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n          (1): Permute()\n          (2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=768, out_features=3072, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=3072, out_features=768, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.1, mode=row)\n      )\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=1)\n  (classifier): Sequential(\n    (0): LayerNorm2d((768,), eps=1e-06, elementwise_affine=True)\n    (1): Flatten(start_dim=1, end_dim=-1)\n    (2): Linear(in_features=768, out_features=10, bias=True)\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"weights_64 = model_64.state_dict()\nfor name, param in weights_64.items():\n    print(name)","metadata":{"execution":{"iopub.status.busy":"2024-04-09T09:54:37.583383Z","iopub.execute_input":"2024-04-09T09:54:37.584442Z","iopub.status.idle":"2024-04-09T09:54:37.594329Z","shell.execute_reply.started":"2024-04-09T09:54:37.584405Z","shell.execute_reply":"2024-04-09T09:54:37.593072Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":71,"outputs":[{"name":"stdout","text":"features.0.0.weight\nfeatures.0.0.bias\nfeatures.0.1.weight\nfeatures.0.1.bias\nfeatures.1.0.layer_scale\nfeatures.1.0.block.0.weight\nfeatures.1.0.block.0.bias\nfeatures.1.0.block.2.weight\nfeatures.1.0.block.2.bias\nfeatures.1.0.block.3.weight\nfeatures.1.0.block.3.bias\nfeatures.1.0.block.5.weight\nfeatures.1.0.block.5.bias\nfeatures.1.1.layer_scale\nfeatures.1.1.block.0.weight\nfeatures.1.1.block.0.bias\nfeatures.1.1.block.2.weight\nfeatures.1.1.block.2.bias\nfeatures.1.1.block.3.weight\nfeatures.1.1.block.3.bias\nfeatures.1.1.block.5.weight\nfeatures.1.1.block.5.bias\nfeatures.1.2.layer_scale\nfeatures.1.2.block.0.weight\nfeatures.1.2.block.0.bias\nfeatures.1.2.block.2.weight\nfeatures.1.2.block.2.bias\nfeatures.1.2.block.3.weight\nfeatures.1.2.block.3.bias\nfeatures.1.2.block.5.weight\nfeatures.1.2.block.5.bias\nfeatures.2.0.weight\nfeatures.2.0.bias\nfeatures.2.1.weight\nfeatures.2.1.bias\nfeatures.3.0.layer_scale\nfeatures.3.0.block.0.weight\nfeatures.3.0.block.0.bias\nfeatures.3.0.block.2.weight\nfeatures.3.0.block.2.bias\nfeatures.3.0.block.3.weight\nfeatures.3.0.block.3.bias\nfeatures.3.0.block.5.weight\nfeatures.3.0.block.5.bias\nfeatures.3.1.layer_scale\nfeatures.3.1.block.0.weight\nfeatures.3.1.block.0.bias\nfeatures.3.1.block.2.weight\nfeatures.3.1.block.2.bias\nfeatures.3.1.block.3.weight\nfeatures.3.1.block.3.bias\nfeatures.3.1.block.5.weight\nfeatures.3.1.block.5.bias\nfeatures.3.2.layer_scale\nfeatures.3.2.block.0.weight\nfeatures.3.2.block.0.bias\nfeatures.3.2.block.2.weight\nfeatures.3.2.block.2.bias\nfeatures.3.2.block.3.weight\nfeatures.3.2.block.3.bias\nfeatures.3.2.block.5.weight\nfeatures.3.2.block.5.bias\nfeatures.4.0.weight\nfeatures.4.0.bias\nfeatures.4.1.weight\nfeatures.4.1.bias\nfeatures.5.0.layer_scale\nfeatures.5.0.block.0.weight\nfeatures.5.0.block.0.bias\nfeatures.5.0.block.2.weight\nfeatures.5.0.block.2.bias\nfeatures.5.0.block.3.weight\nfeatures.5.0.block.3.bias\nfeatures.5.0.block.5.weight\nfeatures.5.0.block.5.bias\nfeatures.5.1.layer_scale\nfeatures.5.1.block.0.weight\nfeatures.5.1.block.0.bias\nfeatures.5.1.block.2.weight\nfeatures.5.1.block.2.bias\nfeatures.5.1.block.3.weight\nfeatures.5.1.block.3.bias\nfeatures.5.1.block.5.weight\nfeatures.5.1.block.5.bias\nfeatures.5.2.layer_scale\nfeatures.5.2.block.0.weight\nfeatures.5.2.block.0.bias\nfeatures.5.2.block.2.weight\nfeatures.5.2.block.2.bias\nfeatures.5.2.block.3.weight\nfeatures.5.2.block.3.bias\nfeatures.5.2.block.5.weight\nfeatures.5.2.block.5.bias\nfeatures.5.3.layer_scale\nfeatures.5.3.block.0.weight\nfeatures.5.3.block.0.bias\nfeatures.5.3.block.2.weight\nfeatures.5.3.block.2.bias\nfeatures.5.3.block.3.weight\nfeatures.5.3.block.3.bias\nfeatures.5.3.block.5.weight\nfeatures.5.3.block.5.bias\nfeatures.5.4.layer_scale\nfeatures.5.4.block.0.weight\nfeatures.5.4.block.0.bias\nfeatures.5.4.block.2.weight\nfeatures.5.4.block.2.bias\nfeatures.5.4.block.3.weight\nfeatures.5.4.block.3.bias\nfeatures.5.4.block.5.weight\nfeatures.5.4.block.5.bias\nfeatures.5.5.layer_scale\nfeatures.5.5.block.0.weight\nfeatures.5.5.block.0.bias\nfeatures.5.5.block.2.weight\nfeatures.5.5.block.2.bias\nfeatures.5.5.block.3.weight\nfeatures.5.5.block.3.bias\nfeatures.5.5.block.5.weight\nfeatures.5.5.block.5.bias\nfeatures.5.6.layer_scale\nfeatures.5.6.block.0.weight\nfeatures.5.6.block.0.bias\nfeatures.5.6.block.2.weight\nfeatures.5.6.block.2.bias\nfeatures.5.6.block.3.weight\nfeatures.5.6.block.3.bias\nfeatures.5.6.block.5.weight\nfeatures.5.6.block.5.bias\nfeatures.5.7.layer_scale\nfeatures.5.7.block.0.weight\nfeatures.5.7.block.0.bias\nfeatures.5.7.block.2.weight\nfeatures.5.7.block.2.bias\nfeatures.5.7.block.3.weight\nfeatures.5.7.block.3.bias\nfeatures.5.7.block.5.weight\nfeatures.5.7.block.5.bias\nfeatures.5.8.layer_scale\nfeatures.5.8.block.0.weight\nfeatures.5.8.block.0.bias\nfeatures.5.8.block.2.weight\nfeatures.5.8.block.2.bias\nfeatures.5.8.block.3.weight\nfeatures.5.8.block.3.bias\nfeatures.5.8.block.5.weight\nfeatures.5.8.block.5.bias\nfeatures.6.0.weight\nfeatures.6.0.bias\nfeatures.6.1.weight\nfeatures.6.1.bias\nfeatures.7.0.layer_scale\nfeatures.7.0.block.0.weight\nfeatures.7.0.block.0.bias\nfeatures.7.0.block.2.weight\nfeatures.7.0.block.2.bias\nfeatures.7.0.block.3.weight\nfeatures.7.0.block.3.bias\nfeatures.7.0.block.5.weight\nfeatures.7.0.block.5.bias\nfeatures.7.1.layer_scale\nfeatures.7.1.block.0.weight\nfeatures.7.1.block.0.bias\nfeatures.7.1.block.2.weight\nfeatures.7.1.block.2.bias\nfeatures.7.1.block.3.weight\nfeatures.7.1.block.3.bias\nfeatures.7.1.block.5.weight\nfeatures.7.1.block.5.bias\nfeatures.7.2.layer_scale\nfeatures.7.2.block.0.weight\nfeatures.7.2.block.0.bias\nfeatures.7.2.block.2.weight\nfeatures.7.2.block.2.bias\nfeatures.7.2.block.3.weight\nfeatures.7.2.block.3.bias\nfeatures.7.2.block.5.weight\nfeatures.7.2.block.5.bias\nclassifier.0.weight\nclassifier.0.bias\nclassifier.2.weight\nclassifier.2.bias\n","output_type":"stream"}]},{"cell_type":"code","source":"weights_64 = model_64.state_dict()\nprint(weights_64['features.0.0.weight'].dtype)\n# print(weights_64['conv1.weight'].dtype)","metadata":{"execution":{"iopub.status.busy":"2024-04-09T09:54:44.817255Z","iopub.execute_input":"2024-04-09T09:54:44.817678Z","iopub.status.idle":"2024-04-09T09:54:44.828428Z","shell.execute_reply.started":"2024-04-09T09:54:44.817644Z","shell.execute_reply":"2024-04-09T09:54:44.827248Z"},"trusted":true},"execution_count":72,"outputs":[{"name":"stdout","text":"torch.float64\n","output_type":"stream"}]},{"cell_type":"code","source":"model_64.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-04-09T09:54:48.983026Z","iopub.execute_input":"2024-04-09T09:54:48.983698Z","iopub.status.idle":"2024-04-09T09:54:48.997624Z","shell.execute_reply.started":"2024-04-09T09:54:48.983667Z","shell.execute_reply":"2024-04-09T09:54:48.996613Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":73,"outputs":[{"execution_count":73,"output_type":"execute_result","data":{"text/plain":"ConvNeXt(\n  (features): Sequential(\n    (0): Conv2dNormActivation(\n      (0): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\n      (1): LayerNorm2d((96,), eps=1e-06, elementwise_affine=True)\n    )\n    (1): Sequential(\n      (0): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n          (1): Permute()\n          (2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=96, out_features=384, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=384, out_features=96, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n      )\n      (1): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n          (1): Permute()\n          (2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=96, out_features=384, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=384, out_features=96, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.0058823529411764705, mode=row)\n      )\n      (2): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n          (1): Permute()\n          (2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=96, out_features=384, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=384, out_features=96, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.011764705882352941, mode=row)\n      )\n    )\n    (2): Sequential(\n      (0): LayerNorm2d((96,), eps=1e-06, elementwise_affine=True)\n      (1): Conv2d(96, 192, kernel_size=(2, 2), stride=(2, 2))\n    )\n    (3): Sequential(\n      (0): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n          (1): Permute()\n          (2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=192, out_features=768, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=768, out_features=192, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.017647058823529415, mode=row)\n      )\n      (1): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n          (1): Permute()\n          (2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=192, out_features=768, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=768, out_features=192, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.023529411764705882, mode=row)\n      )\n      (2): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n          (1): Permute()\n          (2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=192, out_features=768, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=768, out_features=192, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.029411764705882353, mode=row)\n      )\n    )\n    (4): Sequential(\n      (0): LayerNorm2d((192,), eps=1e-06, elementwise_affine=True)\n      (1): Conv2d(192, 384, kernel_size=(2, 2), stride=(2, 2))\n    )\n    (5): Sequential(\n      (0): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n          (1): Permute()\n          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=384, out_features=1536, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=1536, out_features=384, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.03529411764705883, mode=row)\n      )\n      (1): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n          (1): Permute()\n          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=384, out_features=1536, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=1536, out_features=384, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.0411764705882353, mode=row)\n      )\n      (2): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n          (1): Permute()\n          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=384, out_features=1536, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=1536, out_features=384, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.047058823529411764, mode=row)\n      )\n      (3): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n          (1): Permute()\n          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=384, out_features=1536, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=1536, out_features=384, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.052941176470588235, mode=row)\n      )\n      (4): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n          (1): Permute()\n          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=384, out_features=1536, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=1536, out_features=384, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.058823529411764705, mode=row)\n      )\n      (5): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n          (1): Permute()\n          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=384, out_features=1536, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=1536, out_features=384, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.06470588235294118, mode=row)\n      )\n      (6): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n          (1): Permute()\n          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=384, out_features=1536, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=1536, out_features=384, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.07058823529411766, mode=row)\n      )\n      (7): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n          (1): Permute()\n          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=384, out_features=1536, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=1536, out_features=384, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.07647058823529412, mode=row)\n      )\n      (8): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n          (1): Permute()\n          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=384, out_features=1536, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=1536, out_features=384, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.0823529411764706, mode=row)\n      )\n    )\n    (6): Sequential(\n      (0): LayerNorm2d((384,), eps=1e-06, elementwise_affine=True)\n      (1): Conv2d(384, 768, kernel_size=(2, 2), stride=(2, 2))\n    )\n    (7): Sequential(\n      (0): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n          (1): Permute()\n          (2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=768, out_features=3072, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=3072, out_features=768, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.08823529411764706, mode=row)\n      )\n      (1): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n          (1): Permute()\n          (2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=768, out_features=3072, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=3072, out_features=768, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.09411764705882353, mode=row)\n      )\n      (2): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n          (1): Permute()\n          (2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=768, out_features=3072, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=3072, out_features=768, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.1, mode=row)\n      )\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=1)\n  (classifier): Sequential(\n    (0): LayerNorm2d((768,), eps=1e-06, elementwise_affine=True)\n    (1): Flatten(start_dim=1, end_dim=-1)\n    (2): Linear(in_features=768, out_features=10, bias=True)\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"def pred_fp64(Model,Testloader): \n    all_labels = [] \n    all_predictions = [] \n    correct = 0 \n    total = 0 \n    start_time = time() \n    with torch.no_grad(): \n        Model.eval() \n        for images, labels in Testloader: \n            all_labels.extend(labels.numpy()) \n            images, labels = images.to(device), labels.to(device) \n            outputs = Model(images.double())\n            _,predicted = torch.max(outputs.data, 1) \n            total += labels.size(0) \n            correct += (predicted == labels).sum().item() \n            predicted_tensor_cpu = predicted.to('cpu') \n            all_predictions.extend(predicted_tensor_cpu.numpy()) \n        end_time = time() \n        print(\"Time: \",end_time - start_time) \n        print('Accuracy achieved by the network on test images is: %d%%' % (100 * correct / total))\n        return all_labels,all_predictions","metadata":{"execution":{"iopub.status.busy":"2024-04-09T09:54:52.882467Z","iopub.execute_input":"2024-04-09T09:54:52.883399Z","iopub.status.idle":"2024-04-09T09:54:52.892293Z","shell.execute_reply.started":"2024-04-09T09:54:52.883366Z","shell.execute_reply":"2024-04-09T09:54:52.891072Z"},"trusted":true},"execution_count":74,"outputs":[]},{"cell_type":"markdown","source":"def pred_fp64(Model,Testloader):\n    all_labels = []\n    all_predictions = []\n    correct = 0\n    total = 0\n    start_time = time()\n    with torch.no_grad():\n        Model.eval()\n        for images, labels in Testloader:\n            all_labels.extend(labels.numpy())\n            images, labels = images.to(device), labels.to(device)\n            outputs = Model(images.double())\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n            predicted_tensor_cpu = predicted.to('cpu')\n            all_predictions.extend(predicted_tensor_cpu.numpy())\n    end_time = time()\n    print(\"Time: \",end_time - start_time)\n    print('Accuracy achieved by the network on test images is: %d%%' % (100 * correct / total))\n    \n    return all_labels,all_predictions","metadata":{}},{"cell_type":"code","source":"labels_64,predictions_64 = pred_fp64(model_64,testloader)","metadata":{"execution":{"iopub.status.busy":"2024-04-09T09:59:46.830212Z","iopub.execute_input":"2024-04-09T09:59:46.830585Z","iopub.status.idle":"2024-04-09T10:04:17.107240Z","shell.execute_reply.started":"2024-04-09T09:59:46.830557Z","shell.execute_reply":"2024-04-09T10:04:17.106082Z"},"trusted":true},"execution_count":77,"outputs":[{"name":"stdout","text":"Time:  270.2710304260254\nAccuracy achieved by the network on test images is: 89%\n","output_type":"stream"}]},{"cell_type":"markdown","source":" labels,predictions = pred_fp64(model_64,testloader)","metadata":{}},{"cell_type":"code","source":"metrics(labels_64,predictions_64)","metadata":{"execution":{"iopub.status.busy":"2024-04-09T10:04:36.043996Z","iopub.execute_input":"2024-04-09T10:04:36.044419Z","iopub.status.idle":"2024-04-09T10:04:36.961598Z","shell.execute_reply.started":"2024-04-09T10:04:36.044387Z","shell.execute_reply":"2024-04-09T10:04:36.960447Z"},"trusted":true},"execution_count":78,"outputs":[{"name":"stdout","text":"Confusion Matrix:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 800x600 with 2 Axes>","image/png":"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"},"metadata":{"image/png":{"width":793,"height":689}}},{"name":"stdout","text":"----------------------------------------------------------------\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.91      0.86      0.89       568\n           1       0.89      0.96      0.92       610\n           2       0.88      0.91      0.89       627\n           3       0.83      0.79      0.81       457\n           4       0.94      0.94      0.94       491\n           5       0.85      0.87      0.86       417\n           6       0.87      0.86      0.86       508\n           7       0.94      0.94      0.94       620\n           8       0.81      0.80      0.80       477\n           9       0.97      0.93      0.95       625\n\n    accuracy                           0.89      5400\n   macro avg       0.89      0.89      0.89      5400\nweighted avg       0.89      0.89      0.89      5400\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## INT-8","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.ao.quantization import (\n  get_default_qconfig_mapping,\n  get_default_qat_qconfig_mapping,\n  QConfigMapping,\n)\nimport torch.ao.quantization.quantize_fx as quantize_fx","metadata":{"execution":{"iopub.status.busy":"2024-04-09T10:04:46.803567Z","iopub.execute_input":"2024-04-09T10:04:46.804250Z","iopub.status.idle":"2024-04-09T10:04:46.809213Z","shell.execute_reply.started":"2024-04-09T10:04:46.804213Z","shell.execute_reply":"2024-04-09T10:04:46.808120Z"},"trusted":true},"execution_count":80,"outputs":[]},{"cell_type":"markdown","source":"### PTQ","metadata":{}},{"cell_type":"code","source":"model.to('cpu')","metadata":{"execution":{"iopub.status.busy":"2024-04-09T10:04:50.008378Z","iopub.execute_input":"2024-04-09T10:04:50.008765Z","iopub.status.idle":"2024-04-09T10:04:50.057532Z","shell.execute_reply.started":"2024-04-09T10:04:50.008735Z","shell.execute_reply":"2024-04-09T10:04:50.056503Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":81,"outputs":[{"execution_count":81,"output_type":"execute_result","data":{"text/plain":"ConvNeXt(\n  (features): Sequential(\n    (0): Conv2dNormActivation(\n      (0): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\n      (1): LayerNorm2d((96,), eps=1e-06, elementwise_affine=True)\n    )\n    (1): Sequential(\n      (0): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n          (1): Permute()\n          (2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=96, out_features=384, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=384, out_features=96, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n      )\n      (1): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n          (1): Permute()\n          (2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=96, out_features=384, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=384, out_features=96, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.0058823529411764705, mode=row)\n      )\n      (2): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n          (1): Permute()\n          (2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=96, out_features=384, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=384, out_features=96, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.011764705882352941, mode=row)\n      )\n    )\n    (2): Sequential(\n      (0): LayerNorm2d((96,), eps=1e-06, elementwise_affine=True)\n      (1): Conv2d(96, 192, kernel_size=(2, 2), stride=(2, 2))\n    )\n    (3): Sequential(\n      (0): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n          (1): Permute()\n          (2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=192, out_features=768, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=768, out_features=192, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.017647058823529415, mode=row)\n      )\n      (1): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n          (1): Permute()\n          (2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=192, out_features=768, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=768, out_features=192, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.023529411764705882, mode=row)\n      )\n      (2): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n          (1): Permute()\n          (2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=192, out_features=768, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=768, out_features=192, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.029411764705882353, mode=row)\n      )\n    )\n    (4): Sequential(\n      (0): LayerNorm2d((192,), eps=1e-06, elementwise_affine=True)\n      (1): Conv2d(192, 384, kernel_size=(2, 2), stride=(2, 2))\n    )\n    (5): Sequential(\n      (0): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n          (1): Permute()\n          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=384, out_features=1536, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=1536, out_features=384, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.03529411764705883, mode=row)\n      )\n      (1): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n          (1): Permute()\n          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=384, out_features=1536, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=1536, out_features=384, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.0411764705882353, mode=row)\n      )\n      (2): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n          (1): Permute()\n          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=384, out_features=1536, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=1536, out_features=384, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.047058823529411764, mode=row)\n      )\n      (3): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n          (1): Permute()\n          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=384, out_features=1536, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=1536, out_features=384, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.052941176470588235, mode=row)\n      )\n      (4): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n          (1): Permute()\n          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=384, out_features=1536, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=1536, out_features=384, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.058823529411764705, mode=row)\n      )\n      (5): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n          (1): Permute()\n          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=384, out_features=1536, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=1536, out_features=384, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.06470588235294118, mode=row)\n      )\n      (6): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n          (1): Permute()\n          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=384, out_features=1536, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=1536, out_features=384, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.07058823529411766, mode=row)\n      )\n      (7): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n          (1): Permute()\n          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=384, out_features=1536, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=1536, out_features=384, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.07647058823529412, mode=row)\n      )\n      (8): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n          (1): Permute()\n          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=384, out_features=1536, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=1536, out_features=384, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.0823529411764706, mode=row)\n      )\n    )\n    (6): Sequential(\n      (0): LayerNorm2d((384,), eps=1e-06, elementwise_affine=True)\n      (1): Conv2d(384, 768, kernel_size=(2, 2), stride=(2, 2))\n    )\n    (7): Sequential(\n      (0): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n          (1): Permute()\n          (2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=768, out_features=3072, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=3072, out_features=768, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.08823529411764706, mode=row)\n      )\n      (1): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n          (1): Permute()\n          (2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=768, out_features=3072, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=3072, out_features=768, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.09411764705882353, mode=row)\n      )\n      (2): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n          (1): Permute()\n          (2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=768, out_features=3072, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=3072, out_features=768, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.1, mode=row)\n      )\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=1)\n  (classifier): Sequential(\n    (0): LayerNorm2d((768,), eps=1e-06, elementwise_affine=True)\n    (1): Flatten(start_dim=1, end_dim=-1)\n    (2): Linear(in_features=768, out_features=10, bias=True)\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"input_data = next(iter(trainloader))[0] \ncalibrate_data = input_data.to(\"cpu\")","metadata":{"execution":{"iopub.status.busy":"2024-04-09T10:04:56.654991Z","iopub.execute_input":"2024-04-09T10:04:56.655380Z","iopub.status.idle":"2024-04-09T10:04:56.838833Z","shell.execute_reply.started":"2024-04-09T10:04:56.655348Z","shell.execute_reply":"2024-04-09T10:04:56.837627Z"},"trusted":true},"execution_count":82,"outputs":[]},{"cell_type":"code","source":"model_int8 = copy.deepcopy(model)\n\nqconfig_mapping = get_default_qconfig_mapping(\"x86\")\nmodel_int8.eval()\n# prepare\nmodel_prepared = quantize_fx.prepare_fx(model_int8, qconfig_mapping, calibrate_data)\n# calibrate","metadata":{"execution":{"iopub.status.busy":"2024-04-09T10:05:00.003128Z","iopub.execute_input":"2024-04-09T10:05:00.003521Z","iopub.status.idle":"2024-04-09T10:05:01.512093Z","shell.execute_reply.started":"2024-04-09T10:05:00.003491Z","shell.execute_reply":"2024-04-09T10:05:01.511031Z"},"trusted":true},"execution_count":83,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/ao/quantization/observer.py:214: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"with torch.no_grad():\n    for i in range(20):\n        batch = next(iter(trainloader))[0]\n        output = model_prepared(batch.to('cpu'))","metadata":{"execution":{"iopub.status.busy":"2024-04-09T10:05:04.812772Z","iopub.execute_input":"2024-04-09T10:05:04.813194Z","iopub.status.idle":"2024-04-09T10:14:47.888208Z","shell.execute_reply.started":"2024-04-09T10:05:04.813163Z","shell.execute_reply":"2024-04-09T10:14:47.887010Z"},"trusted":true},"execution_count":84,"outputs":[]},{"cell_type":"code","source":"model_quantized_static = quantize_fx.convert_fx(model_prepared)\nmodel_quantized_static.state_dict()","metadata":{"execution":{"iopub.status.busy":"2024-04-09T10:14:47.889771Z","iopub.execute_input":"2024-04-09T10:14:47.890507Z","iopub.status.idle":"2024-04-09T10:14:56.261526Z","shell.execute_reply.started":"2024-04-09T10:14:47.890478Z","shell.execute_reply":"2024-04-09T10:14:56.260369Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":85,"outputs":[{"execution_count":85,"output_type":"execute_result","data":{"text/plain":"OrderedDict([('features_0_0_input_scale_0', tensor(0.0342)),\n             ('features_0_0_input_zero_point_0', tensor(50)),\n             ('features_0_1_scale_1', tensor(0.0100)),\n             ('features_0_1_zero_point_1', tensor(58)),\n             ('features_1_0_scale_0', tensor(0.0034)),\n             ('features_1_0_zero_point_0', tensor(67)),\n             ('features_1_0_block_1_scale_0', tensor(0.0018)),\n             ('features_1_0_block_1_zero_point_0', tensor(66)),\n             ('features_1_0_block_4_scale_0', tensor(0.0534)),\n             ('features_1_0_block_4_zero_point_0', tensor(3)),\n             ('features_1_0_block_6_scale_0', tensor(0.0455)),\n             ('features_1_0_block_6_zero_point_0', tensor(64)),\n             ('features_1_0_scale_1', tensor(0.0085)),\n             ('features_1_0_zero_point_1', tensor(51)),\n             ('features_1_0_stochastic_depth_scale_0', tensor(0.0085)),\n             ('features_1_0_stochastic_depth_zero_point_0', tensor(51)),\n             ('features_1_0_scale_2', tensor(0.0107)),\n             ('features_1_0_zero_point_2', tensor(55)),\n             ('features_1_1_scale_0', tensor(0.0063)),\n             ('features_1_1_zero_point_0', tensor(62)),\n             ('features_1_1_block_1_scale_0', tensor(0.0044)),\n             ('features_1_1_block_1_zero_point_0', tensor(70)),\n             ('features_1_1_block_4_scale_0', tensor(0.0518)),\n             ('features_1_1_block_4_zero_point_0', tensor(3)),\n             ('features_1_1_block_6_scale_0', tensor(0.0505)),\n             ('features_1_1_block_6_zero_point_0', tensor(93)),\n             ('features_1_1_scale_1', tensor(0.0171)),\n             ('features_1_1_zero_point_1', tensor(36)),\n             ('features_1_1_stochastic_depth_scale_0', tensor(0.0171)),\n             ('features_1_1_stochastic_depth_zero_point_0', tensor(36)),\n             ('features_1_1_scale_2', tensor(0.0203)),\n             ('features_1_1_zero_point_2', tensor(22)),\n             ('features_1_2_scale_0', tensor(0.0072)),\n             ('features_1_2_zero_point_0', tensor(64)),\n             ('features_1_2_block_1_scale_0', tensor(0.0018)),\n             ('features_1_2_block_1_zero_point_0', tensor(66)),\n             ('features_1_2_block_4_scale_0', tensor(0.0341)),\n             ('features_1_2_block_4_zero_point_0', tensor(5)),\n             ('features_1_2_block_6_scale_0', tensor(0.0481)),\n             ('features_1_2_block_6_zero_point_0', tensor(80)),\n             ('features_1_2_scale_1', tensor(0.0167)),\n             ('features_1_2_zero_point_1', tensor(81)),\n             ('features_1_2_stochastic_depth_scale_0', tensor(0.0167)),\n             ('features_1_2_stochastic_depth_zero_point_0', tensor(81)),\n             ('features_1_2_scale_2', tensor(0.0315)),\n             ('features_1_2_zero_point_2', tensor(48)),\n             ('features_2_0_scale_1', tensor(0.0059)),\n             ('features_2_0_zero_point_1', tensor(71)),\n             ('features_3_0_scale_0', tensor(0.0047)),\n             ('features_3_0_zero_point_0', tensor(37)),\n             ('features_3_0_block_1_scale_0', tensor(0.0034)),\n             ('features_3_0_block_1_zero_point_0', tensor(65)),\n             ('features_3_0_block_4_scale_0', tensor(0.0351)),\n             ('features_3_0_block_4_zero_point_0', tensor(5)),\n             ('features_3_0_block_6_scale_0', tensor(0.0703)),\n             ('features_3_0_block_6_zero_point_0', tensor(97)),\n             ('features_3_0_scale_1', tensor(0.0252)),\n             ('features_3_0_zero_point_1', tensor(115)),\n             ('features_3_0_stochastic_depth_scale_0', tensor(0.0252)),\n             ('features_3_0_stochastic_depth_zero_point_0', tensor(115)),\n             ('features_3_0_scale_2', tensor(0.0315)),\n             ('features_3_0_zero_point_2', tensor(114)),\n             ('features_3_1_scale_0', tensor(0.0065)),\n             ('features_3_1_zero_point_0', tensor(87)),\n             ('features_3_1_block_1_scale_0', tensor(0.0033)),\n             ('features_3_1_block_1_zero_point_0', tensor(80)),\n             ('features_3_1_block_4_scale_0', tensor(0.0358)),\n             ('features_3_1_block_4_zero_point_0', tensor(5)),\n             ('features_3_1_block_6_scale_0', tensor(0.0601)),\n             ('features_3_1_block_6_zero_point_0', tensor(39)),\n             ('features_3_1_scale_1', tensor(0.0279)),\n             ('features_3_1_zero_point_1', tensor(108)),\n             ('features_3_1_stochastic_depth_scale_0', tensor(0.0279)),\n             ('features_3_1_stochastic_depth_zero_point_0', tensor(108)),\n             ('features_3_1_scale_2', tensor(0.0552)),\n             ('features_3_1_zero_point_2', tensor(117)),\n             ('features_3_2_scale_0', tensor(0.0125)),\n             ('features_3_2_zero_point_0', tensor(66)),\n             ('features_3_2_block_1_scale_0', tensor(0.0059)),\n             ('features_3_2_block_1_zero_point_0', tensor(80)),\n             ('features_3_2_block_4_scale_0', tensor(0.0273)),\n             ('features_3_2_block_4_zero_point_0', tensor(6)),\n             ('features_3_2_block_6_scale_0', tensor(0.0810)),\n             ('features_3_2_block_6_zero_point_0', tensor(63)),\n             ('features_3_2_scale_1', tensor(0.0601)),\n             ('features_3_2_zero_point_1', tensor(66)),\n             ('features_3_2_stochastic_depth_scale_0', tensor(0.0601)),\n             ('features_3_2_stochastic_depth_zero_point_0', tensor(66)),\n             ('features_3_2_scale_2', tensor(0.0982)),\n             ('features_3_2_zero_point_2', tensor(90)),\n             ('features_4_0_scale_1', tensor(0.0031)),\n             ('features_4_0_zero_point_1', tensor(67)),\n             ('features_5_0_scale_0', tensor(0.0016)),\n             ('features_5_0_zero_point_0', tensor(73)),\n             ('features_5_0_block_1_scale_0', tensor(0.0017)),\n             ('features_5_0_block_1_zero_point_0', tensor(61)),\n             ('features_5_0_block_4_scale_0', tensor(0.0295)),\n             ('features_5_0_block_4_zero_point_0', tensor(6)),\n             ('features_5_0_block_6_scale_0', tensor(0.0445)),\n             ('features_5_0_block_6_zero_point_0', tensor(64)),\n             ('features_5_0_scale_1', tensor(0.0031)),\n             ('features_5_0_zero_point_1', tensor(56)),\n             ('features_5_0_stochastic_depth_scale_0', tensor(0.0031)),\n             ('features_5_0_stochastic_depth_zero_point_0', tensor(56)),\n             ('features_5_0_scale_2', tensor(0.0052)),\n             ('features_5_0_zero_point_2', tensor(62)),\n             ('features_5_1_scale_0', tensor(0.0035)),\n             ('features_5_1_zero_point_0', tensor(96)),\n             ('features_5_1_block_1_scale_0', tensor(0.0030)),\n             ('features_5_1_block_1_zero_point_0', tensor(63)),\n             ('features_5_1_block_4_scale_0', tensor(0.0311)),\n             ('features_5_1_block_4_zero_point_0', tensor(5)),\n             ('features_5_1_block_6_scale_0', tensor(0.0400)),\n             ('features_5_1_block_6_zero_point_0', tensor(66)),\n             ('features_5_1_scale_1', tensor(0.0058)),\n             ('features_5_1_zero_point_1', tensor(34)),\n             ('features_5_1_stochastic_depth_scale_0', tensor(0.0058)),\n             ('features_5_1_stochastic_depth_zero_point_0', tensor(34)),\n             ('features_5_1_scale_2', tensor(0.0089)),\n             ('features_5_1_zero_point_2', tensor(44)),\n             ('features_5_2_scale_0', tensor(0.0057)),\n             ('features_5_2_zero_point_0', tensor(27)),\n             ('features_5_2_block_1_scale_0', tensor(0.0041)),\n             ('features_5_2_block_1_zero_point_0', tensor(57)),\n             ('features_5_2_block_4_scale_0', tensor(0.0250)),\n             ('features_5_2_block_4_zero_point_0', tensor(7)),\n             ('features_5_2_block_6_scale_0', tensor(0.0475)),\n             ('features_5_2_block_6_zero_point_0', tensor(48)),\n             ('features_5_2_scale_1', tensor(0.0184)),\n             ('features_5_2_zero_point_1', tensor(14)),\n             ('features_5_2_stochastic_depth_scale_0', tensor(0.0184)),\n             ('features_5_2_stochastic_depth_zero_point_0', tensor(14)),\n             ('features_5_2_scale_2', tensor(0.0240)),\n             ('features_5_2_zero_point_2', tensor(20)),\n             ('features_5_3_scale_0', tensor(0.0083)),\n             ('features_5_3_zero_point_0', tensor(26)),\n             ('features_5_3_block_1_scale_0', tensor(0.0031)),\n             ('features_5_3_block_1_zero_point_0', tensor(63)),\n             ('features_5_3_block_4_scale_0', tensor(0.0325)),\n             ('features_5_3_block_4_zero_point_0', tensor(5)),\n             ('features_5_3_block_6_scale_0', tensor(0.0466)),\n             ('features_5_3_block_6_zero_point_0', tensor(62)),\n             ('features_5_3_scale_1', tensor(0.0227)),\n             ('features_5_3_zero_point_1', tensor(17)),\n             ('features_5_3_stochastic_depth_scale_0', tensor(0.0227)),\n             ('features_5_3_stochastic_depth_zero_point_0', tensor(17)),\n             ('features_5_3_scale_2', tensor(0.0422)),\n             ('features_5_3_zero_point_2', tensor(15)),\n             ('features_5_4_scale_0', tensor(0.0098)),\n             ('features_5_4_zero_point_0', tensor(106)),\n             ('features_5_4_block_1_scale_0', tensor(0.0047)),\n             ('features_5_4_block_1_zero_point_0', tensor(64)),\n             ('features_5_4_block_4_scale_0', tensor(0.0334)),\n             ('features_5_4_block_4_zero_point_0', tensor(5)),\n             ('features_5_4_block_6_scale_0', tensor(0.0428)),\n             ('features_5_4_block_6_zero_point_0', tensor(72)),\n             ('features_5_4_scale_1', tensor(0.0292)),\n             ('features_5_4_zero_point_1', tensor(15)),\n             ('features_5_4_stochastic_depth_scale_0', tensor(0.0292)),\n             ('features_5_4_stochastic_depth_zero_point_0', tensor(15)),\n             ('features_5_4_scale_2', tensor(0.0643)),\n             ('features_5_4_zero_point_2', tensor(12)),\n             ('features_5_5_scale_0', tensor(0.0119)),\n             ('features_5_5_zero_point_0', tensor(27)),\n             ('features_5_5_block_1_scale_0', tensor(0.0096)),\n             ('features_5_5_block_1_zero_point_0', tensor(81)),\n             ('features_5_5_block_4_scale_0', tensor(0.0375)),\n             ('features_5_5_block_4_zero_point_0', tensor(5)),\n             ('features_5_5_block_6_scale_0', tensor(0.0630)),\n             ('features_5_5_block_6_zero_point_0', tensor(57)),\n             ('features_5_5_scale_1', tensor(0.0459)),\n             ('features_5_5_zero_point_1', tensor(20)),\n             ('features_5_5_stochastic_depth_scale_0', tensor(0.0459)),\n             ('features_5_5_stochastic_depth_zero_point_0', tensor(20)),\n             ('features_5_5_scale_2', tensor(0.0952)),\n             ('features_5_5_zero_point_2', tensor(11)),\n             ('features_5_6_scale_0', tensor(0.0148)),\n             ('features_5_6_zero_point_0', tensor(101)),\n             ('features_5_6_block_1_scale_0', tensor(0.0074)),\n             ('features_5_6_block_1_zero_point_0', tensor(59)),\n             ('features_5_6_block_4_scale_0', tensor(0.0357)),\n             ('features_5_6_block_4_zero_point_0', tensor(5)),\n             ('features_5_6_block_6_scale_0', tensor(0.1104)),\n             ('features_5_6_block_6_zero_point_0', tensor(98)),\n             ('features_5_6_scale_1', tensor(0.1435)),\n             ('features_5_6_zero_point_1', tensor(7)),\n             ('features_5_6_stochastic_depth_scale_0', tensor(0.1435)),\n             ('features_5_6_stochastic_depth_zero_point_0', tensor(7)),\n             ('features_5_6_scale_2', tensor(0.2177)),\n             ('features_5_6_zero_point_2', tensor(5)),\n             ('features_5_7_scale_0', tensor(0.0187)),\n             ('features_5_7_zero_point_0', tensor(28)),\n             ('features_5_7_block_1_scale_0', tensor(0.0147)),\n             ('features_5_7_block_1_zero_point_0', tensor(61)),\n             ('features_5_7_block_4_scale_0', tensor(0.0317)),\n             ('features_5_7_block_4_zero_point_0', tensor(5)),\n             ('features_5_7_block_6_scale_0', tensor(0.1276)),\n             ('features_5_7_block_6_zero_point_0', tensor(14)),\n             ('features_5_7_scale_1', tensor(0.2451)),\n             ('features_5_7_zero_point_1', tensor(9)),\n             ('features_5_7_stochastic_depth_scale_0', tensor(0.2451)),\n             ('features_5_7_stochastic_depth_zero_point_0', tensor(9)),\n             ('features_5_7_scale_2', tensor(0.4357)),\n             ('features_5_7_zero_point_2', tensor(7)),\n             ('features_5_8_scale_0', tensor(0.0202)),\n             ('features_5_8_zero_point_0', tensor(98)),\n             ('features_5_8_block_1_scale_0', tensor(0.0140)),\n             ('features_5_8_block_1_zero_point_0', tensor(58)),\n             ('features_5_8_block_4_scale_0', tensor(0.0329)),\n             ('features_5_8_block_4_zero_point_0', tensor(5)),\n             ('features_5_8_block_6_scale_0', tensor(0.1137)),\n             ('features_5_8_block_6_zero_point_0', tensor(94)),\n             ('features_5_8_scale_1', tensor(0.1972)),\n             ('features_5_8_zero_point_1', tensor(22)),\n             ('features_5_8_stochastic_depth_scale_0', tensor(0.1972)),\n             ('features_5_8_stochastic_depth_zero_point_0', tensor(22)),\n             ('features_5_8_scale_2', tensor(0.5693)),\n             ('features_5_8_zero_point_2', tensor(6)),\n             ('features_6_0_scale_1', tensor(0.0084)),\n             ('features_6_0_zero_point_1', tensor(63)),\n             ('features_7_0_scale_0', tensor(0.0137)),\n             ('features_7_0_zero_point_0', tensor(66)),\n             ('features_7_0_block_1_scale_0', tensor(0.0226)),\n             ('features_7_0_block_1_zero_point_0', tensor(86)),\n             ('features_7_0_block_4_scale_0', tensor(0.0772)),\n             ('features_7_0_block_4_zero_point_0', tensor(2)),\n             ('features_7_0_block_6_scale_0', tensor(0.0885)),\n             ('features_7_0_block_6_zero_point_0', tensor(74)),\n             ('features_7_0_scale_1', tensor(0.0419)),\n             ('features_7_0_zero_point_1', tensor(53)),\n             ('features_7_0_stochastic_depth_scale_0', tensor(0.0419)),\n             ('features_7_0_stochastic_depth_zero_point_0', tensor(53)),\n             ('features_7_0_scale_2', tensor(0.0456)),\n             ('features_7_0_zero_point_2', tensor(59)),\n             ('features_7_1_scale_0', tensor(0.0144)),\n             ('features_7_1_zero_point_0', tensor(64)),\n             ('features_7_1_block_1_scale_0', tensor(0.0387)),\n             ('features_7_1_block_1_zero_point_0', tensor(68)),\n             ('features_7_1_block_4_scale_0', tensor(0.0447)),\n             ('features_7_1_block_4_zero_point_0', tensor(4)),\n             ('features_7_1_block_6_scale_0', tensor(0.0529)),\n             ('features_7_1_block_6_zero_point_0', tensor(64)),\n             ('features_7_1_scale_1', tensor(0.0250)),\n             ('features_7_1_zero_point_1', tensor(66)),\n             ('features_7_1_stochastic_depth_scale_0', tensor(0.0250)),\n             ('features_7_1_stochastic_depth_zero_point_0', tensor(66)),\n             ('features_7_1_scale_2', tensor(0.0543)),\n             ('features_7_1_zero_point_2', tensor(60)),\n             ('features_7_2_scale_0', tensor(0.0138)),\n             ('features_7_2_zero_point_0', tensor(64)),\n             ('features_7_2_block_1_scale_0', tensor(0.0376)),\n             ('features_7_2_block_1_zero_point_0', tensor(72)),\n             ('features_7_2_block_4_scale_0', tensor(0.0760)),\n             ('features_7_2_block_4_zero_point_0', tensor(2)),\n             ('features_7_2_block_6_scale_0', tensor(0.1508)),\n             ('features_7_2_block_6_zero_point_0', tensor(78)),\n             ('features_7_2_scale_1', tensor(0.0544)),\n             ('features_7_2_zero_point_1', tensor(65)),\n             ('features_7_2_stochastic_depth_scale_0', tensor(0.0544)),\n             ('features_7_2_stochastic_depth_zero_point_0', tensor(65)),\n             ('features_7_2_scale_2', tensor(0.0847)),\n             ('features_7_2_zero_point_2', tensor(63)),\n             ('classifier_0_scale_1', tensor(0.1129)),\n             ('classifier_0_zero_point_1', tensor(65)),\n             ('classifier_1_scale_0', tensor(0.1129)),\n             ('classifier_1_zero_point_0', tensor(65)),\n             ('features.0.0.weight',\n              tensor([[[[-0.1021, -0.1260, -0.0781, -0.0479],\n                        [ 0.0489, -0.0062, -0.0271,  0.0073],\n                        [ 0.0052, -0.0167,  0.0000, -0.0104],\n                        [ 0.0573,  0.0635,  0.1323,  0.1208]],\n              \n                       [[-0.0156, -0.0417, -0.0021,  0.0219],\n                        [ 0.0812,  0.0312,  0.0187,  0.0208],\n                        [-0.0448, -0.0469, -0.0219, -0.0677],\n                        [-0.0260, -0.0125,  0.0552,  0.0042]],\n              \n                       [[ 0.0469,  0.0271,  0.0760,  0.0656],\n                        [ 0.0823,  0.0573,  0.0406,  0.0250],\n                        [-0.0500, -0.0427, -0.0062, -0.0844],\n                        [-0.0802, -0.0698,  0.0062, -0.0552]]],\n              \n              \n                      [[[ 0.0230,  0.0212,  0.0451, -0.0044],\n                        [ 0.0009, -0.0743,  0.0672,  0.0664],\n                        [-0.0062, -0.0009,  0.0018, -0.0159],\n                        [-0.0619,  0.0177,  0.0142, -0.0929]],\n              \n                       [[ 0.0088, -0.0345,  0.0336, -0.0540],\n                        [-0.0035, -0.1133,  0.0894,  0.0867],\n                        [ 0.0000, -0.0115,  0.0106, -0.0265],\n                        [-0.0557,  0.0513,  0.0867, -0.0973]],\n              \n                       [[-0.0080, -0.0283, -0.0195, -0.0487],\n                        [ 0.0248, -0.0549,  0.0265,  0.0203],\n                        [ 0.0212, -0.0080,  0.0009, -0.0133],\n                        [ 0.0124,  0.0487,  0.0655, -0.0053]]],\n              \n              \n                      [[[ 0.0253, -0.0899,  0.0699, -0.0297],\n                        [ 0.0140,  0.0375,  0.0830, -0.0253],\n                        [-0.0096, -0.0882,  0.0131, -0.0183],\n                        [-0.0279,  0.0244, -0.0437,  0.0672]],\n              \n                       [[ 0.0244, -0.1030,  0.0646, -0.0131],\n                        [-0.0026, -0.0026,  0.0515, -0.0926],\n                        [ 0.0122, -0.0218,  0.0393, -0.0166],\n                        [-0.0140,  0.1074, -0.1118,  0.0506]],\n              \n                       [[ 0.0297, -0.0410,  0.0367,  0.0306],\n                        [-0.0629, -0.0079, -0.0017,  0.0070],\n                        [ 0.0026,  0.0515, -0.0087,  0.0332],\n                        [-0.0323,  0.0646, -0.0568, -0.0122]]],\n              \n              \n                      ...,\n              \n              \n                      [[[ 0.0945,  0.0287, -0.0253,  0.1452],\n                        [ 0.0439, -0.0135, -0.0557,  0.0084],\n                        [-0.1030,  0.0084,  0.0034, -0.0523],\n                        [-0.0912,  0.0000,  0.0219,  0.0101]],\n              \n                       [[-0.0709, -0.1587, -0.2161,  0.0101],\n                        [-0.0219, -0.0642, -0.0709, -0.0321],\n                        [-0.0810,  0.1114,  0.1064, -0.0169],\n                        [ 0.0236,  0.1756,  0.1671,  0.0625]],\n              \n                       [[ 0.0929,  0.0270,  0.0152,  0.1418],\n                        [ 0.0760,  0.0642,  0.0304,  0.0270],\n                        [-0.0506,  0.0929,  0.0709, -0.1148],\n                        [-0.1064, -0.0422, -0.0692, -0.1874]]],\n              \n              \n                      [[[-0.0333,  0.0510,  0.0156, -0.0417],\n                        [-0.0042, -0.0271,  0.0010,  0.0667],\n                        [ 0.0250, -0.0240, -0.0635,  0.0448],\n                        [-0.0750,  0.0583, -0.0094,  0.0135]],\n              \n                       [[-0.0302,  0.0708,  0.0031, -0.1083],\n                        [ 0.0323, -0.0406, -0.0219,  0.0635],\n                        [ 0.1031, -0.0146, -0.1333,  0.0167],\n                        [-0.0552,  0.1083, -0.0146, -0.0031]],\n              \n                       [[-0.0375,  0.0250,  0.0292, -0.0062],\n                        [-0.0167, -0.0385,  0.0156,  0.0667],\n                        [ 0.0198, -0.0156, -0.0448,  0.0312],\n                        [-0.0385,  0.0417, -0.0021,  0.0042]]],\n              \n              \n                      [[[-0.0179, -0.0385,  0.0340,  0.0134],\n                        [-0.0376, -0.0984,  0.0161,  0.1137],\n                        [ 0.0421, -0.1074, -0.0591, -0.0653],\n                        [ 0.1002,  0.0492,  0.0197,  0.0367]],\n              \n                       [[-0.0054,  0.0904, -0.0063,  0.0045],\n                        [ 0.0224,  0.0081,  0.0197,  0.0107],\n                        [ 0.0063,  0.0018,  0.0358, -0.0859],\n                        [-0.0430, -0.0528, -0.0555,  0.0206]],\n              \n                       [[-0.0098,  0.0063, -0.0716, -0.0197],\n                        [-0.0233,  0.0618, -0.0188, -0.0331],\n                        [-0.0215,  0.1065,  0.0823,  0.0188],\n                        [-0.0752, -0.0089,  0.0286,  0.0125]]]], size=(96, 3, 4, 4),\n                     dtype=torch.qint8, quantization_scheme=torch.per_channel_affine,\n                     scale=tensor([1.0414e-03, 8.8478e-04, 8.7320e-04, 1.1863e-03, 1.1024e-03, 1.4774e-03,\n                      8.0798e-05, 1.7137e-03, 5.8931e-04, 9.9786e-04, 9.2581e-04, 1.3190e-03,\n                      1.6479e-03, 8.4004e-04, 7.3771e-04, 1.5522e-03, 6.3253e-04, 5.8552e-04,\n                      2.4480e-04, 1.5861e-04, 9.7245e-04, 1.0673e-03, 2.0376e-04, 6.3877e-04,\n                      1.3370e-03, 1.1909e-03, 4.3109e-04, 4.6855e-04, 7.2621e-04, 9.0372e-04,\n                      8.8228e-04, 5.6963e-04, 8.6600e-04, 1.4162e-03, 8.6273e-04, 1.0486e-03,\n                      8.5967e-04, 7.3220e-04, 7.8009e-04, 9.1330e-04, 1.9878e-04, 1.0286e-03,\n                      8.5758e-04, 1.2504e-03, 1.4213e-03, 1.1471e-03, 8.9677e-04, 1.2796e-03,\n                      6.1946e-04, 1.1367e-03, 8.3109e-04, 3.8302e-04, 5.9089e-04, 1.2144e-03,\n                      8.8616e-04, 7.8700e-04, 9.3271e-04, 9.0968e-04, 1.0153e-03, 7.8587e-04,\n                      8.9301e-04, 1.0630e-03, 1.4763e-03, 8.6386e-04, 1.5357e-03, 1.0122e-03,\n                      1.6263e-04, 4.9615e-04, 1.0140e-03, 6.4411e-04, 1.5103e-03, 5.9498e-04,\n                      8.3483e-04, 3.9848e-04, 8.2918e-04, 7.7084e-04, 8.1384e-04, 1.2152e-03,\n                      1.4429e-03, 1.4668e-03, 4.0733e-04, 1.3093e-03, 1.3318e-03, 1.1228e-03,\n                      8.0529e-04, 6.9251e-04, 2.2818e-04, 5.7244e-04, 1.0967e-04, 7.0305e-04,\n                      7.9677e-04, 7.0004e-04, 8.1418e-04, 1.6882e-03, 1.0415e-03, 8.9493e-04],\n                     dtype=torch.float64),\n                     zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n                     axis=0)),\n             ('features.0.0.bias',\n              Parameter containing:\n              tensor([-6.4387e-03,  8.8262e-03,  2.4495e-03,  4.8466e-03, -1.4076e-03,\n                      -8.2406e-03,  3.9464e-01, -7.7112e-03,  5.1536e-02,  9.1345e-03,\n                       1.0076e-03, -5.5043e-03, -7.6856e-03, -6.2549e-03, -4.1248e-03,\n                      -4.1209e-02,  2.5286e-04, -1.5759e-02,  9.8608e-03, -4.1611e-01,\n                      -7.5028e-04, -7.7876e-03, -1.6168e-03,  4.1149e-02, -3.4023e-02,\n                       1.7942e-03,  6.6819e-01, -3.0017e-01, -2.1487e-02, -2.0332e-03,\n                       1.6394e-03, -5.0809e-03, -2.0922e-02, -4.0823e-03,  1.7331e-03,\n                      -2.1973e-03,  2.4005e-03, -4.4667e-03, -6.3230e-03, -8.5132e-04,\n                       4.1862e-02,  7.4970e-04, -1.6190e-03, -1.6329e-02, -1.4102e-02,\n                      -1.7515e-02, -9.9785e-03, -5.8063e-03, -1.6124e-02, -3.4752e-02,\n                      -3.0050e-03, -6.4955e-01, -9.4460e-03, -3.6052e-03,  1.5011e-02,\n                      -5.9400e-03, -1.0428e-04, -2.4039e-02, -5.6034e-03, -2.5558e-03,\n                      -2.9776e-02, -1.1548e-02,  3.0700e-03,  4.9474e-04, -6.1255e-04,\n                      -1.4147e-02, -4.9282e-01, -1.0175e-02, -2.3791e-04, -2.6325e-03,\n                       3.3759e-03,  2.8210e-02, -4.2575e-03, -9.4875e-02, -3.7355e-03,\n                      -8.0616e-03, -6.8897e-03,  9.1664e-03, -5.3269e-03,  3.2919e-03,\n                      -2.3006e-02, -4.2028e-02,  3.4188e-03,  5.3938e-03, -6.1190e-03,\n                      -1.5883e-03,  6.0202e-01, -4.5479e-03,  4.8198e-01, -1.1921e-02,\n                       2.8457e-03,  7.1854e-04,  9.5020e-04, -5.2071e-03, -5.0878e-03,\n                       4.8469e-04], requires_grad=True)),\n             ('features.0.0.scale', tensor(0.0165)),\n             ('features.0.0.zero_point', tensor(58)),\n             ('features.0.1.weight',\n              tensor([ 1.7539e-02,  1.5790e-01,  3.1611e-01,  9.7921e-02,  4.6657e-01,\n                       1.2553e-03,  2.4422e-01,  1.3143e-01,  1.7136e-01,  2.4889e-01,\n                       3.6359e-01, -3.0819e-04,  1.2902e-01,  1.7831e-01,  2.1197e-01,\n                       7.3794e-02,  1.4260e-01,  3.1327e-02,  9.6708e-02,  4.6191e-01,\n                       4.9814e-01,  5.0429e-02,  1.3249e-01,  2.3367e-01,  1.6829e-03,\n                       3.4817e-01,  1.1500e-01,  4.8251e-02,  1.2731e-02,  1.5265e-01,\n                       1.2657e-02,  2.1735e-02,  2.6220e-02, -8.0109e-05,  1.7715e-01,\n                       1.4433e-01,  3.1304e-01,  1.1465e-01,  2.7579e-02,  1.7304e-01,\n                       1.0227e-01,  2.9164e-01,  2.1140e-02, -1.7835e-04,  5.6111e-02,\n                       4.6350e-04,  1.2360e-04, -2.0066e-04,  6.4291e-02,  4.7685e-02,\n                       3.7162e-01,  5.8772e-02,  9.6129e-02,  2.6645e-04,  3.5103e-01,\n                       1.4677e-01,  5.4387e-02,  4.5435e-02, -3.2230e-04,  2.4379e-02,\n                       1.0254e-03,  2.1851e-01,  2.9764e-02,  2.3753e-01,  1.0565e-01,\n                      -5.8037e-04,  1.8365e-01,  1.2072e-01,  3.8783e-02,  1.4832e-01,\n                       9.1753e-02,  6.7231e-02,  4.8272e-01,  1.1070e-01,  1.9136e-01,\n                       1.8637e-02,  2.1463e-01,  2.6024e-02, -1.2661e-02,  1.5609e-01,\n                       6.4032e-02,  1.1253e-01,  6.5700e-02,  2.3215e-01, -2.6014e-05,\n                       1.0991e-01,  3.9642e-02,  1.4132e-02,  1.3674e-01,  6.1232e-02,\n                       3.6612e-01,  4.2729e-01,  2.2183e-01, -6.2546e-05,  1.9803e-01,\n                       1.9608e-01])),\n             ('features.0.1.bias',\n              tensor([ 4.5490e-04, -7.4247e-03, -6.8785e-03,  1.3628e-03, -1.2462e-03,\n                       1.3132e-03, -4.2913e-01,  4.6608e-03,  3.9655e-02, -1.8407e-02,\n                      -1.8481e-03,  6.6536e-05, -6.2557e-04,  1.2707e-03, -7.4515e-04,\n                       1.4723e-02, -6.7428e-04,  6.6413e-03, -5.3131e-02,  8.9009e-01,\n                      -1.1382e-03, -2.3708e-03,  8.4583e-02,  1.7789e-02, -5.7083e-04,\n                      -6.2013e-03, -2.7355e-01,  6.3373e-02,  2.3636e-03,  2.1301e-03,\n                      -1.0293e-03, -1.5598e-03,  1.6498e-03,  2.3959e-04, -2.3955e-03,\n                       1.4367e-03, -9.5660e-03, -1.0892e-03, -7.7105e-04, -2.4985e-03,\n                      -4.9126e-02, -3.8818e-03,  1.4040e-03, -8.2177e-05,  2.3880e-03,\n                      -1.4037e-03, -1.9375e-03,  2.2608e-02,  3.4673e-03,  7.6120e-03,\n                       1.5384e-03,  1.9239e-01,  1.8395e-03,  6.0619e-05, -3.0330e-02,\n                       4.5211e-03,  2.4755e-03,  4.9486e-03, -6.9053e-02,  6.1531e-04,\n                       4.3967e-04,  8.0910e-03,  1.2079e-03, -6.7862e-03, -4.8967e-04,\n                      -1.8000e-04,  3.8012e-01,  8.1265e-03,  2.9470e-03,  1.9997e-03,\n                      -1.5389e-03, -1.2574e-03,  7.0923e-03,  2.5421e-02,  3.2092e-03,\n                       7.5043e-04,  1.1717e-02, -1.0040e-03, -7.1260e-04,  2.3939e-04,\n                      -1.4039e-02,  2.6617e-02,  4.4025e-03,  3.7243e-03,  1.0068e-05,\n                       2.8712e-03, -1.2337e-01, -9.0090e-03, -2.6573e-01,  2.1809e-03,\n                      -7.7251e-03, -1.0029e-04, -3.8877e-03, -4.7775e-02,  4.1939e-03,\n                      -5.2581e-05])),\n             ('features.1.0.layer_scale',\n              tensor([[[-7.5177e-02]],\n              \n                      [[ 1.0265e-01]],\n              \n                      [[-1.6823e-01]],\n              \n                      [[-9.7049e-02]],\n              \n                      [[ 1.7201e-01]],\n              \n                      [[ 3.3834e-02]],\n              \n                      [[ 4.9831e-02]],\n              \n                      [[ 9.7022e-02]],\n              \n                      [[ 3.9249e-04]],\n              \n                      [[ 1.5487e-01]],\n              \n                      [[-1.2750e-01]],\n              \n                      [[ 5.9854e-02]],\n              \n                      [[-1.1672e-01]],\n              \n                      [[ 8.0862e-02]],\n              \n                      [[ 1.3386e-01]],\n              \n                      [[-1.2826e-01]],\n              \n                      [[-7.4422e-02]],\n              \n                      [[ 3.5452e-02]],\n              \n                      [[ 5.6314e-03]],\n              \n                      [[ 5.3631e-02]],\n              \n                      [[ 2.0152e-01]],\n              \n                      [[-5.4761e-02]],\n              \n                      [[-1.7177e-02]],\n              \n                      [[ 2.7336e-03]],\n              \n                      [[-5.7065e-02]],\n              \n                      [[-1.7449e-01]],\n              \n                      [[-2.0736e-01]],\n              \n                      [[-1.2930e-01]],\n              \n                      [[-8.0410e-02]],\n              \n                      [[ 1.0383e-01]],\n              \n                      [[ 4.6413e-02]],\n              \n                      [[ 7.2875e-02]],\n              \n                      [[ 9.5623e-02]],\n              \n                      [[-5.5574e-02]],\n              \n                      [[-1.4701e-01]],\n              \n                      [[ 1.6587e-01]],\n              \n                      [[ 1.7596e-01]],\n              \n                      [[ 8.0937e-02]],\n              \n                      [[ 5.8820e-02]],\n              \n                      [[ 1.0601e-01]],\n              \n                      [[ 4.4332e-02]],\n              \n                      [[-1.8312e-01]],\n              \n                      [[-7.4338e-02]],\n              \n                      [[-4.4760e-02]],\n              \n                      [[ 6.5746e-02]],\n              \n                      [[-6.6895e-02]],\n              \n                      [[-4.4016e-02]],\n              \n                      [[ 7.1302e-02]],\n              \n                      [[-5.3673e-02]],\n              \n                      [[-1.6928e-02]],\n              \n                      [[-1.8879e-01]],\n              \n                      [[ 3.9795e-02]],\n              \n                      [[ 6.6937e-02]],\n              \n                      [[-4.0315e-02]],\n              \n                      [[-2.2663e-01]],\n              \n                      [[ 1.5484e-01]],\n              \n                      [[ 1.0565e-01]],\n              \n                      [[-1.1246e-01]],\n              \n                      [[-3.6289e-03]],\n              \n                      [[-6.8492e-02]],\n              \n                      [[ 7.3473e-02]],\n              \n                      [[-1.2418e-01]],\n              \n                      [[ 1.4278e-01]],\n              \n                      [[ 1.2423e-01]],\n              \n                      [[-1.5184e-01]],\n              \n                      [[ 3.6157e-02]],\n              \n                      [[-2.0402e-04]],\n              \n                      [[ 1.2117e-01]],\n              \n                      [[ 5.0930e-02]],\n              \n                      [[-7.5133e-02]],\n              \n                      [[-1.7084e-01]],\n              \n                      [[-3.2794e-04]],\n              \n                      [[-9.7555e-02]],\n              \n                      [[ 8.5748e-02]],\n              \n                      [[ 9.1191e-02]],\n              \n                      [[-4.9002e-02]],\n              \n                      [[-1.3244e-01]],\n              \n                      [[ 8.6358e-02]],\n              \n                      [[ 8.3878e-02]],\n              \n                      [[ 1.1930e-01]],\n              \n                      [[ 4.5551e-04]],\n              \n                      [[-1.2350e-01]],\n              \n                      [[-6.6072e-02]],\n              \n                      [[ 6.6053e-02]],\n              \n                      [[ 5.4590e-02]],\n              \n                      [[ 9.8519e-02]],\n              \n                      [[-8.0864e-02]],\n              \n                      [[-1.4490e-01]],\n              \n                      [[-3.2305e-02]],\n              \n                      [[ 7.6698e-02]],\n              \n                      [[ 1.9298e-01]],\n              \n                      [[-1.8871e-01]],\n              \n                      [[-1.6027e-01]],\n              \n                      [[-5.9606e-02]],\n              \n                      [[ 1.0063e-01]],\n              \n                      [[ 1.3917e-01]]])),\n             ('features.1.0.block.0.weight',\n              tensor([[[[-0.0290,  0.0000,  0.0145,  ...,  0.0145, -0.0097, -0.0048],\n                        [-0.0145,  0.0000,  0.0048,  ...,  0.0000, -0.0097, -0.0097],\n                        [-0.0048,  0.0290,  0.0290,  ...,  0.0290,  0.0290,  0.0000],\n                        ...,\n                        [ 0.0097,  0.0435,  0.0821,  ...,  0.0000,  0.0048, -0.0048],\n                        [-0.0145, -0.0097,  0.0193,  ...,  0.0000, -0.0048, -0.0290],\n                        [-0.0145,  0.0000,  0.0193,  ...,  0.0145,  0.0048, -0.0097]]],\n              \n              \n                      [[[ 0.0045,  0.0045,  0.0045,  ...,  0.0045,  0.0045,  0.0045],\n                        [ 0.0045,  0.0090,  0.0090,  ...,  0.0090,  0.0045,  0.0045],\n                        [ 0.0045,  0.0090,  0.0179,  ...,  0.0134,  0.0090,  0.0045],\n                        ...,\n                        [ 0.0045,  0.0090,  0.0134,  ...,  0.0134,  0.0090,  0.0045],\n                        [ 0.0045,  0.0045,  0.0090,  ...,  0.0090,  0.0045,  0.0045],\n                        [ 0.0045,  0.0045,  0.0045,  ...,  0.0045,  0.0045,  0.0045]]],\n              \n              \n                      [[[ 0.0032,  0.0032,  0.0064,  ...,  0.0032,  0.0032,  0.0032],\n                        [ 0.0032,  0.0064,  0.0095,  ...,  0.0064,  0.0064,  0.0064],\n                        [ 0.0064,  0.0095,  0.0159,  ...,  0.0159,  0.0095,  0.0064],\n                        ...,\n                        [ 0.0064,  0.0095,  0.0159,  ...,  0.0127,  0.0095,  0.0064],\n                        [ 0.0032,  0.0064,  0.0095,  ...,  0.0064,  0.0064,  0.0032],\n                        [ 0.0032,  0.0064,  0.0064,  ...,  0.0032,  0.0032,  0.0032]]],\n              \n              \n                      ...,\n              \n              \n                      [[[ 0.0111,  0.0221,  0.0909,  ..., -0.0455, -0.0074, -0.0037],\n                        [ 0.0246,  0.0344,  0.1045,  ..., -0.0369,  0.0037,  0.0086],\n                        [ 0.0700,  0.0811,  0.1561,  ...,  0.0111,  0.0504,  0.0528],\n                        ...,\n                        [-0.0025,  0.0049,  0.0713,  ..., -0.0737, -0.0319, -0.0295],\n                        [ 0.0147,  0.0221,  0.0860,  ..., -0.0553, -0.0135, -0.0086],\n                        [ 0.0172,  0.0233,  0.0860,  ..., -0.0541, -0.0111, -0.0061]]],\n              \n              \n                      [[[-0.0040, -0.0040, -0.0081,  ..., -0.0081, -0.0081, -0.0040],\n                        [-0.0040, -0.0081, -0.0121,  ..., -0.0081, -0.0081, -0.0040],\n                        [-0.0081, -0.0121, -0.0161,  ..., -0.0121, -0.0081, -0.0081],\n                        ...,\n                        [-0.0040, -0.0081, -0.0121,  ..., -0.0121, -0.0081, -0.0081],\n                        [-0.0040, -0.0081, -0.0081,  ..., -0.0081, -0.0081, -0.0081],\n                        [-0.0040, -0.0040, -0.0081,  ..., -0.0081, -0.0081, -0.0040]]],\n              \n              \n                      [[[ 0.0036,  0.0036,  0.0072,  ...,  0.0036,  0.0036,  0.0036],\n                        [ 0.0036,  0.0072,  0.0108,  ...,  0.0072,  0.0036,  0.0036],\n                        [ 0.0072,  0.0108,  0.0217,  ...,  0.0144,  0.0072,  0.0072],\n                        ...,\n                        [ 0.0036,  0.0072,  0.0144,  ...,  0.0180,  0.0072,  0.0036],\n                        [ 0.0036,  0.0072,  0.0072,  ...,  0.0072,  0.0036,  0.0036],\n                        [ 0.0036,  0.0036,  0.0036,  ...,  0.0036,  0.0036,  0.0036]]]],\n                     size=(96, 1, 7, 7), dtype=torch.qint8,\n                     quantization_scheme=torch.per_channel_affine,\n                     scale=tensor([4.8311e-03, 4.4825e-03, 3.1828e-03, 3.0543e-03, 2.6529e-03, 2.2790e-03,\n                      1.1918e-03, 4.0175e-03, 1.0532e-04, 2.6893e-03, 3.0985e-03, 2.7397e-04,\n                      2.7910e-03, 1.7228e-03, 3.0906e-03, 3.5426e-03, 2.8807e-03, 2.8138e-03,\n                      2.1557e-03, 3.6604e-04, 2.4796e-03, 3.6700e-03, 1.8632e-03, 6.6764e-05,\n                      1.9619e-03, 2.9454e-03, 2.3873e-03, 2.5404e-03, 5.3190e-03, 1.7493e-03,\n                      4.0039e-03, 2.4253e-03, 3.5297e-03, 3.4259e-04, 3.7181e-03, 3.9446e-03,\n                      2.8811e-03, 1.7912e-03, 5.4853e-03, 2.4981e-03, 1.8691e-03, 3.1595e-03,\n                      2.8752e-03, 3.0719e-04, 4.3073e-03, 1.0819e-03, 1.3191e-03, 8.0322e-04,\n                      2.2986e-03, 8.3259e-04, 2.5204e-03, 3.1264e-03, 2.2795e-03, 2.7570e-04,\n                      2.9733e-03, 3.7499e-03, 3.1104e-03, 4.1640e-03, 1.2046e-03, 4.4357e-03,\n                      2.1163e-03, 3.5699e-03, 5.1337e-03, 2.3918e-03, 4.0823e-03, 1.9263e-03,\n                      1.5932e-03, 2.2426e-03, 3.9198e-03, 2.9598e-03, 4.2560e-03, 1.3684e-03,\n                      1.2669e-03, 2.5240e-03, 1.1838e-03, 3.3565e-03, 3.0469e-03, 3.1061e-03,\n                      4.7563e-03, 3.0741e-03, 1.1878e-03, 2.8625e-03, 3.8043e-03, 1.5326e-03,\n                      4.2666e-04, 1.8524e-03, 2.4077e-03, 2.0874e-03, 2.0178e-03, 2.2314e-03,\n                      2.2849e-03, 2.1856e-03, 3.5425e-03, 1.2288e-03, 4.0256e-03, 3.6083e-03],\n                     dtype=torch.float64),\n                     zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n                     axis=0)),\n             ('features.1.0.block.0.bias',\n              Parameter containing:\n              tensor([ 1.0715e-03,  1.0740e-03,  1.5665e-04,  3.1654e-04,  5.8940e-04,\n                      -1.5425e-03,  1.4487e-02,  7.7674e-04,  7.5077e-04, -2.4054e-04,\n                       4.3158e-04, -7.2016e-03,  1.6772e-03,  3.8388e-04,  2.4403e-03,\n                      -4.6558e-04,  7.8841e-04,  6.1459e-04, -2.7923e-04, -1.1826e-03,\n                       4.5525e-04, -3.4070e-05,  4.0791e-03,  7.3748e-04, -8.1194e-03,\n                       5.5330e-04, -2.9376e-03,  9.1643e-04,  6.8372e-04,  1.2865e-03,\n                       3.4230e-03,  3.5674e-03, -1.6040e-04, -3.0762e-02,  6.5500e-04,\n                       6.8800e-04,  9.2560e-04, -1.3072e-03,  8.5170e-04,  1.6014e-04,\n                       2.9112e-04, -2.0411e-04,  4.5455e-03,  2.7211e-02,  2.5580e-05,\n                       7.7191e-03, -8.5927e-03, -4.6105e-02,  1.5960e-03, -5.1743e-03,\n                      -2.1712e-04,  3.4012e-03,  1.2114e-03, -2.4508e-02,  6.5809e-04,\n                       1.0368e-03, -1.3603e-04,  6.8418e-04, -1.9376e-02,  5.3708e-04,\n                       1.6789e-03, -1.3794e-04,  2.5197e-04, -6.2281e-04,  5.4860e-04,\n                      -5.8448e-04,  2.2136e-03,  2.2977e-04,  7.1575e-04,  3.5796e-04,\n                       1.1038e-04,  6.8803e-04,  8.4130e-04,  4.2828e-04,  4.5582e-04,\n                       1.5075e-04,  6.8390e-04,  8.3762e-04,  5.3032e-04,  1.2225e-03,\n                       6.7597e-04, -2.7294e-04,  5.4068e-04, -1.2718e-04,  1.3725e-03,\n                      -1.4374e-03,  3.6268e-02, -1.1895e-02, -3.6139e-03,  2.7198e-04,\n                       4.2697e-04, -8.3766e-04,  5.3138e-04,  2.5226e-02,  5.0780e-04,\n                       7.4934e-04], requires_grad=True)),\n             ('features.1.0.block.0.scale', tensor(0.0018)),\n             ('features.1.0.block.0.zero_point', tensor(66)),\n             ('features.1.0.block.2.weight',\n              tensor([ 1.4512,  3.1833,  3.2210,  2.3384,  3.3073,  0.4586,  1.1434,  2.4023,\n                       0.2931,  2.7822,  4.4526,  0.6768,  2.3961,  2.2536,  2.0381,  1.7391,\n                       3.0607,  2.3703, -2.3395,  0.8049,  3.2867,  1.9554,  0.9068, -0.2936,\n                       0.8045,  3.3200,  2.3959,  1.6420,  3.7920,  1.4313,  2.0278,  1.2806,\n                       2.1735,  1.6146,  3.7545,  3.6293,  2.4183,  1.5960,  2.7247,  3.0392,\n                      -2.1308,  2.6812,  1.3505,  1.3864,  3.0987,  0.4572,  0.4660,  0.7917,\n                       1.5863,  0.2929,  2.6215,  2.5376,  2.7870,  1.2206,  3.3152,  2.4261,\n                       2.0229,  1.9075,  1.3777,  1.7019,  0.4282,  2.3104,  4.8042,  1.8302,\n                       2.6464,  0.4831,  1.3770,  2.0400,  3.1320,  2.9886,  2.8012,  2.3173,\n                       2.5478,  1.6904,  2.1369,  2.2237,  2.5707,  3.2846,  0.8023,  2.4538,\n                       1.4282,  1.7588,  3.1896, -1.5469, -0.2487,  1.7840,  1.0964,  1.3750,\n                       1.6504,  2.0940,  4.0245,  2.0496,  2.5046,  1.1565,  2.9907,  4.3280])),\n             ('features.1.0.block.2.bias',\n              tensor([ 4.8347e-02,  1.5469e-01,  2.2655e-01,  1.5957e-01,  1.2139e-01,\n                      -2.0456e-02, -5.3099e-01,  9.2568e-02,  5.3072e-01,  3.0698e-01,\n                       1.7300e-01,  2.4992e+00, -1.6094e-03,  1.1737e-01,  1.7038e-01,\n                       2.0927e-01,  2.4458e-01,  7.6315e-02, -2.8054e-01,  2.5857e-01,\n                       2.0015e-01,  5.8302e-02,  4.8432e-01, -4.9255e-01,  3.7852e-01,\n                       8.0294e-02,  5.9674e-01,  9.5811e-02, -1.8631e-02,  2.9795e-01,\n                      -6.9007e-02,  1.3373e-01,  7.5551e-02,  1.8248e-01,  1.2936e-01,\n                       5.0626e-02,  2.3110e-01,  2.5200e-01,  3.3065e-02,  1.6806e-01,\n                      -1.6744e-01,  3.8501e-01, -3.6019e-02,  3.3228e-02,  1.7199e-01,\n                       5.2774e-02,  7.3568e-02,  2.5228e-01,  5.1627e-02,  1.9491e+00,\n                       4.2529e-01, -4.2889e-01,  6.0642e-02,  2.9367e+00,  1.2591e-01,\n                       1.6912e-01,  2.8136e-01,  9.2649e-02,  1.0713e-01,  1.6595e-02,\n                       8.8323e-02,  1.3048e-01,  1.4333e-01,  2.6754e-01,  1.4318e-01,\n                       1.4564e-01,  1.2064e-01,  2.5722e-01, -8.0677e-03,  3.4048e-01,\n                       1.9935e-01,  1.4282e-01,  7.1313e-02,  1.3583e-01,  1.2530e-01,\n                       1.0483e-01,  3.5344e-01,  2.5490e-02,  1.4199e-01,  6.1960e-02,\n                       1.6167e-01,  2.4170e-01,  1.1084e-01, -2.5735e-01, -2.0328e+00,\n                       1.8875e-01, -1.8095e-01, -6.1355e-02,  4.0999e-01,  1.4277e-01,\n                       1.7999e-01,  3.8804e-01,  2.2460e-01, -5.7170e-02,  1.4325e-01,\n                       9.2403e-02])),\n             ('features.1.0.block.2.scale', tensor(0.3096)),\n             ('features.1.0.block.2.zero_point', tensor(62)),\n             ('features.1.0.block.3.scale', tensor(0.1135)),\n             ('features.1.0.block.3.zero_point', tensor(67)),\n             ('features.1.0.block.3._packed_params.dtype', torch.qint8),\n             ('features.1.0.block.3._packed_params._packed_params',\n              (tensor([[ 0.0133, -0.0133, -0.1137,  ...,  0.0080, -0.0764,  0.0204],\n                       [ 0.0873, -0.0718,  0.0484,  ...,  0.0047,  0.0827, -0.0484],\n                       [-0.0272, -0.0176, -0.0256,  ...,  0.0016,  0.0048,  0.0048],\n                       ...,\n                       [ 0.0012,  0.0120, -0.1267,  ..., -0.0179, -0.0705, -0.0036],\n                       [ 0.0606, -0.0511,  0.0076,  ...,  0.0189,  0.0246,  0.0057],\n                       [-0.0623, -0.0284,  0.0474,  ..., -0.0894, -0.0867, -0.0068]],\n                      size=(384, 96), dtype=torch.qint8,\n                      quantization_scheme=torch.per_channel_affine,\n                      scale=tensor([0.0009, 0.0016, 0.0016, 0.0019, 0.0028, 0.0017, 0.0014, 0.0013, 0.0014,\n                       0.0015, 0.0022, 0.0025, 0.0012, 0.0029, 0.0032, 0.0010, 0.0011, 0.0019,\n                       0.0021, 0.0015, 0.0010, 0.0018, 0.0019, 0.0009, 0.0014, 0.0015, 0.0029,\n                       0.0013, 0.0013, 0.0014, 0.0014, 0.0025, 0.0015, 0.0012, 0.0021, 0.0023,\n                       0.0014, 0.0029, 0.0010, 0.0026, 0.0021, 0.0016, 0.0014, 0.0015, 0.0014,\n                       0.0008, 0.0027, 0.0020, 0.0012, 0.0011, 0.0011, 0.0023, 0.0013, 0.0011,\n                       0.0011, 0.0012, 0.0011, 0.0019, 0.0014, 0.0013, 0.0026, 0.0015, 0.0021,\n                       0.0013, 0.0023, 0.0010, 0.0016, 0.0014, 0.0022, 0.0021, 0.0013, 0.0015,\n                       0.0020, 0.0015, 0.0022, 0.0019, 0.0023, 0.0011, 0.0016, 0.0018, 0.0010,\n                       0.0011, 0.0028, 0.0019, 0.0024, 0.0013, 0.0012, 0.0013, 0.0021, 0.0017,\n                       0.0014, 0.0020, 0.0024, 0.0024, 0.0022, 0.0023, 0.0010, 0.0015, 0.0008,\n                       0.0014, 0.0020, 0.0012, 0.0022, 0.0010, 0.0040, 0.0019, 0.0040, 0.0016,\n                       0.0018, 0.0023, 0.0017, 0.0023, 0.0014, 0.0013, 0.0023, 0.0024, 0.0016,\n                       0.0018, 0.0013, 0.0020, 0.0017, 0.0009, 0.0008, 0.0013, 0.0011, 0.0015,\n                       0.0014, 0.0015, 0.0027, 0.0013, 0.0010, 0.0020, 0.0014, 0.0031, 0.0014,\n                       0.0014, 0.0012, 0.0017, 0.0025, 0.0022, 0.0008, 0.0032, 0.0025, 0.0012,\n                       0.0021, 0.0017, 0.0013, 0.0024, 0.0027, 0.0010, 0.0019, 0.0022, 0.0015,\n                       0.0022, 0.0021, 0.0012, 0.0014, 0.0019, 0.0010, 0.0010, 0.0012, 0.0012,\n                       0.0027, 0.0012, 0.0009, 0.0015, 0.0024, 0.0020, 0.0026, 0.0011, 0.0011,\n                       0.0013, 0.0013, 0.0012, 0.0028, 0.0013, 0.0024, 0.0012, 0.0018, 0.0020,\n                       0.0010, 0.0012, 0.0008, 0.0014, 0.0011, 0.0022, 0.0016, 0.0014, 0.0013,\n                       0.0011, 0.0022, 0.0016, 0.0019, 0.0012, 0.0011, 0.0024, 0.0019, 0.0017,\n                       0.0020, 0.0027, 0.0009, 0.0020, 0.0022, 0.0011, 0.0013, 0.0028, 0.0013,\n                       0.0014, 0.0022, 0.0014, 0.0015, 0.0022, 0.0010, 0.0011, 0.0017, 0.0012,\n                       0.0027, 0.0023, 0.0013, 0.0020, 0.0011, 0.0016, 0.0017, 0.0024, 0.0009,\n                       0.0018, 0.0012, 0.0014, 0.0017, 0.0010, 0.0017, 0.0025, 0.0016, 0.0016,\n                       0.0016, 0.0013, 0.0015, 0.0011, 0.0024, 0.0021, 0.0014, 0.0027, 0.0019,\n                       0.0014, 0.0015, 0.0028, 0.0010, 0.0019, 0.0008, 0.0015, 0.0023, 0.0025,\n                       0.0013, 0.0020, 0.0013, 0.0022, 0.0013, 0.0011, 0.0013, 0.0014, 0.0014,\n                       0.0011, 0.0012, 0.0029, 0.0018, 0.0011, 0.0015, 0.0009, 0.0021, 0.0020,\n                       0.0014, 0.0021, 0.0023, 0.0032, 0.0012, 0.0016, 0.0014, 0.0012, 0.0014,\n                       0.0017, 0.0011, 0.0021, 0.0020, 0.0024, 0.0019, 0.0015, 0.0009, 0.0011,\n                       0.0014, 0.0010, 0.0014, 0.0011, 0.0016, 0.0014, 0.0010, 0.0010, 0.0009,\n                       0.0015, 0.0011, 0.0013, 0.0011, 0.0020, 0.0011, 0.0023, 0.0024, 0.0015,\n                       0.0020, 0.0023, 0.0020, 0.0026, 0.0021, 0.0015, 0.0014, 0.0009, 0.0015,\n                       0.0012, 0.0014, 0.0012, 0.0015, 0.0013, 0.0014, 0.0024, 0.0017, 0.0009,\n                       0.0012, 0.0015, 0.0022, 0.0012, 0.0009, 0.0018, 0.0013, 0.0014, 0.0019,\n                       0.0021, 0.0020, 0.0023, 0.0011, 0.0019, 0.0018, 0.0011, 0.0015, 0.0008,\n                       0.0018, 0.0029, 0.0023, 0.0013, 0.0019, 0.0030, 0.0021, 0.0013, 0.0011,\n                       0.0013, 0.0013, 0.0020, 0.0029, 0.0024, 0.0010, 0.0024, 0.0016, 0.0015,\n                       0.0015, 0.0020, 0.0016, 0.0021, 0.0013, 0.0012, 0.0010, 0.0013, 0.0010,\n                       0.0016, 0.0008, 0.0012, 0.0012, 0.0021, 0.0027, 0.0017, 0.0015, 0.0010,\n                       0.0017, 0.0015, 0.0016, 0.0012, 0.0019, 0.0014], dtype=torch.float64),\n                      zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n                      axis=0),\n               Parameter containing:\n               tensor([-1.0029e-01, -1.1179e-02, -5.0794e-02,  2.6718e-03,  2.1680e-02,\n                       -7.6490e-02, -9.1704e-02, -7.9125e-03, -4.7927e-02, -2.4490e-02,\n                        3.0237e-02, -4.4639e-02, -3.0426e-02, -1.0769e-02, -3.4268e-02,\n                       -9.8819e-02, -9.6919e-02, -8.6644e-03, -8.9458e-03,  1.1112e-02,\n                       -9.2536e-02, -1.5136e-02, -9.5629e-02, -9.5275e-02,  6.1565e-03,\n                       -3.3358e-02, -1.0842e-04, -3.7580e-02, -1.2235e-01, -5.6057e-02,\n                       -3.4557e-02, -1.3047e-02, -9.9382e-02,  2.7188e-02, -1.0742e-01,\n                       -2.2333e-02,  8.3598e-03, -4.2797e-03,  2.5290e-02, -4.2127e-02,\n                       -6.3414e-02, -1.3709e-02, -1.4516e-02, -7.1793e-02, -1.0453e-01,\n                       -8.8066e-02, -5.6263e-02, -3.1068e-02, -1.8740e-02, -1.0567e-01,\n                       -3.6659e-02, -1.7021e-02, -8.8744e-02, -8.8912e-02, -9.5352e-02,\n                       -1.1744e-02, -1.5855e-02, -1.2303e-02, -1.1189e-01, -4.5134e-03,\n                       -1.6264e-02, -6.7242e-02, -4.5794e-02,  1.2318e-04, -6.6944e-03,\n                       -6.7191e-02, -2.4052e-02, -1.4672e-02, -1.6771e-02, -6.3333e-02,\n                       -6.0117e-02, -1.2876e-02, -3.1860e-02, -3.5512e-03, -7.4067e-02,\n                       -1.3735e-02,  9.9392e-03, -4.0793e-02, -1.5905e-02,  9.1886e-03,\n                        2.7361e-02, -1.0731e-01, -1.6501e-02, -3.6703e-02,  4.7401e-02,\n                       -8.4000e-02, -1.7182e-02, -1.0440e-01, -8.7221e-02, -1.7195e-02,\n                       -1.2065e-02,  1.0705e-03, -1.0043e-02, -1.4146e-02, -5.9978e-02,\n                       -5.8350e-02, -7.5252e-02, -1.2651e-02,  9.7787e-03, -2.8771e-02,\n                       -3.1080e-02, -7.8357e-02, -6.2284e-02, -8.9712e-02, -5.5126e-02,\n                        3.5453e-02, -2.0292e-02, -8.8573e-03, -5.0109e-03, -1.4753e-02,\n                        2.7296e-02, -9.3383e-03, -9.9229e-03,  3.2542e-03, -1.7688e-02,\n                       -4.4484e-02, -4.4694e-02, -1.4217e-02, -9.1608e-03, -4.6085e-03,\n                       -7.1301e-02, -9.9931e-02, -9.5408e-02, -1.2415e-01, -1.0508e-01,\n                       -8.1054e-02, -8.8264e-02,  2.1405e-02, -4.8784e-02, -1.1847e-01,\n                       -1.0453e-01, -6.5688e-02, -1.6369e-02, -8.7689e-03, -1.0321e-01,\n                       -8.5686e-03, -9.9974e-02, -9.7645e-02, -2.4181e-02, -9.7622e-03,\n                       -1.0291e-01, -6.4834e-02, -5.3459e-02, -1.1376e-01, -4.9693e-03,\n                        3.8000e-02,  7.1212e-04, -5.8780e-02, -1.6233e-02, -1.2802e-01,\n                       -9.3418e-03, -5.1244e-02, -3.6931e-02,  6.3458e-02, -1.9080e-02,\n                       -1.1409e-01, -9.4924e-03, -2.3673e-02, -9.9646e-02, -9.7148e-02,\n                        2.8205e-02, -3.1907e-02, -2.3052e-02,  2.4580e-03, -1.0284e-01,\n                       -1.1365e-01, -1.9961e-02, -1.4502e-02, -1.3869e-02, -7.3231e-03,\n                       -2.6783e-02, -1.9318e-02, -7.6357e-03, -5.2997e-02, -5.3893e-02,\n                       -1.0319e-01, -2.5113e-02, -8.9642e-02, -6.7116e-02, -1.3927e-02,\n                       -8.4768e-02, -7.9864e-03, -9.7174e-02, -1.8903e-02, -8.3510e-02,\n                       -9.4295e-03, -2.7230e-03, -5.9719e-02, -1.1867e-02, -1.0487e-01,\n                       -7.8875e-02, -1.1476e-02, -6.6079e-02,  2.0840e-03, -1.2060e-01,\n                        2.0004e-02, -2.0824e-02, -3.1825e-02, -7.3017e-02, -8.1078e-02,\n                       -1.0264e-01, -6.2755e-02, -5.9133e-02, -8.6508e-02, -2.2512e-02,\n                        8.3198e-02, -4.7643e-02, -8.2578e-02, -8.3266e-02, -1.7035e-02,\n                        1.4419e-02, -4.8059e-02, -9.7955e-02, -1.0454e-01, -1.5742e-02,\n                       -1.1880e-01, -1.3617e-02, -7.7952e-02, -2.3261e-03, -7.8970e-03,\n                       -1.0729e-01, -7.7216e-02, -1.1212e-01,  2.1086e-02, -1.0452e-01,\n                       -2.0046e-02, -9.6762e-02, -4.0477e-02, -1.9556e-02, -1.1038e-01,\n                       -1.8381e-02, -1.7692e-02, -2.1080e-02, -4.8623e-02, -1.9451e-02,\n                       -1.8003e-02, -4.5552e-02,  2.9328e-02, -4.7218e-02, -4.2532e-02,\n                        1.5921e-02,  1.3406e-02, -1.0695e-02, -3.2615e-02, -1.6925e-02,\n                       -1.3209e-02, -1.0042e-01, -1.5274e-02, -9.5328e-02, -9.3246e-02,\n                       -1.7867e-02, -2.6145e-02, -3.1532e-03, -7.0730e-03, -2.1585e-02,\n                       -2.1316e-02, -3.7563e-02, -9.3140e-02, -3.9109e-03, -1.3280e-02,\n                       -2.6489e-02, -7.4626e-02, -1.1900e-01, -7.6723e-03, -2.4648e-02,\n                       -1.2953e-01, -2.8692e-02, -1.1407e-01, -3.5192e-02, -9.8075e-02,\n                       -1.1796e-01, -2.4822e-02,  5.6971e-02, -2.4021e-02, -1.0477e-01,\n                       -8.5820e-02, -1.1192e-01, -1.1232e-01, -8.3183e-02, -3.1485e-02,\n                       -9.2585e-02,  2.6401e-02, -3.8651e-02, -8.3001e-03, -3.0801e-02,\n                       -4.5466e-02, -1.0753e-01, -9.6523e-02, -1.3651e-02, -1.0305e-01,\n                       -2.8384e-03, -1.9311e-02, -3.5030e-02, -4.8785e-02, -1.2495e-01,\n                       -9.5359e-02, -1.0447e-01, -7.4388e-03, -1.2604e-01, -4.8456e-02,\n                       -1.0022e-01, -6.7773e-02, -1.1837e-01, -3.6165e-02, -1.3866e-02,\n                       -1.0465e-01, -1.0376e-01,  3.9553e-02, -1.0529e-01, -2.4835e-02,\n                       -1.0013e-01, -1.1000e-01, -2.0203e-02, -9.9754e-02, -5.5764e-05,\n                       -4.1850e-02, -4.5826e-02, -1.2153e-01, -5.4193e-02, -4.0104e-02,\n                        2.1708e-02, -9.5585e-03, -3.6209e-02, -1.0085e-01, -1.2536e-02,\n                        2.6109e-03, -1.4377e-02, -3.6427e-02, -8.7311e-02, -6.9305e-03,\n                       -5.8305e-03, -1.5818e-02, -1.0227e-02, -7.3642e-03, -3.7570e-02,\n                       -1.0145e-01, -8.7693e-02, -1.8655e-02, -4.7462e-02, -2.1185e-02,\n                       -2.3405e-02, -1.3110e-03,  1.6866e-02,  4.5086e-02, -1.9854e-02,\n                       -9.2204e-02, -5.4070e-02, -1.3008e-02, -2.6232e-02, -3.0710e-02,\n                       -9.7934e-02, -1.1197e-02,  2.1532e-02, -2.4607e-02, -4.0878e-03,\n                        5.0633e-02, -9.4221e-02, -1.0360e-02, -1.5675e-02, -1.0398e-02,\n                       -8.8711e-02, -2.0063e-02, -1.0389e-02, -3.6506e-02, -6.8013e-03,\n                       -1.2119e-01, -9.5952e-02, -2.2669e-02, -8.2832e-02, -1.9121e-02,\n                       -9.9339e-02, -8.1007e-02, -3.5126e-04, -8.5800e-02,  9.2928e-02,\n                       -2.1929e-02, -2.3772e-02, -8.5836e-02,  4.8624e-02, -1.5018e-02,\n                       -1.0469e-01, -1.1156e-02, -3.2771e-03, -8.9967e-02],\n                      requires_grad=True))),\n             ('features.1.0.block.5.scale', tensor(0.0455)),\n             ('features.1.0.block.5.zero_point', tensor(64)),\n             ('features.1.0.block.5._packed_params.dtype', torch.qint8),\n             ('features.1.0.block.5._packed_params._packed_params',\n              (tensor([[ 0.0094, -0.0172, -0.0485,  ..., -0.0297,  0.0595, -0.0469],\n                       [ 0.0276,  0.0713,  0.0184,  ..., -0.0506,  0.0138, -0.0161],\n                       [ 0.1740, -0.0129, -0.0516,  ..., -0.0483,  0.0322, -0.0226],\n                       ...,\n                       [-0.0080, -0.0186, -0.0305,  ..., -0.0066, -0.0106,  0.0464],\n                       [-0.0088, -0.0352,  0.0682,  ..., -0.0022, -0.0528,  0.0440],\n                       [ 0.0033, -0.0248, -0.0363,  ..., -0.0363, -0.0627, -0.0182]],\n                      size=(96, 384), dtype=torch.qint8,\n                      quantization_scheme=torch.per_channel_affine,\n                      scale=tensor([0.0016, 0.0023, 0.0032, 0.0024, 0.0023, 0.0012, 0.0020, 0.0030, 0.0005,\n                       0.0019, 0.0018, 0.0024, 0.0019, 0.0034, 0.0026, 0.0029, 0.0024, 0.0010,\n                       0.0011, 0.0029, 0.0035, 0.0024, 0.0027, 0.0010, 0.0015, 0.0030, 0.0014,\n                       0.0017, 0.0016, 0.0022, 0.0012, 0.0024, 0.0019, 0.0023, 0.0021, 0.0026,\n                       0.0023, 0.0018, 0.0031, 0.0019, 0.0023, 0.0022, 0.0014, 0.0016, 0.0033,\n                       0.0014, 0.0017, 0.0017, 0.0018, 0.0033, 0.0024, 0.0023, 0.0025, 0.0018,\n                       0.0018, 0.0041, 0.0016, 0.0014, 0.0012, 0.0012, 0.0018, 0.0023, 0.0032,\n                       0.0024, 0.0035, 0.0015, 0.0006, 0.0020, 0.0019, 0.0021, 0.0029, 0.0006,\n                       0.0041, 0.0030, 0.0027, 0.0011, 0.0025, 0.0012, 0.0026, 0.0023, 0.0007,\n                       0.0033, 0.0022, 0.0038, 0.0017, 0.0016, 0.0013, 0.0011, 0.0020, 0.0019,\n                       0.0034, 0.0038, 0.0019, 0.0013, 0.0022, 0.0017], dtype=torch.float64),\n                      zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n                      axis=0),\n               Parameter containing:\n               tensor([-0.0033,  0.0827,  0.0006, -0.0013, -0.0642,  0.0559,  0.1117,  0.0207,\n                        0.0035, -0.0757, -0.1004,  0.0043,  0.0042, -0.0123, -0.0559,  0.0040,\n                       -0.0168,  0.1709, -0.0064, -0.1520,  0.0320,  0.0024,  0.0203,  0.0042,\n                        0.0415, -0.0072, -0.1645, -0.2514, -0.0470,  0.0293, -0.0311, -0.0126,\n                        0.0019, -0.0355, -0.0131,  0.0069, -0.1007, -0.0164, -0.0240, -0.0369,\n                        0.0220, -0.0399, -0.0137, -0.0008, -0.0116,  0.0205,  0.0497,  0.0197,\n                       -0.0052,  0.0132,  0.0879, -0.0361,  0.0003, -0.0053,  0.0907,  0.0470,\n                        0.0195,  0.0115, -0.0010, -0.0077,  0.0053,  0.0937, -0.0067, -0.0650,\n                        0.0522,  0.0038,  0.0156,  0.0006, -0.0036,  0.0148,  0.0126, -0.0005,\n                        0.0298, -0.0757,  0.0123, -0.0057, -0.0333, -0.0054, -0.0025,  0.0208,\n                       -0.0010,  0.0551,  0.0037,  0.0473,  0.0079,  0.0422,  0.1051,  0.0819,\n                       -0.1016,  0.0058, -0.0328, -0.0434, -0.0197,  0.0177,  0.0307,  0.1226],\n                      requires_grad=True))),\n             ('features.1.1.layer_scale',\n              tensor([[[ 1.9186e-02]],\n              \n                      [[-1.7064e-01]],\n              \n                      [[-1.4611e-01]],\n              \n                      [[-8.8352e-02]],\n              \n                      [[ 2.0887e-01]],\n              \n                      [[ 3.5426e-02]],\n              \n                      [[ 3.6077e-04]],\n              \n                      [[ 8.2835e-02]],\n              \n                      [[-3.1038e-01]],\n              \n                      [[ 2.3660e-01]],\n              \n                      [[ 2.1918e-01]],\n              \n                      [[-4.0813e-02]],\n              \n                      [[ 1.8158e-01]],\n              \n                      [[-1.9852e-01]],\n              \n                      [[-1.4839e-01]],\n              \n                      [[-1.8818e-01]],\n              \n                      [[ 1.9995e-01]],\n              \n                      [[ 1.5852e-01]],\n              \n                      [[-1.1603e-01]],\n              \n                      [[ 3.1181e-01]],\n              \n                      [[ 8.3440e-02]],\n              \n                      [[ 7.7485e-02]],\n              \n                      [[ 4.0916e-01]],\n              \n                      [[ 3.1201e-01]],\n              \n                      [[-3.3755e-02]],\n              \n                      [[ 1.4715e-01]],\n              \n                      [[-3.2717e-01]],\n              \n                      [[-3.8835e-01]],\n              \n                      [[ 6.4416e-02]],\n              \n                      [[ 1.3494e-01]],\n              \n                      [[-2.7765e-02]],\n              \n                      [[-3.8181e-02]],\n              \n                      [[-1.5797e-01]],\n              \n                      [[-3.2933e-02]],\n              \n                      [[ 1.8009e-01]],\n              \n                      [[-1.9718e-01]],\n              \n                      [[-2.0654e-01]],\n              \n                      [[-1.1428e-01]],\n              \n                      [[ 7.7982e-02]],\n              \n                      [[ 1.9504e-01]],\n              \n                      [[-2.3846e-01]],\n              \n                      [[-2.0931e-01]],\n              \n                      [[ 3.3344e-02]],\n              \n                      [[-1.7614e-04]],\n              \n                      [[ 4.7531e-02]],\n              \n                      [[ 2.4637e-02]],\n              \n                      [[ 2.4813e-02]],\n              \n                      [[-3.9820e-02]],\n              \n                      [[-9.2640e-02]],\n              \n                      [[ 1.8884e-01]],\n              \n                      [[-2.1632e-01]],\n              \n                      [[-2.1388e-01]],\n              \n                      [[ 1.0060e-01]],\n              \n                      [[ 7.3408e-03]],\n              \n                      [[-2.0974e-01]],\n              \n                      [[-2.9082e-01]],\n              \n                      [[ 1.5680e-01]],\n              \n                      [[-1.4100e-01]],\n              \n                      [[-7.0019e-02]],\n              \n                      [[ 3.8823e-02]],\n              \n                      [[-3.9660e-02]],\n              \n                      [[ 1.9204e-01]],\n              \n                      [[ 6.1968e-02]],\n              \n                      [[ 1.9361e-01]],\n              \n                      [[ 2.3992e-01]],\n              \n                      [[ 2.1947e-02]],\n              \n                      [[ 3.3701e-01]],\n              \n                      [[ 8.0316e-02]],\n              \n                      [[-6.4472e-02]],\n              \n                      [[ 2.1318e-01]],\n              \n                      [[-2.0178e-01]],\n              \n                      [[ 4.1318e-02]],\n              \n                      [[ 2.6513e-01]],\n              \n                      [[ 2.1289e-01]],\n              \n                      [[-2.3171e-01]],\n              \n                      [[ 1.3611e-02]],\n              \n                      [[ 1.7584e-01]],\n              \n                      [[-3.7455e-02]],\n              \n                      [[-4.5111e-02]],\n              \n                      [[ 1.3555e-01]],\n              \n                      [[ 5.2032e-02]],\n              \n                      [[-1.2297e-01]],\n              \n                      [[-9.0871e-02]],\n              \n                      [[-2.1690e-01]],\n              \n                      [[-5.5516e-02]],\n              \n                      [[ 1.1057e-01]],\n              \n                      [[ 2.1687e-01]],\n              \n                      [[-4.5328e-02]],\n              \n                      [[-2.0326e-01]],\n              \n                      [[-1.2188e-01]],\n              \n                      [[ 1.4128e-01]],\n              \n                      [[-1.0345e-01]],\n              \n                      [[ 1.8889e-01]],\n              \n                      [[ 3.4655e-02]],\n              \n                      [[ 3.3757e-01]],\n              \n                      [[-2.0102e-01]]])),\n             ('features.1.1.block.0.weight',\n              tensor([[[[ 0.0199,  0.0164,  0.0164,  ...,  0.0224,  0.0250,  0.0078],\n                        [ 0.0190,  0.0190,  0.0138,  ...,  0.0311,  0.0173,  0.0121],\n                        [ 0.0259,  0.0302,  0.0397,  ...,  0.0311,  0.0311,  0.0259],\n                        ...,\n                        [ 0.0328,  0.0268,  0.0250,  ...,  0.0380,  0.0406,  0.0363],\n                        [ 0.0207,  0.0164,  0.0319,  ...,  0.0199,  0.0138,  0.0207],\n                        [ 0.0086,  0.0199,  0.0259,  ...,  0.0199,  0.0112,  0.0216]]],\n              \n              \n                      [[[ 0.0042,  0.0059,  0.0067,  ...,  0.0084,  0.0051,  0.0051],\n                        [ 0.0025,  0.0093,  0.0118,  ...,  0.0084,  0.0135,  0.0042],\n                        [ 0.0034,  0.0017,  0.0143,  ...,  0.0244,  0.0101,  0.0025],\n                        ...,\n                        [ 0.0000,  0.0034,  0.0101,  ...,  0.0295,  0.0017,  0.0034],\n                        [ 0.0034,  0.0126,  0.0101,  ...,  0.0093,  0.0101,  0.0059],\n                        [ 0.0034,  0.0042,  0.0076,  ...,  0.0042,  0.0084,  0.0059]]],\n              \n              \n                      [[[ 0.0000,  0.0015,  0.0015,  ...,  0.0015, -0.0030,  0.0015],\n                        [-0.0015,  0.0015,  0.0044,  ...,  0.0118,  0.0030,  0.0000],\n                        [ 0.0044,  0.0089,  0.0148,  ...,  0.0222,  0.0104,  0.0044],\n                        ...,\n                        [-0.0074,  0.0000,  0.0059,  ...,  0.0193,  0.0074, -0.0015],\n                        [-0.0015, -0.0030,  0.0000,  ...,  0.0044, -0.0030,  0.0015],\n                        [-0.0030, -0.0030, -0.0015,  ..., -0.0015, -0.0030,  0.0000]]],\n              \n              \n                      ...,\n              \n              \n                      [[[-0.0110, -0.0137, -0.0110,  ..., -0.0219, -0.0110, -0.0137],\n                        [-0.0110, -0.0082, -0.0137,  ..., -0.0219, -0.0082, -0.0137],\n                        [-0.0110, -0.0137, -0.0439,  ..., -0.0603, -0.0082, -0.0164],\n                        ...,\n                        [-0.0192, -0.0164, -0.0411,  ..., -0.0548, -0.0137, -0.0192],\n                        [-0.0055, -0.0082, -0.0164,  ..., -0.0247, -0.0082, -0.0055],\n                        [-0.0082, -0.0110, -0.0164,  ..., -0.0219, -0.0082, -0.0110]]],\n              \n              \n                      [[[-0.0044,  0.0033, -0.0065,  ...,  0.0044, -0.0109, -0.0033],\n                        [-0.0033,  0.0098, -0.0011,  ..., -0.0261,  0.0163,  0.0163],\n                        [ 0.0326,  0.0239,  0.0163,  ...,  0.0729,  0.0283,  0.0305],\n                        ...,\n                        [-0.0152, -0.0261, -0.0642,  ...,  0.0054, -0.0174, -0.0468],\n                        [ 0.0054,  0.0098,  0.0141,  ..., -0.0054,  0.0065, -0.0120],\n                        [-0.0033,  0.0065, -0.0294,  ...,  0.0076,  0.0054, -0.0087]]],\n              \n              \n                      [[[ 0.0000, -0.0014,  0.0027,  ...,  0.0014, -0.0014,  0.0027],\n                        [ 0.0014,  0.0041,  0.0014,  ..., -0.0014,  0.0054,  0.0000],\n                        [-0.0014,  0.0068,  0.0163,  ...,  0.0149,  0.0041,  0.0000],\n                        ...,\n                        [-0.0041,  0.0014, -0.0285,  ..., -0.0217, -0.0014, -0.0014],\n                        [-0.0041, -0.0041, -0.0095,  ..., -0.0108, -0.0041, -0.0027],\n                        [-0.0041, -0.0014, -0.0041,  ..., -0.0068,  0.0000, -0.0014]]]],\n                     size=(96, 1, 7, 7), dtype=torch.qint8,\n                     quantization_scheme=torch.per_channel_affine,\n                     scale=tensor([8.6314e-04, 8.4281e-04, 1.4812e-03, 1.6167e-03, 8.0967e-04, 3.8629e-03,\n                      1.4984e-03, 2.6577e-03, 3.2443e-03, 1.6029e-03, 1.4343e-03, 1.1920e-03,\n                      1.8883e-03, 1.1344e-03, 2.1614e-03, 2.1719e-03, 1.0844e-03, 2.4877e-03,\n                      1.2934e-03, 2.1385e-03, 1.2109e-03, 2.8464e-03, 2.5702e-03, 2.9332e-03,\n                      1.1693e-03, 1.1277e-03, 1.9710e-03, 2.7220e-03, 8.9374e-04, 2.3363e-03,\n                      1.2883e-03, 9.6837e-04, 3.1277e-03, 2.2744e-03, 1.1274e-03, 2.0563e-03,\n                      1.7635e-03, 2.6164e-03, 1.5508e-03, 1.3750e-03, 1.0109e-03, 7.9866e-04,\n                      1.3353e-03, 7.1628e-05, 2.0261e-03, 1.2940e-03, 1.1619e-03, 2.6574e-03,\n                      3.2251e-03, 4.8621e-03, 1.6154e-03, 1.0568e-03, 2.7230e-03, 3.5697e-04,\n                      9.0416e-04, 2.3373e-03, 1.7136e-03, 2.0077e-03, 1.1195e-03, 1.5300e-03,\n                      8.7511e-04, 9.7054e-04, 2.7365e-03, 1.9685e-03, 2.2580e-03, 2.3931e-04,\n                      2.9000e-03, 2.6558e-03, 2.5876e-03, 7.7476e-04, 1.9681e-03, 6.6657e-04,\n                      1.8690e-03, 1.4622e-03, 1.1385e-03, 1.7951e-03, 2.0575e-03, 1.4343e-03,\n                      3.0258e-03, 2.1548e-03, 9.1554e-04, 9.5593e-04, 2.3083e-03, 2.1160e-03,\n                      3.3968e-03, 6.0729e-04, 2.8576e-03, 3.2543e-03, 1.7281e-03, 2.0442e-03,\n                      9.6715e-04, 1.6637e-03, 1.0803e-03, 2.7407e-03, 1.0882e-03, 1.3557e-03],\n                     dtype=torch.float64),\n                     zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n                     axis=0)),\n             ('features.1.1.block.0.bias',\n              Parameter containing:\n              tensor([-1.1436e-02, -1.5927e-03,  6.6274e-05, -1.7751e-03, -2.0805e-02,\n                      -5.2641e-03, -1.0306e-02,  2.1136e-04, -4.0080e-03,  7.8716e-04,\n                      -7.2847e-04,  4.0869e-04, -9.9607e-03,  1.4807e-03,  8.0458e-03,\n                      -2.2958e-03, -1.1930e-03, -1.1981e-02,  2.7446e-03, -1.1798e-02,\n                      -1.3142e-03, -1.1849e-02,  4.2703e-03, -1.0606e-03,  1.0378e-02,\n                       3.5996e-03,  8.3993e-02, -4.6052e-03, -1.6627e-02, -7.0654e-03,\n                      -1.8222e-02, -4.0081e-03, -8.7041e-03,  2.2143e-03,  1.3702e-02,\n                       4.8684e-03, -5.9884e-03, -1.4016e-02,  4.6975e-03,  2.1674e-03,\n                      -2.2091e-03,  9.3506e-03,  2.1264e-02,  8.2172e-02,  8.2581e-04,\n                      -1.0353e-02,  1.0691e-02, -1.7756e-02,  8.3266e-04, -4.6400e-03,\n                      -2.2694e-02, -3.7759e-03,  7.2041e-04, -4.5510e-02, -5.3052e-02,\n                      -8.7162e-04,  8.7367e-03, -6.0361e-03, -6.7361e-02, -8.1312e-03,\n                      -1.5398e-02, -3.0131e-03, -1.3356e-04,  2.5442e-03,  9.7442e-03,\n                       7.6833e-02,  4.9448e-02, -1.1658e-04, -2.2237e-02,  2.4294e-03,\n                       7.7199e-03, -3.0805e-04, -7.7654e-04,  1.2745e-03,  1.5741e-03,\n                      -1.0657e-02,  2.6711e-03,  4.3624e-03,  4.4026e-04,  5.7216e-03,\n                      -1.4180e-03, -7.5689e-03,  4.6769e-03, -1.1931e-03,  1.6652e-03,\n                      -8.9692e-04, -5.2339e-03,  1.5026e-03,  6.2204e-03, -3.7027e-03,\n                       4.3169e-04, -3.2235e-03,  4.4648e-03, -4.4431e-03, -4.7147e-03,\n                      -4.9360e-03], requires_grad=True)),\n             ('features.1.1.block.0.scale', tensor(0.0044)),\n             ('features.1.1.block.0.zero_point', tensor(70)),\n             ('features.1.1.block.2.weight',\n              tensor([ 1.0948,  1.4026,  0.6770,  0.6041,  0.3715,  4.0585,  1.9060, -1.0762,\n                       3.9355,  1.0555,  1.0482,  1.9835,  1.0538,  1.0391,  1.1277,  0.9488,\n                       1.0178,  4.9991,  2.0594,  3.0934,  0.3159,  1.6175,  4.0484,  3.7749,\n                       1.8652,  0.5811,  1.5943,  2.3156,  2.0633,  1.5715,  2.0093,  1.9329,\n                       2.8241,  1.4697,  0.9499,  0.9050, -0.9500,  1.4207,  2.0346,  0.9436,\n                       1.2091,  0.7969,  1.3480,  2.7020,  0.1080,  1.5001,  2.2557,  2.1978,\n                       2.5008,  3.2742,  0.9206,  1.4277,  2.7174,  3.0624,  0.3629,  1.1834,\n                       0.9748,  1.5706,  4.9178,  2.0831,  1.6857,  0.5808,  2.2394,  1.5399,\n                      -1.0006,  2.7263,  2.7981,  2.1635,  1.3292,  0.9472,  0.9707,  2.1805,\n                       1.9705,  0.8128,  0.9826,  2.3757,  1.3547,  1.3406,  2.8897,  0.9804,\n                       2.0723,  0.4823,  1.1354,  2.2644,  4.3323,  0.0776,  2.7604,  1.7540,\n                       1.8995,  1.0159,  0.6436, -0.5742,  0.9054,  2.1825,  0.1362,  1.2947])),\n             ('features.1.1.block.2.bias',\n              tensor([ 5.5957e-01,  1.7116e-02,  1.2817e-02,  7.0019e-02,  2.0966e-01,\n                      -2.4989e-01,  6.6050e-01,  5.6378e-02,  9.4752e-03, -5.8576e-02,\n                       1.2833e-01, -4.4791e-03,  3.1889e-01, -2.4481e-02, -2.2603e-01,\n                       6.6255e-02,  1.2511e-01,  7.6407e-01,  5.9143e-01,  3.1425e-01,\n                       3.0199e-02,  4.2004e-01, -2.8123e-01,  1.1264e-01,  1.4303e+00,\n                      -2.8258e-02,  1.0185e+00, -3.7694e-01,  2.6974e-01,  1.9806e-01,\n                      -9.5803e-01, -2.4900e-01,  9.4988e-01, -3.9877e-01, -3.1090e-01,\n                      -6.5818e-03, -2.1482e-01,  5.2630e-01, -2.2060e-01, -5.3704e-04,\n                       6.7862e-02, -6.5758e-02, -9.4762e-01, -1.4344e+00,  1.9881e+00,\n                       6.0301e-01, -4.0778e-01,  7.3745e-01,  2.6818e-01,  2.7759e-01,\n                       6.6948e-01,  4.7125e-02,  5.1039e-02,  7.4017e-01,  1.0259e+00,\n                       1.4073e-01, -1.3496e-01,  1.1831e-01, -1.7159e+00,  1.2742e-01,\n                       8.8342e-02,  1.1056e-01, -1.4376e-01, -2.1112e-01, -1.4394e-01,\n                      -8.3145e-01,  1.2458e-01,  2.5922e-02,  7.5053e-01,  1.1058e-01,\n                      -1.1965e-01,  2.2083e-02,  4.6631e-02,  2.1985e-02, -6.6302e-02,\n                       1.7008e-01,  2.9971e-02, -5.3048e-01, -4.2058e-02, -1.0575e-01,\n                      -2.7507e-02,  1.2644e-01, -1.4773e-01,  1.0667e-01,  2.6913e-02,\n                       1.8799e+00, -6.2116e-01,  9.5965e-01, -2.5991e-02,  9.2224e-02,\n                       2.4906e-02, -9.8779e-02, -6.4287e-02, -9.4822e-01,  1.8354e+00,\n                       1.4978e-01])),\n             ('features.1.1.block.2.scale', tensor(0.2539)),\n             ('features.1.1.block.2.zero_point', tensor(67)),\n             ('features.1.1.block.3.scale', tensor(0.1135)),\n             ('features.1.1.block.3.zero_point', tensor(74)),\n             ('features.1.1.block.3._packed_params.dtype', torch.qint8),\n             ('features.1.1.block.3._packed_params._packed_params',\n              (tensor([[-0.0669, -0.0562, -0.0936,  ...,  0.0094, -0.0241, -0.0361],\n                       [-0.0476,  0.0122,  0.0190,  ...,  0.0721, -0.0626, -0.0531],\n                       [ 0.0048,  0.0339,  0.0145,  ...,  0.0174, -0.0542, -0.0068],\n                       ...,\n                       [-0.0010,  0.0078, -0.0166,  ...,  0.0430, -0.0488,  0.0029],\n                       [ 0.0328, -0.0207, -0.0138,  ..., -0.0190, -0.0155,  0.0000],\n                       [ 0.0634,  0.0734, -0.0200,  ...,  0.0934, -0.0634,  0.0133]],\n                      size=(384, 96), dtype=torch.qint8,\n                      quantization_scheme=torch.per_channel_affine,\n                      scale=tensor([0.0013, 0.0014, 0.0010, 0.0013, 0.0016, 0.0020, 0.0018, 0.0018, 0.0010,\n                       0.0016, 0.0019, 0.0011, 0.0010, 0.0018, 0.0024, 0.0013, 0.0016, 0.0026,\n                       0.0017, 0.0018, 0.0018, 0.0014, 0.0012, 0.0014, 0.0025, 0.0012, 0.0031,\n                       0.0015, 0.0014, 0.0019, 0.0014, 0.0014, 0.0012, 0.0012, 0.0011, 0.0012,\n                       0.0012, 0.0018, 0.0016, 0.0011, 0.0011, 0.0016, 0.0016, 0.0019, 0.0014,\n                       0.0010, 0.0011, 0.0014, 0.0009, 0.0015, 0.0011, 0.0018, 0.0008, 0.0015,\n                       0.0018, 0.0015, 0.0012, 0.0023, 0.0018, 0.0021, 0.0035, 0.0022, 0.0013,\n                       0.0012, 0.0009, 0.0030, 0.0013, 0.0015, 0.0013, 0.0013, 0.0010, 0.0018,\n                       0.0008, 0.0010, 0.0010, 0.0010, 0.0014, 0.0011, 0.0017, 0.0013, 0.0012,\n                       0.0012, 0.0020, 0.0021, 0.0013, 0.0020, 0.0026, 0.0012, 0.0021, 0.0014,\n                       0.0024, 0.0021, 0.0010, 0.0015, 0.0018, 0.0010, 0.0016, 0.0012, 0.0021,\n                       0.0013, 0.0017, 0.0012, 0.0021, 0.0023, 0.0019, 0.0014, 0.0011, 0.0021,\n                       0.0019, 0.0017, 0.0018, 0.0014, 0.0015, 0.0014, 0.0009, 0.0022, 0.0024,\n                       0.0024, 0.0021, 0.0023, 0.0019, 0.0014, 0.0015, 0.0022, 0.0010, 0.0018,\n                       0.0016, 0.0012, 0.0010, 0.0022, 0.0013, 0.0018, 0.0016, 0.0026, 0.0011,\n                       0.0014, 0.0015, 0.0014, 0.0020, 0.0012, 0.0023, 0.0011, 0.0016, 0.0017,\n                       0.0010, 0.0016, 0.0014, 0.0014, 0.0009, 0.0009, 0.0010, 0.0021, 0.0034,\n                       0.0018, 0.0016, 0.0019, 0.0021, 0.0024, 0.0019, 0.0010, 0.0021, 0.0016,\n                       0.0008, 0.0012, 0.0012, 0.0011, 0.0017, 0.0015, 0.0011, 0.0014, 0.0018,\n                       0.0010, 0.0012, 0.0021, 0.0019, 0.0019, 0.0022, 0.0027, 0.0018, 0.0013,\n                       0.0022, 0.0014, 0.0013, 0.0018, 0.0009, 0.0013, 0.0011, 0.0019, 0.0015,\n                       0.0010, 0.0009, 0.0018, 0.0011, 0.0022, 0.0023, 0.0012, 0.0020, 0.0021,\n                       0.0019, 0.0016, 0.0024, 0.0013, 0.0017, 0.0011, 0.0013, 0.0014, 0.0021,\n                       0.0018, 0.0014, 0.0013, 0.0015, 0.0013, 0.0013, 0.0015, 0.0013, 0.0019,\n                       0.0021, 0.0019, 0.0010, 0.0009, 0.0013, 0.0017, 0.0024, 0.0015, 0.0008,\n                       0.0015, 0.0009, 0.0017, 0.0017, 0.0014, 0.0012, 0.0015, 0.0011, 0.0018,\n                       0.0028, 0.0017, 0.0011, 0.0012, 0.0026, 0.0020, 0.0011, 0.0014, 0.0014,\n                       0.0010, 0.0017, 0.0022, 0.0020, 0.0012, 0.0013, 0.0011, 0.0016, 0.0016,\n                       0.0031, 0.0015, 0.0013, 0.0013, 0.0012, 0.0017, 0.0018, 0.0017, 0.0017,\n                       0.0013, 0.0021, 0.0010, 0.0012, 0.0015, 0.0015, 0.0021, 0.0011, 0.0015,\n                       0.0012, 0.0019, 0.0013, 0.0017, 0.0017, 0.0017, 0.0012, 0.0017, 0.0015,\n                       0.0014, 0.0020, 0.0023, 0.0021, 0.0012, 0.0014, 0.0013, 0.0010, 0.0031,\n                       0.0016, 0.0018, 0.0011, 0.0027, 0.0020, 0.0011, 0.0009, 0.0015, 0.0022,\n                       0.0009, 0.0024, 0.0018, 0.0011, 0.0016, 0.0016, 0.0026, 0.0013, 0.0013,\n                       0.0019, 0.0013, 0.0023, 0.0022, 0.0014, 0.0014, 0.0014, 0.0012, 0.0018,\n                       0.0010, 0.0015, 0.0019, 0.0016, 0.0017, 0.0010, 0.0012, 0.0017, 0.0010,\n                       0.0012, 0.0020, 0.0013, 0.0009, 0.0022, 0.0012, 0.0016, 0.0011, 0.0012,\n                       0.0015, 0.0013, 0.0022, 0.0017, 0.0019, 0.0023, 0.0011, 0.0016, 0.0009,\n                       0.0012, 0.0013, 0.0013, 0.0012, 0.0015, 0.0013, 0.0011, 0.0011, 0.0010,\n                       0.0022, 0.0016, 0.0022, 0.0017, 0.0026, 0.0013, 0.0016, 0.0025, 0.0013,\n                       0.0028, 0.0020, 0.0028, 0.0010, 0.0016, 0.0010, 0.0025, 0.0015, 0.0022,\n                       0.0016, 0.0031, 0.0014, 0.0017, 0.0029, 0.0016, 0.0026, 0.0014, 0.0013,\n                       0.0017, 0.0013, 0.0014, 0.0010, 0.0017, 0.0017], dtype=torch.float64),\n                      zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n                      axis=0),\n               Parameter containing:\n               tensor([-2.6751e-02, -6.2781e-02, -5.4946e-02, -6.6003e-02, -2.6490e-02,\n                       -7.4056e-02, -1.2515e-02, -4.0610e-02, -4.2027e-02, -4.7187e-02,\n                        9.8382e-03, -1.6980e-02, -8.9056e-02, -1.2158e-02, -4.5938e-02,\n                       -4.3323e-02, -3.6473e-02, -6.1718e-02, -2.8149e-02, -3.1738e-03,\n                       -1.0794e-02,  2.9648e-02, -3.1483e-03, -9.0391e-02,  5.0832e-03,\n                       -5.0107e-02,  7.8456e-03, -6.2390e-02, -2.8969e-02, -3.7123e-02,\n                       -2.1114e-02, -7.3767e-02, -3.2201e-02, -9.3144e-02, -1.5077e-02,\n                       -2.6845e-02, -5.7601e-02, -1.3298e-02, -4.4431e-02,  1.5385e-03,\n                       -2.5674e-02,  2.6200e-03, -3.3374e-02, -7.2950e-02, -2.7714e-02,\n                       -2.0520e-02, -1.6827e-02, -1.3027e-02, -5.7447e-02, -3.7022e-02,\n                        9.0564e-03, -6.2003e-02, -3.5361e-02, -2.8123e-02, -7.7704e-02,\n                       -1.7993e-02, -1.5947e-02,  8.6176e-03, -1.1429e-02, -3.6475e-02,\n                       -4.4075e-03, -5.1525e-02, -4.3902e-02,  1.9013e-02, -6.7653e-02,\n                        2.0432e-02, -3.1031e-02, -5.5920e-02, -2.4080e-02, -9.3207e-02,\n                       -1.2540e-02, -3.6125e-02, -3.1433e-02, -6.7409e-02, -9.3361e-02,\n                       -1.3349e-02, -1.4464e-02, -8.0934e-02, -1.4999e-02, -8.5247e-02,\n                       -2.9512e-02, -5.7969e-02, -3.7465e-02, -1.3448e-02, -3.6390e-02,\n                       -3.1673e-02, -4.3973e-03,  8.5142e-03, -5.1849e-02, -5.1552e-02,\n                       -4.8784e-02, -6.1201e-03, -1.1895e-01, -4.7547e-03, -4.6889e-02,\n                       -7.0056e-03, -1.5748e-02, -7.1989e-02, -9.3896e-02, -4.9128e-02,\n                        7.9275e-02, -2.4267e-02, -9.2158e-02, -7.4312e-02,  4.7474e-02,\n                        4.3091e-02, -8.4193e-02, -4.9656e-02, -1.4259e-02, -4.3492e-02,\n                       -6.1170e-02, -1.3708e-02, -1.4533e-03, -4.5957e-02, -1.6429e-02,\n                        4.7767e-02, -1.2782e-01,  2.0838e-02, -6.5911e-02, -1.5354e-02,\n                       -1.0975e-01, -1.7283e-02, -3.0118e-02, -5.6997e-02, -8.2370e-02,\n                        2.6017e-03, -4.7860e-03, -1.4282e-02, -8.5129e-02, -4.4367e-02,\n                       -4.7622e-02, -7.0908e-02, -4.7541e-02, -5.6125e-02,  5.5940e-02,\n                       -1.0975e-01, -2.1887e-02, -5.5091e-02, -1.9787e-01, -2.5881e-02,\n                       -7.1621e-02,  1.1476e-02, -1.0259e-02, -1.1014e-02,  3.7887e-02,\n                        1.4217e-02, -8.8093e-02, -2.9816e-02, -4.1428e-02, -2.7107e-02,\n                       -9.4839e-02, -5.5516e-02, -6.1294e-02,  3.6995e-02, -3.8077e-02,\n                       -2.0715e-02,  1.1098e-03, -4.0750e-02, -2.7217e-02, -1.1669e-01,\n                       -3.4705e-02, -5.4833e-02, -6.7030e-02, -6.0622e-02, -7.0261e-02,\n                       -6.9347e-02,  4.2783e-03, -2.9799e-02, -1.0033e-02, -1.7199e-02,\n                       -1.0786e-02, -8.7018e-02,  5.1455e-03, -2.0395e-02, -1.4832e-02,\n                       -4.1530e-02,  1.3835e-02, -2.5020e-02, -4.5748e-02, -1.0142e-01,\n                       -2.3403e-02, -9.8288e-03, -8.7768e-02, -7.0687e-02, -3.3565e-02,\n                       -1.8196e-02, -4.7830e-02, -9.5178e-03, -6.8874e-02, -3.2433e-02,\n                       -5.6468e-02, -4.7885e-02, -1.9207e-02, -1.9124e-02, -2.4779e-02,\n                       -1.4498e-02, -5.6125e-02, -8.4007e-02,  2.4586e-02, -5.4928e-02,\n                       -3.8514e-02, -5.2173e-02, -3.6999e-02, -5.0251e-02, -5.6422e-02,\n                       -6.7379e-02, -3.8074e-02, -1.3201e-02, -4.6835e-02, -3.6537e-02,\n                       -7.3607e-03, -6.2893e-02, -5.5847e-02, -3.8558e-02, -4.4790e-02,\n                       -2.0695e-02, -5.5000e-02,  4.2497e-03, -7.4407e-02, -1.2369e-02,\n                       -5.0801e-02, -3.7552e-02,  8.2962e-03, -4.8329e-02, -4.1393e-02,\n                        5.8052e-02, -5.1954e-02, -6.2452e-02, -3.8659e-02, -6.8883e-02,\n                       -2.5913e-02, -8.5032e-02, -9.6562e-02, -2.3042e-02, -2.1108e-02,\n                        2.5823e-02, -7.6074e-03, -4.8394e-02, -2.4748e-03, -1.1210e-02,\n                       -9.0034e-02, -1.0362e-01, -7.9381e-02, -1.6574e-02, -1.5864e-03,\n                       -4.2760e-02, -5.2877e-02, -5.8071e-03, -2.5041e-02, -8.6033e-02,\n                       -4.8483e-02, -3.0754e-02, -1.3772e-01, -3.9641e-02, -3.6238e-02,\n                       -4.7303e-02, -7.6572e-02, -1.1184e-02, -8.0173e-02, -1.1591e-01,\n                       -6.2049e-02, -5.9357e-02, -5.8772e-03, -2.2325e-02, -1.1530e-01,\n                       -1.0286e-01, -1.3541e-01, -2.6114e-02, -8.4245e-02, -1.4990e-01,\n                       -4.4457e-02, -3.5512e-02, -5.9784e-02, -1.4262e-03,  4.5693e-04,\n                       -1.0067e-02, -8.9782e-02, -5.7727e-02,  3.5671e-03, -1.3311e-01,\n                       -1.3956e-02, -3.1348e-02, -4.0547e-02,  9.4138e-03, -2.7920e-02,\n                       -4.1329e-02, -3.4771e-02,  5.1132e-02, -8.7755e-02, -3.6677e-02,\n                       -2.1806e-02, -5.4815e-02, -2.8984e-02, -7.7053e-02, -1.5914e-02,\n                       -6.0975e-02, -6.5774e-02, -6.3999e-02, -5.3253e-02, -4.1702e-02,\n                       -7.7523e-02, -2.2105e-02,  7.6798e-02, -7.0292e-02, -5.5159e-02,\n                       -5.4864e-03, -5.5521e-02, -6.4092e-02, -9.9386e-02, -4.4743e-02,\n                       -8.3439e-02, -2.6254e-02, -3.5027e-02, -6.1371e-02, -7.8107e-02,\n                       -1.0350e-01, -8.4276e-02, -2.3083e-02, -1.3595e-03, -3.6043e-02,\n                       -6.0589e-02, -3.9115e-02, -2.0385e-02,  6.3032e-02, -4.9972e-02,\n                       -3.9797e-02, -6.3009e-02, -8.1123e-02, -2.1072e-02, -4.9682e-02,\n                       -4.4457e-02, -5.4312e-02, -2.5561e-02, -4.4113e-02,  4.5458e-02,\n                       -8.0766e-02, -3.3995e-02,  1.2766e-02,  6.4964e-03, -9.9850e-02,\n                       -6.6481e-02,  1.6782e-02, -1.7933e-03, -5.9431e-02, -3.8884e-02,\n                       -3.4790e-02,  3.5109e-02, -3.6310e-02, -7.1639e-02, -6.7693e-02,\n                       -7.9832e-02, -2.0555e-02, -4.8944e-02, -8.5229e-02, -5.6571e-02,\n                       -4.9006e-02, -3.7561e-02, -4.5228e-02, -9.6640e-03, -5.5746e-02,\n                       -3.2792e-02, -4.3082e-02, -2.4781e-02, -7.0207e-02,  5.2691e-03,\n                       -5.7669e-02, -8.3134e-05,  2.2469e-02, -2.1688e-02, -4.4749e-02,\n                       -3.2638e-03, -5.2262e-02, -2.1762e-02, -4.3473e-02, -6.3694e-02,\n                       -8.9400e-03, -1.0354e-01, -5.1038e-02, -5.2354e-02, -8.8042e-02,\n                       -6.2228e-02, -5.1126e-02, -1.7643e-02, -6.3682e-02],\n                      requires_grad=True))),\n             ('features.1.1.block.5.scale', tensor(0.0505)),\n             ('features.1.1.block.5.zero_point', tensor(93)),\n             ('features.1.1.block.5._packed_params.dtype', torch.qint8),\n             ('features.1.1.block.5._packed_params._packed_params',\n              (tensor([[ 0.0541,  0.0158, -0.0135,  ..., -0.0090,  0.0135,  0.0429],\n                       [-0.0422, -0.0292, -0.0260,  ..., -0.0390,  0.0000,  0.0422],\n                       [-0.2019, -0.0049,  0.0170,  ..., -0.0146,  0.0681,  0.0122],\n                       ...,\n                       [ 0.0363,  0.0490, -0.0309,  ...,  0.0091,  0.0018,  0.1107],\n                       [-0.0219, -0.0146, -0.0024,  ...,  0.0244, -0.0122,  0.0219],\n                       [ 0.0245, -0.0178, -0.0044,  ..., -0.0245,  0.0623,  0.0334]],\n                      size=(96, 384), dtype=torch.qint8,\n                      quantization_scheme=torch.per_channel_affine,\n                      scale=tensor([0.0023, 0.0032, 0.0024, 0.0023, 0.0023, 0.0032, 0.0005, 0.0025, 0.0018,\n                       0.0026, 0.0019, 0.0026, 0.0018, 0.0024, 0.0020, 0.0027, 0.0027, 0.0029,\n                       0.0021, 0.0029, 0.0024, 0.0015, 0.0033, 0.0020, 0.0025, 0.0030, 0.0035,\n                       0.0027, 0.0018, 0.0018, 0.0029, 0.0022, 0.0029, 0.0022, 0.0026, 0.0018,\n                       0.0021, 0.0020, 0.0017, 0.0016, 0.0021, 0.0019, 0.0017, 0.0006, 0.0017,\n                       0.0018, 0.0022, 0.0018, 0.0016, 0.0032, 0.0023, 0.0026, 0.0028, 0.0014,\n                       0.0018, 0.0018, 0.0020, 0.0019, 0.0024, 0.0025, 0.0019, 0.0023, 0.0036,\n                       0.0022, 0.0023, 0.0019, 0.0040, 0.0028, 0.0018, 0.0028, 0.0017, 0.0025,\n                       0.0031, 0.0019, 0.0029, 0.0014, 0.0020, 0.0016, 0.0030, 0.0022, 0.0022,\n                       0.0020, 0.0017, 0.0023, 0.0021, 0.0024, 0.0021, 0.0020, 0.0020, 0.0021,\n                       0.0031, 0.0021, 0.0021, 0.0018, 0.0024, 0.0022], dtype=torch.float64),\n                      zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n                      axis=0),\n               Parameter containing:\n               tensor([-1.1174e-02, -1.3840e-02, -3.0757e-03, -4.0130e-03,  3.6124e-02,\n                       -2.4389e-02, -1.9778e-03, -7.4861e-03,  8.7642e-03, -1.9878e-02,\n                       -5.3461e-02,  3.5275e-03,  8.0939e-02, -9.4165e-04,  3.3659e-02,\n                        5.9533e-03, -5.2228e-06, -1.4391e-02, -2.4644e-03, -1.0601e-02,\n                        4.3301e-02,  3.3342e-03,  4.3044e-03, -1.5754e-02, -2.1834e-02,\n                        7.0428e-03, -3.5197e-01,  1.3612e-02,  2.6630e-02,  1.1382e-03,\n                       -1.8778e-02,  8.0120e-03, -9.4587e-03,  9.8970e-03,  1.4608e-02,\n                       -1.3127e-03,  2.7521e-02,  1.8393e-02, -4.1450e-02, -1.5333e-02,\n                        9.9282e-03,  9.7828e-03, -7.1323e-03,  2.3930e-03,  4.4680e-03,\n                        2.0858e-03, -1.9367e-03, -1.7993e-02,  2.0588e-02,  3.2019e-02,\n                       -1.2799e-02,  4.8217e-02, -9.0309e-03, -2.5529e-03,  4.0348e-02,\n                        2.1013e-03, -2.8128e-02,  4.1608e-03, -1.7733e-02, -1.5925e-02,\n                       -1.2623e-02, -8.4769e-02,  3.1549e-03,  9.7589e-04, -4.8895e-03,\n                        1.2182e-02, -1.5266e-02,  4.3416e-03, -4.5019e-03,  2.4597e-03,\n                       -9.0661e-03, -1.9445e-03, -1.5838e-02,  3.6083e-02, -1.1952e-02,\n                       -3.2437e-03,  1.0345e-02, -8.8199e-03,  8.7726e-04,  1.1915e-02,\n                       -7.0476e-03,  8.0207e-03, -6.0724e-03,  1.6474e-03, -5.6592e-03,\n                        2.1012e-02, -8.7716e-03,  1.8572e-02, -4.2997e-02, -7.1633e-04,\n                       -4.4242e-03, -8.3118e-03, -1.3693e-02,  7.3279e-03, -5.2738e-04,\n                       -2.3449e-03], requires_grad=True))),\n             ('features.1.2.layer_scale',\n              tensor([[[ 1.2340e-01]],\n              \n                      [[ 3.2527e-01]],\n              \n                      [[ 3.5419e-01]],\n              \n                      [[-4.5002e-01]],\n              \n                      [[ 3.2488e-01]],\n              \n                      [[-7.9413e-02]],\n              \n                      [[ 1.9839e-05]],\n              \n                      [[ 4.5074e-01]],\n              \n                      [[ 8.1076e-03]],\n              \n                      [[ 3.4547e-01]],\n              \n                      [[-3.5743e-01]],\n              \n                      [[ 1.2824e-01]],\n              \n                      [[ 3.9367e-01]],\n              \n                      [[-1.1380e-01]],\n              \n                      [[-2.7566e-01]],\n              \n                      [[ 3.4070e-01]],\n              \n                      [[ 2.1162e-01]],\n              \n                      [[-1.1385e-01]],\n              \n                      [[-9.5240e-04]],\n              \n                      [[ 1.2934e-01]],\n              \n                      [[-4.6155e-01]],\n              \n                      [[ 3.1729e-01]],\n              \n                      [[-2.4544e-01]],\n              \n                      [[ 7.5482e-03]],\n              \n                      [[ 3.0029e-05]],\n              \n                      [[-3.9965e-01]],\n              \n                      [[ 3.0064e-01]],\n              \n                      [[-4.2591e-01]],\n              \n                      [[-1.0501e-01]],\n              \n                      [[-3.1946e-01]],\n              \n                      [[ 5.3003e-02]],\n              \n                      [[ 1.3095e-01]],\n              \n                      [[-2.5787e-01]],\n              \n                      [[-1.2744e-01]],\n              \n                      [[-2.6076e-01]],\n              \n                      [[ 4.2305e-01]],\n              \n                      [[-3.7976e-01]],\n              \n                      [[ 3.3691e-01]],\n              \n                      [[ 2.6557e-01]],\n              \n                      [[-1.7653e-01]],\n              \n                      [[ 1.1380e-01]],\n              \n                      [[ 2.3203e-01]],\n              \n                      [[-1.2375e-01]],\n              \n                      [[-7.9651e-02]],\n              \n                      [[-4.0797e-01]],\n              \n                      [[ 9.8273e-02]],\n              \n                      [[-5.5924e-02]],\n              \n                      [[-1.3603e-01]],\n              \n                      [[ 3.3174e-01]],\n              \n                      [[ 2.6988e-01]],\n              \n                      [[ 4.0066e-01]],\n              \n                      [[ 3.4180e-01]],\n              \n                      [[ 2.9728e-01]],\n              \n                      [[-9.9007e-02]],\n              \n                      [[ 4.0094e-01]],\n              \n                      [[ 3.5247e-01]],\n              \n                      [[ 2.3224e-01]],\n              \n                      [[-2.1325e-01]],\n              \n                      [[ 2.4550e-01]],\n              \n                      [[ 1.1923e-01]],\n              \n                      [[ 1.1233e-01]],\n              \n                      [[ 2.6964e-01]],\n              \n                      [[ 1.8428e-01]],\n              \n                      [[ 3.6624e-01]],\n              \n                      [[-3.2882e-01]],\n              \n                      [[ 1.7425e-01]],\n              \n                      [[ 3.2948e-01]],\n              \n                      [[-8.0651e-02]],\n              \n                      [[-2.2235e-01]],\n              \n                      [[-2.0919e-01]],\n              \n                      [[-3.5641e-01]],\n              \n                      [[ 8.5984e-06]],\n              \n                      [[ 6.2238e-02]],\n              \n                      [[-3.7037e-01]],\n              \n                      [[-2.5371e-02]],\n              \n                      [[ 7.0476e-02]],\n              \n                      [[-3.3572e-01]],\n              \n                      [[ 1.5096e-01]],\n              \n                      [[-1.2482e-01]],\n              \n                      [[-3.7047e-01]],\n              \n                      [[-2.7300e-04]],\n              \n                      [[-4.0002e-01]],\n              \n                      [[-3.1889e-01]],\n              \n                      [[-2.7107e-03]],\n              \n                      [[-1.0558e-01]],\n              \n                      [[ 2.8133e-01]],\n              \n                      [[-1.7795e-01]],\n              \n                      [[-1.3665e-01]],\n              \n                      [[ 1.0819e-01]],\n              \n                      [[ 1.8318e-01]],\n              \n                      [[ 4.3668e-02]],\n              \n                      [[-3.5836e-01]],\n              \n                      [[ 2.7071e-01]],\n              \n                      [[-1.0293e-01]],\n              \n                      [[ 3.8541e-01]],\n              \n                      [[ 2.5866e-01]]])),\n             ('features.1.2.block.0.weight',\n              tensor([[[[ 0.0087,  0.0058,  0.0029,  ...,  0.0029,  0.0029,  0.0087],\n                        [ 0.0029,  0.0058,  0.0029,  ...,  0.0087,  0.0058,  0.0058],\n                        [ 0.0116,  0.0145,  0.0000,  ...,  0.0029,  0.0058,  0.0029],\n                        ...,\n                        [ 0.0029,  0.0087, -0.0058,  ...,  0.0087,  0.0116,  0.0087],\n                        [ 0.0058,  0.0058,  0.0145,  ...,  0.0087,  0.0087,  0.0058],\n                        [ 0.0087,  0.0058,  0.0000,  ...,  0.0058,  0.0058,  0.0058]]],\n              \n              \n                      [[[ 0.0000,  0.0000, -0.0024,  ..., -0.0024, -0.0024,  0.0000],\n                        [ 0.0000, -0.0024, -0.0048,  ...,  0.0000,  0.0000,  0.0000],\n                        [-0.0024, -0.0048, -0.0215,  ..., -0.0167, -0.0048, -0.0024],\n                        ...,\n                        [-0.0024, -0.0072, -0.0191,  ..., -0.0143, -0.0048, -0.0024],\n                        [-0.0024,  0.0000, -0.0024,  ..., -0.0024, -0.0024,  0.0000],\n                        [ 0.0000,  0.0000, -0.0024,  ..., -0.0024,  0.0024, -0.0024]]],\n              \n              \n                      [[[-0.0019, -0.0019, -0.0019,  ..., -0.0039, -0.0019, -0.0019],\n                        [ 0.0000,  0.0000,  0.0000,  ..., -0.0019,  0.0000,  0.0019],\n                        [-0.0019, -0.0019, -0.0058,  ..., -0.0058, -0.0019, -0.0019],\n                        ...,\n                        [-0.0058, -0.0077, -0.0174,  ..., -0.0077, -0.0096, -0.0058],\n                        [-0.0019, -0.0058, -0.0096,  ..., -0.0077, -0.0039, -0.0058],\n                        [-0.0019, -0.0019, -0.0039,  ..., -0.0039, -0.0019, -0.0019]]],\n              \n              \n                      ...,\n              \n              \n                      [[[ 0.0089, -0.0030, -0.0059,  ...,  0.0000, -0.0059,  0.0089],\n                        [ 0.0059, -0.0030, -0.0177,  ..., -0.0089, -0.0059,  0.0030],\n                        [-0.0059, -0.0148, -0.0384,  ..., -0.0384, -0.0236, -0.0089],\n                        ...,\n                        [ 0.0059, -0.0148, -0.0414,  ..., -0.0295, -0.0148,  0.0059],\n                        [ 0.0030, -0.0059, -0.0207,  ..., -0.0236, -0.0059,  0.0000],\n                        [ 0.0118,  0.0000, -0.0030,  ..., -0.0059,  0.0030,  0.0089]]],\n              \n              \n                      [[[-0.0025, -0.0025, -0.0050,  ..., -0.0050, -0.0025, -0.0025],\n                        [-0.0050, -0.0025, -0.0100,  ..., -0.0025, -0.0050, -0.0025],\n                        [-0.0125, -0.0150, -0.0399,  ..., -0.0275, -0.0175, -0.0150],\n                        ...,\n                        [ 0.0000, -0.0025, -0.0050,  ..., -0.0150, -0.0025,  0.0000],\n                        [ 0.0000,  0.0000, -0.0025,  ..., -0.0050,  0.0000,  0.0000],\n                        [ 0.0000, -0.0025, -0.0025,  ..., -0.0025, -0.0025, -0.0025]]],\n              \n              \n                      [[[ 0.0000,  0.0000,  0.0021,  ...,  0.0021,  0.0021,  0.0021],\n                        [ 0.0021,  0.0021,  0.0021,  ...,  0.0041,  0.0000,  0.0021],\n                        [ 0.0000,  0.0021,  0.0082,  ...,  0.0082,  0.0000,  0.0000],\n                        ...,\n                        [-0.0021,  0.0000,  0.0041,  ...,  0.0165,  0.0000,  0.0021],\n                        [ 0.0000,  0.0021,  0.0021,  ...,  0.0041,  0.0000,  0.0000],\n                        [ 0.0000,  0.0021,  0.0021,  ...,  0.0000,  0.0000,  0.0021]]]],\n                     size=(96, 1, 7, 7), dtype=torch.qint8,\n                     quantization_scheme=torch.per_channel_affine,\n                     scale=tensor([2.8961e-03, 2.3899e-03, 1.9284e-03, 2.6974e-03, 1.6156e-03, 1.3907e-03,\n                      1.1663e-03, 2.8028e-03, 2.1592e-03, 2.0134e-03, 1.4118e-03, 5.1726e-04,\n                      2.4299e-03, 2.5884e-03, 2.3713e-03, 2.8052e-03, 3.0522e-03, 1.7617e-03,\n                      8.3779e-04, 1.7426e-03, 1.6829e-03, 3.2779e-03, 2.7029e-03, 2.3279e-03,\n                      1.6321e-03, 1.9281e-03, 7.5153e-05, 5.5571e-04, 2.6880e-03, 2.7327e-03,\n                      9.3728e-04, 6.7090e-04, 1.3883e-03, 1.3428e-03, 1.9204e-03, 2.6878e-03,\n                      1.9868e-03, 2.7931e-03, 2.1087e-03, 2.4164e-03, 2.2408e-03, 1.8166e-03,\n                      2.8305e-03, 9.3303e-04, 2.9630e-03, 2.9669e-03, 8.4682e-04, 2.8468e-03,\n                      2.5680e-03, 2.7068e-03, 1.8153e-03, 3.1928e-03, 2.4105e-03, 1.0162e-04,\n                      1.6572e-03, 2.3813e-03, 2.1378e-03, 2.1084e-03, 2.0850e-03, 2.2025e-03,\n                      3.1909e-03, 2.3325e-03, 2.6690e-03, 2.7071e-03, 2.3496e-03, 4.2637e-03,\n                      3.5348e-04, 1.7056e-03, 3.4819e-03, 2.8840e-03, 2.7121e-03, 1.7045e-04,\n                      2.0392e-03, 2.5659e-03, 2.2822e-03, 1.2519e-03, 2.4556e-03, 2.7554e-03,\n                      8.5088e-04, 2.6491e-03, 6.5454e-04, 2.2603e-03, 2.9686e-03, 1.9548e-03,\n                      1.4099e-03, 2.6165e-03, 1.9754e-03, 2.7552e-03, 2.2830e-03, 2.6891e-03,\n                      1.8408e-03, 2.0364e-03, 1.9704e-03, 2.9548e-03, 2.4958e-03, 2.0602e-03],\n                     dtype=torch.float64),\n                     zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n                     axis=0)),\n             ('features.1.2.block.0.bias',\n              Parameter containing:\n              tensor([ 0.0006,  0.0037,  0.0009,  0.0006,  0.0007, -0.0063,  0.0022,  0.0023,\n                       0.0005,  0.0002,  0.0009,  0.0071,  0.0015,  0.0010,  0.0019,  0.0009,\n                       0.0012,  0.0012,  0.0006,  0.0003,  0.0009,  0.0002,  0.0007,  0.0005,\n                      -0.0063,  0.0012,  0.0024, -0.0033, -0.0004,  0.0021,  0.0110,  0.0013,\n                       0.0001,  0.0177,  0.0004,  0.0007, -0.0006,  0.0010,  0.0024,  0.0021,\n                      -0.0008,  0.0028,  0.0002,  0.0032,  0.0006, -0.0005,  0.0066, -0.0002,\n                       0.0018,  0.0158,  0.0016,  0.0004,  0.0004, -0.0413,  0.0041,  0.0014,\n                      -0.0002,  0.0008, -0.0049,  0.0006,  0.0017, -0.0009,  0.0010, -0.0007,\n                       0.0009,  0.0015, -0.0136,  0.0009,  0.0016,  0.0005,  0.0004,  0.0004,\n                       0.0008,  0.0009,  0.0007,  0.0009,  0.0025,  0.0026,  0.0019,  0.0025,\n                       0.0010,  0.0007,  0.0016,  0.0005, -0.0059,  0.0003, -0.0013,  0.0007,\n                       0.0017,  0.0010,  0.0004,  0.0003,  0.0012,  0.0035,  0.0004,  0.0012],\n                     requires_grad=True)),\n             ('features.1.2.block.0.scale', tensor(0.0018)),\n             ('features.1.2.block.0.zero_point', tensor(66)),\n             ('features.1.2.block.2.weight',\n              tensor([ 2.9273,  2.4334,  1.6077,  2.1000,  1.2474,  2.1109,  1.0276,  2.6203,\n                      -3.3424,  1.9761,  1.7732,  0.5010,  2.1766,  2.7354,  2.2481,  1.8672,\n                       2.5320,  1.4162,  1.3801,  1.4442,  1.8614,  2.5057,  2.2632, -3.0617,\n                       2.3909,  1.9364, -0.0254, -1.2464,  2.6992,  2.5124,  1.8611,  0.5405,\n                       2.5409,  1.7101,  2.1799,  1.8458,  1.8683,  3.0491,  2.1619,  2.1572,\n                      -1.4536,  1.8462,  2.5652,  1.1256,  2.6831,  1.9173,  1.2670,  2.5957,\n                       2.1360,  1.7664,  1.9335,  2.9452,  1.8843,  3.0866,  1.5729,  1.9326,\n                       1.4380,  2.9551,  2.1034,  4.9414,  2.8170,  2.2053,  2.1812,  2.2929,\n                       1.6013,  4.5687,  1.0028,  1.1892,  3.0169,  2.4282,  1.8447, -0.0238,\n                       2.9007,  1.8133,  3.0365,  1.9190,  2.2775,  2.1421,  0.6069,  2.1985,\n                      -0.3833,  1.7844,  2.2986,  2.4291,  1.3495,  2.8389,  3.2536,  3.1853,\n                       2.0414,  1.4583,  2.3691,  2.2603,  1.8752,  1.8150,  1.8788,  2.1233])),\n             ('features.1.2.block.2.bias',\n              tensor([-3.1859e-02, -4.1200e-01, -1.8493e-02,  2.0036e-02, -1.4800e-03,\n                       1.0091e+00, -4.7252e-02, -1.3484e-01, -1.6095e-02,  2.5128e-03,\n                       2.5673e-02, -3.5843e-01, -1.0417e-02,  3.8722e-02, -1.1098e-01,\n                       2.2300e-03,  7.3168e-02, -2.3808e-02, -6.6952e-03,  1.3196e-02,\n                       9.9825e-03,  8.1451e-02,  9.7525e-02, -6.3738e-03,  9.7200e-01,\n                      -4.4542e-02, -2.1981e+00, -4.9146e-01,  5.6712e-02, -1.4557e-01,\n                      -8.9549e-02, -5.8452e-02,  1.9746e-01, -9.9194e-01,  6.8046e-02,\n                       6.0760e-02,  1.8145e-02, -3.1053e-02, -2.2769e-01, -4.7783e-02,\n                      -1.2069e-01, -2.6508e-01,  4.5657e-02, -2.1496e-01,  9.5581e-03,\n                       9.9469e-02, -3.0822e-01,  2.3706e-01, -7.5371e-02, -9.2900e-01,\n                       3.2773e-02,  7.2908e-02,  5.7457e-05, -4.1977e-01, -1.4753e-01,\n                      -3.2999e-02,  5.8817e-02,  1.6930e-01, -5.0782e-01, -3.0942e-03,\n                      -1.3167e-01,  1.8437e-01, -4.6731e-03,  1.1975e-01,  1.1709e-02,\n                      -1.7977e-01,  8.5407e-01, -8.3996e-03, -9.8270e-02,  1.3024e-01,\n                       5.5384e-02, -2.5368e+00,  1.4718e-02,  1.3420e-03,  9.1276e-03,\n                       5.5910e-01, -1.0973e-01, -1.0703e-01, -3.6583e-03, -1.3064e-01,\n                       1.7492e-03,  3.6229e-02, -5.1549e-02,  3.5865e-02,  3.8839e-01,\n                       1.0194e-01,  1.1821e-01,  4.5422e-01, -7.7330e-02, -9.7423e-03,\n                       5.2296e-02,  5.9309e-02, -8.0603e-03, -2.0521e-01,  4.3864e-02,\n                      -3.1984e-02])),\n             ('features.1.2.block.2.scale', tensor(0.2628)),\n             ('features.1.2.block.2.zero_point', tensor(76)),\n             ('features.1.2.block.3.scale', tensor(0.0809)),\n             ('features.1.2.block.3.zero_point', tensor(78)),\n             ('features.1.2.block.3._packed_params.dtype', torch.qint8),\n             ('features.1.2.block.3._packed_params._packed_params',\n              (tensor([[-0.0020,  0.1162,  0.0137,  ...,  0.0205, -0.0156, -0.1250],\n                       [-0.0172,  0.0396, -0.0103,  ...,  0.2188, -0.0069, -0.0207],\n                       [ 0.0148, -0.1058,  0.0019,  ..., -0.0594, -0.0019,  0.0687],\n                       ...,\n                       [-0.0099, -0.0565, -0.0692,  ...,  0.0339,  0.0311, -0.0042],\n                       [-0.0651, -0.0610,  0.0097,  ..., -0.0429,  0.0014, -0.0097],\n                       [-0.0083,  0.0125, -0.0195,  ...,  0.0250, -0.0974, -0.0042]],\n                      size=(384, 96), dtype=torch.qint8,\n                      quantization_scheme=torch.per_channel_affine,\n                      scale=tensor([0.0010, 0.0017, 0.0019, 0.0011, 0.0011, 0.0016, 0.0015, 0.0016, 0.0020,\n                       0.0010, 0.0028, 0.0019, 0.0014, 0.0011, 0.0012, 0.0009, 0.0019, 0.0012,\n                       0.0010, 0.0012, 0.0013, 0.0009, 0.0011, 0.0017, 0.0013, 0.0012, 0.0014,\n                       0.0011, 0.0015, 0.0020, 0.0013, 0.0016, 0.0017, 0.0016, 0.0017, 0.0030,\n                       0.0013, 0.0010, 0.0023, 0.0012, 0.0014, 0.0013, 0.0013, 0.0012, 0.0015,\n                       0.0010, 0.0017, 0.0011, 0.0009, 0.0014, 0.0022, 0.0014, 0.0013, 0.0020,\n                       0.0009, 0.0015, 0.0016, 0.0015, 0.0010, 0.0011, 0.0011, 0.0011, 0.0015,\n                       0.0010, 0.0011, 0.0016, 0.0012, 0.0014, 0.0021, 0.0009, 0.0009, 0.0010,\n                       0.0009, 0.0014, 0.0009, 0.0016, 0.0015, 0.0011, 0.0010, 0.0018, 0.0011,\n                       0.0020, 0.0015, 0.0012, 0.0012, 0.0019, 0.0020, 0.0024, 0.0018, 0.0012,\n                       0.0016, 0.0009, 0.0014, 0.0013, 0.0010, 0.0016, 0.0010, 0.0011, 0.0016,\n                       0.0016, 0.0018, 0.0015, 0.0013, 0.0016, 0.0016, 0.0014, 0.0013, 0.0017,\n                       0.0015, 0.0018, 0.0014, 0.0010, 0.0011, 0.0010, 0.0022, 0.0016, 0.0017,\n                       0.0015, 0.0011, 0.0009, 0.0018, 0.0013, 0.0011, 0.0012, 0.0012, 0.0014,\n                       0.0011, 0.0011, 0.0014, 0.0017, 0.0017, 0.0014, 0.0010, 0.0016, 0.0022,\n                       0.0016, 0.0008, 0.0012, 0.0009, 0.0020, 0.0010, 0.0018, 0.0018, 0.0009,\n                       0.0014, 0.0013, 0.0009, 0.0010, 0.0013, 0.0014, 0.0013, 0.0014, 0.0019,\n                       0.0011, 0.0011, 0.0022, 0.0012, 0.0011, 0.0011, 0.0013, 0.0009, 0.0013,\n                       0.0014, 0.0013, 0.0016, 0.0013, 0.0020, 0.0023, 0.0013, 0.0015, 0.0017,\n                       0.0014, 0.0011, 0.0019, 0.0012, 0.0010, 0.0013, 0.0014, 0.0020, 0.0015,\n                       0.0017, 0.0019, 0.0015, 0.0022, 0.0019, 0.0011, 0.0015, 0.0009, 0.0017,\n                       0.0012, 0.0015, 0.0011, 0.0012, 0.0013, 0.0011, 0.0023, 0.0014, 0.0018,\n                       0.0008, 0.0019, 0.0016, 0.0015, 0.0018, 0.0016, 0.0013, 0.0013, 0.0016,\n                       0.0013, 0.0010, 0.0014, 0.0019, 0.0016, 0.0010, 0.0012, 0.0014, 0.0008,\n                       0.0012, 0.0014, 0.0011, 0.0011, 0.0009, 0.0014, 0.0012, 0.0013, 0.0016,\n                       0.0019, 0.0013, 0.0012, 0.0010, 0.0013, 0.0012, 0.0011, 0.0012, 0.0014,\n                       0.0012, 0.0011, 0.0014, 0.0010, 0.0012, 0.0009, 0.0022, 0.0010, 0.0015,\n                       0.0009, 0.0010, 0.0014, 0.0011, 0.0018, 0.0017, 0.0021, 0.0011, 0.0016,\n                       0.0029, 0.0018, 0.0010, 0.0012, 0.0011, 0.0017, 0.0023, 0.0009, 0.0015,\n                       0.0015, 0.0014, 0.0015, 0.0013, 0.0014, 0.0012, 0.0016, 0.0011, 0.0014,\n                       0.0015, 0.0013, 0.0009, 0.0013, 0.0015, 0.0017, 0.0014, 0.0025, 0.0020,\n                       0.0010, 0.0010, 0.0019, 0.0020, 0.0011, 0.0010, 0.0011, 0.0016, 0.0024,\n                       0.0017, 0.0016, 0.0015, 0.0011, 0.0014, 0.0013, 0.0011, 0.0015, 0.0012,\n                       0.0019, 0.0013, 0.0016, 0.0009, 0.0020, 0.0012, 0.0010, 0.0020, 0.0022,\n                       0.0016, 0.0013, 0.0010, 0.0019, 0.0014, 0.0020, 0.0012, 0.0012, 0.0018,\n                       0.0025, 0.0012, 0.0012, 0.0014, 0.0011, 0.0015, 0.0015, 0.0014, 0.0011,\n                       0.0014, 0.0011, 0.0013, 0.0021, 0.0010, 0.0011, 0.0016, 0.0019, 0.0012,\n                       0.0019, 0.0011, 0.0014, 0.0014, 0.0013, 0.0016, 0.0012, 0.0036, 0.0015,\n                       0.0013, 0.0016, 0.0013, 0.0017, 0.0011, 0.0014, 0.0013, 0.0014, 0.0012,\n                       0.0016, 0.0014, 0.0012, 0.0013, 0.0011, 0.0011, 0.0014, 0.0015, 0.0012,\n                       0.0013, 0.0010, 0.0016, 0.0021, 0.0013, 0.0013, 0.0015, 0.0015, 0.0009,\n                       0.0012, 0.0015, 0.0013, 0.0013, 0.0021, 0.0010, 0.0011, 0.0011, 0.0012,\n                       0.0017, 0.0009, 0.0020, 0.0014, 0.0014, 0.0014], dtype=torch.float64),\n                      zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n                      axis=0),\n               Parameter containing:\n               tensor([-6.9776e-02, -2.5721e-02, -1.5352e-02, -5.8988e-02, -7.5220e-02,\n                       -8.6582e-03, -1.3095e-01, -3.9267e-02, -4.6750e-02, -4.1486e-02,\n                       -3.7400e-02, -3.9452e-02, -1.3387e-01, -5.6266e-02, -5.6231e-02,\n                       -3.5971e-02, -4.1249e-02, -4.1207e-02, -8.2600e-02, -3.5325e-02,\n                       -5.3758e-02, -4.7023e-02, -5.0671e-02, -5.7897e-02, -2.2861e-02,\n                       -5.9625e-02, -5.0187e-02, -4.7330e-02, -3.5454e-02,  7.5428e-02,\n                       -4.2265e-02, -2.8324e-02, -8.8409e-03, -3.1793e-02, -2.3563e-02,\n                       -1.6879e-02,  2.4687e-02, -9.7606e-02, -1.5279e-02, -2.8031e-02,\n                       -4.0208e-02, -9.9689e-02, -9.7131e-02, -4.0026e-02, -1.6454e-02,\n                       -4.1182e-02, -1.6758e-02, -1.0633e-01, -5.9485e-02, -1.6531e-02,\n                       -1.8745e-02, -6.7317e-02, -7.1771e-02, -6.1661e-02, -5.8577e-02,\n                       -4.8263e-02, -1.4522e-02, -3.3059e-02, -5.1754e-02, -2.7625e-02,\n                       -5.4429e-02, -9.6962e-02, -4.8958e-02, -2.9246e-02, -5.0616e-02,\n                       -7.8486e-03, -7.0304e-02, -1.8698e-02, -3.3441e-02, -6.5156e-02,\n                       -4.9013e-02, -9.0245e-02, -4.2464e-02, -3.1886e-02, -5.5040e-02,\n                        7.5144e-04, -2.0909e-02, -4.2927e-02, -5.2077e-02, -7.9515e-03,\n                       -8.1818e-02, -2.3916e-02, -1.9077e-02, -4.7561e-02, -7.8375e-02,\n                       -4.7300e-02, -6.7166e-03, -1.1185e-01, -4.6096e-02, -8.9762e-02,\n                       -4.2558e-02, -8.5156e-02, -8.4065e-03, -7.0334e-02, -7.9959e-02,\n                       -6.6383e-02, -5.0895e-02, -3.9451e-02, -2.5529e-02, -4.5973e-02,\n                       -6.1088e-02, -4.7815e-02, -6.1226e-02, -7.9303e-02, -6.7016e-02,\n                       -2.2444e-02, -3.1440e-02, -1.3429e-02, -5.1636e-02, -2.0018e-03,\n                       -2.3664e-02, -8.4248e-02, -6.4988e-02, -1.1350e-02, -1.5153e-02,\n                       -4.9751e-02, -9.6735e-02, -3.9781e-02, -5.6200e-02, -6.6052e-02,\n                        2.7071e-02, -6.5426e-02, -5.0292e-02, -5.9007e-02, -8.8275e-02,\n                       -5.9194e-02, -3.7830e-02, -2.3196e-02, -4.3591e-02, -4.2976e-02,\n                       -7.1033e-03, -4.5027e-02, -1.1232e-01, -3.2245e-02, -4.2506e-02,\n                       -3.6652e-02, -4.2857e-02, -3.3474e-02,  2.9430e-05, -1.8401e-02,\n                       -5.7830e-02, -3.9225e-02, -3.7183e-02, -5.7618e-02, -1.6257e-02,\n                       -4.3648e-02, -6.5200e-02, -8.0051e-02, -5.4962e-02, -5.3410e-02,\n                       -3.4264e-02, -4.9637e-02, -7.5288e-03, -5.3860e-02, -5.0987e-02,\n                       -4.5962e-02, -3.5569e-02, -5.3751e-02, -5.1565e-02, -8.8787e-02,\n                       -8.3587e-02, -4.5636e-02, -3.2946e-02, -5.8751e-02, -2.7509e-02,\n                       -6.7073e-02, -4.2323e-02, -4.6845e-02, -3.7534e-02, -1.0816e-01,\n                       -2.9246e-02, -4.8397e-02, -7.1625e-02, -3.4258e-02, -4.1701e-02,\n                       -9.8314e-02, -7.7627e-02, -8.1916e-02, -4.0587e-02, -4.3696e-02,\n                       -3.2927e-02, -1.6215e-02, -3.4335e-02, -1.4419e-02, -3.3114e-02,\n                       -4.1419e-02, -2.8965e-02, -7.4448e-02, -5.2940e-02, -6.1396e-02,\n                       -9.1907e-02, -3.7358e-02, -2.2871e-02, -4.2157e-02, -7.2932e-02,\n                       -4.5049e-02, -4.3622e-02, -2.2090e-02, -4.7669e-02, -1.5185e-02,\n                       -6.2507e-02, -6.4003e-02, -4.0967e-02, -4.3087e-02, -4.4581e-02,\n                       -6.0183e-02, -2.0083e-02, -4.3518e-02, -6.0286e-02, -5.7835e-02,\n                       -3.2247e-02, -1.7303e-02, -5.4567e-02, -9.0599e-02, -8.9711e-02,\n                       -4.9246e-02, -4.8253e-02, -2.6322e-02, -1.0258e-01, -3.8347e-02,\n                       -8.3599e-02, -2.5655e-02, -3.1109e-02, -3.5131e-02, -3.5306e-02,\n                       -1.4409e-01, -4.7237e-02, -4.0876e-02, -4.6004e-02, -4.3256e-03,\n                       -3.5397e-02, -4.0918e-02, -4.3987e-02, -7.8161e-02, -3.5320e-02,\n                       -5.2036e-02, -5.4392e-02, -4.1289e-02, -8.3538e-02, -8.7546e-02,\n                       -6.8367e-03, -1.0255e-01, -1.3468e-01, -4.2936e-02, -5.8626e-02,\n                       -1.1845e-02, -3.9940e-02, -2.1139e-02, -4.0275e-03, -4.4798e-02,\n                       -5.0178e-02, -3.7833e-02, -9.3436e-02, -2.5830e-02, -6.8780e-02,\n                       -3.5249e-02, -7.9998e-02, -8.3052e-02, -5.7102e-02, -2.1053e-02,\n                       -2.4293e-02, -4.4431e-02, -7.4509e-02, -1.6117e-02, -4.9234e-02,\n                       -3.0011e-02,  2.9886e-03, -6.4982e-02, -6.9263e-02, -1.3442e-01,\n                       -2.4754e-02, -5.4579e-02, -3.3162e-02, -2.7480e-02, -2.0140e-02,\n                       -3.6375e-02, -4.2842e-02, -6.5343e-02, -1.5488e-02, -8.6638e-02,\n                       -3.2145e-02, -8.3823e-02, -5.7876e-02, -5.2706e-02, -4.6129e-02,\n                       -5.6206e-02, -5.0811e-02, -5.0281e-02, -2.5547e-02, -1.2039e-02,\n                       -5.8767e-02, -3.4924e-02, -5.8094e-02, -4.6104e-02, -2.3842e-02,\n                       -2.5957e-02, -1.7425e-02,  9.2709e-03, -4.6300e-02, -2.4742e-02,\n                       -9.6699e-02, -2.6102e-02, -3.4959e-02, -8.6928e-02, -1.7725e-02,\n                       -1.7186e-02, -2.3018e-02, -3.3098e-02,  1.7494e-02, -2.7885e-02,\n                       -2.2343e-02, -1.8256e-02, -5.4536e-02, -1.1367e-01, -2.3460e-02,\n                       -4.4949e-02, -9.4404e-02, -4.0288e-02, -5.9904e-02, -5.0214e-02,\n                       -4.0167e-02, -6.4473e-02, -1.3823e-01, -5.8323e-02, -7.4760e-02,\n                       -5.4848e-02, -3.7877e-02, -5.3352e-02, -5.0337e-02, -5.1382e-02,\n                       -5.9443e-03, -4.9310e-02, -4.7236e-02, -5.4865e-02, -2.7327e-02,\n                       -2.4564e-02, -5.4230e-02, -9.4584e-03, -3.4641e-02, -9.4062e-02,\n                       -1.2958e-01, -9.2187e-02, -3.5398e-02, -1.9131e-02, -4.8105e-02,\n                       -4.1458e-02, -4.1695e-02, -3.0597e-02, -1.1567e-01, -3.8869e-02,\n                       -3.9595e-02, -3.4188e-02, -1.0062e-01, -3.2564e-02, -1.9237e-02,\n                       -5.1949e-02, -4.2463e-02, -1.3749e-02, -7.9042e-02, -4.6803e-02,\n                       -9.2899e-02, -4.8799e-02, -3.1467e-02, -4.7262e-02, -6.4221e-02,\n                       -5.8386e-02, -4.1044e-02, -3.9382e-02, -5.8366e-02, -7.5472e-02,\n                       -2.7850e-02, -4.3035e-02, -3.2524e-02, -1.5065e-02, -1.0760e-01,\n                       -3.3096e-02, -4.9798e-02, -5.2847e-02, -5.2721e-02, -1.1311e-01,\n                       -9.2207e-02, -4.5563e-02, -1.7275e-02, -5.2020e-02],\n                      requires_grad=True))),\n             ('features.1.2.block.5.scale', tensor(0.0481)),\n             ('features.1.2.block.5.zero_point', tensor(80)),\n             ('features.1.2.block.5._packed_params.dtype', torch.qint8),\n             ('features.1.2.block.5._packed_params._packed_params',\n              (tensor([[-0.0274,  0.0578, -0.0457,  ...,  0.0518,  0.0091, -0.1172],\n                       [-0.0258, -0.1503, -0.0610,  ...,  0.0728, -0.1197,  0.0423],\n                       [-0.0846, -0.0635, -0.0442,  ..., -0.0788, -0.0096, -0.0654],\n                       ...,\n                       [ 0.0604, -0.0434,  0.0226,  ..., -0.0057, -0.0094,  0.0302],\n                       [-0.0511,  0.1022,  0.0170,  ...,  0.0213, -0.0724, -0.0469],\n                       [ 0.0291, -0.0421,  0.0146,  ...,  0.0551, -0.0664, -0.0275]],\n                      size=(96, 384), dtype=torch.qint8,\n                      quantization_scheme=torch.per_channel_affine,\n                      scale=tensor([0.0015, 0.0023, 0.0019, 0.0029, 0.0040, 0.0015, 0.0007, 0.0016, 0.0018,\n                       0.0019, 0.0027, 0.0024, 0.0034, 0.0028, 0.0021, 0.0023, 0.0023, 0.0022,\n                       0.0005, 0.0023, 0.0042, 0.0017, 0.0019, 0.0009, 0.0005, 0.0031, 0.0042,\n                       0.0048, 0.0018, 0.0023, 0.0014, 0.0025, 0.0025, 0.0022, 0.0018, 0.0021,\n                       0.0019, 0.0021, 0.0015, 0.0022, 0.0017, 0.0014, 0.0015, 0.0017, 0.0016,\n                       0.0011, 0.0015, 0.0016, 0.0017, 0.0035, 0.0031, 0.0018, 0.0016, 0.0014,\n                       0.0028, 0.0028, 0.0015, 0.0014, 0.0024, 0.0016, 0.0017, 0.0022, 0.0020,\n                       0.0022, 0.0025, 0.0016, 0.0037, 0.0019, 0.0018, 0.0023, 0.0028, 0.0007,\n                       0.0019, 0.0037, 0.0017, 0.0011, 0.0014, 0.0015, 0.0020, 0.0021, 0.0005,\n                       0.0041, 0.0018, 0.0008, 0.0013, 0.0019, 0.0020, 0.0017, 0.0016, 0.0023,\n                       0.0020, 0.0020, 0.0018, 0.0019, 0.0043, 0.0016], dtype=torch.float64),\n                      zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n                      axis=0),\n               Parameter containing:\n               tensor([-1.1249e-02, -3.3849e-02, -2.8441e-02,  2.5201e-02, -2.8277e-01,\n                        5.7714e-02,  3.1689e-03,  3.1404e-03, -1.7223e-02,  1.3116e-02,\n                       -5.4354e-02,  1.5302e-04, -4.3952e-01,  9.7015e-04,  5.0354e-03,\n                       -7.0770e-02,  2.9844e-02, -2.2699e-02, -8.0959e-03, -6.1111e-02,\n                        1.9483e-01,  3.5368e-02,  9.7743e-02, -6.6301e-03, -2.4831e-03,\n                       -1.6921e-03,  3.7587e-01,  2.1910e-01, -4.3954e-02, -5.1922e-03,\n                        3.4229e-02, -1.7467e-03,  5.4062e-02, -3.3115e-03, -3.9381e-02,\n                        2.7144e-02, -3.5947e-02, -1.0160e-02,  2.9798e-02,  3.1272e-02,\n                        1.1199e-02,  2.3176e-02, -2.2522e-02, -3.0473e-04, -2.7700e-02,\n                       -1.2182e-02,  1.5355e-02, -1.9170e-02,  3.2107e-02,  1.9396e-03,\n                       -2.2696e-01,  1.8303e-02,  3.1302e-03, -8.3706e-03, -1.1143e-02,\n                        2.2359e-02,  3.5367e-03,  8.4921e-03,  4.5107e-02, -2.4491e-02,\n                       -1.3543e-02, -1.0201e-02, -4.4659e-03,  1.7354e-02,  3.3759e-01,\n                        4.1190e-03, -3.6046e-01, -1.2838e-02, -9.8121e-02, -2.1034e-02,\n                        6.1875e-02,  3.5188e-03,  2.1402e-02,  3.4818e-01,  1.2628e-02,\n                       -2.3766e-02,  3.7375e-02, -8.5187e-03, -1.8991e-03, -5.4881e-02,\n                       -7.1784e-03,  1.8146e-01,  1.6252e-02, -8.0753e-03,  2.9243e-02,\n                        9.2614e-04, -8.4037e-02,  2.7741e-02,  7.6008e-02,  8.7485e-03,\n                       -4.8100e-03, -3.6338e-02,  2.3534e-02, -5.1051e-03, -2.3883e-02,\n                        4.3407e-02], requires_grad=True))),\n             ('features.2.0.weight',\n              tensor([ 2.7580e-01,  6.6828e-02,  1.8363e-02,  3.8064e-02, -2.8669e-05,\n                       5.2845e-01,  1.0139e-01,  5.7857e-02,  1.7928e-01,  2.6769e-02,\n                       3.3916e-02,  2.7512e-01,  6.8029e-05,  6.7787e-02,  5.1485e-02,\n                       3.0819e-02,  1.0188e-01,  1.2794e-01,  1.8646e-01,  6.8682e-02,\n                       3.4513e-04,  9.6470e-02,  2.6198e-02,  1.5023e-01,  3.1537e-01,\n                       2.4826e-02,  5.0056e-02, -5.0042e-05,  2.2861e-01,  5.3669e-02,\n                       5.6990e-01,  2.5165e-01,  1.2472e-01,  3.2988e-01,  4.6504e-02,\n                       2.7510e-02,  2.5665e-02,  6.7163e-02,  1.1134e-01,  5.2673e-02,\n                       8.5909e-02,  4.0533e-02,  2.8921e-01,  6.5537e-01,  7.3956e-02,\n                       4.4330e-01,  5.1946e-01,  3.0169e-01,  1.0266e-01,  1.0273e-01,\n                      -1.7750e-06,  4.0790e-02,  1.0152e-01,  3.6471e-01,  1.5359e-02,\n                       2.8774e-02,  6.6735e-02,  7.1144e-02,  1.6864e-01,  3.0740e-01,\n                       3.2057e-01,  4.4205e-02,  2.0384e-01,  4.1791e-02, -1.9442e-05,\n                       2.6925e-01,  6.5761e-05,  1.2221e-01,  1.1471e-01,  9.1669e-02,\n                       3.1328e-02,  2.6135e-01,  4.1826e-02,  2.2894e-05,  6.6794e-02,\n                       5.3432e-01,  4.8072e-02,  2.6995e-01,  3.6475e-01,  4.0987e-02,\n                       2.7340e-01,  6.5377e-05,  7.8783e-02,  4.1088e-02,  2.7376e-01,\n                       6.8806e-02,  8.0497e-02,  2.5909e-01,  9.8054e-02,  9.5452e-02,\n                       3.4053e-02,  2.3918e-02,  4.4866e-02,  3.6799e-01,  2.2757e-02,\n                       5.2720e-02])),\n             ('features.2.0.bias',\n              tensor([ 1.1091e-02,  9.8900e-03,  1.0475e-03,  1.3304e-03, -8.6513e-06,\n                       1.4287e-01, -1.6399e-02,  3.5921e-03,  9.0215e-04, -4.0729e-03,\n                      -2.5719e-03,  4.1231e-03,  9.0095e-05,  1.3614e-03, -8.2132e-03,\n                       1.8457e-03, -3.1744e-03, -1.5541e-02, -2.2184e-03,  1.3722e-02,\n                       1.3363e-04, -7.2431e-03,  2.3804e-04,  2.4595e-03, -6.5025e-02,\n                       3.1202e-04, -3.5950e-01, -3.4284e-05,  2.9670e-02,  2.3362e-04,\n                      -1.4628e-01,  3.4965e-03,  4.4236e-03,  2.3759e-02, -3.7298e-03,\n                      -1.1532e-03, -4.1506e-03,  5.6565e-04, -1.3260e-02,  9.5849e-04,\n                       3.5464e-03, -3.4798e-03, -1.1893e-02,  1.9876e-02,  4.6636e-04,\n                       6.2356e-03, -3.0465e-02,  3.6316e-03, -5.6142e-04,  8.0745e-04,\n                       7.7479e-06,  4.6398e-03, -4.9355e-03,  5.3423e-03,  5.6808e-04,\n                      -2.1399e-03,  1.8738e-03, -4.0508e-03,  2.9810e-02,  3.2948e-02,\n                       2.8921e-02, -6.0375e-03, -4.1426e-04, -2.2342e-03, -9.3166e-06,\n                       2.7503e-03,  1.3229e-04, -2.8709e-04, -3.2385e-02,  1.6692e-03,\n                       4.2565e-03,  3.0420e-03,  1.5000e-04,  1.8239e-05,  1.2467e-03,\n                       5.3102e-02, -6.7388e-04,  7.1801e-03,  5.9415e-03,  2.9792e-03,\n                       1.6320e-02,  5.1825e-06, -2.4274e-03,  7.6883e-04,  1.4300e-02,\n                       4.6796e-03, -9.0641e-03, -3.5106e-02, -2.1062e-02, -4.1693e-03,\n                       9.1342e-04,  1.3318e-04, -2.5093e-03, -5.7947e-03, -7.3964e-04,\n                      -1.8832e-03])),\n             ('features.2.1.weight',\n              tensor([[[[-0.0173,  0.0029],\n                        [ 0.0014, -0.0130]],\n              \n                       [[-0.0547,  0.1108],\n                        [-0.0691,  0.1209]],\n              \n                       [[-0.0259, -0.0086],\n                        [-0.0302, -0.0101]],\n              \n                       ...,\n              \n                       [[-0.0245,  0.0676],\n                        [-0.0561,  0.0317]],\n              \n                       [[-0.0345,  0.0144],\n                        [-0.0086,  0.0014]],\n              \n                       [[-0.0273, -0.0432],\n                        [-0.0144, -0.0273]]],\n              \n              \n                      [[[ 0.0000, -0.0206],\n                        [-0.0113, -0.0281]],\n              \n                       [[ 0.0131, -0.0094],\n                        [-0.1557, -0.0938]],\n              \n                       [[-0.0563, -0.0488],\n                        [-0.0319, -0.1182]],\n              \n                       ...,\n              \n                       [[-0.0019, -0.0150],\n                        [-0.0300, -0.0413]],\n              \n                       [[ 0.1182,  0.0900],\n                        [ 0.1463,  0.1407]],\n              \n                       [[-0.0150, -0.0431],\n                        [ 0.0094, -0.0056]]],\n              \n              \n                      [[[ 0.0554,  0.0756],\n                        [ 0.1999,  0.2133]],\n              \n                       [[ 0.0437,  0.0353],\n                        [ 0.0370,  0.0655]],\n              \n                       [[-0.0403, -0.0437],\n                        [-0.0168, -0.0319]],\n              \n                       ...,\n              \n                       [[ 0.0252,  0.0403],\n                        [ 0.0302,  0.0252]],\n              \n                       [[ 0.1058,  0.0538],\n                        [ 0.0706,  0.0302]],\n              \n                       [[-0.0050,  0.0084],\n                        [ 0.0370, -0.0050]]],\n              \n              \n                      ...,\n              \n              \n                      [[[ 0.0107,  0.0015],\n                        [-0.0046, -0.0184]],\n              \n                       [[-0.0445, -0.0307],\n                        [-0.0399, -0.0905]],\n              \n                       [[-0.0153,  0.0353],\n                        [ 0.0813,  0.0645]],\n              \n                       ...,\n              \n                       [[ 0.0061, -0.0061],\n                        [ 0.0200,  0.0015]],\n              \n                       [[-0.1105,  0.0169],\n                        [-0.0015,  0.0200]],\n              \n                       [[ 0.0552, -0.0537],\n                        [-0.0430, -0.0368]]],\n              \n              \n                      [[[-0.0052, -0.0052],\n                        [ 0.0000,  0.0000]],\n              \n                       [[ 0.0078,  0.0052],\n                        [ 0.0000,  0.0026]],\n              \n                       [[ 0.0104, -0.0052],\n                        [-0.0130, -0.0234]],\n              \n                       ...,\n              \n                       [[-0.0052, -0.0052],\n                        [-0.0078, -0.0078]],\n              \n                       [[-0.0026, -0.0156],\n                        [ 0.0260,  0.0234]],\n              \n                       [[-0.0182, -0.0130],\n                        [-0.0026,  0.0156]]],\n              \n              \n                      [[[-0.0230, -0.0481],\n                        [-0.0230, -0.0042]],\n              \n                       [[-0.0439,  0.0167],\n                        [ 0.0230,  0.0251]],\n              \n                       [[ 0.0000,  0.0084],\n                        [ 0.0209,  0.0272]],\n              \n                       ...,\n              \n                       [[ 0.0293,  0.0209],\n                        [ 0.0125,  0.0125]],\n              \n                       [[ 0.0544, -0.0335],\n                        [ 0.0042, -0.0356]],\n              \n                       [[ 0.0606, -0.0105],\n                        [ 0.0356, -0.0105]]]], size=(192, 96, 2, 2), dtype=torch.qint8,\n                     quantization_scheme=torch.per_channel_affine,\n                     scale=tensor([0.0014, 0.0019, 0.0017, 0.0009, 0.0013, 0.0013, 0.0021, 0.0012, 0.0023,\n                      0.0016, 0.0013, 0.0014, 0.0011, 0.0018, 0.0018, 0.0014, 0.0012, 0.0012,\n                      0.0015, 0.0021, 0.0018, 0.0020, 0.0017, 0.0009, 0.0015, 0.0021, 0.0014,\n                      0.0018, 0.0010, 0.0012, 0.0015, 0.0013, 0.0015, 0.0018, 0.0016, 0.0009,\n                      0.0008, 0.0014, 0.0025, 0.0013, 0.0009, 0.0010, 0.0018, 0.0016, 0.0018,\n                      0.0009, 0.0010, 0.0014, 0.0012, 0.0011, 0.0017, 0.0014, 0.0011, 0.0019,\n                      0.0015, 0.0015, 0.0016, 0.0013, 0.0021, 0.0013, 0.0012, 0.0012, 0.0014,\n                      0.0016, 0.0013, 0.0014, 0.0009, 0.0015, 0.0012, 0.0013, 0.0014, 0.0014,\n                      0.0013, 0.0018, 0.0015, 0.0018, 0.0025, 0.0013, 0.0011, 0.0013, 0.0014,\n                      0.0013, 0.0014, 0.0010, 0.0012, 0.0013, 0.0009, 0.0012, 0.0011, 0.0017,\n                      0.0018, 0.0011, 0.0019, 0.0014, 0.0021, 0.0017, 0.0014, 0.0016, 0.0008,\n                      0.0022, 0.0012, 0.0015, 0.0019, 0.0011, 0.0015, 0.0024, 0.0016, 0.0020,\n                      0.0014, 0.0019, 0.0013, 0.0010, 0.0012, 0.0021, 0.0018, 0.0012, 0.0016,\n                      0.0019, 0.0022, 0.0023, 0.0013, 0.0016, 0.0015, 0.0010, 0.0012, 0.0012,\n                      0.0013, 0.0024, 0.0024, 0.0014, 0.0010, 0.0017, 0.0011, 0.0015, 0.0018,\n                      0.0021, 0.0008, 0.0022, 0.0021, 0.0015, 0.0018, 0.0013, 0.0022, 0.0013,\n                      0.0014, 0.0017, 0.0009, 0.0012, 0.0010, 0.0010, 0.0013, 0.0014, 0.0017,\n                      0.0013, 0.0019, 0.0015, 0.0010, 0.0020, 0.0011, 0.0015, 0.0023, 0.0012,\n                      0.0015, 0.0012, 0.0014, 0.0023, 0.0004, 0.0011, 0.0019, 0.0014, 0.0011,\n                      0.0020, 0.0013, 0.0018, 0.0017, 0.0008, 0.0028, 0.0014, 0.0020, 0.0019,\n                      0.0012, 0.0012, 0.0015, 0.0014, 0.0019, 0.0009, 0.0016, 0.0011, 0.0013,\n                      0.0015, 0.0026, 0.0021], dtype=torch.float64),\n                     zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n                     axis=0)),\n             ('features.2.1.bias',\n              Parameter containing:\n              tensor([-5.2683e-06, -1.2612e-02,  5.5280e-03, -8.5285e-02,  1.9487e-03,\n                      -4.7177e-03,  2.0535e-04,  3.7524e-03,  1.1152e-02, -1.2505e-03,\n                      -2.4515e-03,  7.0370e-03,  1.1148e-02,  5.5336e-03,  9.4750e-05,\n                      -1.0762e-02,  4.6166e-03, -5.0392e-03, -3.4107e-03, -3.6591e-02,\n                      -6.2308e-03,  3.0482e-03, -1.8093e-03, -2.6921e-02,  3.1702e-03,\n                       6.1707e-03,  1.7966e-02, -3.2913e-03, -2.2504e-03, -3.2500e-03,\n                       3.1321e-03,  6.9297e-03, -9.6563e-04,  2.6616e-02, -3.2644e-03,\n                       3.3563e-04,  1.2747e-02,  1.4492e-02,  2.1025e-03,  8.3672e-03,\n                      -2.5398e-02,  7.3626e-04,  1.9185e-04,  1.3614e-03,  1.4337e-02,\n                      -4.6612e-03, -2.0221e-03, -2.4602e-02,  4.8329e-03,  1.0616e-02,\n                      -2.8637e-03,  1.8176e-03, -1.8511e-02, -1.6908e-02, -2.5946e-03,\n                      -1.9456e-03,  3.2045e-02, -8.3986e-03, -5.5762e-03, -5.3469e-03,\n                       2.0652e-04, -1.5473e-03,  7.8491e-04, -7.8145e-03,  6.0778e-03,\n                      -1.4715e-03,  6.2710e-03, -9.3202e-04, -2.1297e-03, -4.6418e-03,\n                      -6.4293e-03, -7.3584e-03,  2.8115e-04,  8.3302e-03, -5.4133e-04,\n                       1.4556e-03, -1.4982e-02,  5.6119e-04,  3.2303e-03, -1.5592e-03,\n                      -8.8715e-03,  1.9399e-02, -9.6698e-04,  7.2049e-03, -1.9786e-03,\n                       5.2506e-04,  2.2048e-02,  1.0169e-02,  6.8144e-03, -6.8057e-01,\n                       1.6966e-03, -1.3141e-03,  1.0852e-02,  4.1364e-03,  1.1819e-02,\n                      -7.1427e-03,  1.1730e-02, -1.6946e-02,  5.7370e-03, -1.2578e-02,\n                       3.0157e-02, -3.2092e-02,  4.3370e-03, -5.8155e-03, -6.7623e-04,\n                       3.1296e-03,  1.1075e-03, -7.8417e-03, -4.9385e-03,  1.6555e-02,\n                      -1.2659e-02,  2.1984e-02,  1.2268e-02,  3.1867e-03, -3.6814e-03,\n                      -6.0504e-04,  1.7409e-02,  1.0930e-03,  3.3551e-02, -3.5876e-04,\n                       2.3605e-03,  4.0395e-03,  4.2876e-03, -5.8041e-03, -3.7243e-03,\n                       3.9800e-03,  1.7644e-04,  9.9735e-04,  8.1414e-02,  5.3142e-03,\n                       2.9355e-03,  1.6320e-02, -5.7190e-03, -1.1017e-02,  1.4305e-03,\n                      -5.3511e-02,  6.7472e-03,  1.1908e-03,  3.0251e-02,  2.7646e-03,\n                      -4.8272e-03, -4.4972e-03,  7.7776e-03,  2.6053e-03, -3.8847e-03,\n                       2.9270e-03, -1.1401e-03,  2.0675e-03,  3.1641e-03,  1.4353e-02,\n                       3.9609e-03,  7.1625e-03,  2.7331e-03,  1.0068e-02, -2.4776e-03,\n                      -1.4675e-02,  5.8124e-04,  1.0886e-02, -3.8806e-03,  3.1466e-03,\n                       4.5844e-03, -3.9915e-03, -3.4238e-03,  3.7715e-03,  3.7657e-03,\n                      -3.9984e-03, -9.1262e-02,  5.0724e-02,  1.0379e-03,  1.8546e-03,\n                      -3.3587e-03,  5.1015e-03, -6.5487e-03,  3.6969e-02, -1.8368e-03,\n                      -5.6835e-02, -2.3895e-03,  2.4476e-03,  1.3875e-02, -1.5887e-03,\n                       3.5466e-03,  5.0643e-05, -2.9347e-03,  2.6792e-02, -7.8575e-02,\n                       9.2900e-03,  7.2604e-03, -6.4193e-04,  3.0175e-03, -3.4338e-03,\n                      -1.7063e-02,  9.3836e-04], requires_grad=True)),\n             ('features.2.1.scale', tensor(0.0089)),\n             ('features.2.1.zero_point', tensor(91)),\n             ('features.3.0.layer_scale',\n              tensor([[[-0.1075]],\n              \n                      [[-0.1300]],\n              \n                      [[ 0.0686]],\n              \n                      [[ 0.1619]],\n              \n                      [[ 0.0952]],\n              \n                      [[-0.0725]],\n              \n                      [[-0.1215]],\n              \n                      [[-0.1036]],\n              \n                      [[ 0.1356]],\n              \n                      [[ 0.0642]],\n              \n                      [[-0.0940]],\n              \n                      [[-0.0708]],\n              \n                      [[-0.1416]],\n              \n                      [[-0.0747]],\n              \n                      [[-0.0350]],\n              \n                      [[-0.1061]],\n              \n                      [[-0.0562]],\n              \n                      [[-0.0774]],\n              \n                      [[ 0.1141]],\n              \n                      [[-0.0548]],\n              \n                      [[ 0.1281]],\n              \n                      [[ 0.1039]],\n              \n                      [[ 0.1161]],\n              \n                      [[-0.0867]],\n              \n                      [[ 0.0915]],\n              \n                      [[-0.0875]],\n              \n                      [[-0.0404]],\n              \n                      [[ 0.1217]],\n              \n                      [[-0.0573]],\n              \n                      [[ 0.1028]],\n              \n                      [[ 0.1273]],\n              \n                      [[ 0.1228]],\n              \n                      [[-0.0600]],\n              \n                      [[ 0.1574]],\n              \n                      [[ 0.0714]],\n              \n                      [[-0.0601]],\n              \n                      [[ 0.1781]],\n              \n                      [[ 0.0649]],\n              \n                      [[-0.1331]],\n              \n                      [[-0.0749]],\n              \n                      [[-0.0501]],\n              \n                      [[ 0.0614]],\n              \n                      [[ 0.0725]],\n              \n                      [[ 0.1308]],\n              \n                      [[-0.1696]],\n              \n                      [[-0.0836]],\n              \n                      [[-0.0728]],\n              \n                      [[-0.1656]],\n              \n                      [[ 0.1368]],\n              \n                      [[ 0.1020]],\n              \n                      [[-0.1087]],\n              \n                      [[-0.1009]],\n              \n                      [[ 0.0413]],\n              \n                      [[ 0.1279]],\n              \n                      [[-0.0841]],\n              \n                      [[-0.0944]],\n              \n                      [[ 0.0444]],\n              \n                      [[-0.1078]],\n              \n                      [[ 0.1366]],\n              \n                      [[ 0.0722]],\n              \n                      [[-0.1458]],\n              \n                      [[ 0.0754]],\n              \n                      [[ 0.0862]],\n              \n                      [[ 0.1611]],\n              \n                      [[-0.0925]],\n              \n                      [[-0.0712]],\n              \n                      [[ 0.0850]],\n              \n                      [[ 0.0757]],\n              \n                      [[-0.0860]],\n              \n                      [[ 0.0586]],\n              \n                      [[ 0.1415]],\n              \n                      [[-0.1477]],\n              \n                      [[-0.1380]],\n              \n                      [[-0.1096]],\n              \n                      [[ 0.0756]],\n              \n                      [[-0.0675]],\n              \n                      [[ 0.2292]],\n              \n                      [[-0.0641]],\n              \n                      [[-0.0785]],\n              \n                      [[ 0.0900]],\n              \n                      [[-0.1090]],\n              \n                      [[-0.1248]],\n              \n                      [[-0.0618]],\n              \n                      [[-0.0818]],\n              \n                      [[-0.0974]],\n              \n                      [[-0.0853]],\n              \n                      [[ 0.0989]],\n              \n                      [[-0.1011]],\n              \n                      [[-0.1076]],\n              \n                      [[ 0.4225]],\n              \n                      [[-0.0480]],\n              \n                      [[ 0.1501]],\n              \n                      [[ 0.1589]],\n              \n                      [[ 0.0792]],\n              \n                      [[-0.0568]],\n              \n                      [[-0.1239]],\n              \n                      [[-0.0657]],\n              \n                      [[-0.1310]],\n              \n                      [[-0.1302]],\n              \n                      [[-0.1640]],\n              \n                      [[ 0.0593]],\n              \n                      [[ 0.1233]],\n              \n                      [[ 0.1235]],\n              \n                      [[ 0.0728]],\n              \n                      [[-0.0624]],\n              \n                      [[ 0.0535]],\n              \n                      [[-0.0980]],\n              \n                      [[ 0.1275]],\n              \n                      [[ 0.1107]],\n              \n                      [[-0.0642]],\n              \n                      [[-0.0539]],\n              \n                      [[ 0.1587]],\n              \n                      [[ 0.1588]],\n              \n                      [[-0.1087]],\n              \n                      [[ 0.1174]],\n              \n                      [[ 0.0455]],\n              \n                      [[ 0.1358]],\n              \n                      [[-0.1421]],\n              \n                      [[-0.1320]],\n              \n                      [[-0.0583]],\n              \n                      [[-0.1270]],\n              \n                      [[-0.0567]],\n              \n                      [[-0.0458]],\n              \n                      [[ 0.0659]],\n              \n                      [[-0.1071]],\n              \n                      [[-0.0636]],\n              \n                      [[ 0.0709]],\n              \n                      [[ 0.1697]],\n              \n                      [[-0.1220]],\n              \n                      [[ 0.0622]],\n              \n                      [[-0.0499]],\n              \n                      [[-0.1730]],\n              \n                      [[-0.1157]],\n              \n                      [[ 0.0960]],\n              \n                      [[ 0.1081]],\n              \n                      [[ 0.1633]],\n              \n                      [[-0.1104]],\n              \n                      [[-0.0372]],\n              \n                      [[ 0.1498]],\n              \n                      [[-0.0702]],\n              \n                      [[-0.1320]],\n              \n                      [[-0.0727]],\n              \n                      [[ 0.0493]],\n              \n                      [[-0.0909]],\n              \n                      [[-0.0738]],\n              \n                      [[-0.0663]],\n              \n                      [[ 0.0992]],\n              \n                      [[ 0.0811]],\n              \n                      [[ 0.0871]],\n              \n                      [[ 0.0958]],\n              \n                      [[ 0.0890]],\n              \n                      [[ 0.0783]],\n              \n                      [[-0.0626]],\n              \n                      [[ 0.0832]],\n              \n                      [[-0.1001]],\n              \n                      [[-0.1216]],\n              \n                      [[-0.0891]],\n              \n                      [[-0.1581]],\n              \n                      [[-0.0718]],\n              \n                      [[ 0.0650]],\n              \n                      [[-0.0585]],\n              \n                      [[-0.0624]],\n              \n                      [[-0.0756]],\n              \n                      [[-0.0818]],\n              \n                      [[-0.0681]],\n              \n                      [[-0.1097]],\n              \n                      [[-0.0655]],\n              \n                      [[-0.1643]],\n              \n                      [[-0.0904]],\n              \n                      [[-0.1368]],\n              \n                      [[ 0.0862]],\n              \n                      [[ 0.0633]],\n              \n                      [[-0.1072]],\n              \n                      [[-0.1749]],\n              \n                      [[-0.0771]],\n              \n                      [[ 0.0629]],\n              \n                      [[ 0.2155]],\n              \n                      [[-0.0931]],\n              \n                      [[-0.1197]],\n              \n                      [[ 0.1575]],\n              \n                      [[ 0.0822]],\n              \n                      [[ 0.0762]],\n              \n                      [[ 0.0680]],\n              \n                      [[ 0.1424]],\n              \n                      [[-0.0894]],\n              \n                      [[-0.0812]],\n              \n                      [[-0.1237]],\n              \n                      [[ 0.1255]],\n              \n                      [[-0.0637]],\n              \n                      [[-0.1033]],\n              \n                      [[ 0.1765]],\n              \n                      [[ 0.1216]]])),\n             ('features.3.0.block.0.weight',\n              tensor([[[[ 0.0000,  0.0022,  0.0065,  ...,  0.0022,  0.0043,  0.0086],\n                        [ 0.0022,  0.0000,  0.0130,  ...,  0.0086,  0.0043,  0.0000],\n                        [ 0.0000,  0.0022, -0.0043,  ...,  0.0000, -0.0022,  0.0000],\n                        ...,\n                        [-0.0086, -0.0195, -0.0800,  ...,  0.0108,  0.0022,  0.0043],\n                        [ 0.0000,  0.0000,  0.0130,  ...,  0.0022,  0.0000,  0.0000],\n                        [ 0.0043,  0.0022,  0.0173,  ..., -0.0022, -0.0022,  0.0000]]],\n              \n              \n                      [[[ 0.0000,  0.0048,  0.0000,  ..., -0.0048,  0.0000,  0.0000],\n                        [ 0.0048,  0.0024,  0.0073,  ..., -0.0097,  0.0000,  0.0048],\n                        [ 0.0073,  0.0073, -0.0024,  ..., -0.0121,  0.0097,  0.0073],\n                        ...,\n                        [ 0.0024,  0.0024,  0.0194,  ...,  0.0097,  0.0048,  0.0024],\n                        [ 0.0048,  0.0048,  0.0097,  ...,  0.0073,  0.0048,  0.0024],\n                        [ 0.0097,  0.0048,  0.0048,  ...,  0.0097,  0.0073,  0.0073]]],\n              \n              \n                      [[[ 0.0101,  0.0114,  0.0127,  ...,  0.0202,  0.0127,  0.0152],\n                        [ 0.0164,  0.0101,  0.0101,  ...,  0.0114,  0.0152,  0.0101],\n                        [ 0.0291,  0.0278,  0.0405,  ...,  0.0304,  0.0152,  0.0202],\n                        ...,\n                        [ 0.0139,  0.0089,  0.0354,  ...,  0.0430,  0.0253,  0.0215],\n                        [ 0.0101,  0.0025,  0.0228,  ...,  0.0152,  0.0051,  0.0190],\n                        [ 0.0101,  0.0101,  0.0152,  ...,  0.0152,  0.0114,  0.0089]]],\n              \n              \n                      ...,\n              \n              \n                      [[[ 0.0044,  0.0000, -0.0022,  ...,  0.0022, -0.0066, -0.0022],\n                        [-0.0044, -0.0022, -0.0044,  ...,  0.0022,  0.0044, -0.0022],\n                        [-0.0066, -0.0066,  0.0022,  ..., -0.0155, -0.0066, -0.0066],\n                        ...,\n                        [-0.0022,  0.0044,  0.0066,  ..., -0.0221, -0.0110, -0.0044],\n                        [ 0.0000, -0.0044,  0.0000,  ..., -0.0022, -0.0044,  0.0022],\n                        [ 0.0000, -0.0022,  0.0022,  ..., -0.0022, -0.0066, -0.0022]]],\n              \n              \n                      [[[ 0.0059,  0.0000,  0.0119,  ...,  0.0178,  0.0000,  0.0030],\n                        [ 0.0059,  0.0000,  0.0000,  ...,  0.0000, -0.0030,  0.0059],\n                        [ 0.0178,  0.0208, -0.0475,  ..., -0.0534,  0.0238,  0.0148],\n                        ...,\n                        [ 0.0089,  0.0059,  0.0089,  ...,  0.0148,  0.0119,  0.0119],\n                        [ 0.0030,  0.0000,  0.0089,  ...,  0.0119,  0.0030,  0.0059],\n                        [ 0.0059,  0.0030,  0.0119,  ...,  0.0148,  0.0059,  0.0059]]],\n              \n              \n                      [[[ 0.0028,  0.0056,  0.0000,  ...,  0.0056,  0.0000,  0.0000],\n                        [ 0.0028,  0.0000,  0.0000,  ...,  0.0028,  0.0000,  0.0028],\n                        [ 0.0000,  0.0028,  0.0056,  ...,  0.0056,  0.0028,  0.0028],\n                        ...,\n                        [ 0.0056,  0.0056,  0.0056,  ...,  0.0084,  0.0167,  0.0056],\n                        [ 0.0028,  0.0000,  0.0000,  ...,  0.0056,  0.0000,  0.0028],\n                        [ 0.0000,  0.0028,  0.0000,  ...,  0.0056,  0.0084,  0.0000]]]],\n                     size=(192, 1, 7, 7), dtype=torch.qint8,\n                     quantization_scheme=torch.per_channel_affine,\n                     scale=tensor([2.1615e-03, 2.4233e-03, 1.2653e-03, 2.5226e-03, 2.5908e-03, 2.8711e-03,\n                      1.8961e-03, 2.5144e-03, 2.9907e-03, 1.6264e-03, 2.8490e-03, 1.5596e-03,\n                      3.1423e-03, 2.3381e-03, 4.3671e-05, 2.0779e-03, 3.0749e-03, 2.6341e-03,\n                      1.2585e-03, 2.5958e-03, 2.8609e-03, 2.9425e-03, 2.8240e-03, 9.9398e-04,\n                      2.9615e-03, 2.0396e-03, 1.9694e-03, 2.6581e-03, 2.4945e-04, 2.5364e-03,\n                      2.1545e-03, 2.7104e-03, 8.7796e-04, 2.5246e-03, 9.1507e-04, 2.4446e-03,\n                      2.0825e-03, 1.6467e-03, 2.8883e-03, 1.8414e-03, 5.8202e-04, 3.1572e-03,\n                      2.5543e-03, 2.6868e-03, 2.3088e-03, 1.2727e-03, 3.0560e-03, 1.9734e-03,\n                      2.1157e-03, 1.0598e-03, 2.6581e-03, 2.9703e-03, 2.1247e-03, 2.8496e-03,\n                      2.3666e-03, 7.2519e-04, 8.9860e-04, 2.6217e-03, 2.9099e-03, 1.7152e-03,\n                      2.4359e-03, 1.5575e-03, 1.6311e-03, 2.0797e-03, 2.2356e-03, 9.4248e-04,\n                      1.7222e-03, 1.5513e-03, 2.6512e-03, 2.2573e-03, 2.6647e-03, 2.0245e-03,\n                      2.8160e-03, 2.0688e-03, 1.9479e-03, 2.4750e-03, 1.9833e-03, 1.3488e-03,\n                      1.4054e-03, 2.4077e-03, 2.6061e-03, 2.8583e-03, 3.0128e-03, 2.9316e-03,\n                      2.7157e-03, 2.5380e-03, 2.5752e-03, 1.1446e-03, 1.2654e-03, 9.1248e-04,\n                      2.7053e-03, 2.7852e-03, 2.1969e-03, 1.7689e-03, 2.1452e-03, 2.5934e-03,\n                      2.5614e-03, 2.7241e-03, 9.5042e-04, 2.4228e-03, 2.6792e-03, 1.2421e-03,\n                      2.4128e-03, 3.0840e-03, 2.3334e-03, 7.7924e-04, 2.6551e-03, 1.7389e-03,\n                      2.4281e-03, 1.4276e-03, 1.0280e-03, 1.9203e-03, 2.9137e-03, 2.9529e-03,\n                      2.4689e-03, 1.4019e-03, 2.5842e-03, 2.5215e-03, 2.6418e-03, 1.9011e-03,\n                      2.1861e-03, 2.4210e-03, 1.2221e-03, 1.3916e-03, 1.7537e-03, 1.5972e-03,\n                      3.0173e-03, 1.9960e-03, 2.3062e-03, 6.4031e-04, 2.6442e-03, 2.4762e-03,\n                      2.6899e-03, 2.8596e-03, 1.9556e-03, 2.3408e-03, 3.2184e-03, 1.9366e-03,\n                      2.4155e-03, 2.4003e-03, 1.9457e-03, 2.8565e-03, 2.3435e-03, 2.9030e-03,\n                      1.2161e-03, 2.5401e-03, 1.5586e-03, 2.7375e-03, 2.1940e-03, 2.9186e-03,\n                      4.2714e-04, 2.1707e-03, 2.2546e-03, 1.3844e-03, 2.6449e-03, 2.5430e-03,\n                      1.4319e-03, 2.3101e-03, 2.9101e-03, 1.7198e-03, 1.9141e-03, 2.8780e-03,\n                      1.6262e-03, 2.3227e-03, 8.7276e-04, 2.7478e-03, 8.5860e-04, 1.6572e-03,\n                      1.3179e-03, 2.8368e-03, 8.7476e-04, 2.7800e-03, 2.4393e-03, 2.5942e-03,\n                      1.2916e-03, 9.6811e-04, 2.6252e-03, 2.2173e-03, 2.3996e-03, 3.0141e-03,\n                      2.8171e-03, 2.8895e-03, 2.0063e-03, 2.7725e-03, 2.4633e-03, 1.0235e-03,\n                      2.4804e-03, 2.2605e-03, 1.0205e-03, 2.2077e-03, 2.9694e-03, 2.7908e-03],\n                     dtype=torch.float64),\n                     zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n                     axis=0)),\n             ('features.3.0.block.0.bias',\n              Parameter containing:\n              tensor([ 1.3970e-02, -4.0571e-03, -1.2108e-02, -1.2388e-04,  3.2277e-03,\n                       1.7131e-03,  6.4237e-03,  1.7110e-02,  8.2494e-03,  1.9956e-03,\n                       5.2651e-03, -1.1544e-02,  4.5014e-04,  1.2896e-03, -1.1440e-01,\n                      -4.4527e-03, -1.4790e-03,  2.5821e-03,  2.4928e-03,  3.2355e-03,\n                       8.3983e-05, -9.8790e-03,  3.7119e-03, -1.3871e-02, -1.2928e-02,\n                       4.7161e-02, -7.7276e-03,  7.9271e-03,  3.1288e-02, -9.9314e-03,\n                       5.2601e-03,  2.6455e-03,  6.6617e-03, -1.6347e-03, -4.3849e-03,\n                       2.4410e-02, -4.0283e-03, -1.7775e-03, -1.2522e-02,  9.6514e-03,\n                      -8.9393e-03,  8.6453e-04,  5.5158e-03,  1.9623e-03, -8.8810e-03,\n                       7.8437e-03,  1.6950e-03, -1.2605e-03, -5.8206e-03, -3.3015e-03,\n                      -8.7981e-03, -3.6413e-03,  3.6243e-03,  2.9779e-03,  2.5635e-03,\n                      -5.5242e-04, -1.8728e-02, -5.8162e-04, -7.4003e-03,  1.1985e-02,\n                       2.0994e-02, -5.2248e-03,  1.1859e-02,  1.4930e-03,  9.5327e-03,\n                       3.3046e-03,  2.0338e-03, -9.2770e-04, -5.0997e-03,  2.2561e-02,\n                      -6.1809e-03, -1.1744e-03,  1.5728e-03,  1.3796e-02,  2.4529e-03,\n                       1.5700e-02,  1.7859e-03,  1.1214e-02,  9.2569e-03, -3.2203e-02,\n                       9.1214e-03, -2.4267e-03, -6.3461e-03, -7.9437e-03, -3.4241e-03,\n                       5.0443e-03,  2.0966e-03, -6.4477e-03, -9.9090e-03,  3.4934e-02,\n                       2.0830e-02,  1.0572e-03,  2.2782e-02, -5.0101e-03,  6.2664e-04,\n                      -7.6040e-03, -9.1784e-03, -2.3768e-02, -1.1140e-04,  1.4272e-02,\n                       4.6266e-03, -2.2031e-02,  7.3921e-03,  3.6515e-03,  1.0352e-02,\n                       1.1926e-02,  1.5750e-03,  1.7171e-04,  1.8598e-02,  2.0737e-03,\n                      -1.7592e-02, -5.9410e-03,  8.9972e-03, -7.1446e-03, -5.3884e-03,\n                       5.5118e-03, -9.8322e-05,  9.6096e-03,  2.7190e-04, -2.5070e-03,\n                      -3.9259e-02,  2.2705e-03, -2.5263e-03,  5.2409e-03, -1.7637e-03,\n                       1.5741e-02, -4.9968e-03,  3.7786e-03, -1.8040e-03,  2.0139e-02,\n                       9.4395e-03,  9.7218e-03, -7.0891e-03,  1.1231e-02,  2.4564e-03,\n                       1.1494e-03,  1.9469e-04,  1.5353e-03, -1.6701e-02,  1.1908e-02,\n                      -8.3304e-04, -9.8534e-03,  1.1768e-03,  6.9315e-03,  4.1615e-03,\n                       2.0484e-02, -6.7713e-03,  3.1012e-03,  1.8675e-02,  2.1219e-02,\n                       8.0378e-05,  7.5348e-03,  3.3161e-03, -7.3728e-03,  4.4961e-03,\n                      -2.4905e-04, -1.1507e-03,  1.0852e-02, -6.6046e-03, -6.8327e-03,\n                       4.7781e-03,  1.7770e-02, -2.5444e-03,  2.2240e-02,  6.5833e-04,\n                       2.6652e-03,  7.0059e-02,  5.3422e-03,  1.0503e-03, -3.1618e-03,\n                      -9.6014e-03, -3.3825e-02,  3.4512e-04,  6.2750e-03, -5.2003e-03,\n                       2.6313e-03,  9.8644e-04, -3.6284e-03, -5.8760e-04,  1.2108e-03,\n                       1.6412e-03, -1.2646e-03, -5.1797e-03,  5.0803e-03, -1.1294e-03,\n                      -4.8044e-04,  6.0928e-03,  2.2700e-03, -3.4787e-03,  2.3049e-02,\n                       4.0326e-03,  8.4826e-03], requires_grad=True)),\n             ('features.3.0.block.0.scale', tensor(0.0034)),\n             ('features.3.0.block.0.zero_point', tensor(65)),\n             ('features.3.0.block.2.weight',\n              tensor([ 0.9791,  1.1396,  0.9049,  1.6589,  1.2612,  1.1806,  1.0108,  1.2647,\n                       1.4262,  1.0008,  1.2451,  1.0145,  2.0974,  1.1740,  3.0574,  0.9669,\n                       1.2073,  1.4379,  0.8537,  1.6784,  1.6502,  1.2998,  1.7625,  0.9847,\n                       1.3312,  1.3198,  0.9484,  1.1438,  0.8909,  1.2189,  1.0348,  1.3059,\n                       0.8431,  1.2491,  0.9083,  1.1042,  1.4260,  0.9457,  1.7864,  1.0079,\n                       1.1898,  1.3147,  1.3551,  1.1864,  1.0443,  1.0137,  1.4901,  1.4438,\n                       0.8627,  0.9963,  1.3533,  1.5199,  1.0902,  1.3279,  1.2451,  0.1108,\n                       0.9658,  1.2686,  1.6435,  0.9664,  1.0038,  0.8942,  0.9430,  1.1081,\n                       1.1072,  0.8476,  0.7969,  0.9592,  1.1006,  0.6987,  1.2485,  1.4603,\n                       1.6194,  1.1298,  1.0097,  1.0798,  1.5972,  0.9088,  1.0394,  1.3548,\n                       1.1992,  1.3820,  1.2787,  1.3227,  1.1398,  1.2001,  1.6267,  1.1173,\n                       0.9410,  0.6818,  1.0807,  1.6880,  1.0517,  1.0288,  1.5906,  1.4127,\n                       1.4555,  1.3094,  1.1477,  1.1374,  1.2935,  0.8284,  1.0343,  1.6875,\n                       1.0610,  0.7998,  1.3902,  1.3468,  0.9486,  0.7785,  1.0652,  1.4121,\n                       1.6789,  1.7535,  1.1796,  0.8635,  1.6102,  1.1498,  1.1882,  1.0411,\n                       1.3786,  0.8192,  0.6667,  0.8155,  0.8757,  0.8051,  1.6239,  0.7134,\n                       1.1910,  0.7598,  1.3046,  1.3553,  1.2723,  1.3232,  1.3943,  1.5472,\n                       1.5101,  1.5053,  1.6187,  1.0629,  1.0341,  1.3743,  1.5764,  1.2629,\n                       1.1217,  1.2000,  1.0064,  1.3272,  1.2282,  1.6195,  0.0498, -0.8604,\n                       1.0692,  0.8681,  1.2758,  1.1995,  0.6401,  1.0275,  1.3691,  0.9686,\n                       0.9616,  1.2893,  0.9411,  1.0357,  0.8204,  1.4560,  1.7401,  1.4399,\n                       0.7545,  1.4637,  1.0261,  1.2091,  0.9552,  1.1100,  0.8673,  1.1715,\n                       2.0875,  0.7806, -1.1330,  1.6240,  1.1452,  0.8945,  0.8508,  1.6045,\n                       1.4075,  0.8526,  1.0952,  1.0609,  0.9236,  1.2601,  1.5309,  1.4495])),\n             ('features.3.0.block.2.bias',\n              tensor([-4.0750e-01,  1.5070e-01,  2.6374e-01,  8.0414e-02, -1.0668e-01,\n                      -1.1116e-01, -1.7470e-01, -4.5808e-01, -1.0660e-01, -1.5731e-01,\n                      -1.4531e-01,  4.9706e-01,  2.1885e-01, -5.1758e-02, -4.1626e-01,\n                      -1.5413e-01,  4.4203e-02, -6.2103e-02, -9.8328e-02, -9.8284e-02,\n                       2.9348e-02,  3.6053e-01, -1.4768e-01,  5.1958e-01,  4.5569e-01,\n                      -1.5787e+00,  3.1473e-01, -1.5385e-01,  2.4256e+00,  3.5480e-01,\n                      -1.9970e-01, -6.8588e-02, -1.4196e-01,  6.5666e-02,  2.0488e-01,\n                      -7.6508e-01,  1.7114e-01,  3.7130e-01,  6.6014e-01, -6.0190e-01,\n                       1.4071e-01,  2.1570e-02, -8.7653e-02, -5.4354e-02,  2.3760e-01,\n                      -1.6177e-02, -5.5452e-02,  2.0709e-02,  1.5772e-01,  5.5809e-01,\n                       2.4848e-01,  1.8850e-01, -1.0960e-01,  4.1052e-01, -7.5509e-02,\n                      -2.2638e+00,  6.6144e-01,  6.4397e-02,  3.1751e-01, -1.0662e-01,\n                      -5.5461e-01,  3.6742e-01, -2.9907e-01,  1.4807e-02, -2.8298e-01,\n                       2.0387e-02, -4.0632e-02, -2.8367e-01,  1.5286e-01, -5.5465e-01,\n                       2.6375e-01,  5.3491e-02, -2.6359e-01, -3.9691e-01, -3.8910e-02,\n                      -3.5839e-01, -7.2043e-02, -2.2749e-01, -5.8415e-03,  1.2319e+00,\n                      -2.7574e-01,  2.0150e-01,  2.3841e-01,  2.8782e-01,  1.5733e-01,\n                      -2.5476e-01,  5.8656e-02,  2.0154e-01, -1.0443e-03, -3.7973e-02,\n                      -6.4949e-01,  2.0221e-01, -6.7190e-01, -9.8100e-02, -1.2373e-02,\n                       3.1701e-01,  4.3716e-01,  7.6196e-01,  6.8638e-02, -4.1837e-01,\n                      -1.3394e-01,  6.3586e-01, -2.2768e-01, -1.4994e-01, -2.6039e-01,\n                      -1.3213e-01,  1.1773e-03,  1.7539e-01, -5.0780e-01, -2.6658e-02,\n                       1.3851e-01,  1.6954e-01, -2.5255e-01,  3.1468e-01,  1.6041e-01,\n                      -1.0120e-01, -1.9665e-02, -2.4288e-01, -1.5604e-01,  8.2503e-02,\n                       1.3664e+00, -3.3619e-02, -4.3741e-02, -7.9000e-02,  1.9601e-01,\n                       1.2938e+00,  2.3256e-01, -4.8567e-02,  2.4359e-02, -5.6897e-01,\n                      -3.0638e-01, -2.9642e-01,  3.1822e-01, -4.3617e-01, -7.8475e-02,\n                      -3.6191e-02, -1.5289e-02, -2.9129e-02,  3.5979e-02, -3.9047e-01,\n                       2.5784e-02,  3.8950e-01, -2.1216e-03, -1.8154e-01, -1.8299e-01,\n                      -6.3610e-01,  1.6964e-01, -4.6630e-02, -6.8030e-01, -1.0002e+00,\n                      -2.4436e+00,  1.5604e-01,  4.9205e-02,  5.5458e-01, -1.8989e-01,\n                       1.0140e-01,  1.7721e-02, -3.0013e-01,  2.2098e-01,  1.3739e-01,\n                      -1.1236e-01, -5.6063e-01,  9.9464e-02, -6.4436e-01,  1.0248e-01,\n                       5.8323e-03,  2.4176e+00, -1.4615e-01, -1.6806e-02,  1.3469e-01,\n                       3.8139e-01,  1.1882e+00,  1.0451e-01, -2.1996e-01, -4.5429e-02,\n                      -1.4282e+00,  9.0105e-03,  6.2419e-02, -4.9365e-02, -3.3928e-02,\n                       2.0018e-02,  1.2239e-01,  3.6044e-02,  5.8714e-02, -1.5258e-02,\n                      -2.8402e-01, -1.4115e-01, -7.8330e-02, -2.5198e-02, -7.5147e-01,\n                      -1.3859e-01, -2.9276e-01])),\n             ('features.3.0.block.2.scale', tensor(0.1773)),\n             ('features.3.0.block.2.zero_point', tensor(86)),\n             ('features.3.0.block.3.scale', tensor(0.0746)),\n             ('features.3.0.block.3.zero_point', tensor(75)),\n             ('features.3.0.block.3._packed_params.dtype', torch.qint8),\n             ('features.3.0.block.3._packed_params._packed_params',\n              (tensor([[ 0.0114, -0.0571,  0.0419,  ...,  0.0698, -0.0431, -0.0330],\n                       [ 0.1557, -0.0054,  0.0072,  ..., -0.0036, -0.0054, -0.1394],\n                       [ 0.0284, -0.0485, -0.1205,  ...,  0.0485,  0.0117,  0.0050],\n                       ...,\n                       [ 0.0139,  0.0154,  0.0355,  ...,  0.0062, -0.0493,  0.0447],\n                       [-0.0366, -0.0340, -0.0288,  ...,  0.0995,  0.0131,  0.0288],\n                       [ 0.0183,  0.0037,  0.0281,  ..., -0.0244, -0.0208,  0.0782]],\n                      size=(768, 192), dtype=torch.qint8,\n                      quantization_scheme=torch.per_channel_affine,\n                      scale=tensor([0.0013, 0.0018, 0.0017, 0.0019, 0.0013, 0.0016, 0.0020, 0.0013, 0.0012,\n                       0.0015, 0.0017, 0.0011, 0.0019, 0.0023, 0.0021, 0.0014, 0.0022, 0.0012,\n                       0.0010, 0.0012, 0.0018, 0.0016, 0.0013, 0.0018, 0.0017, 0.0014, 0.0016,\n                       0.0026, 0.0012, 0.0030, 0.0013, 0.0015, 0.0018, 0.0026, 0.0019, 0.0017,\n                       0.0016, 0.0018, 0.0017, 0.0020, 0.0033, 0.0020, 0.0015, 0.0018, 0.0017,\n                       0.0013, 0.0018, 0.0019, 0.0031, 0.0016, 0.0009, 0.0011, 0.0018, 0.0015,\n                       0.0010, 0.0028, 0.0019, 0.0016, 0.0011, 0.0015, 0.0014, 0.0014, 0.0019,\n                       0.0015, 0.0015, 0.0011, 0.0017, 0.0020, 0.0010, 0.0015, 0.0014, 0.0013,\n                       0.0011, 0.0014, 0.0023, 0.0020, 0.0016, 0.0017, 0.0016, 0.0022, 0.0011,\n                       0.0030, 0.0016, 0.0016, 0.0017, 0.0019, 0.0014, 0.0015, 0.0010, 0.0017,\n                       0.0013, 0.0016, 0.0014, 0.0018, 0.0015, 0.0012, 0.0016, 0.0011, 0.0014,\n                       0.0014, 0.0020, 0.0027, 0.0013, 0.0014, 0.0013, 0.0012, 0.0028, 0.0018,\n                       0.0016, 0.0014, 0.0014, 0.0020, 0.0021, 0.0017, 0.0016, 0.0013, 0.0012,\n                       0.0014, 0.0019, 0.0010, 0.0021, 0.0021, 0.0015, 0.0017, 0.0023, 0.0020,\n                       0.0016, 0.0021, 0.0016, 0.0017, 0.0013, 0.0015, 0.0014, 0.0018, 0.0021,\n                       0.0030, 0.0013, 0.0020, 0.0013, 0.0014, 0.0013, 0.0015, 0.0011, 0.0017,\n                       0.0011, 0.0018, 0.0015, 0.0015, 0.0016, 0.0010, 0.0024, 0.0016, 0.0021,\n                       0.0016, 0.0018, 0.0018, 0.0017, 0.0013, 0.0022, 0.0017, 0.0011, 0.0015,\n                       0.0031, 0.0019, 0.0016, 0.0016, 0.0016, 0.0015, 0.0023, 0.0014, 0.0019,\n                       0.0013, 0.0014, 0.0017, 0.0015, 0.0015, 0.0018, 0.0016, 0.0016, 0.0016,\n                       0.0015, 0.0027, 0.0017, 0.0016, 0.0019, 0.0020, 0.0011, 0.0013, 0.0015,\n                       0.0015, 0.0032, 0.0016, 0.0011, 0.0014, 0.0015, 0.0017, 0.0018, 0.0012,\n                       0.0014, 0.0023, 0.0017, 0.0014, 0.0014, 0.0010, 0.0013, 0.0018, 0.0016,\n                       0.0029, 0.0025, 0.0012, 0.0018, 0.0018, 0.0024, 0.0014, 0.0015, 0.0018,\n                       0.0027, 0.0017, 0.0037, 0.0017, 0.0010, 0.0018, 0.0022, 0.0013, 0.0014,\n                       0.0014, 0.0011, 0.0016, 0.0012, 0.0016, 0.0019, 0.0020, 0.0014, 0.0014,\n                       0.0012, 0.0014, 0.0014, 0.0018, 0.0018, 0.0012, 0.0022, 0.0015, 0.0014,\n                       0.0019, 0.0016, 0.0019, 0.0018, 0.0011, 0.0021, 0.0024, 0.0020, 0.0014,\n                       0.0014, 0.0018, 0.0011, 0.0017, 0.0022, 0.0016, 0.0016, 0.0018, 0.0015,\n                       0.0026, 0.0013, 0.0014, 0.0019, 0.0013, 0.0018, 0.0013, 0.0018, 0.0015,\n                       0.0024, 0.0017, 0.0017, 0.0010, 0.0021, 0.0014, 0.0012, 0.0011, 0.0013,\n                       0.0014, 0.0012, 0.0014, 0.0018, 0.0017, 0.0012, 0.0028, 0.0014, 0.0009,\n                       0.0020, 0.0016, 0.0019, 0.0014, 0.0018, 0.0017, 0.0015, 0.0023, 0.0011,\n                       0.0016, 0.0014, 0.0017, 0.0020, 0.0028, 0.0013, 0.0035, 0.0014, 0.0016,\n                       0.0030, 0.0026, 0.0014, 0.0021, 0.0017, 0.0015, 0.0019, 0.0013, 0.0013,\n                       0.0012, 0.0017, 0.0018, 0.0017, 0.0017, 0.0017, 0.0033, 0.0015, 0.0020,\n                       0.0027, 0.0022, 0.0027, 0.0013, 0.0021, 0.0011, 0.0018, 0.0015, 0.0017,\n                       0.0016, 0.0013, 0.0020, 0.0013, 0.0023, 0.0016, 0.0013, 0.0017, 0.0017,\n                       0.0013, 0.0017, 0.0014, 0.0014, 0.0015, 0.0020, 0.0015, 0.0010, 0.0023,\n                       0.0012, 0.0014, 0.0019, 0.0020, 0.0015, 0.0013, 0.0018, 0.0011, 0.0017,\n                       0.0015, 0.0020, 0.0012, 0.0016, 0.0013, 0.0015, 0.0028, 0.0032, 0.0013,\n                       0.0033, 0.0015, 0.0017, 0.0021, 0.0015, 0.0015, 0.0030, 0.0018, 0.0013,\n                       0.0014, 0.0014, 0.0018, 0.0026, 0.0015, 0.0014, 0.0014, 0.0028, 0.0021,\n                       0.0017, 0.0019, 0.0017, 0.0017, 0.0020, 0.0014, 0.0010, 0.0016, 0.0019,\n                       0.0019, 0.0020, 0.0011, 0.0018, 0.0018, 0.0015, 0.0012, 0.0018, 0.0016,\n                       0.0028, 0.0013, 0.0014, 0.0016, 0.0022, 0.0012, 0.0031, 0.0020, 0.0014,\n                       0.0027, 0.0013, 0.0016, 0.0018, 0.0011, 0.0029, 0.0018, 0.0015, 0.0018,\n                       0.0021, 0.0014, 0.0023, 0.0026, 0.0016, 0.0013, 0.0016, 0.0014, 0.0016,\n                       0.0013, 0.0031, 0.0017, 0.0010, 0.0016, 0.0014, 0.0017, 0.0012, 0.0012,\n                       0.0016, 0.0015, 0.0016, 0.0020, 0.0026, 0.0015, 0.0014, 0.0017, 0.0018,\n                       0.0022, 0.0020, 0.0022, 0.0022, 0.0017, 0.0012, 0.0014, 0.0017, 0.0015,\n                       0.0029, 0.0017, 0.0019, 0.0023, 0.0015, 0.0013, 0.0015, 0.0015, 0.0016,\n                       0.0014, 0.0019, 0.0019, 0.0013, 0.0028, 0.0015, 0.0019, 0.0014, 0.0014,\n                       0.0032, 0.0012, 0.0017, 0.0015, 0.0011, 0.0015, 0.0018, 0.0016, 0.0015,\n                       0.0019, 0.0012, 0.0015, 0.0012, 0.0012, 0.0015, 0.0016, 0.0021, 0.0014,\n                       0.0013, 0.0013, 0.0019, 0.0012, 0.0017, 0.0017, 0.0022, 0.0013, 0.0013,\n                       0.0015, 0.0022, 0.0014, 0.0027, 0.0020, 0.0014, 0.0015, 0.0014, 0.0019,\n                       0.0015, 0.0015, 0.0014, 0.0020, 0.0014, 0.0019, 0.0013, 0.0031, 0.0016,\n                       0.0019, 0.0016, 0.0016, 0.0013, 0.0012, 0.0014, 0.0020, 0.0018, 0.0018,\n                       0.0013, 0.0013, 0.0012, 0.0015, 0.0015, 0.0029, 0.0012, 0.0013, 0.0013,\n                       0.0032, 0.0015, 0.0021, 0.0013, 0.0019, 0.0018, 0.0013, 0.0019, 0.0012,\n                       0.0018, 0.0015, 0.0015, 0.0014, 0.0024, 0.0017, 0.0016, 0.0012, 0.0014,\n                       0.0015, 0.0027, 0.0018, 0.0012, 0.0015, 0.0018, 0.0011, 0.0012, 0.0019,\n                       0.0013, 0.0020, 0.0027, 0.0013, 0.0018, 0.0015, 0.0022, 0.0012, 0.0014,\n                       0.0027, 0.0025, 0.0022, 0.0014, 0.0016, 0.0026, 0.0014, 0.0027, 0.0015,\n                       0.0016, 0.0013, 0.0018, 0.0017, 0.0012, 0.0017, 0.0019, 0.0018, 0.0013,\n                       0.0016, 0.0018, 0.0015, 0.0019, 0.0019, 0.0015, 0.0016, 0.0013, 0.0017,\n                       0.0016, 0.0012, 0.0013, 0.0013, 0.0022, 0.0018, 0.0020, 0.0014, 0.0017,\n                       0.0015, 0.0016, 0.0016, 0.0026, 0.0015, 0.0019, 0.0019, 0.0020, 0.0012,\n                       0.0013, 0.0013, 0.0012, 0.0018, 0.0015, 0.0013, 0.0016, 0.0014, 0.0013,\n                       0.0026, 0.0013, 0.0014, 0.0018, 0.0013, 0.0016, 0.0019, 0.0014, 0.0017,\n                       0.0016, 0.0016, 0.0017, 0.0019, 0.0014, 0.0013, 0.0027, 0.0010, 0.0014,\n                       0.0013, 0.0016, 0.0015, 0.0012, 0.0016, 0.0016, 0.0018, 0.0015, 0.0024,\n                       0.0027, 0.0019, 0.0014, 0.0022, 0.0016, 0.0013, 0.0017, 0.0016, 0.0012,\n                       0.0025, 0.0017, 0.0014, 0.0013, 0.0024, 0.0014, 0.0017, 0.0013, 0.0013,\n                       0.0021, 0.0015, 0.0019, 0.0011, 0.0010, 0.0015, 0.0013, 0.0025, 0.0017,\n                       0.0017, 0.0017, 0.0017, 0.0017, 0.0014, 0.0028, 0.0020, 0.0017, 0.0018,\n                       0.0019, 0.0012, 0.0014, 0.0015, 0.0016, 0.0013, 0.0019, 0.0013, 0.0013,\n                       0.0033, 0.0017, 0.0025, 0.0015, 0.0025, 0.0014, 0.0010, 0.0018, 0.0012,\n                       0.0012, 0.0027, 0.0022, 0.0015, 0.0014, 0.0012, 0.0018, 0.0013, 0.0019,\n                       0.0014, 0.0018, 0.0013, 0.0013, 0.0012, 0.0016, 0.0012, 0.0015, 0.0027,\n                       0.0010, 0.0012, 0.0017, 0.0017, 0.0018, 0.0013, 0.0020, 0.0014, 0.0012,\n                       0.0015, 0.0024, 0.0013, 0.0014, 0.0013, 0.0013, 0.0010, 0.0027, 0.0012,\n                       0.0016, 0.0009, 0.0016, 0.0016, 0.0014, 0.0026, 0.0013, 0.0013, 0.0019,\n                       0.0010, 0.0015, 0.0014, 0.0018, 0.0016, 0.0018, 0.0012, 0.0018, 0.0012,\n                       0.0015, 0.0026, 0.0012], dtype=torch.float64),\n                      zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n                      axis=0),\n               Parameter containing:\n               tensor([-0.0590, -0.0525, -0.1467, -0.0513, -0.0809, -0.0107, -0.0729, -0.0632,\n                       -0.0147, -0.0857, -0.0435, -0.0224, -0.0685, -0.0435, -0.0830, -0.0462,\n                       -0.1278, -0.0851, -0.1173, -0.0169, -0.0261, -0.0623, -0.1012, -0.0497,\n                       -0.0295, -0.0575, -0.0891, -0.0680, -0.0566, -0.0501, -0.0385, -0.0211,\n                       -0.0360, -0.0665, -0.0798, -0.0639, -0.1154, -0.0752, -0.0686, -0.0408,\n                       -0.0361, -0.0357, -0.0012, -0.0828, -0.0243,  0.0011, -0.0357, -0.0494,\n                       -0.0203, -0.0393, -0.0213, -0.0271, -0.0243, -0.0611, -0.0175, -0.0392,\n                       -0.0569, -0.0221, -0.0326, -0.0437, -0.0401, -0.0197, -0.0450, -0.0684,\n                       -0.0329, -0.0781, -0.0535, -0.0371, -0.0788, -0.0806, -0.0102, -0.0131,\n                       -0.0652, -0.0014, -0.0447, -0.0467, -0.0235, -0.0199, -0.0250, -0.0566,\n                       -0.0662,  0.0260, -0.0574, -0.0429, -0.0638, -0.0296, -0.0645, -0.0643,\n                       -0.0330,  0.0024, -0.0791, -0.0244, -0.0589, -0.0621, -0.0429, -0.0838,\n                       -0.0411, -0.0532, -0.0423, -0.0232, -0.0401, -0.0625, -0.0317, -0.0976,\n                       -0.0768, -0.0587, -0.0403, -0.0106, -0.0051, -0.0334, -0.0312, -0.0618,\n                       -0.0439, -0.0388, -0.1188, -0.0506, -0.0640, -0.0930, -0.0610, -0.0416,\n                       -0.0177, -0.0309, -0.0472, -0.0430, -0.1019, -0.0126, -0.0174, -0.0178,\n                       -0.0281, -0.0255, -0.0334, -0.0638, -0.0220, -0.0197, -0.0486, -0.0327,\n                       -0.0906, -0.0197, -0.0764, -0.0633, -0.0215, -0.0769, -0.0899, -0.0430,\n                       -0.0734, -0.0409, -0.0540, -0.0490, -0.0311, -0.0565, -0.0490,  0.0002,\n                       -0.0601, -0.0369, -0.0305, -0.0966, -0.0568, -0.0346, -0.0248, -0.0347,\n                       -0.0424, -0.0209,  0.0091, -0.0430, -0.0335, -0.0721, -0.0224, -0.0720,\n                       -0.0354, -0.0621, -0.0404, -0.0378, -0.0377, -0.0170, -0.0227, -0.0433,\n                       -0.0314,  0.0032, -0.1035, -0.0454, -0.0678, -0.0422, -0.0182, -0.0108,\n                       -0.0878, -0.1016, -0.0259, -0.0714, -0.0744, -0.1035, -0.0434, -0.0554,\n                       -0.0557, -0.0892, -0.0055, -0.0982, -0.0359, -0.0679, -0.0534, -0.0455,\n                       -0.0516, -0.0666, -0.0425, -0.0229, -0.0804, -0.0545, -0.0092, -0.0441,\n                       -0.0395, -0.0741, -0.0283, -0.0331, -0.0448, -0.0143, -0.0600, -0.0493,\n                       -0.0273, -0.0235, -0.0456, -0.0593, -0.0172, -0.0441, -0.0349, -0.0575,\n                       -0.0295, -0.0625, -0.0696, -0.0437, -0.0186, -0.0239, -0.0428, -0.0332,\n                       -0.0609, -0.0521, -0.0540, -0.0504, -0.0021, -0.0303, -0.0279, -0.0342,\n                       -0.0544, -0.0640, -0.0691, -0.0602, -0.0748, -0.0088, -0.0593, -0.0672,\n                       -0.0908, -0.0320, -0.0751, -0.0580, -0.0690, -0.0614, -0.0343, -0.0531,\n                       -0.0170, -0.0447, -0.0594, -0.0425, -0.0597, -0.0588, -0.0256, -0.0475,\n                       -0.0699, -0.0042, -0.0717, -0.0446, -0.0730,  0.0239, -0.0255, -0.0382,\n                       -0.0292, -0.0739, -0.0649, -0.0342, -0.0715, -0.0680, -0.0453, -0.0336,\n                       -0.0132, -0.0587, -0.0625, -0.0589, -0.0687, -0.0387, -0.0590, -0.0713,\n                       -0.0324, -0.0237, -0.0106, -0.0342, -0.0311, -0.0362, -0.0437, -0.0454,\n                       -0.0729, -0.0495, -0.0277, -0.0609, -0.0404, -0.0342, -0.0268,  0.0170,\n                       -0.0147, -0.0037, -0.0199, -0.0632, -0.0172, -0.0262, -0.0278, -0.0115,\n                       -0.0409, -0.0415, -0.0243, -0.0827, -0.0665, -0.0892, -0.0486, -0.0639,\n                       -0.0399,  0.0229, -0.0577, -0.0936, -0.0367, -0.0551, -0.0492, -0.0786,\n                       -0.0644, -0.0662, -0.0430, -0.0400, -0.0635, -0.0703, -0.0361, -0.0395,\n                       -0.0646, -0.1026, -0.0522, -0.0190, -0.0691, -0.0524, -0.0729, -0.0405,\n                       -0.0751, -0.0586, -0.0474, -0.0515, -0.0339, -0.0251, -0.0060, -0.0943,\n                       -0.0807, -0.0468, -0.0276, -0.0646, -0.0708, -0.1169, -0.0280, -0.0192,\n                       -0.0349, -0.0145, -0.0896, -0.0924, -0.0774,  0.0125, -0.0296, -0.0122,\n                       -0.0482, -0.0380, -0.0663, -0.0132, -0.0694, -0.0661, -0.0517, -0.0439,\n                       -0.0283, -0.0705, -0.0546, -0.0472, -0.0123, -0.0433, -0.1216, -0.0601,\n                       -0.0324,  0.0088, -0.0123, -0.0229, -0.0679, -0.0513, -0.0075, -0.0752,\n                       -0.0155, -0.0574, -0.0382, -0.0583, -0.0820, -0.0313, -0.0164, -0.0586,\n                       -0.0708, -0.0410, -0.0782, -0.0451, -0.0156, -0.0654, -0.0487, -0.0809,\n                       -0.0381, -0.0386, -0.0311,  0.0095, -0.0639, -0.0499, -0.1210, -0.0357,\n                       -0.0500, -0.0184, -0.0439, -0.0270, -0.0408, -0.0699, -0.0263, -0.0813,\n                       -0.0469, -0.0606, -0.0219, -0.0467, -0.0153, -0.0324, -0.0120, -0.0583,\n                       -0.0293,  0.0128, -0.0788, -0.0057, -0.0088, -0.0472, -0.0567, -0.0569,\n                       -0.0400, -0.0739, -0.0060, -0.0482, -0.0643, -0.0379, -0.0235, -0.0275,\n                       -0.0956, -0.0755, -0.0572, -0.0493, -0.0724, -0.0179, -0.0571, -0.0079,\n                       -0.0679, -0.0424, -0.1185, -0.0520, -0.0650, -0.0444, -0.0361, -0.0153,\n                       -0.0416, -0.0444, -0.0297, -0.0300, -0.0524, -0.0401, -0.0359, -0.0439,\n                        0.0163, -0.0693, -0.0222, -0.0484, -0.0255, -0.0609, -0.0195, -0.0194,\n                       -0.0705, -0.0102, -0.0881, -0.0532, -0.0216, -0.0583, -0.0492, -0.0560,\n                       -0.0318, -0.0859, -0.0610, -0.0531, -0.1028, -0.0703, -0.0536, -0.0343,\n                       -0.0454, -0.0697, -0.0374,  0.0036, -0.0358, -0.0875, -0.0163, -0.0253,\n                       -0.0370, -0.0157, -0.0127, -0.0477, -0.0889, -0.0819, -0.0555, -0.0414,\n                       -0.0961, -0.0342, -0.0707, -0.0943, -0.0399, -0.0031, -0.0771, -0.0543,\n                       -0.0445, -0.0219, -0.0677, -0.0642, -0.0774, -0.0163, -0.0648, -0.0574,\n                       -0.0395, -0.0336, -0.0597, -0.0563, -0.0357, -0.0269, -0.0371, -0.0590,\n                       -0.0492, -0.0575, -0.0709, -0.0306, -0.0300, -0.0408, -0.0691, -0.0272,\n                       -0.0743, -0.1082, -0.0265, -0.0620, -0.0760, -0.0843, -0.0630, -0.0650,\n                       -0.0565, -0.0091, -0.0787, -0.1001, -0.0198, -0.0523, -0.0510, -0.0350,\n                       -0.0784, -0.0171, -0.0383, -0.0895, -0.0232, -0.0600, -0.0961, -0.0672,\n                       -0.0142, -0.0256, -0.0296, -0.0186, -0.0378, -0.0634, -0.0649, -0.0594,\n                       -0.0743, -0.0169, -0.0521, -0.0527, -0.0872, -0.0370, -0.0874,  0.0128,\n                       -0.0467, -0.0431, -0.0344, -0.0697, -0.0585, -0.0719, -0.0472, -0.0244,\n                       -0.0297, -0.0303, -0.0315, -0.0690, -0.0750, -0.0598, -0.0582, -0.0278,\n                       -0.0747, -0.0301, -0.0264, -0.0611, -0.0642, -0.0126, -0.0548, -0.0249,\n                       -0.0901, -0.0405, -0.0124, -0.0595, -0.0505, -0.0394, -0.0333, -0.0153,\n                       -0.0087, -0.0201, -0.0020, -0.0478, -0.0811, -0.0143, -0.0488,  0.0598,\n                       -0.0240, -0.0171, -0.0611, -0.0809, -0.0373, -0.0847, -0.0140, -0.0703,\n                       -0.0365, -0.0664, -0.0325, -0.0214, -0.0067, -0.0784, -0.0040,  0.0030,\n                       -0.0204, -0.1136, -0.0294, -0.0579, -0.0458, -0.0303, -0.0826, -0.0384,\n                       -0.0459, -0.0268, -0.0494, -0.0538, -0.0460, -0.0819, -0.0913, -0.0786,\n                       -0.0281, -0.0582, -0.0359, -0.0665, -0.0412, -0.0504, -0.0226, -0.0413,\n                       -0.0615, -0.0124, -0.0333, -0.0211, -0.0672, -0.0777, -0.0389, -0.0331,\n                       -0.0684, -0.0080, -0.0234, -0.0617, -0.0069, -0.0810, -0.0709, -0.0752,\n                       -0.0847, -0.0451, -0.0262, -0.0371, -0.0311, -0.0822, -0.0351, -0.0211,\n                       -0.0240, -0.0291, -0.0619, -0.0631, -0.0099, -0.0573, -0.0103, -0.0851,\n                       -0.0969, -0.0256, -0.0717, -0.0494, -0.0505, -0.0144,  0.0108, -0.0495,\n                       -0.0546, -0.0274, -0.0163, -0.0770, -0.0038, -0.0450, -0.0779, -0.0810,\n                        0.0099, -0.0375, -0.0904, -0.0461, -0.0789, -0.0747, -0.0209, -0.0460,\n                       -0.0319, -0.0798, -0.0167, -0.0284, -0.0495, -0.0611, -0.0738, -0.0895,\n                       -0.0313, -0.0100, -0.0789, -0.0481, -0.0762, -0.0920, -0.0392, -0.0572,\n                       -0.0365, -0.0220, -0.0264, -0.1039, -0.0627, -0.0435, -0.0482, -0.0755,\n                       -0.0131, -0.0240, -0.0355, -0.0221, -0.0501, -0.1247, -0.0501, -0.0852,\n                       -0.0595, -0.0405, -0.0958, -0.0281, -0.0371, -0.0806, -0.0762, -0.0811,\n                       -0.0684, -0.0295, -0.0723, -0.0242, -0.0679, -0.0446, -0.0577, -0.0124],\n                      requires_grad=True))),\n             ('features.3.0.block.5.scale', tensor(0.0703)),\n             ('features.3.0.block.5.zero_point', tensor(97)),\n             ('features.3.0.block.5._packed_params.dtype', torch.qint8),\n             ('features.3.0.block.5._packed_params._packed_params',\n              (tensor([[-0.0046, -0.1811,  0.0229,  ..., -0.0023, -0.0413, -0.0298],\n                       [-0.0242, -0.0372,  0.0149,  ..., -0.0037,  0.0372,  0.0688],\n                       [ 0.0115,  0.0016, -0.1522,  ..., -0.0311, -0.0246, -0.0540],\n                       ...,\n                       [-0.0087, -0.0070,  0.1085,  ..., -0.0157, -0.0017,  0.0035],\n                       [ 0.0049,  0.0000, -0.0392,  ...,  0.0490, -0.0074,  0.0147],\n                       [-0.0200,  0.1426, -0.0100,  ..., -0.0242, -0.1369, -0.0157]],\n                      size=(192, 768), dtype=torch.qint8,\n                      quantization_scheme=torch.per_channel_affine,\n                      scale=tensor([0.0023, 0.0019, 0.0016, 0.0023, 0.0021, 0.0016, 0.0024, 0.0017, 0.0018,\n                       0.0015, 0.0017, 0.0015, 0.0030, 0.0018, 0.0018, 0.0019, 0.0017, 0.0025,\n                       0.0023, 0.0025, 0.0019, 0.0019, 0.0029, 0.0018, 0.0015, 0.0015, 0.0028,\n                       0.0019, 0.0017, 0.0019, 0.0019, 0.0016, 0.0027, 0.0019, 0.0016, 0.0020,\n                       0.0014, 0.0019, 0.0026, 0.0017, 0.0018, 0.0018, 0.0017, 0.0016, 0.0027,\n                       0.0017, 0.0015, 0.0018, 0.0015, 0.0017, 0.0017, 0.0020, 0.0016, 0.0030,\n                       0.0023, 0.0023, 0.0018, 0.0015, 0.0018, 0.0020, 0.0028, 0.0018, 0.0013,\n                       0.0017, 0.0015, 0.0019, 0.0017, 0.0015, 0.0014, 0.0015, 0.0022, 0.0021,\n                       0.0020, 0.0018, 0.0017, 0.0016, 0.0020, 0.0019, 0.0017, 0.0018, 0.0017,\n                       0.0019, 0.0015, 0.0020, 0.0023, 0.0023, 0.0018, 0.0026, 0.0023, 0.0039,\n                       0.0021, 0.0015, 0.0025, 0.0015, 0.0023, 0.0021, 0.0020, 0.0020, 0.0015,\n                       0.0025, 0.0018, 0.0017, 0.0024, 0.0013, 0.0016, 0.0032, 0.0016, 0.0040,\n                       0.0016, 0.0018, 0.0018, 0.0026, 0.0018, 0.0017, 0.0020, 0.0019, 0.0015,\n                       0.0017, 0.0025, 0.0024, 0.0015, 0.0019, 0.0026, 0.0021, 0.0014, 0.0017,\n                       0.0015, 0.0025, 0.0015, 0.0019, 0.0018, 0.0028, 0.0024, 0.0014, 0.0020,\n                       0.0023, 0.0021, 0.0033, 0.0031, 0.0018, 0.0018, 0.0018, 0.0028, 0.0017,\n                       0.0021, 0.0020, 0.0019, 0.0016, 0.0020, 0.0027, 0.0024, 0.0018, 0.0021,\n                       0.0021, 0.0022, 0.0020, 0.0021, 0.0025, 0.0019, 0.0016, 0.0018, 0.0015,\n                       0.0017, 0.0019, 0.0016, 0.0015, 0.0014, 0.0022, 0.0030, 0.0015, 0.0015,\n                       0.0017, 0.0015, 0.0022, 0.0018, 0.0022, 0.0020, 0.0019, 0.0019, 0.0014,\n                       0.0020, 0.0018, 0.0022, 0.0016, 0.0022, 0.0015, 0.0023, 0.0017, 0.0021,\n                       0.0017, 0.0025, 0.0014], dtype=torch.float64),\n                      zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n                      axis=0),\n               Parameter containing:\n               tensor([-6.1623e-02,  1.9995e-02,  4.1015e-06,  1.1449e-01,  1.0776e-02,\n                        3.9946e-03,  1.5960e-02, -6.3712e-02,  5.8321e-02, -3.1742e-02,\n                       -4.4751e-02, -1.0560e-02,  2.2220e-02,  6.9677e-03, -7.4022e-03,\n                       -3.4031e-02, -2.2603e-02, -7.2341e-03, -2.9722e-02,  1.2109e-02,\n                        2.6767e-02,  5.7199e-04, -1.8040e-02,  2.7842e-02, -8.6475e-03,\n                        9.7157e-03, -1.0417e-03,  5.2974e-02, -2.5661e-02, -6.5215e-03,\n                        4.6892e-02,  2.2365e-02,  9.9338e-03, -5.2784e-02,  1.1091e-02,\n                        1.2716e-02, -3.0968e-02,  1.0301e-02, -6.9191e-03, -1.6696e-02,\n                        3.0915e-02, -1.5995e-02, -1.4947e-02,  2.0734e-02, -2.8406e-02,\n                       -9.9421e-03, -2.3884e-02,  3.8150e-03,  2.0219e-02, -2.1348e-02,\n                        5.2705e-02, -5.7729e-03, -4.0309e-03, -4.6321e-02, -1.0576e-02,\n                       -1.6172e-02,  2.0459e-02, -3.3445e-03, -1.0580e-02,  6.6543e-04,\n                       -3.3387e-03,  2.1776e-02, -6.3359e-03,  3.9531e-02,  1.9620e-03,\n                       -1.8118e-03,  9.0667e-03, -2.1507e-02,  7.8047e-03, -8.2566e-03,\n                        5.2183e-02, -3.1381e-02, -2.3246e-02, -2.9309e-02, -2.3482e-02,\n                        5.3426e-03,  4.1285e-02,  1.5001e-02, -3.4474e-03,  4.9233e-02,\n                        8.5536e-03, -2.8457e-02,  2.3422e-02, -3.5372e-02, -3.2195e-02,\n                       -2.2519e-02,  7.2991e-03, -1.3163e-02, -5.8514e-04, -6.2597e-01,\n                        1.5822e-02,  3.8178e-02, -1.3214e-02, -4.8907e-03,  9.4913e-03,\n                        9.1288e-03,  2.6800e-03,  2.5795e-02, -2.1540e-02, -5.8221e-02,\n                       -5.4356e-02, -2.2269e-02,  5.1586e-03, -1.5468e-03,  8.7635e-03,\n                       -2.5951e-03,  5.4178e-02, -5.0643e-02, -4.5150e-02,  7.2622e-03,\n                        7.7189e-04, -5.6141e-02,  6.5265e-02, -6.3240e-02, -4.8239e-03,\n                        1.2618e-04,  4.4607e-03,  8.3137e-02, -1.8600e-03,  3.6267e-03,\n                        4.1586e-03, -6.5701e-03, -2.5591e-03, -2.5930e-03,  2.4566e-04,\n                        1.4088e-03, -1.5990e-02,  4.0110e-03,  9.8644e-02,  2.7840e-02,\n                        4.7195e-02, -2.6926e-02, -2.6459e-02, -7.5540e-03,  1.4381e-02,\n                        6.5473e-02,  4.4800e-03, -7.9864e-03,  1.4571e-01,  9.1045e-04,\n                        7.2852e-03,  4.4615e-02, -5.3317e-03, -1.5126e-02, -5.1176e-03,\n                       -1.5064e-02,  1.0550e-02, -2.1814e-02,  3.0312e-03,  6.8559e-03,\n                       -6.6707e-03,  5.6620e-02,  3.2137e-03, -1.8969e-03,  2.9231e-02,\n                       -2.9253e-02,  8.1641e-03,  3.7531e-02,  6.7940e-02, -8.9756e-03,\n                       -8.1335e-03,  2.4349e-02, -2.8714e-03,  8.6201e-03, -1.7962e-03,\n                        9.8995e-03, -1.1789e-01,  1.0527e-02, -6.6284e-03, -2.3737e-02,\n                        2.5049e-03,  2.2645e-02,  3.5956e-02,  5.5279e-02, -7.5152e-03,\n                       -4.9019e-02,  1.3246e-02, -3.5240e-02,  2.2649e-02,  8.5555e-03,\n                        1.2650e-02, -2.6455e-02, -1.0326e-02,  1.7632e-02,  1.3252e-02,\n                        1.1714e-02, -7.1126e-03, -2.5298e-02, -7.4589e-03, -9.1866e-03,\n                        1.9633e-02,  3.1272e-03], requires_grad=True))),\n             ('features.3.1.layer_scale',\n              tensor([[[ 0.1516]],\n              \n                      [[-0.1666]],\n              \n                      [[ 0.1419]],\n              \n                      [[ 0.1531]],\n              \n                      [[ 0.0690]],\n              \n                      [[ 0.2174]],\n              \n                      [[ 0.1746]],\n              \n                      [[-0.1286]],\n              \n                      [[ 0.2433]],\n              \n                      [[ 0.1183]],\n              \n                      [[-0.1485]],\n              \n                      [[-0.0915]],\n              \n                      [[ 0.1983]],\n              \n                      [[ 0.0802]],\n              \n                      [[-0.2347]],\n              \n                      [[-0.1649]],\n              \n                      [[ 0.2225]],\n              \n                      [[-0.1476]],\n              \n                      [[-0.0846]],\n              \n                      [[ 0.0204]],\n              \n                      [[ 0.2237]],\n              \n                      [[-0.2618]],\n              \n                      [[-0.1982]],\n              \n                      [[ 0.0910]],\n              \n                      [[-0.1914]],\n              \n                      [[-0.0896]],\n              \n                      [[ 0.0327]],\n              \n                      [[-0.1553]],\n              \n                      [[ 0.1935]],\n              \n                      [[-0.1725]],\n              \n                      [[-0.0927]],\n              \n                      [[ 0.1303]],\n              \n                      [[ 0.1594]],\n              \n                      [[-0.1383]],\n              \n                      [[ 0.1303]],\n              \n                      [[-0.2211]],\n              \n                      [[-0.1453]],\n              \n                      [[-0.0864]],\n              \n                      [[ 0.1889]],\n              \n                      [[ 0.0813]],\n              \n                      [[ 0.0614]],\n              \n                      [[ 0.1821]],\n              \n                      [[-0.0589]],\n              \n                      [[ 0.1901]],\n              \n                      [[-0.2035]],\n              \n                      [[ 0.1466]],\n              \n                      [[ 0.2513]],\n              \n                      [[-0.1019]],\n              \n                      [[-0.1196]],\n              \n                      [[ 0.1604]],\n              \n                      [[ 0.1467]],\n              \n                      [[-0.1719]],\n              \n                      [[ 0.0437]],\n              \n                      [[ 0.2226]],\n              \n                      [[-0.1749]],\n              \n                      [[-0.2768]],\n              \n                      [[-0.0495]],\n              \n                      [[ 0.0906]],\n              \n                      [[-0.1362]],\n              \n                      [[-0.0824]],\n              \n                      [[ 0.0861]],\n              \n                      [[ 0.0984]],\n              \n                      [[ 0.2500]],\n              \n                      [[ 0.1176]],\n              \n                      [[ 0.1237]],\n              \n                      [[ 0.1523]],\n              \n                      [[ 0.1509]],\n              \n                      [[-0.0887]],\n              \n                      [[ 0.1557]],\n              \n                      [[-0.1901]],\n              \n                      [[-0.1225]],\n              \n                      [[ 0.1127]],\n              \n                      [[ 0.1267]],\n              \n                      [[-0.1647]],\n              \n                      [[ 0.1497]],\n              \n                      [[ 0.0937]],\n              \n                      [[-0.1524]],\n              \n                      [[-0.1393]],\n              \n                      [[-0.1414]],\n              \n                      [[ 0.1496]],\n              \n                      [[ 0.1116]],\n              \n                      [[ 0.1264]],\n              \n                      [[ 0.2005]],\n              \n                      [[ 0.1751]],\n              \n                      [[ 0.1928]],\n              \n                      [[-0.0787]],\n              \n                      [[-0.1469]],\n              \n                      [[-0.2032]],\n              \n                      [[-0.1865]],\n              \n                      [[-0.5665]],\n              \n                      [[ 0.2138]],\n              \n                      [[ 0.1112]],\n              \n                      [[-0.1121]],\n              \n                      [[ 0.1183]],\n              \n                      [[-0.0399]],\n              \n                      [[ 0.1193]],\n              \n                      [[-0.1621]],\n              \n                      [[-0.1285]],\n              \n                      [[-0.1871]],\n              \n                      [[-0.1429]],\n              \n                      [[ 0.1485]],\n              \n                      [[-0.1844]],\n              \n                      [[ 0.1189]],\n              \n                      [[-0.2699]],\n              \n                      [[ 0.0842]],\n              \n                      [[ 0.0729]],\n              \n                      [[-0.1496]],\n              \n                      [[ 0.2075]],\n              \n                      [[-0.1311]],\n              \n                      [[ 0.1650]],\n              \n                      [[ 0.0387]],\n              \n                      [[-0.1532]],\n              \n                      [[-0.1851]],\n              \n                      [[ 0.1967]],\n              \n                      [[-0.1145]],\n              \n                      [[ 0.0487]],\n              \n                      [[ 0.2136]],\n              \n                      [[ 0.1644]],\n              \n                      [[ 0.1658]],\n              \n                      [[-0.1980]],\n              \n                      [[-0.1363]],\n              \n                      [[-0.2269]],\n              \n                      [[ 0.1321]],\n              \n                      [[-0.0548]],\n              \n                      [[ 0.1523]],\n              \n                      [[-0.1735]],\n              \n                      [[-0.2278]],\n              \n                      [[ 0.0713]],\n              \n                      [[ 0.2366]],\n              \n                      [[ 0.1928]],\n              \n                      [[-0.2271]],\n              \n                      [[ 0.1375]],\n              \n                      [[-0.0946]],\n              \n                      [[-0.1431]],\n              \n                      [[ 0.0770]],\n              \n                      [[ 0.1071]],\n              \n                      [[-0.1538]],\n              \n                      [[-0.0308]],\n              \n                      [[ 0.1351]],\n              \n                      [[ 0.0907]],\n              \n                      [[-0.2036]],\n              \n                      [[ 0.2330]],\n              \n                      [[-0.0314]],\n              \n                      [[-0.1660]],\n              \n                      [[-0.1645]],\n              \n                      [[-0.2550]],\n              \n                      [[ 0.1557]],\n              \n                      [[-0.1956]],\n              \n                      [[ 0.2485]],\n              \n                      [[-0.1739]],\n              \n                      [[ 0.1149]],\n              \n                      [[-0.2723]],\n              \n                      [[ 0.0789]],\n              \n                      [[ 0.2175]],\n              \n                      [[ 0.1291]],\n              \n                      [[ 0.1462]],\n              \n                      [[-0.2003]],\n              \n                      [[-0.1758]],\n              \n                      [[-0.1984]],\n              \n                      [[-0.0882]],\n              \n                      [[-0.0972]],\n              \n                      [[ 0.2172]],\n              \n                      [[-0.1035]],\n              \n                      [[ 0.0793]],\n              \n                      [[ 0.1086]],\n              \n                      [[-0.1298]],\n              \n                      [[ 0.0928]],\n              \n                      [[-0.1382]],\n              \n                      [[ 0.1613]],\n              \n                      [[-0.1140]],\n              \n                      [[-0.0867]],\n              \n                      [[ 0.2622]],\n              \n                      [[-0.1143]],\n              \n                      [[-0.1262]],\n              \n                      [[-0.1178]],\n              \n                      [[-0.0641]],\n              \n                      [[ 0.0523]],\n              \n                      [[-0.1425]],\n              \n                      [[-0.1724]],\n              \n                      [[ 0.2153]],\n              \n                      [[ 0.1742]],\n              \n                      [[ 0.2043]],\n              \n                      [[-0.1623]],\n              \n                      [[-0.1100]],\n              \n                      [[ 0.1925]],\n              \n                      [[-0.0933]],\n              \n                      [[ 0.1387]],\n              \n                      [[ 0.1079]],\n              \n                      [[ 0.0762]],\n              \n                      [[-0.1426]],\n              \n                      [[-0.3123]],\n              \n                      [[-0.1330]]])),\n             ('features.3.1.block.0.weight',\n              tensor([[[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n                        [ 0.0024,  0.0047,  0.0047,  ...,  0.0000,  0.0000,  0.0000],\n                        [ 0.0000,  0.0024,  0.0071,  ...,  0.0188,  0.0024,  0.0024],\n                        ...,\n                        [ 0.0000,  0.0024,  0.0118,  ..., -0.0071,  0.0000,  0.0000],\n                        [ 0.0000,  0.0000,  0.0000,  ...,  0.0024,  0.0024,  0.0024],\n                        [ 0.0024,  0.0000,  0.0000,  ...,  0.0000,  0.0024,  0.0024]]],\n              \n              \n                      [[[ 0.0000,  0.0000, -0.0026,  ...,  0.0026,  0.0000, -0.0026],\n                        [ 0.0000,  0.0000, -0.0026,  ...,  0.0000, -0.0026, -0.0026],\n                        [ 0.0000,  0.0000, -0.0103,  ..., -0.0077, -0.0026,  0.0000],\n                        ...,\n                        [-0.0026, -0.0026, -0.0051,  ...,  0.0000, -0.0077,  0.0000],\n                        [-0.0026,  0.0000,  0.0000,  ..., -0.0026, -0.0026, -0.0051],\n                        [-0.0026,  0.0000,  0.0000,  ..., -0.0026, -0.0026, -0.0026]]],\n              \n              \n                      [[[-0.0081,  0.0027, -0.0027,  ...,  0.0027,  0.0027, -0.0027],\n                        [ 0.0000, -0.0027, -0.0027,  ...,  0.0054,  0.0000,  0.0000],\n                        [ 0.0000,  0.0000, -0.0054,  ..., -0.0108,  0.0000,  0.0000],\n                        ...,\n                        [ 0.0000, -0.0054, -0.0108,  ..., -0.0162,  0.0027, -0.0027],\n                        [-0.0027,  0.0027, -0.0027,  ...,  0.0000,  0.0027, -0.0027],\n                        [ 0.0000, -0.0027,  0.0027,  ...,  0.0000, -0.0027,  0.0000]]],\n              \n              \n                      ...,\n              \n              \n                      [[[-0.0022,  0.0000,  0.0000,  ...,  0.0000,  0.0000, -0.0022],\n                        [ 0.0000,  0.0000,  0.0000,  ...,  0.0022,  0.0000, -0.0022],\n                        [ 0.0022,  0.0044,  0.0066,  ..., -0.0022,  0.0022,  0.0022],\n                        ...,\n                        [ 0.0022,  0.0022,  0.0022,  ..., -0.0110, -0.0044,  0.0000],\n                        [ 0.0000,  0.0000,  0.0044,  ..., -0.0022, -0.0022,  0.0022],\n                        [ 0.0000,  0.0000,  0.0022,  ...,  0.0000,  0.0022,  0.0000]]],\n              \n              \n                      [[[ 0.0000,  0.0000,  0.0115,  ...,  0.0023,  0.0023,  0.0046],\n                        [ 0.0046, -0.0023,  0.0161,  ...,  0.0023,  0.0023,  0.0000],\n                        [ 0.0000,  0.0092, -0.0046,  ...,  0.0092,  0.0000,  0.0000],\n                        ...,\n                        [ 0.0092,  0.0207, -0.0598,  ...,  0.0138,  0.0023,  0.0046],\n                        [ 0.0000,  0.0023,  0.0184,  ..., -0.0023,  0.0000,  0.0023],\n                        [ 0.0023,  0.0000,  0.0138,  ...,  0.0023,  0.0023,  0.0069]]],\n              \n              \n                      [[[ 0.0024,  0.0000,  0.0000,  ...,  0.0000,  0.0049,  0.0024],\n                        [ 0.0024,  0.0049,  0.0000,  ..., -0.0024,  0.0000,  0.0000],\n                        [ 0.0000,  0.0024, -0.0024,  ..., -0.0049,  0.0000,  0.0024],\n                        ...,\n                        [ 0.0049,  0.0121,  0.0267,  ...,  0.0243,  0.0097,  0.0000],\n                        [ 0.0000,  0.0000,  0.0121,  ...,  0.0049,  0.0049,  0.0024],\n                        [ 0.0024,  0.0024,  0.0049,  ...,  0.0073,  0.0049,  0.0024]]]],\n                     size=(192, 1, 7, 7), dtype=torch.qint8,\n                     quantization_scheme=torch.per_channel_affine,\n                     scale=tensor([0.0024, 0.0026, 0.0027, 0.0015, 0.0023, 0.0028, 0.0025, 0.0025, 0.0027,\n                      0.0022, 0.0027, 0.0023, 0.0020, 0.0020, 0.0026, 0.0017, 0.0027, 0.0020,\n                      0.0017, 0.0016, 0.0026, 0.0026, 0.0027, 0.0013, 0.0031, 0.0023, 0.0014,\n                      0.0026, 0.0028, 0.0026, 0.0023, 0.0027, 0.0023, 0.0026, 0.0023, 0.0027,\n                      0.0021, 0.0019, 0.0024, 0.0020, 0.0012, 0.0029, 0.0022, 0.0025, 0.0023,\n                      0.0001, 0.0028, 0.0024, 0.0025, 0.0019, 0.0026, 0.0028, 0.0005, 0.0017,\n                      0.0027, 0.0025, 0.0008, 0.0024, 0.0025, 0.0012, 0.0021, 0.0017, 0.0030,\n                      0.0023, 0.0026, 0.0023, 0.0018, 0.0021, 0.0027, 0.0029, 0.0023, 0.0025,\n                      0.0027, 0.0026, 0.0025, 0.0021, 0.0022, 0.0021, 0.0023, 0.0027, 0.0023,\n                      0.0025, 0.0031, 0.0029, 0.0029, 0.0023, 0.0009, 0.0029, 0.0028, 0.0001,\n                      0.0028, 0.0024, 0.0023, 0.0022, 0.0015, 0.0026, 0.0024, 0.0025, 0.0027,\n                      0.0023, 0.0029, 0.0024, 0.0026, 0.0029, 0.0021, 0.0011, 0.0026, 0.0026,\n                      0.0026, 0.0024, 0.0016, 0.0010, 0.0026, 0.0026, 0.0025, 0.0004, 0.0026,\n                      0.0025, 0.0015, 0.0025, 0.0024, 0.0025, 0.0016, 0.0014, 0.0027, 0.0025,\n                      0.0030, 0.0019, 0.0026, 0.0029, 0.0030, 0.0021, 0.0025, 0.0026, 0.0022,\n                      0.0021, 0.0010, 0.0014, 0.0019, 0.0021, 0.0027, 0.0028, 0.0015, 0.0026,\n                      0.0018, 0.0027, 0.0013, 0.0023, 0.0029, 0.0017, 0.0027, 0.0025, 0.0021,\n                      0.0028, 0.0025, 0.0022, 0.0025, 0.0024, 0.0025, 0.0021, 0.0020, 0.0029,\n                      0.0022, 0.0022, 0.0008, 0.0023, 0.0006, 0.0011, 0.0025, 0.0026, 0.0018,\n                      0.0027, 0.0026, 0.0025, 0.0021, 0.0010, 0.0021, 0.0025, 0.0023, 0.0025,\n                      0.0027, 0.0028, 0.0029, 0.0024, 0.0015, 0.0006, 0.0024, 0.0027, 0.0021,\n                      0.0022, 0.0023, 0.0024], dtype=torch.float64),\n                     zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n                     axis=0)),\n             ('features.3.1.block.0.bias',\n              Parameter containing:\n              tensor([-0.0163,  0.0013, -0.0014, -0.0001,  0.0024, -0.0018,  0.0018, -0.0208,\n                       0.0031,  0.0019,  0.0022,  0.0012,  0.0070, -0.0058,  0.0019,  0.0220,\n                       0.0019,  0.0026,  0.0026, -0.0091, -0.0018,  0.0032, -0.0020,  0.0130,\n                      -0.0041,  0.0040, -0.0042,  0.0065, -0.0001,  0.0070, -0.0038,  0.0078,\n                      -0.0053, -0.0098, -0.0072, -0.0030, -0.0043, -0.0057,  0.0007, -0.0046,\n                      -0.0251, -0.0090, -0.0023, -0.0003, -0.0027,  0.0791, -0.0004, -0.0006,\n                      -0.0041,  0.0028, -0.0005,  0.0010, -0.0204, -0.0021, -0.0019, -0.0029,\n                      -0.0312,  0.0052, -0.0016,  0.0020,  0.0005,  0.0094, -0.0001,  0.0157,\n                       0.0057, -0.0005, -0.0117, -0.0025, -0.0008, -0.0066,  0.0025,  0.0004,\n                      -0.0050, -0.0045,  0.0013, -0.0055, -0.0035, -0.0184,  0.0076,  0.0022,\n                      -0.0002, -0.0118, -0.0053, -0.0114, -0.0033, -0.0060, -0.0003, -0.0010,\n                      -0.0003, -0.0163,  0.0034, -0.0003, -0.0045, -0.0083,  0.0005,  0.0081,\n                       0.0021, -0.0033,  0.0034, -0.0030,  0.0120,  0.0058,  0.0017,  0.0057,\n                       0.0044, -0.0053,  0.0084, -0.0059, -0.0028, -0.0035, -0.0076,  0.0205,\n                      -0.0009, -0.0031, -0.0024,  0.0103,  0.0025, -0.0021,  0.0012, -0.0034,\n                       0.0007,  0.0070, -0.0019, -0.0017,  0.0008,  0.0006, -0.0042, -0.0023,\n                      -0.0053,  0.0022,  0.0093, -0.0024, -0.0022, -0.0068, -0.0041,  0.0016,\n                      -0.0126, -0.0011,  0.0191,  0.0003, -0.0001, -0.0094,  0.0017, -0.0036,\n                       0.0018,  0.0047, -0.0043, -0.0076,  0.0006, -0.0222,  0.0059,  0.0027,\n                      -0.0027,  0.0021, -0.0012,  0.0170, -0.0059, -0.0040,  0.0166, -0.0034,\n                      -0.0042,  0.0055, -0.0095, -0.0036, -0.0051, -0.0013,  0.0157,  0.0037,\n                       0.0009,  0.0039, -0.0021, -0.0081, -0.0033, -0.0091,  0.0044, -0.0374,\n                       0.0025, -0.0044,  0.0036, -0.0056, -0.0009,  0.0018,  0.0021,  0.0032,\n                      -0.0029, -0.0349, -0.0010,  0.0022, -0.0046, -0.0016,  0.0008, -0.0056],\n                     requires_grad=True)),\n             ('features.3.1.block.0.scale', tensor(0.0033)),\n             ('features.3.1.block.0.zero_point', tensor(80)),\n             ('features.3.1.block.2.weight',\n              tensor([2.2053, 1.4554, 1.3168, 0.7365, 1.3705, 2.0486, 1.5745, 2.0763, 1.5547,\n                      0.9849, 1.8222, 1.1253, 0.8805, 1.2404, 2.4620, 0.6834, 2.2865, 0.9326,\n                      0.9743, 0.8429, 0.8755, 1.6943, 2.4084, 0.5971, 1.5939, 1.1114, 0.8514,\n                      1.4894, 1.7650, 2.0485, 1.4229, 1.5263, 1.4565, 1.4297, 1.2013, 1.9741,\n                      1.0116, 1.0970, 2.7668, 1.0746, 0.9875, 2.0559, 1.3468, 1.4438, 1.2962,\n                      2.2377, 2.0144, 1.1205, 1.2442, 0.7908, 2.0815, 1.7796, 0.7299, 1.2140,\n                      2.0729, 1.7459, 0.9097, 1.5065, 1.5308, 0.6757, 1.1049, 0.8716, 1.8499,\n                      0.9190, 1.7236, 1.2626, 0.9074, 1.1586, 1.6542, 1.3859, 0.9182, 1.1842,\n                      1.4596, 1.7517, 1.9126, 1.1796, 1.0109, 1.1211, 0.9102, 1.7827, 1.1895,\n                      1.5069, 1.7949, 1.4946, 1.7405, 1.4270, 0.6932, 1.2131, 1.1213, 0.2927,\n                      2.0083, 1.1515, 1.4277, 0.9752, 1.2427, 1.6511, 1.1880, 1.2774, 1.2115,\n                      1.4597, 1.9605, 1.2312, 1.5144, 1.9924, 1.1727, 0.4882, 2.1828, 1.1241,\n                      1.5612, 1.3488, 0.8036, 1.6044, 1.4361, 1.8431, 1.5417, 0.5755, 1.1316,\n                      1.6975, 1.4890, 1.5086, 1.3102, 1.8653, 0.9288, 0.7791, 1.2220, 1.0904,\n                      2.3974, 1.2132, 1.5658, 1.5593, 2.1290, 1.0722, 1.2961, 2.0567, 1.1331,\n                      1.0879, 0.7309, 1.5187, 1.0106, 1.2726, 1.6325, 2.1121, 1.2945, 2.2039,\n                      0.7314, 1.8100, 0.6769, 1.3210, 2.0542, 0.8904, 1.7677, 1.8122, 1.2051,\n                      1.4746, 1.9857, 1.2495, 1.4570, 1.4445, 2.1689, 1.2940, 1.1032, 2.1103,\n                      0.8856, 1.3387, 0.5705, 1.8339, 1.1199, 1.3285, 1.5996, 1.3940, 0.7597,\n                      1.7659, 1.3973, 1.2949, 1.1816, 0.8467, 2.2745, 1.6218, 1.1523, 1.2356,\n                      1.9610, 1.4097, 1.4600, 1.0684, 0.7210, 1.0183, 1.2106, 1.2283, 0.8004,\n                      1.7067, 1.4788, 1.5056])),\n             ('features.3.1.block.2.bias',\n              tensor([ 8.9119e-01, -3.8070e-01, -1.3015e-01, -9.5050e-02, -3.3806e-01,\n                      -1.5278e-01, -3.4790e-01,  9.7463e-01, -7.4123e-01, -2.2679e-01,\n                      -3.7778e-01, -2.5681e-01, -2.9017e-01,  1.1676e-01, -4.2569e-01,\n                      -5.2857e-01, -3.9473e-01, -2.4995e-01, -2.7396e-01, -3.5259e-02,\n                      -9.4780e-02, -3.7069e-01, -1.7466e-01, -2.6769e-01,  1.5598e-01,\n                      -2.8759e-01, -3.0277e-01, -6.6599e-01, -2.3113e-01, -8.3490e-01,\n                       4.6645e-02, -7.5516e-01,  2.5213e-01,  3.4363e-01,  1.7376e-01,\n                      -5.8529e-02,  9.0703e-02,  5.1676e-02, -3.0224e-01,  1.4445e-02,\n                       4.6958e-01,  4.2917e-01, -6.2808e-02, -2.1292e-01, -1.1261e-01,\n                      -5.7220e-01, -2.7722e-01, -1.4921e-01,  2.4637e-03, -2.1188e-01,\n                      -2.7497e-01, -3.9691e-01,  3.8982e-01, -3.4801e-01, -1.1092e-02,\n                       3.3123e-02,  7.7180e-01, -5.4616e-01, -1.7123e-01, -2.3823e-01,\n                      -2.2478e-01, -4.3763e-01, -2.2068e-01, -5.0957e-01, -5.3950e-01,\n                      -5.6087e-02,  1.2627e-02, -8.4216e-02, -3.0586e-01,  1.6208e-01,\n                      -9.6813e-02, -4.3164e-02,  6.6543e-02,  1.6574e-01, -3.0349e-01,\n                       7.6884e-02, -1.8450e-01,  4.8679e-01, -3.8471e-01, -4.3213e-01,\n                      -2.2526e-01,  4.9713e-01,  1.3420e-01,  5.5514e-01, -1.8062e-01,\n                       1.0105e-01, -2.0648e-01, -1.5806e-01, -2.0918e-01, -6.6154e-01,\n                      -3.9392e-01, -2.1255e-01, -6.3640e-03,  9.6255e-02, -2.5522e-01,\n                      -9.5073e-01, -2.4130e-01, -4.9326e-02, -4.1814e-01, -1.5933e-01,\n                      -9.2304e-01, -4.6123e-01, -3.0397e-01, -7.9087e-01, -3.6449e-01,\n                      -6.2679e-02, -9.5786e-01, -2.0015e-01, -3.2907e-02, -8.5823e-03,\n                       6.6665e-03, -6.8201e-01, -4.4859e-01,  1.4850e-01, -2.1990e-01,\n                      -4.2311e-01, -3.2507e-01, -2.5501e-01, -2.3692e-01,  2.1717e-02,\n                      -3.1238e-01, -7.4213e-01, -7.7017e-02, -1.2509e-01, -1.9884e-01,\n                      -1.8392e-01,  1.1961e-01, -6.8396e-02,  4.3215e-03, -3.5653e-01,\n                      -8.7233e-01, -1.2967e-01, -7.8102e-02,  1.4951e-01, -7.8394e-02,\n                      -2.3558e-01,  2.5223e-04, -1.9169e-01,  1.3692e+00, -2.2980e-01,\n                      -2.3048e-01,  3.9397e-01, -2.7868e-01, -2.5569e-02, -1.8322e-01,\n                      -5.9558e-01, -3.3534e-02,  1.8970e-01, -3.8319e-01,  1.2571e+00,\n                      -8.3250e-01, -3.8121e-01, -8.8864e-02, -4.7034e-01, -7.2758e-02,\n                      -1.0930e+00,  1.0077e-01, -1.8720e-02, -1.2916e+00, -1.3028e-01,\n                      -2.5465e-02, -5.6504e-01,  1.3511e-01, -2.0225e-02, -1.3661e-01,\n                      -5.8401e-02,  3.3767e-01, -3.1149e-01, -2.7102e-01, -5.7154e-01,\n                      -8.9412e-02,  3.7415e-01, -8.7782e-02,  2.6262e-01, -3.4119e-01,\n                       1.0267e+00, -4.9468e-01,  6.8256e-02, -3.4086e-01,  3.9603e-02,\n                      -9.9339e-02, -2.7452e-01, -3.3877e-01, -4.2759e-01, -2.6290e-01,\n                      -7.8528e-03, -1.5416e-01, -2.8214e-01, -7.9856e-02, -9.6164e-02,\n                      -2.4442e-01,  1.2773e-01])),\n             ('features.3.1.block.2.scale', tensor(0.1725)),\n             ('features.3.1.block.2.zero_point', tensor(45)),\n             ('features.3.1.block.3.scale', tensor(0.0766)),\n             ('features.3.1.block.3.zero_point', tensor(70)),\n             ('features.3.1.block.3._packed_params.dtype', torch.qint8),\n             ('features.3.1.block.3._packed_params._packed_params',\n              (tensor([[-0.0185, -0.0079,  0.0092,  ...,  0.0026,  0.0013, -0.0198],\n                       [ 0.0593, -0.0208, -0.0222,  ...,  0.0237,  0.0816,  0.1883],\n                       [ 0.0039,  0.0620,  0.0155,  ...,  0.0658,  0.0174, -0.0019],\n                       ...,\n                       [-0.0155, -0.0017,  0.0361,  ...,  0.0069,  0.0000, -0.0499],\n                       [ 0.0305,  0.0292, -0.1000,  ..., -0.0014,  0.0083,  0.0722],\n                       [-0.0015,  0.0393,  0.0151,  ..., -0.0726, -0.1165,  0.0424]],\n                      size=(768, 192), dtype=torch.qint8,\n                      quantization_scheme=torch.per_channel_affine,\n                      scale=tensor([0.0013, 0.0015, 0.0019, 0.0014, 0.0014, 0.0018, 0.0021, 0.0011, 0.0013,\n                       0.0013, 0.0014, 0.0031, 0.0015, 0.0016, 0.0031, 0.0014, 0.0020, 0.0013,\n                       0.0016, 0.0016, 0.0014, 0.0014, 0.0019, 0.0015, 0.0017, 0.0014, 0.0025,\n                       0.0017, 0.0011, 0.0013, 0.0015, 0.0014, 0.0016, 0.0021, 0.0018, 0.0021,\n                       0.0017, 0.0012, 0.0012, 0.0015, 0.0015, 0.0014, 0.0012, 0.0028, 0.0025,\n                       0.0015, 0.0013, 0.0016, 0.0013, 0.0014, 0.0022, 0.0013, 0.0014, 0.0012,\n                       0.0012, 0.0014, 0.0015, 0.0011, 0.0010, 0.0017, 0.0018, 0.0013, 0.0016,\n                       0.0014, 0.0016, 0.0013, 0.0011, 0.0013, 0.0014, 0.0020, 0.0014, 0.0022,\n                       0.0013, 0.0014, 0.0018, 0.0015, 0.0012, 0.0016, 0.0017, 0.0013, 0.0018,\n                       0.0013, 0.0019, 0.0017, 0.0013, 0.0015, 0.0016, 0.0026, 0.0015, 0.0012,\n                       0.0015, 0.0014, 0.0011, 0.0016, 0.0014, 0.0015, 0.0014, 0.0014, 0.0018,\n                       0.0014, 0.0013, 0.0018, 0.0019, 0.0013, 0.0013, 0.0014, 0.0013, 0.0012,\n                       0.0016, 0.0013, 0.0014, 0.0013, 0.0024, 0.0015, 0.0018, 0.0012, 0.0024,\n                       0.0016, 0.0012, 0.0017, 0.0016, 0.0013, 0.0014, 0.0022, 0.0015, 0.0014,\n                       0.0016, 0.0028, 0.0014, 0.0015, 0.0022, 0.0017, 0.0014, 0.0012, 0.0027,\n                       0.0013, 0.0014, 0.0017, 0.0023, 0.0017, 0.0012, 0.0014, 0.0017, 0.0015,\n                       0.0012, 0.0018, 0.0018, 0.0010, 0.0018, 0.0011, 0.0011, 0.0020, 0.0016,\n                       0.0015, 0.0022, 0.0014, 0.0014, 0.0015, 0.0016, 0.0022, 0.0014, 0.0012,\n                       0.0016, 0.0019, 0.0010, 0.0012, 0.0014, 0.0016, 0.0013, 0.0016, 0.0017,\n                       0.0026, 0.0014, 0.0026, 0.0012, 0.0013, 0.0019, 0.0015, 0.0010, 0.0017,\n                       0.0014, 0.0013, 0.0023, 0.0016, 0.0012, 0.0011, 0.0013, 0.0031, 0.0014,\n                       0.0015, 0.0013, 0.0012, 0.0018, 0.0018, 0.0016, 0.0013, 0.0021, 0.0023,\n                       0.0022, 0.0013, 0.0017, 0.0036, 0.0015, 0.0016, 0.0015, 0.0013, 0.0014,\n                       0.0018, 0.0018, 0.0014, 0.0011, 0.0013, 0.0012, 0.0014, 0.0014, 0.0015,\n                       0.0012, 0.0013, 0.0017, 0.0013, 0.0017, 0.0013, 0.0019, 0.0014, 0.0023,\n                       0.0012, 0.0016, 0.0017, 0.0024, 0.0025, 0.0015, 0.0016, 0.0015, 0.0017,\n                       0.0026, 0.0013, 0.0012, 0.0014, 0.0013, 0.0018, 0.0014, 0.0014, 0.0013,\n                       0.0020, 0.0013, 0.0012, 0.0016, 0.0015, 0.0013, 0.0016, 0.0018, 0.0015,\n                       0.0013, 0.0019, 0.0011, 0.0011, 0.0016, 0.0017, 0.0015, 0.0014, 0.0019,\n                       0.0014, 0.0014, 0.0014, 0.0012, 0.0014, 0.0015, 0.0017, 0.0016, 0.0013,\n                       0.0012, 0.0019, 0.0015, 0.0024, 0.0013, 0.0013, 0.0014, 0.0010, 0.0019,\n                       0.0014, 0.0012, 0.0016, 0.0018, 0.0013, 0.0021, 0.0012, 0.0012, 0.0010,\n                       0.0019, 0.0026, 0.0033, 0.0038, 0.0017, 0.0015, 0.0016, 0.0045, 0.0011,\n                       0.0018, 0.0011, 0.0016, 0.0014, 0.0013, 0.0011, 0.0017, 0.0011, 0.0015,\n                       0.0018, 0.0012, 0.0018, 0.0018, 0.0021, 0.0021, 0.0011, 0.0015, 0.0013,\n                       0.0014, 0.0015, 0.0019, 0.0014, 0.0015, 0.0017, 0.0018, 0.0019, 0.0011,\n                       0.0015, 0.0009, 0.0017, 0.0013, 0.0021, 0.0026, 0.0013, 0.0018, 0.0020,\n                       0.0013, 0.0014, 0.0026, 0.0021, 0.0013, 0.0014, 0.0014, 0.0013, 0.0014,\n                       0.0017, 0.0012, 0.0013, 0.0013, 0.0014, 0.0020, 0.0035, 0.0020, 0.0016,\n                       0.0012, 0.0015, 0.0015, 0.0030, 0.0015, 0.0013, 0.0017, 0.0016, 0.0015,\n                       0.0014, 0.0013, 0.0014, 0.0015, 0.0014, 0.0018, 0.0015, 0.0016, 0.0012,\n                       0.0013, 0.0016, 0.0017, 0.0011, 0.0010, 0.0026, 0.0015, 0.0015, 0.0016,\n                       0.0013, 0.0023, 0.0018, 0.0015, 0.0016, 0.0012, 0.0021, 0.0016, 0.0015,\n                       0.0015, 0.0019, 0.0013, 0.0019, 0.0011, 0.0018, 0.0021, 0.0014, 0.0017,\n                       0.0014, 0.0025, 0.0018, 0.0017, 0.0009, 0.0021, 0.0012, 0.0013, 0.0016,\n                       0.0018, 0.0022, 0.0011, 0.0029, 0.0019, 0.0015, 0.0014, 0.0019, 0.0029,\n                       0.0019, 0.0018, 0.0017, 0.0017, 0.0014, 0.0014, 0.0017, 0.0010, 0.0015,\n                       0.0029, 0.0019, 0.0016, 0.0012, 0.0016, 0.0020, 0.0020, 0.0027, 0.0010,\n                       0.0011, 0.0014, 0.0015, 0.0015, 0.0013, 0.0014, 0.0011, 0.0021, 0.0013,\n                       0.0014, 0.0009, 0.0013, 0.0013, 0.0015, 0.0013, 0.0014, 0.0018, 0.0014,\n                       0.0013, 0.0011, 0.0017, 0.0017, 0.0011, 0.0025, 0.0017, 0.0020, 0.0015,\n                       0.0019, 0.0012, 0.0014, 0.0013, 0.0014, 0.0013, 0.0018, 0.0017, 0.0012,\n                       0.0022, 0.0021, 0.0022, 0.0015, 0.0017, 0.0021, 0.0016, 0.0033, 0.0013,\n                       0.0010, 0.0017, 0.0014, 0.0032, 0.0016, 0.0015, 0.0015, 0.0022, 0.0015,\n                       0.0021, 0.0015, 0.0025, 0.0016, 0.0012, 0.0017, 0.0015, 0.0014, 0.0020,\n                       0.0014, 0.0024, 0.0017, 0.0027, 0.0019, 0.0019, 0.0013, 0.0024, 0.0011,\n                       0.0013, 0.0015, 0.0012, 0.0014, 0.0018, 0.0013, 0.0015, 0.0012, 0.0025,\n                       0.0015, 0.0014, 0.0019, 0.0021, 0.0016, 0.0016, 0.0018, 0.0020, 0.0011,\n                       0.0012, 0.0017, 0.0027, 0.0028, 0.0016, 0.0014, 0.0016, 0.0013, 0.0016,\n                       0.0020, 0.0013, 0.0023, 0.0013, 0.0014, 0.0017, 0.0013, 0.0031, 0.0012,\n                       0.0014, 0.0014, 0.0013, 0.0017, 0.0021, 0.0015, 0.0022, 0.0019, 0.0017,\n                       0.0011, 0.0012, 0.0015, 0.0017, 0.0011, 0.0016, 0.0011, 0.0014, 0.0014,\n                       0.0015, 0.0017, 0.0013, 0.0012, 0.0012, 0.0011, 0.0019, 0.0011, 0.0016,\n                       0.0010, 0.0016, 0.0012, 0.0018, 0.0014, 0.0020, 0.0021, 0.0016, 0.0012,\n                       0.0013, 0.0015, 0.0010, 0.0020, 0.0010, 0.0014, 0.0024, 0.0016, 0.0020,\n                       0.0013, 0.0026, 0.0016, 0.0014, 0.0016, 0.0018, 0.0014, 0.0035, 0.0016,\n                       0.0011, 0.0012, 0.0017, 0.0013, 0.0011, 0.0015, 0.0016, 0.0015, 0.0016,\n                       0.0013, 0.0030, 0.0010, 0.0010, 0.0015, 0.0019, 0.0021, 0.0011, 0.0029,\n                       0.0026, 0.0010, 0.0013, 0.0014, 0.0010, 0.0013, 0.0015, 0.0011, 0.0011,\n                       0.0013, 0.0013, 0.0012, 0.0014, 0.0013, 0.0015, 0.0018, 0.0018, 0.0030,\n                       0.0016, 0.0019, 0.0013, 0.0021, 0.0015, 0.0019, 0.0014, 0.0012, 0.0012,\n                       0.0025, 0.0011, 0.0017, 0.0016, 0.0025, 0.0018, 0.0011, 0.0019, 0.0016,\n                       0.0015, 0.0015, 0.0017, 0.0020, 0.0024, 0.0012, 0.0011, 0.0014, 0.0014,\n                       0.0013, 0.0023, 0.0017, 0.0012, 0.0017, 0.0018, 0.0015, 0.0016, 0.0016,\n                       0.0017, 0.0010, 0.0024, 0.0021, 0.0010, 0.0012, 0.0013, 0.0012, 0.0017,\n                       0.0015, 0.0015, 0.0014, 0.0020, 0.0013, 0.0015, 0.0026, 0.0015, 0.0015,\n                       0.0012, 0.0017, 0.0012, 0.0019, 0.0021, 0.0016, 0.0013, 0.0021, 0.0015,\n                       0.0013, 0.0027, 0.0016, 0.0011, 0.0014, 0.0017, 0.0011, 0.0014, 0.0028,\n                       0.0011, 0.0019, 0.0025, 0.0014, 0.0023, 0.0016, 0.0010, 0.0012, 0.0019,\n                       0.0013, 0.0014, 0.0013, 0.0027, 0.0010, 0.0013, 0.0015, 0.0016, 0.0022,\n                       0.0036, 0.0019, 0.0018, 0.0013, 0.0022, 0.0014, 0.0012, 0.0012, 0.0014,\n                       0.0020, 0.0016, 0.0016, 0.0012, 0.0021, 0.0020, 0.0014, 0.0012, 0.0013,\n                       0.0015, 0.0012, 0.0012, 0.0013, 0.0011, 0.0020, 0.0014, 0.0015, 0.0013,\n                       0.0014, 0.0019, 0.0012, 0.0013, 0.0014, 0.0015, 0.0015, 0.0012, 0.0020,\n                       0.0012, 0.0013, 0.0012, 0.0014, 0.0011, 0.0012, 0.0016, 0.0022, 0.0018,\n                       0.0017, 0.0014, 0.0015], dtype=torch.float64),\n                      zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n                      axis=0),\n               Parameter containing:\n               tensor([-2.7953e-02, -2.8028e-02, -3.0410e-02, -4.1383e-02, -3.2658e-02,\n                       -9.6715e-03, -8.0399e-02, -4.4759e-02, -9.0242e-02, -4.6464e-02,\n                       -4.2809e-02, -3.0351e-02, -2.7362e-02, -3.7434e-02, -3.3181e-02,\n                       -2.9061e-02, -3.3037e-02, -4.2635e-02, -9.7060e-03, -3.9830e-02,\n                       -4.0829e-02, -3.8157e-02, -3.6850e-02, -3.6094e-02, -2.8470e-02,\n                       -5.5939e-02, -7.0436e-02, -3.4571e-02, -2.8728e-02,  1.6855e-04,\n                       -1.9636e-02, -2.0757e-02, -6.1046e-02, -3.0768e-02, -2.6339e-02,\n                       -2.1935e-03, -4.3594e-02, -3.4987e-02, -4.2888e-02, -3.2666e-02,\n                       -3.6137e-02, -5.3191e-02, -4.9111e-02, -5.8753e-02, -2.0362e-02,\n                       -3.3582e-02, -3.2271e-02, -7.5450e-02, -4.7464e-02, -3.2477e-02,\n                       -3.9672e-02, -5.7128e-02, -3.4409e-02, -3.9462e-02, -2.7919e-02,\n                       -3.7797e-02, -4.3236e-02, -3.2279e-02, -4.6788e-02, -3.2844e-02,\n                       -1.3988e-01, -3.4507e-02, -3.3833e-02, -3.3744e-02, -2.9362e-02,\n                       -3.3540e-02, -4.8968e-02, -3.1971e-02, -2.9091e-02, -2.8634e-02,\n                       -3.6076e-02, -2.7972e-02, -4.5649e-02, -3.9331e-02, -6.7332e-02,\n                       -5.8346e-02, -3.2222e-02, -3.8470e-02, -5.1850e-02, -5.6852e-02,\n                       -3.9918e-02, -4.9079e-02, -8.4655e-02, -6.0108e-02, -2.9999e-02,\n                       -3.7373e-02, -1.3385e-02, -3.0017e-02, -3.0207e-02, -1.0597e-02,\n                       -3.3161e-02, -4.9884e-02, -3.9579e-02, -4.4585e-02, -4.3189e-02,\n                       -1.1019e-01, -5.6401e-02, -7.8117e-02, -3.2225e-02, -3.5365e-02,\n                       -3.6394e-02, -2.5417e-02, -1.5413e-02, -3.9531e-02, -4.6168e-02,\n                       -4.4326e-02, -2.4200e-02, -4.5196e-02, -3.4921e-02, -4.3804e-02,\n                       -3.0624e-02, -5.2648e-02, -2.8250e-02, -2.9200e-02, -3.4190e-02,\n                       -3.9149e-02, -2.6217e-02, -1.8507e-02, -3.6022e-02, -3.9573e-02,\n                       -2.8532e-02, -3.1084e-02, -3.4590e-02, -4.4080e-02, -9.7039e-02,\n                       -6.2148e-02, -1.0859e-02, -8.2049e-02, -3.6232e-02, -2.5941e-02,\n                       -2.4976e-02, -3.9323e-02, -3.7688e-02, -3.8742e-02, -2.4748e-02,\n                       -3.5987e-02, -5.3431e-02, -4.4987e-02, -3.7686e-02, -4.3313e-02,\n                       -3.5645e-02, -5.3962e-02, -2.7838e-02, -3.6240e-02, -3.9529e-02,\n                       -3.4915e-02, -6.0070e-02, -3.2803e-02, -6.6664e-02, -5.1658e-02,\n                       -4.0819e-02, -5.9941e-02, -3.0325e-02, -4.6435e-02, -7.5138e-02,\n                       -5.6702e-02, -7.4429e-02, -3.6539e-02, -5.4826e-02, -3.5008e-02,\n                       -3.5023e-02, -4.1378e-02, -3.3346e-02, -7.7301e-02, -5.4368e-02,\n                       -3.4380e-02, -4.9621e-02, -5.1570e-02, -4.5829e-02, -3.9835e-02,\n                       -3.8909e-02, -4.4895e-02, -2.4513e-02, -2.4990e-02, -2.9242e-02,\n                       -3.8451e-02, -2.7344e-02, -2.7934e-02, -3.4988e-02, -3.5846e-02,\n                       -3.2075e-02, -6.0584e-02, -3.8121e-02, -3.5720e-02, -3.4795e-02,\n                       -3.5114e-02, -4.4686e-02, -3.0250e-02, -2.3379e-02, -3.3493e-02,\n                       -3.3292e-02, -3.6981e-02, -5.3892e-02, -4.8393e-02, -2.9242e-02,\n                       -2.8921e-02, -5.3411e-02, -3.2404e-02, -2.4590e-02, -2.4693e-02,\n                       -4.5122e-02, -1.5612e-03, -4.5334e-02, -2.4272e-02, -6.5959e-02,\n                       -4.2613e-02, -3.3986e-02, -5.0342e-02, -4.5242e-02, -4.0541e-02,\n                       -3.0314e-02, -2.8734e-02, -2.6717e-02, -3.4848e-02, -3.2948e-02,\n                       -5.0540e-02, -5.2398e-02, -2.9802e-02, -3.0770e-02, -4.5939e-02,\n                       -6.4852e-02, -2.2154e-02, -6.5831e-02, -2.1931e-02, -5.4847e-02,\n                       -5.1728e-02, -4.6168e-02, -3.5691e-02, -1.9033e-02, -7.0794e-02,\n                       -4.8378e-02, -9.7352e-02, -5.4266e-02, -1.1826e-02, -3.0519e-02,\n                       -4.9422e-02, -4.1327e-02, -2.3565e-02, -7.1056e-02, -5.1367e-02,\n                       -3.5705e-02, -4.6293e-02, -4.0129e-02, -1.9666e-02, -9.0735e-02,\n                       -4.0789e-02, -5.5597e-02, -3.9682e-02, -5.1507e-02, -2.7293e-02,\n                       -1.9702e-02, -3.9840e-02, -6.1973e-02, -2.5652e-02, -3.5258e-02,\n                       -4.3883e-02, -3.0360e-02, -4.3320e-02, -3.1791e-02, -5.4957e-02,\n                       -3.5927e-02, -3.2916e-02, -7.2063e-02, -6.1605e-02, -4.1901e-02,\n                       -3.4424e-02, -2.9738e-02, -5.2767e-02, -4.3877e-02, -4.1237e-02,\n                       -4.1712e-02, -3.7097e-02, -5.3826e-02, -3.7744e-02, -3.1024e-02,\n                       -3.9066e-02, -4.5102e-02, -4.3292e-02, -5.3505e-02, -2.2775e-02,\n                       -6.6239e-02, -3.9666e-02, -3.1260e-02, -3.7223e-02, -1.3374e-02,\n                       -4.1739e-02, -3.0645e-02, -3.7508e-02, -1.2833e-02, -3.7480e-02,\n                       -2.4938e-02, -5.9920e-02, -2.1378e-02, -3.4182e-02, -2.8991e-02,\n                       -2.7538e-02, -4.5864e-02, -3.7116e-02, -3.0087e-02, -3.8112e-02,\n                       -2.4079e-02, -1.5410e-02, -5.1136e-02, -4.8195e-02, -5.4706e-02,\n                       -5.7151e-02, -2.5203e-02, -3.8973e-02, -5.2782e-02, -4.5861e-02,\n                       -6.4510e-02, -3.0165e-02, -3.2918e-02, -4.5062e-02, -4.0470e-02,\n                       -3.8851e-02, -7.9900e-02, -4.0230e-02, -6.7876e-02, -5.1725e-02,\n                       -3.4218e-02, -2.3983e-02, -3.9089e-02, -4.6210e-02, -3.7971e-02,\n                       -4.4555e-02, -3.9120e-02, -3.9922e-02, -3.7575e-02, -7.7001e-02,\n                       -4.6431e-02, -4.0622e-02, -4.1868e-02, -6.0349e-02, -3.4337e-02,\n                       -5.6368e-02, -4.0692e-02, -3.4119e-02, -7.3960e-02, -1.9379e-02,\n                       -4.2295e-02, -2.9409e-02, -3.5942e-02, -2.2187e-02, -3.6198e-02,\n                       -2.7822e-02, -3.6390e-02, -2.7998e-02, -4.9118e-02, -1.7409e-01,\n                       -3.3703e-02, -3.2684e-02, -4.9406e-02, -4.0333e-02, -3.5058e-02,\n                       -2.9333e-02, -3.2124e-02, -3.8187e-02, -5.1829e-02, -4.8831e-02,\n                       -3.2786e-02, -1.3352e-02, -3.1505e-02, -3.5544e-02, -3.3146e-02,\n                       -4.3465e-02, -2.7459e-02, -3.9839e-02, -3.6980e-02, -1.1736e-02,\n                       -2.5030e-02, -4.0104e-02, -6.6517e-02, -3.5511e-02, -2.4385e-02,\n                       -5.0047e-02, -3.4521e-02, -5.5982e-02, -4.1741e-02, -3.6825e-02,\n                       -3.6743e-02, -3.3982e-02, -4.2821e-02, -4.8498e-02, -3.8420e-02,\n                       -2.2834e-02, -2.7015e-02, -4.0841e-02, -3.0170e-02, -6.4202e-02,\n                       -3.5223e-02, -3.8516e-02, -3.4353e-02, -2.4919e-02, -4.3322e-02,\n                       -4.6860e-02, -3.6497e-02, -1.5628e-02, -3.1507e-02, -2.9258e-02,\n                       -2.6518e-02, -3.4818e-02, -4.2463e-02, -4.3035e-02, -3.6277e-02,\n                       -4.2352e-02, -2.8122e-02, -4.2507e-02, -2.1507e-02, -6.1495e-02,\n                       -3.7326e-02, -3.3868e-02, -1.4143e-02, -4.2840e-02, -3.2216e-02,\n                       -2.3738e-02, -4.8334e-02, -3.1188e-02, -3.9423e-02, -1.5693e-02,\n                       -3.8224e-02, -4.6836e-02, -3.8177e-02, -1.3581e-02, -2.6527e-02,\n                       -4.4280e-02, -2.8876e-02, -4.1115e-02, -4.1567e-02, -2.7197e-02,\n                       -2.2153e-02, -2.9307e-02, -2.9927e-02, -3.4025e-02, -3.2184e-02,\n                       -3.0318e-02, -6.5856e-02, -4.9043e-02, -1.4010e-02, -5.2310e-02,\n                       -3.1239e-02, -3.5643e-02, -2.7782e-02, -5.0240e-02, -2.8656e-02,\n                       -3.7372e-02, -5.1833e-02, -5.7765e-02, -2.9140e-02, -4.2038e-02,\n                       -9.5746e-02, -1.7186e-02, -3.5334e-02, -4.9695e-02, -6.2583e-02,\n                       -5.8040e-02, -1.9763e-02, -1.3469e-02, -3.1954e-02, -5.1489e-02,\n                       -2.7341e-02, -2.7806e-02, -3.7660e-02, -7.9166e-02, -3.3938e-02,\n                       -2.7838e-02, -8.0635e-02, -2.4420e-02, -4.0151e-02, -3.5610e-02,\n                       -3.0209e-02, -4.0494e-02, -5.3035e-02, -2.6689e-02, -4.5070e-02,\n                       -8.6152e-02, -3.7492e-02, -4.1814e-02, -3.0041e-02, -2.1728e-02,\n                       -4.8833e-02, -3.8560e-02, -3.3399e-02, -3.7000e-02, -9.8020e-02,\n                       -3.4909e-02, -1.3119e-02, -2.5444e-02, -2.6989e-02, -1.5484e-02,\n                       -3.6567e-02, -4.4272e-02, -4.1215e-02, -3.4921e-02, -4.3048e-02,\n                       -4.9908e-02, -7.6005e-02, -2.6414e-02, -4.3992e-02, -3.4536e-02,\n                       -3.5980e-02, -5.3110e-02, -3.7197e-02, -2.6460e-02, -3.0670e-02,\n                       -3.9982e-02, -2.8527e-02, -3.5415e-02, -1.9182e-02, -4.7446e-02,\n                       -3.8723e-02, -6.2489e-02, -4.2220e-02, -5.4538e-02, -8.7111e-02,\n                       -6.2673e-02, -4.7623e-02, -5.3548e-02, -2.0156e-02, -3.8067e-02,\n                       -3.5088e-02, -3.5574e-02, -3.7409e-02, -3.8856e-02, -2.7633e-02,\n                       -2.1791e-02, -4.3274e-02, -3.2594e-02, -4.2478e-02, -4.0399e-02,\n                       -2.2972e-02, -2.0959e-02, -1.5933e-02, -3.4776e-02, -2.5298e-02,\n                       -4.6095e-02, -3.9800e-02, -5.1435e-02, -1.1026e-02, -3.9250e-02,\n                       -4.0987e-02, -4.7014e-02, -5.8839e-02, -2.0879e-02, -3.9428e-02,\n                       -3.3987e-02, -4.6161e-02, -5.0646e-02, -3.4723e-02, -3.4490e-02,\n                       -3.8608e-02, -2.2980e-02, -4.9094e-02, -4.1158e-02, -2.0065e-02,\n                       -3.2586e-02, -3.3917e-02, -3.8629e-02, -3.1057e-02, -3.9671e-02,\n                       -4.6974e-02, -2.7445e-02, -3.3398e-02, -4.2281e-02, -5.6554e-02,\n                       -3.7157e-02, -2.3778e-02, -3.2474e-02, -2.5199e-02, -4.2922e-02,\n                       -2.8478e-02, -3.7167e-02, -4.2436e-02, -5.1608e-02, -3.8775e-02,\n                       -4.3177e-02, -3.0003e-02, -1.9921e-02, -3.4644e-02, -4.6303e-02,\n                       -5.0619e-02, -3.8332e-02, -2.4204e-02, -3.7506e-02, -5.4417e-02,\n                       -5.6059e-02, -6.6362e-02, -3.8985e-02, -3.2908e-02, -3.1604e-02,\n                       -3.3962e-02, -3.9737e-02, -9.4849e-02, -4.5120e-02, -4.5950e-02,\n                       -2.9911e-02, -4.6498e-02, -3.7423e-02, -2.2759e-02, -4.0273e-02,\n                       -3.0825e-02, -3.7385e-02, -4.6949e-02, -5.5339e-02, -3.5284e-02,\n                       -3.9096e-02, -3.0142e-02, -4.0953e-02, -6.6701e-02, -2.3079e-02,\n                       -5.1014e-02, -2.7461e-02, -4.1515e-02, -3.5128e-02, -7.3606e-02,\n                       -3.9665e-02, -4.0842e-02, -4.6345e-02, -3.1472e-02, -2.7368e-02,\n                       -4.6905e-02, -5.8890e-02, -3.3393e-02, -4.2086e-02, -8.6643e-02,\n                       -4.7088e-02, -4.5541e-02, -3.1464e-02, -4.5975e-02,  1.0916e-01,\n                       -2.0107e-02, -2.8659e-02, -3.0899e-02, -1.0248e-01, -8.1963e-02,\n                       -2.8897e-02, -4.0726e-02, -4.2750e-02, -4.2408e-02, -1.7803e-02,\n                       -3.1025e-02, -5.0686e-02, -3.0190e-02, -2.8526e-02, -5.4457e-02,\n                       -5.5827e-02, -1.9113e-02, -3.4936e-02, -4.1603e-02, -5.2221e-02,\n                       -2.6106e-02, -3.1046e-02, -7.2967e-02, -3.7117e-02,  1.0034e-03,\n                       -3.0338e-02, -4.3576e-02, -4.9636e-02, -2.3886e-02, -2.6075e-02,\n                       -5.2695e-02, -3.7280e-02, -4.1286e-02, -4.7114e-02, -3.9788e-02,\n                       -6.0411e-02, -4.1343e-02, -3.4409e-02, -3.1286e-02, -2.4261e-02,\n                        2.9244e-02, -3.7037e-02, -4.6696e-02, -4.4384e-02, -4.1270e-02,\n                       -3.8507e-02, -5.7811e-02, -3.0101e-02, -3.5757e-02, -3.3344e-02,\n                       -3.7142e-02, -3.2367e-02, -2.3496e-02, -3.6641e-02, -4.0032e-02,\n                       -4.4391e-02, -3.1869e-02, -1.8350e-01, -4.2198e-02, -3.4129e-02,\n                       -5.2073e-02, -2.3716e-02, -2.8705e-02, -4.5759e-02, -3.6053e-02,\n                       -2.5399e-02, -4.8495e-02, -2.4968e-02, -2.9164e-02, -3.4802e-02,\n                       -4.6493e-02, -2.4662e-02, -2.7377e-02, -2.3122e-02, -3.9056e-02,\n                       -2.4298e-02, -3.4680e-02, -7.9965e-02, -5.2135e-02, -3.6535e-02,\n                       -1.7717e-02, -3.0838e-02, -2.9984e-02, -3.2734e-02, -3.9000e-02,\n                       -3.7754e-02, -4.5447e-02, -5.4005e-02, -3.2345e-02, -1.0901e-02,\n                       -2.4134e-02, -4.0857e-02, -2.3725e-02, -4.4647e-02, -1.1049e-02,\n                       -3.9646e-02, -2.4309e-02, -5.6797e-02, -4.7203e-02, -5.8578e-02,\n                       -3.5263e-02, -4.6492e-02, -3.3631e-02, -2.7857e-02, -1.1295e-02,\n                       -4.7885e-02, -3.5897e-02, -5.4226e-02, -3.2468e-02, -5.1810e-02,\n                       -3.2307e-02, -4.6719e-02, -2.9519e-02, -5.2384e-02, -3.4673e-02,\n                       -4.5059e-02, -2.6654e-02, -3.4526e-02, -4.0742e-02, -2.0405e-02,\n                       -4.3395e-02, -4.8247e-02, -5.8394e-02, -5.0182e-02, -4.6778e-02,\n                       -2.9314e-02, -4.3962e-02, -2.6199e-02, -5.0720e-02, -3.0584e-02,\n                       -5.8629e-02, -8.6273e-02, -1.4852e-02, -3.6799e-02, -3.6601e-02,\n                       -7.8357e-03, -2.1153e-02, -4.2886e-02], requires_grad=True))),\n             ('features.3.1.block.5.scale', tensor(0.0601)),\n             ('features.3.1.block.5.zero_point', tensor(39)),\n             ('features.3.1.block.5._packed_params.dtype', torch.qint8),\n             ('features.3.1.block.5._packed_params._packed_params',\n              (tensor([[ 0.0777,  0.0343, -0.0108,  ..., -0.0090, -0.0361, -0.0054],\n                       [ 0.1355,  0.0197,  0.0350,  ...,  0.0153, -0.0415, -0.0372],\n                       [ 0.0567, -0.0178,  0.0016,  ..., -0.0972,  0.1167, -0.0551],\n                       ...,\n                       [ 0.1664,  0.0238,  0.0034,  ...,  0.0340,  0.0951, -0.0051],\n                       [ 0.0729,  0.0410, -0.0387,  ...,  0.0068,  0.0091,  0.0296],\n                       [-0.0102,  0.0017,  0.0594,  ...,  0.0441,  0.0187,  0.0424]],\n                      size=(192, 768), dtype=torch.qint8,\n                      quantization_scheme=torch.per_channel_affine,\n                      scale=tensor([0.0018, 0.0022, 0.0016, 0.0021, 0.0015, 0.0018, 0.0025, 0.0017, 0.0017,\n                       0.0017, 0.0013, 0.0015, 0.0029, 0.0017, 0.0027, 0.0016, 0.0014, 0.0014,\n                       0.0017, 0.0031, 0.0021, 0.0020, 0.0039, 0.0017, 0.0013, 0.0016, 0.0019,\n                       0.0015, 0.0019, 0.0024, 0.0013, 0.0013, 0.0025, 0.0018, 0.0015, 0.0016,\n                       0.0019, 0.0015, 0.0039, 0.0018, 0.0015, 0.0018, 0.0021, 0.0020, 0.0023,\n                       0.0024, 0.0018, 0.0019, 0.0017, 0.0018, 0.0022, 0.0018, 0.0021, 0.0016,\n                       0.0022, 0.0023, 0.0020, 0.0013, 0.0014, 0.0014, 0.0019, 0.0012, 0.0019,\n                       0.0015, 0.0014, 0.0014, 0.0014, 0.0017, 0.0014, 0.0018, 0.0018, 0.0017,\n                       0.0017, 0.0020, 0.0019, 0.0015, 0.0021, 0.0016, 0.0024, 0.0013, 0.0016,\n                       0.0015, 0.0015, 0.0014, 0.0024, 0.0016, 0.0014, 0.0019, 0.0027, 0.0051,\n                       0.0018, 0.0018, 0.0022, 0.0019, 0.0032, 0.0014, 0.0014, 0.0018, 0.0016,\n                       0.0025, 0.0015, 0.0016, 0.0016, 0.0016, 0.0017, 0.0024, 0.0019, 0.0017,\n                       0.0017, 0.0016, 0.0018, 0.0016, 0.0021, 0.0018, 0.0013, 0.0019, 0.0022,\n                       0.0019, 0.0018, 0.0015, 0.0015, 0.0017, 0.0024, 0.0024, 0.0017, 0.0014,\n                       0.0015, 0.0022, 0.0020, 0.0017, 0.0022, 0.0027, 0.0013, 0.0016, 0.0030,\n                       0.0029, 0.0022, 0.0030, 0.0024, 0.0014, 0.0015, 0.0015, 0.0031, 0.0015,\n                       0.0018, 0.0023, 0.0018, 0.0017, 0.0025, 0.0016, 0.0017, 0.0023, 0.0016,\n                       0.0013, 0.0021, 0.0028, 0.0016, 0.0018, 0.0016, 0.0015, 0.0021, 0.0017,\n                       0.0017, 0.0018, 0.0019, 0.0019, 0.0016, 0.0018, 0.0032, 0.0017, 0.0014,\n                       0.0033, 0.0013, 0.0018, 0.0017, 0.0014, 0.0024, 0.0015, 0.0020, 0.0015,\n                       0.0020, 0.0015, 0.0014, 0.0015, 0.0024, 0.0013, 0.0027, 0.0018, 0.0018,\n                       0.0017, 0.0023, 0.0017], dtype=torch.float64),\n                      zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n                      axis=0),\n               Parameter containing:\n               tensor([-1.6960e-02,  4.3416e-02, -1.3244e-02, -1.8078e-02, -1.1448e-03,\n                       -2.7339e-02,  2.4142e-02, -1.0798e-03, -1.1196e-01, -1.2150e-02,\n                        2.0426e-02,  1.9886e-02, -1.3082e-02, -3.5618e-03,  4.2773e-02,\n                       -2.4714e-02, -9.1998e-03, -2.5776e-03,  5.1646e-03, -4.3871e-03,\n                        6.0283e-03,  8.9490e-03,  2.2477e-03, -4.8091e-02,  2.8571e-02,\n                        1.3189e-02,  4.9688e-03, -2.4478e-02,  4.4971e-02, -3.9234e-02,\n                        1.0971e-02, -1.2059e-02,  1.0668e-02,  3.2172e-02, -1.0187e-02,\n                        9.4574e-03,  1.7609e-02, -1.4375e-03,  6.2550e-04, -8.7004e-03,\n                       -7.3408e-02, -6.5550e-02, -5.3608e-04,  2.5235e-02, -4.2267e-02,\n                        1.1197e-02,  2.4957e-02,  1.1778e-02, -3.3525e-02, -3.9021e-02,\n                       -9.5222e-04,  1.1236e-02, -2.0372e-02,  2.0504e-02, -1.1782e-02,\n                       -5.2116e-02, -6.8014e-02, -8.3683e-03, -3.0296e-02, -2.0545e-02,\n                       -1.1906e-02, -7.8875e-03,  5.2696e-03,  2.4787e-02, -9.2505e-03,\n                        5.4598e-03,  1.7934e-03,  1.0014e-02, -2.2804e-02,  8.1963e-03,\n                        6.2138e-02,  2.9206e-02,  2.5100e-03, -3.3951e-02,  1.2585e-02,\n                        4.4738e-03,  1.7287e-02,  1.6599e-02, -3.9537e-02, -5.2087e-02,\n                       -1.4467e-03, -6.5005e-03,  7.6315e-03, -6.8856e-02, -2.0530e-03,\n                       -1.7449e-02, -2.2407e-02,  2.5450e-02,  1.3338e-04,  6.7224e-01,\n                       -7.5390e-03, -5.8150e-03,  2.0900e-02, -2.0746e-02, -3.9168e-04,\n                        2.0047e-02, -6.2748e-03, -1.6136e-03, -6.5078e-04, -2.8302e-02,\n                        1.3450e-01,  2.6938e-01, -1.0085e-02, -3.1121e-02,  8.4198e-03,\n                       -1.0257e-03, -1.6394e-02,  6.5589e-02,  9.6950e-03,  1.5853e-02,\n                       -8.5991e-03, -3.8474e-02, -2.4725e-02,  4.4429e-02,  1.1227e-04,\n                        8.6952e-03,  2.7449e-02, -1.3544e-01, -8.4386e-03, -7.4424e-04,\n                        5.3822e-03,  1.7033e-02,  8.5281e-03,  5.8743e-03,  1.8032e-02,\n                       -1.4467e-02, -9.7835e-03,  1.6386e-04, -3.6742e-02, -1.4828e-02,\n                       -1.0157e-02,  9.3882e-03, -3.0324e-03, -2.7190e-03,  2.5135e-03,\n                        2.6857e-03, -3.0180e-02,  6.8775e-03, -1.0829e-01,  1.0404e-02,\n                       -6.6988e-02, -4.7780e-03,  1.7251e-04, -9.9870e-03, -4.5946e-03,\n                       -1.1174e-01, -1.4084e-02, -3.1326e-03, -1.7279e-02,  4.8123e-02,\n                       -2.3516e-02, -1.0783e-02, -1.3890e-03, -2.0193e-02, -2.0732e-02,\n                       -8.7520e-03,  9.2241e-03, -1.1987e-02, -2.1634e-02, -6.1761e-04,\n                       -4.1130e-03, -1.3212e-02,  4.6536e-03, -2.2642e-02,  1.9466e-02,\n                       -2.2112e-02,  1.6636e-01, -2.3391e-02,  1.1329e-02, -2.1683e-02,\n                       -9.3834e-03,  4.5248e-02, -3.1672e-03, -6.7086e-02,  3.0472e-02,\n                        1.1303e-01, -4.5119e-03, -1.1534e-02, -3.3181e-02, -2.4695e-02,\n                       -7.0536e-04, -1.0715e-02, -5.6972e-03,  8.3464e-03,  2.3336e-01,\n                       -1.8220e-03, -9.6033e-03, -3.0603e-02, -4.0521e-03,  2.3802e-03,\n                        1.7425e-01,  1.0912e-02], requires_grad=True))),\n             ('features.3.2.layer_scale',\n              tensor([[[-1.7799e-01]],\n              \n                      [[ 4.2584e-01]],\n              \n                      [[ 1.4746e-01]],\n              \n                      [[-2.7229e-01]],\n              \n                      [[ 1.2476e-01]],\n              \n                      [[-2.2036e-01]],\n              \n                      [[ 2.5984e-01]],\n              \n                      [[-2.2656e-01]],\n              \n                      [[ 2.8856e-01]],\n              \n                      [[ 1.5343e-01]],\n              \n                      [[ 1.8632e-01]],\n              \n                      [[ 1.2563e-01]],\n              \n                      [[ 2.2113e-01]],\n              \n                      [[ 1.0756e-01]],\n              \n                      [[-1.2518e-01]],\n              \n                      [[-1.5169e-01]],\n              \n                      [[ 2.1665e-01]],\n              \n                      [[ 1.5370e-01]],\n              \n                      [[-9.0777e-02]],\n              \n                      [[-2.0364e-05]],\n              \n                      [[-2.4296e-01]],\n              \n                      [[-2.3076e-01]],\n              \n                      [[-6.2933e-02]],\n              \n                      [[-1.2483e-01]],\n              \n                      [[-2.1227e-01]],\n              \n                      [[ 1.6499e-01]],\n              \n                      [[-4.7511e-02]],\n              \n                      [[ 1.1490e-01]],\n              \n                      [[-2.4934e-01]],\n              \n                      [[ 2.6761e-01]],\n              \n                      [[ 2.1162e-01]],\n              \n                      [[ 2.0248e-01]],\n              \n                      [[ 1.8079e-01]],\n              \n                      [[ 2.5836e-01]],\n              \n                      [[ 1.4306e-01]],\n              \n                      [[ 2.4078e-01]],\n              \n                      [[-2.2384e-01]],\n              \n                      [[ 9.7283e-02]],\n              \n                      [[-6.2869e-02]],\n              \n                      [[ 9.3034e-02]],\n              \n                      [[ 5.3982e-02]],\n              \n                      [[ 2.0039e-01]],\n              \n                      [[ 9.8928e-02]],\n              \n                      [[-1.6824e-01]],\n              \n                      [[ 1.9186e-01]],\n              \n                      [[ 1.8788e-01]],\n              \n                      [[-2.5013e-01]],\n              \n                      [[-2.6661e-01]],\n              \n                      [[ 2.1702e-01]],\n              \n                      [[ 1.7342e-01]],\n              \n                      [[ 2.4499e-01]],\n              \n                      [[ 1.7603e-01]],\n              \n                      [[-5.2846e-02]],\n              \n                      [[ 2.3333e-01]],\n              \n                      [[ 2.7282e-01]],\n              \n                      [[-5.4339e-01]],\n              \n                      [[-4.0150e-04]],\n              \n                      [[-1.7322e-01]],\n              \n                      [[ 2.7585e-01]],\n              \n                      [[-1.2473e-01]],\n              \n                      [[-9.4426e-02]],\n              \n                      [[ 1.1340e-01]],\n              \n                      [[ 2.4830e-01]],\n              \n                      [[ 1.9683e-01]],\n              \n                      [[ 1.9519e-01]],\n              \n                      [[ 1.6367e-01]],\n              \n                      [[-1.4343e-01]],\n              \n                      [[ 1.1962e-01]],\n              \n                      [[ 2.0183e-01]],\n              \n                      [[ 1.7798e-01]],\n              \n                      [[ 7.2173e-01]],\n              \n                      [[-2.6608e-01]],\n              \n                      [[-2.4371e-01]],\n              \n                      [[ 2.9357e-01]],\n              \n                      [[ 2.3091e-01]],\n              \n                      [[ 1.3669e-01]],\n              \n                      [[-2.2296e-01]],\n              \n                      [[-1.2700e-01]],\n              \n                      [[ 1.6731e-01]],\n              \n                      [[ 1.3269e-01]],\n              \n                      [[-9.5815e-02]],\n              \n                      [[-2.7994e-01]],\n              \n                      [[ 1.7674e-01]],\n              \n                      [[-2.3862e-01]],\n              \n                      [[ 1.5725e-01]],\n              \n                      [[ 1.8408e-01]],\n              \n                      [[-1.4611e-01]],\n              \n                      [[-2.0962e-01]],\n              \n                      [[ 2.2743e-01]],\n              \n                      [[-8.1587e-01]],\n              \n                      [[ 2.1192e-01]],\n              \n                      [[ 2.1360e-01]],\n              \n                      [[ 1.7454e-01]],\n              \n                      [[ 1.4831e-01]],\n              \n                      [[-4.5855e-02]],\n              \n                      [[ 2.6217e-01]],\n              \n                      [[ 1.8695e-01]],\n              \n                      [[ 7.3170e-01]],\n              \n                      [[ 1.9346e-01]],\n              \n                      [[ 1.9291e-01]],\n              \n                      [[-2.3243e-01]],\n              \n                      [[ 6.3737e-01]],\n              \n                      [[-2.7833e-01]],\n              \n                      [[-2.4811e-01]],\n              \n                      [[ 1.1094e-01]],\n              \n                      [[-8.8438e-02]],\n              \n                      [[-1.7465e-01]],\n              \n                      [[-8.1682e-01]],\n              \n                      [[-2.2983e-01]],\n              \n                      [[ 1.6350e-01]],\n              \n                      [[-5.5109e-02]],\n              \n                      [[-2.9845e-01]],\n              \n                      [[ 3.2419e-01]],\n              \n                      [[ 2.9094e-01]],\n              \n                      [[ 1.0924e-01]],\n              \n                      [[-4.7329e-02]],\n              \n                      [[-2.1532e-01]],\n              \n                      [[ 1.3492e-01]],\n              \n                      [[-2.8280e-01]],\n              \n                      [[ 2.1008e-01]],\n              \n                      [[-1.3416e-01]],\n              \n                      [[-2.0923e-01]],\n              \n                      [[ 1.2438e-01]],\n              \n                      [[ 7.4886e-02]],\n              \n                      [[-1.8227e-01]],\n              \n                      [[ 1.7839e-01]],\n              \n                      [[-1.8762e-01]],\n              \n                      [[ 3.2925e-02]],\n              \n                      [[ 3.1407e-01]],\n              \n                      [[ 2.1178e-01]],\n              \n                      [[ 1.7805e-01]],\n              \n                      [[-2.5379e-01]],\n              \n                      [[ 2.1754e-01]],\n              \n                      [[ 2.2230e-01]],\n              \n                      [[-1.1448e-01]],\n              \n                      [[-2.0352e-01]],\n              \n                      [[-1.4154e-01]],\n              \n                      [[ 2.2503e-02]],\n              \n                      [[-2.4800e-01]],\n              \n                      [[ 1.1048e-01]],\n              \n                      [[ 1.5157e-01]],\n              \n                      [[ 2.2877e-01]],\n              \n                      [[-1.1496e-03]],\n              \n                      [[-2.7205e-01]],\n              \n                      [[ 1.7486e-01]],\n              \n                      [[-4.4967e-01]],\n              \n                      [[ 1.4591e-01]],\n              \n                      [[ 2.3180e-01]],\n              \n                      [[-1.9084e-01]],\n              \n                      [[-2.0314e-01]],\n              \n                      [[-2.7746e-01]],\n              \n                      [[ 4.6467e-01]],\n              \n                      [[-9.6789e-02]],\n              \n                      [[-2.0016e-01]],\n              \n                      [[-1.9260e-01]],\n              \n                      [[ 3.0173e-01]],\n              \n                      [[ 2.2551e-01]],\n              \n                      [[ 5.5114e-01]],\n              \n                      [[-2.1993e-01]],\n              \n                      [[ 1.0087e-01]],\n              \n                      [[-1.0254e-01]],\n              \n                      [[ 2.3262e-01]],\n              \n                      [[-1.4723e-01]],\n              \n                      [[ 1.2066e-01]],\n              \n                      [[ 9.4882e-02]],\n              \n                      [[ 2.4648e-01]],\n              \n                      [[ 1.2722e-01]],\n              \n                      [[-2.7092e-01]],\n              \n                      [[-2.7843e-02]],\n              \n                      [[-2.6043e-01]],\n              \n                      [[ 1.3540e-01]],\n              \n                      [[-1.3025e-01]],\n              \n                      [[-2.2008e-01]],\n              \n                      [[ 6.3137e-01]],\n              \n                      [[-1.4437e-01]],\n              \n                      [[-6.5710e-02]],\n              \n                      [[ 4.6356e-02]],\n              \n                      [[ 2.5270e-01]],\n              \n                      [[ 2.7694e-01]],\n              \n                      [[-2.1656e-01]],\n              \n                      [[ 2.0558e-01]],\n              \n                      [[ 1.8878e-01]],\n              \n                      [[-1.3442e-01]],\n              \n                      [[-2.4275e-01]],\n              \n                      [[ 7.6549e-01]],\n              \n                      [[-8.6613e-02]],\n              \n                      [[-2.6824e-01]],\n              \n                      [[ 2.3351e-01]],\n              \n                      [[-9.0233e-02]],\n              \n                      [[-1.4839e-01]],\n              \n                      [[ 6.1847e-01]],\n              \n                      [[ 1.9717e-01]]])),\n             ('features.3.2.block.0.weight',\n              tensor([[[[ 0.0016,  0.0000,  0.0049,  ...,  0.0016, -0.0033,  0.0033],\n                        [ 0.0000,  0.0099,  0.0082,  ..., -0.0066, -0.0066, -0.0016],\n                        [ 0.0000, -0.0016, -0.1036,  ..., -0.0444, -0.0066,  0.0000],\n                        ...,\n                        [ 0.0033,  0.0000,  0.0000,  ...,  0.0049, -0.0049,  0.0016],\n                        [ 0.0000, -0.0066,  0.0049,  ..., -0.0016, -0.0016,  0.0016],\n                        [-0.0016, -0.0049, -0.0033,  ...,  0.0016, -0.0033, -0.0066]]],\n              \n              \n                      [[[-0.0025, -0.0050, -0.0050,  ...,  0.0000, -0.0025,  0.0000],\n                        [-0.0025,  0.0000, -0.0175,  ..., -0.0025, -0.0075, -0.0025],\n                        [-0.0025, -0.0075,  0.0000,  ..., -0.0025, -0.0150, -0.0050],\n                        ...,\n                        [ 0.0000, -0.0075, -0.0050,  ...,  0.0050, -0.0050, -0.0025],\n                        [ 0.0025, -0.0050,  0.0000,  ..., -0.0050,  0.0000, -0.0025],\n                        [-0.0050, -0.0050, -0.0025,  ..., -0.0025, -0.0025, -0.0050]]],\n              \n              \n                      [[[ 0.0090,  0.0124,  0.0034,  ...,  0.0169,  0.0146,  0.0067],\n                        [ 0.0124,  0.0022, -0.0011,  ...,  0.0180,  0.0067,  0.0000],\n                        [ 0.0236,  0.0270,  0.0685,  ...,  0.0708, -0.0034,  0.0090],\n                        ...,\n                        [ 0.0090, -0.0034,  0.0348,  ...,  0.0517,  0.0225,  0.0247],\n                        [ 0.0056,  0.0056,  0.0236,  ...,  0.0000,  0.0022,  0.0124],\n                        [ 0.0067,  0.0079,  0.0146,  ...,  0.0146,  0.0124,  0.0034]]],\n              \n              \n                      ...,\n              \n              \n                      [[[-0.0021,  0.0000, -0.0021,  ...,  0.0021,  0.0000,  0.0021],\n                        [ 0.0021,  0.0000,  0.0021,  ...,  0.0106,  0.0043,  0.0021],\n                        [ 0.0021,  0.0106, -0.0064,  ..., -0.0234,  0.0021,  0.0000],\n                        ...,\n                        [ 0.0000, -0.0021, -0.0149,  ..., -0.0106, -0.0085,  0.0000],\n                        [ 0.0021,  0.0000,  0.0043,  ..., -0.0021,  0.0043,  0.0000],\n                        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0043]]],\n              \n              \n                      [[[ 0.0024, -0.0024,  0.0082,  ...,  0.0106,  0.0000,  0.0033],\n                        [ 0.0008, -0.0008,  0.0090,  ...,  0.0098, -0.0049,  0.0000],\n                        [-0.0016,  0.0033,  0.0539,  ...,  0.0604, -0.0008, -0.0008],\n                        ...,\n                        [ 0.0000,  0.0049,  0.0326,  ...,  0.0229,  0.0024,  0.0033],\n                        [ 0.0049,  0.0016,  0.0139,  ...,  0.0122, -0.0016,  0.0041],\n                        [ 0.0033,  0.0016,  0.0131,  ...,  0.0106,  0.0000,  0.0073]]],\n              \n              \n                      [[[-0.0027,  0.0027,  0.0000,  ..., -0.0027, -0.0027,  0.0027],\n                        [ 0.0000, -0.0027, -0.0080,  ..., -0.0027,  0.0000,  0.0000],\n                        [ 0.0027, -0.0027, -0.0293,  ..., -0.0267,  0.0000,  0.0000],\n                        ...,\n                        [-0.0053, -0.0027, -0.0293,  ..., -0.0240, -0.0027, -0.0027],\n                        [ 0.0000, -0.0027, -0.0053,  ...,  0.0000,  0.0000,  0.0000],\n                        [-0.0027, -0.0027, -0.0027,  ..., -0.0080,  0.0000, -0.0027]]]],\n                     size=(192, 1, 7, 7), dtype=torch.qint8,\n                     quantization_scheme=torch.per_channel_affine,\n                     scale=tensor([0.0016, 0.0025, 0.0011, 0.0023, 0.0027, 0.0023, 0.0023, 0.0025, 0.0019,\n                      0.0028, 0.0026, 0.0029, 0.0008, 0.0026, 0.0013, 0.0019, 0.0018, 0.0022,\n                      0.0021, 0.0012, 0.0008, 0.0018, 0.0018, 0.0021, 0.0022, 0.0029, 0.0017,\n                      0.0025, 0.0013, 0.0025, 0.0026, 0.0027, 0.0014, 0.0023, 0.0026, 0.0015,\n                      0.0026, 0.0028, 0.0016, 0.0029, 0.0020, 0.0012, 0.0028, 0.0024, 0.0023,\n                      0.0008, 0.0016, 0.0026, 0.0028, 0.0016, 0.0025, 0.0016, 0.0014, 0.0006,\n                      0.0018, 0.0015, 0.0023, 0.0029, 0.0026, 0.0027, 0.0018, 0.0028, 0.0018,\n                      0.0026, 0.0026, 0.0024, 0.0014, 0.0027, 0.0025, 0.0014, 0.0025, 0.0026,\n                      0.0026, 0.0024, 0.0022, 0.0026, 0.0024, 0.0025, 0.0022, 0.0025, 0.0026,\n                      0.0027, 0.0012, 0.0017, 0.0023, 0.0030, 0.0013, 0.0009, 0.0010, 0.0011,\n                      0.0016, 0.0028, 0.0023, 0.0025, 0.0013, 0.0027, 0.0019, 0.0025, 0.0013,\n                      0.0023, 0.0010, 0.0027, 0.0025, 0.0017, 0.0029, 0.0007, 0.0017, 0.0020,\n                      0.0025, 0.0008, 0.0028, 0.0010, 0.0018, 0.0016, 0.0026, 0.0008, 0.0010,\n                      0.0024, 0.0019, 0.0008, 0.0027, 0.0010, 0.0023, 0.0027, 0.0009, 0.0009,\n                      0.0016, 0.0019, 0.0019, 0.0019, 0.0020, 0.0024, 0.0028, 0.0027, 0.0027,\n                      0.0028, 0.0014, 0.0016, 0.0021, 0.0029, 0.0025, 0.0020, 0.0018, 0.0025,\n                      0.0021, 0.0025, 0.0015, 0.0022, 0.0019, 0.0023, 0.0022, 0.0023, 0.0027,\n                      0.0017, 0.0026, 0.0025, 0.0007, 0.0022, 0.0017, 0.0028, 0.0028, 0.0023,\n                      0.0026, 0.0028, 0.0008, 0.0025, 0.0012, 0.0015, 0.0018, 0.0026, 0.0027,\n                      0.0016, 0.0028, 0.0023, 0.0024, 0.0017, 0.0015, 0.0024, 0.0025, 0.0023,\n                      0.0027, 0.0014, 0.0019, 0.0028, 0.0023, 0.0012, 0.0025, 0.0029, 0.0028,\n                      0.0021, 0.0008, 0.0027], dtype=torch.float64),\n                     zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n                     axis=0)),\n             ('features.3.2.block.0.bias',\n              Parameter containing:\n              tensor([ 6.6841e-03,  6.1065e-04, -5.0078e-03, -3.1807e-02, -1.7347e-03,\n                       1.6822e-02, -2.7989e-03, -1.4881e-02, -5.1645e-03,  1.8064e-02,\n                       1.4999e-02,  5.5600e-03,  1.1169e-02,  2.1872e-03,  6.5660e-03,\n                      -1.4897e-02,  1.3297e-02,  1.8038e-02,  3.7907e-02,  1.4541e-02,\n                      -1.4532e-02,  7.3757e-03,  3.9205e-03, -3.8513e-02,  9.2295e-04,\n                       1.2171e-02,  1.9141e-02, -1.3712e-02,  1.4650e-02,  3.6998e-03,\n                      -1.8947e-02,  1.1394e-02,  6.2078e-03,  4.9437e-03,  1.6268e-02,\n                       6.9196e-03, -2.0268e-02, -1.3813e-02, -1.6458e-03, -1.6727e-03,\n                      -3.8299e-02, -1.9133e-02, -7.2821e-04,  1.0098e-02,  1.0392e-03,\n                       4.1035e-03,  1.5834e-02,  3.5077e-03,  4.8393e-03, -9.4325e-03,\n                       1.9235e-02, -6.5462e-03, -1.7183e-02, -1.4857e-03,  3.6061e-03,\n                      -6.8077e-03, -2.9959e-02, -1.2240e-02,  2.3365e-02,  1.9904e-03,\n                       1.5369e-02,  1.6384e-02, -7.3153e-03,  7.1546e-03, -2.2057e-02,\n                      -1.2251e-02,  6.9512e-03,  1.8761e-02, -6.8944e-03, -1.2161e-02,\n                      -2.1218e-04, -1.1213e-02,  3.0970e-03,  6.5479e-03,  8.9649e-03,\n                      -1.4722e-02, -9.6308e-03, -2.7168e-03, -6.3515e-03,  2.8677e-03,\n                      -1.4905e-02, -9.6569e-03, -6.1132e-03,  4.9891e-03, -3.0781e-03,\n                      -2.6822e-04,  7.6026e-04,  2.2575e-04,  1.3528e-02,  3.7865e-02,\n                       1.4100e-02,  1.3782e-03, -1.6571e-02, -5.4818e-03,  7.8836e-04,\n                      -1.4122e-02, -3.3132e-03,  1.0305e-02,  5.7610e-04, -6.6744e-04,\n                      -2.8039e-02,  5.4758e-03, -1.2985e-02,  1.0321e-03, -2.2766e-03,\n                      -6.0180e-04, -2.0897e-03,  5.2476e-03, -3.0709e-03,  2.1007e-03,\n                       7.1583e-03,  1.7255e-02,  2.2436e-02,  1.3380e-03,  1.4689e-02,\n                      -8.6365e-03,  2.1616e-02, -1.5236e-02,  1.5867e-02,  7.2014e-06,\n                      -1.9300e-02, -1.3126e-02,  3.0006e-03, -8.5142e-03, -9.0459e-03,\n                      -7.5528e-03,  8.2177e-03,  4.3941e-03,  1.9291e-02, -6.4767e-03,\n                       8.5524e-03, -1.2869e-03,  5.3960e-04, -1.1740e-02, -1.9696e-03,\n                       7.8998e-03, -9.3437e-03,  1.0292e-02,  1.6682e-02,  9.5345e-03,\n                      -3.5342e-03, -4.2747e-03, -6.5263e-03,  2.1996e-02,  8.9704e-04,\n                      -5.6311e-03, -2.8987e-03,  2.5431e-02,  1.2717e-02,  1.1145e-02,\n                      -6.8607e-03,  2.8469e-03, -1.3994e-02,  6.3501e-03,  6.1260e-03,\n                       1.5028e-03,  1.9444e-04,  6.2490e-03,  1.1244e-02,  7.0908e-03,\n                      -2.6525e-03,  1.2520e-02, -2.0533e-02, -1.2113e-02,  5.5538e-03,\n                      -1.5164e-02, -1.2783e-02, -1.4104e-03, -1.5318e-03,  9.2957e-03,\n                       2.9233e-02,  3.1698e-03,  5.1171e-03,  1.0695e-02, -5.4785e-03,\n                       3.7113e-02,  3.8738e-03, -3.9677e-02, -8.6036e-05, -3.0321e-03,\n                       4.5659e-03,  5.3956e-03,  4.7120e-03, -1.8449e-02, -1.7265e-02,\n                       1.2442e-02,  1.0469e-02,  4.9850e-03, -7.1302e-03, -1.0579e-02,\n                       3.7997e-02, -7.4455e-04], requires_grad=True)),\n             ('features.3.2.block.0.scale', tensor(0.0059)),\n             ('features.3.2.block.0.zero_point', tensor(80)),\n             ('features.3.2.block.2.weight',\n              tensor([1.6161, 2.3503, 1.0065, 1.1249, 2.8243, 1.7104, 1.8874, 2.3778, 0.9704,\n                      1.7858, 1.8880, 2.1195, 0.8445, 2.5263, 1.0259, 1.0583, 0.8520, 1.5213,\n                      1.8810, 0.9749, 0.8952, 1.2421, 0.9691, 1.5549, 1.4949, 2.3508, 1.3431,\n                      1.7103, 1.0532, 2.3817, 2.8917, 2.1689, 0.9952, 1.7965, 1.9524, 0.8765,\n                      1.9301, 2.4745, 1.0631, 2.7686, 1.8283, 0.9419, 3.5783, 1.4249, 1.7101,\n                      1.0765, 0.9128, 2.6748, 2.2495, 1.0333, 1.8459, 1.1591, 1.1593, 0.8283,\n                      1.9156, 1.0795, 1.5791, 2.5234, 1.8860, 2.1124, 1.0526, 1.7806, 0.9065,\n                      2.3785, 2.2046, 1.5527, 1.0742, 2.0366, 1.9432, 0.7695, 1.9090, 2.7658,\n                      2.7070, 1.5125, 1.8331, 2.5572, 1.7474, 1.4586, 1.2477, 1.9567, 1.8912,\n                      2.3329, 0.7928, 1.1948, 1.5915, 2.5350, 0.8943, 0.9107, 1.0605, 0.4822,\n                      0.9263, 2.6422, 1.8245, 1.3521, 0.9291, 2.2513, 1.0541, 2.3282, 0.8730,\n                      1.9009, 1.2018, 1.6135, 1.8676, 0.8025, 2.6674, 0.8688, 1.3544, 1.2867,\n                      1.9476, 0.8800, 2.1525, 1.4485, 1.4782, 1.4166, 2.0060, 1.1011, 0.7765,\n                      1.8921, 1.7264, 0.9364, 1.7503, 0.8211, 1.3790, 1.7501, 0.8339, 0.8588,\n                      0.7586, 1.1934, 1.1547, 1.1062, 1.4500, 2.1621, 2.3726, 1.8340, 1.4921,\n                      2.1837, 0.9790, 1.1501, 1.1195, 2.2844, 1.5180, 0.9913, 1.1555, 1.7534,\n                      1.0361, 1.0309, 0.9619, 1.2323, 1.2869, 1.2153, 2.4875, 1.3764, 2.9917,\n                      1.0257, 2.2582, 2.3023, 0.9232, 1.6222, 1.0456, 2.4603, 2.3115, 1.0056,\n                      2.2989, 2.5939, 0.8258, 2.1332, 1.4802, 1.5638, 1.4003, 2.4215, 2.4615,\n                      1.1648, 2.1448, 1.7011, 1.7141, 1.5273, 1.2118, 2.2012, 2.1724, 1.2854,\n                      1.8000, 0.9421, 0.9104, 2.3402, 1.2583, 1.0106, 1.8487, 2.4190, 2.3893,\n                      2.3911, 1.0000, 1.9017])),\n             ('features.3.2.block.2.bias',\n              tensor([-3.3204e-01, -1.6734e-01, -5.6345e-01,  7.0796e-01,  4.8127e-02,\n                      -7.3576e-01,  6.2797e-02,  5.6695e-01, -3.6774e-01, -4.0496e-01,\n                      -6.7416e-01, -9.9151e-02,  1.6105e-02, -2.6061e-01, -5.3090e-02,\n                      -8.6512e-01, -6.1841e-01, -4.0448e-01, -6.8437e-01, -2.4214e-01,\n                      -1.9267e-01, -2.8684e-01, -2.7304e-01, -4.0721e-01, -6.9444e-02,\n                      -5.3326e-01,  8.8294e-01,  1.4959e-01, -2.3340e-01, -2.4920e-01,\n                       8.5349e-01, -5.5647e-01, -4.7172e-01, -3.1273e-01, -4.7459e-01,\n                      -4.1260e-01,  4.5994e-01,  3.1459e-01, -2.1237e-01, -2.9466e-02,\n                      -7.7376e-01, -4.0222e-01, -4.7140e-02, -4.5371e-01, -1.6058e-01,\n                      -1.9629e-01, -5.5613e-01, -5.5096e-01, -3.3204e-01,  3.9843e-02,\n                      -7.9199e-01, -6.5994e-02,  4.3774e-02, -7.2911e-02, -2.3966e-01,\n                      -4.7725e-02, -2.6343e-01,  1.3028e-01, -9.5322e-01, -1.5984e-01,\n                      -6.3057e-01, -3.7875e-01,  6.5107e-04, -8.5489e-02,  7.6727e-01,\n                      -2.2478e-02, -2.5679e-01, -2.6841e-01,  1.3206e-01, -1.9323e-01,\n                      -1.7146e-01,  4.2105e-01, -4.8011e-01, -2.7810e-01, -4.1742e-01,\n                       4.8693e-01,  1.4474e-01, -3.2621e-01,  1.1269e-01, -2.5114e-01,\n                       3.5691e-01,  1.9271e-01, -3.7316e-01, -5.4591e-01, -1.8962e-01,\n                      -1.5648e-01, -5.9111e-01, -1.3052e-01, -2.5343e-01, -8.2074e-01,\n                      -6.0921e-01, -2.9114e-01,  3.8556e-01, -6.1567e-01, -4.5798e-01,\n                       3.5610e-01, -8.4154e-01, -4.6155e-01, -1.7445e-01, -1.7756e-01,\n                       1.7119e+00, -3.2987e-01,  3.3708e-01, -3.3300e-02, -8.5538e-02,\n                      -2.7470e-01, -1.6608e-01, -4.1160e-01,  9.6636e-02, -3.3975e-01,\n                      -2.0887e-01, -5.8454e-01, -7.7971e-01, -2.4450e-01, -5.0982e-01,\n                      -4.2639e-01, -6.9990e-01,  5.0504e-01, -6.1591e-01, -2.5102e-01,\n                       4.9819e-01, -2.8467e-03, -1.9519e-01,  5.6161e-02, -1.1706e-01,\n                      -1.6111e-01, -8.0315e-02, -3.1180e-01, -6.0169e-01,  4.5632e-02,\n                      -3.7289e-01, -2.0834e-01, -1.3287e-01,  2.8988e-01, -8.7511e-02,\n                      -5.8944e-01,  7.4802e-02, -4.0726e-01, -9.4301e-01, -4.1383e-01,\n                       4.1900e-02, -1.7524e-01,  3.7425e-02, -7.7700e-01, -3.6852e-01,\n                       9.6806e-02, -3.3233e-01, -6.9038e-01, -3.9817e-01, -9.3065e-01,\n                       1.2665e-01, -1.7274e-01,  3.1572e-01, -6.7316e-01, -3.8985e-01,\n                      -2.2893e-01, -3.1844e-01, -2.9058e-01, -5.2470e-01, -4.4728e-01,\n                       1.6542e-02, -3.1481e-01,  5.1908e-01,  2.2547e-01, -4.5276e-01,\n                       4.9521e-01,  1.3668e+00, -2.6282e-01, -1.7927e-01, -4.7676e-01,\n                      -8.8166e-01, -6.0358e-01, -3.4501e-01, -4.8365e-01,  8.1184e-02,\n                      -3.8837e-01, -2.4043e-01,  7.0691e-01, -1.3208e-01, -4.8574e-02,\n                      -3.2983e-01, -3.7223e-01, -1.7206e-01,  1.5088e-01,  2.7155e-01,\n                       2.0197e-01, -4.8344e-01, -3.8341e-01,  1.2958e-01,  2.7131e-01,\n                      -8.4918e-01, -1.6067e-02])),\n             ('features.3.2.block.2.scale', tensor(0.1255)),\n             ('features.3.2.block.2.zero_point', tensor(71)),\n             ('features.3.2.block.3.scale', tensor(0.0680)),\n             ('features.3.2.block.3.zero_point', tensor(83)),\n             ('features.3.2.block.3._packed_params.dtype', torch.qint8),\n             ('features.3.2.block.3._packed_params._packed_params',\n              (tensor([[ 0.1330,  0.0163,  0.0105,  ..., -0.0420,  0.0653,  0.0140],\n                       [-0.0166,  0.0090,  0.0307,  ..., -0.0333, -0.0512,  0.0345],\n                       [-0.0165,  0.0288, -0.0165,  ..., -0.0082,  0.0535,  0.0370],\n                       ...,\n                       [-0.0193, -0.0123,  0.0070,  ...,  0.0316,  0.0474,  0.0264],\n                       [-0.0250,  0.0811,  0.0187,  ...,  0.0749,  0.0296, -0.0640],\n                       [ 0.0597,  0.0710,  0.0312,  ..., -0.0625,  0.0028,  0.0057]],\n                      size=(768, 192), dtype=torch.qint8,\n                      quantization_scheme=torch.per_channel_affine,\n                      scale=tensor([0.0012, 0.0013, 0.0021, 0.0016, 0.0013, 0.0012, 0.0011, 0.0013, 0.0015,\n                       0.0016, 0.0015, 0.0015, 0.0016, 0.0020, 0.0021, 0.0024, 0.0016, 0.0023,\n                       0.0018, 0.0012, 0.0013, 0.0010, 0.0012, 0.0017, 0.0012, 0.0015, 0.0013,\n                       0.0016, 0.0014, 0.0019, 0.0022, 0.0014, 0.0010, 0.0016, 0.0013, 0.0018,\n                       0.0015, 0.0014, 0.0013, 0.0017, 0.0017, 0.0013, 0.0014, 0.0016, 0.0012,\n                       0.0020, 0.0010, 0.0016, 0.0011, 0.0013, 0.0010, 0.0011, 0.0012, 0.0015,\n                       0.0021, 0.0014, 0.0014, 0.0012, 0.0016, 0.0016, 0.0014, 0.0020, 0.0022,\n                       0.0013, 0.0012, 0.0015, 0.0013, 0.0017, 0.0013, 0.0014, 0.0023, 0.0019,\n                       0.0014, 0.0014, 0.0018, 0.0015, 0.0017, 0.0015, 0.0011, 0.0015, 0.0015,\n                       0.0012, 0.0013, 0.0009, 0.0018, 0.0020, 0.0018, 0.0016, 0.0024, 0.0019,\n                       0.0012, 0.0016, 0.0013, 0.0013, 0.0021, 0.0020, 0.0012, 0.0018, 0.0017,\n                       0.0012, 0.0017, 0.0015, 0.0023, 0.0015, 0.0012, 0.0013, 0.0015, 0.0030,\n                       0.0011, 0.0012, 0.0012, 0.0017, 0.0016, 0.0021, 0.0013, 0.0017, 0.0013,\n                       0.0017, 0.0019, 0.0022, 0.0019, 0.0023, 0.0012, 0.0019, 0.0016, 0.0019,\n                       0.0014, 0.0016, 0.0011, 0.0017, 0.0012, 0.0013, 0.0021, 0.0017, 0.0020,\n                       0.0013, 0.0014, 0.0012, 0.0012, 0.0015, 0.0012, 0.0013, 0.0014, 0.0014,\n                       0.0023, 0.0024, 0.0028, 0.0014, 0.0013, 0.0022, 0.0010, 0.0016, 0.0015,\n                       0.0016, 0.0014, 0.0011, 0.0013, 0.0021, 0.0018, 0.0016, 0.0014, 0.0013,\n                       0.0019, 0.0011, 0.0012, 0.0017, 0.0011, 0.0027, 0.0012, 0.0012, 0.0009,\n                       0.0015, 0.0018, 0.0016, 0.0020, 0.0011, 0.0016, 0.0012, 0.0025, 0.0013,\n                       0.0014, 0.0019, 0.0015, 0.0017, 0.0012, 0.0028, 0.0017, 0.0021, 0.0012,\n                       0.0012, 0.0011, 0.0016, 0.0015, 0.0015, 0.0026, 0.0015, 0.0011, 0.0015,\n                       0.0015, 0.0012, 0.0016, 0.0019, 0.0011, 0.0018, 0.0015, 0.0011, 0.0012,\n                       0.0019, 0.0015, 0.0011, 0.0017, 0.0014, 0.0015, 0.0016, 0.0019, 0.0015,\n                       0.0016, 0.0017, 0.0014, 0.0013, 0.0012, 0.0014, 0.0015, 0.0013, 0.0016,\n                       0.0012, 0.0015, 0.0017, 0.0017, 0.0010, 0.0017, 0.0014, 0.0020, 0.0016,\n                       0.0012, 0.0017, 0.0015, 0.0016, 0.0015, 0.0020, 0.0013, 0.0015, 0.0020,\n                       0.0015, 0.0014, 0.0020, 0.0014, 0.0011, 0.0017, 0.0013, 0.0013, 0.0015,\n                       0.0017, 0.0010, 0.0024, 0.0013, 0.0016, 0.0012, 0.0011, 0.0023, 0.0023,\n                       0.0011, 0.0018, 0.0016, 0.0012, 0.0015, 0.0027, 0.0015, 0.0018, 0.0015,\n                       0.0022, 0.0019, 0.0015, 0.0017, 0.0014, 0.0018, 0.0022, 0.0013, 0.0015,\n                       0.0015, 0.0017, 0.0015, 0.0015, 0.0016, 0.0016, 0.0015, 0.0016, 0.0013,\n                       0.0013, 0.0015, 0.0014, 0.0021, 0.0012, 0.0009, 0.0016, 0.0011, 0.0013,\n                       0.0020, 0.0016, 0.0019, 0.0033, 0.0015, 0.0011, 0.0013, 0.0017, 0.0012,\n                       0.0017, 0.0014, 0.0018, 0.0015, 0.0013, 0.0016, 0.0019, 0.0015, 0.0011,\n                       0.0015, 0.0011, 0.0015, 0.0013, 0.0012, 0.0014, 0.0015, 0.0016, 0.0015,\n                       0.0015, 0.0013, 0.0016, 0.0012, 0.0012, 0.0013, 0.0018, 0.0014, 0.0019,\n                       0.0011, 0.0023, 0.0021, 0.0018, 0.0012, 0.0011, 0.0013, 0.0015, 0.0014,\n                       0.0030, 0.0019, 0.0012, 0.0010, 0.0017, 0.0020, 0.0011, 0.0011, 0.0013,\n                       0.0013, 0.0015, 0.0010, 0.0014, 0.0017, 0.0013, 0.0012, 0.0013, 0.0013,\n                       0.0012, 0.0017, 0.0018, 0.0014, 0.0028, 0.0020, 0.0016, 0.0020, 0.0018,\n                       0.0019, 0.0017, 0.0019, 0.0015, 0.0012, 0.0019, 0.0012, 0.0019, 0.0018,\n                       0.0021, 0.0016, 0.0013, 0.0012, 0.0014, 0.0013, 0.0015, 0.0012, 0.0013,\n                       0.0018, 0.0017, 0.0010, 0.0019, 0.0013, 0.0011, 0.0015, 0.0022, 0.0021,\n                       0.0016, 0.0020, 0.0017, 0.0013, 0.0010, 0.0015, 0.0012, 0.0014, 0.0014,\n                       0.0012, 0.0012, 0.0012, 0.0018, 0.0021, 0.0012, 0.0019, 0.0014, 0.0031,\n                       0.0016, 0.0014, 0.0018, 0.0015, 0.0015, 0.0014, 0.0016, 0.0010, 0.0022,\n                       0.0012, 0.0011, 0.0011, 0.0019, 0.0010, 0.0012, 0.0019, 0.0013, 0.0013,\n                       0.0010, 0.0015, 0.0013, 0.0023, 0.0013, 0.0018, 0.0019, 0.0020, 0.0016,\n                       0.0020, 0.0013, 0.0020, 0.0017, 0.0021, 0.0014, 0.0010, 0.0011, 0.0014,\n                       0.0012, 0.0013, 0.0013, 0.0020, 0.0015, 0.0023, 0.0015, 0.0015, 0.0014,\n                       0.0016, 0.0018, 0.0019, 0.0013, 0.0012, 0.0016, 0.0021, 0.0018, 0.0012,\n                       0.0013, 0.0014, 0.0012, 0.0011, 0.0009, 0.0017, 0.0014, 0.0018, 0.0015,\n                       0.0019, 0.0011, 0.0013, 0.0012, 0.0013, 0.0015, 0.0012, 0.0014, 0.0016,\n                       0.0014, 0.0019, 0.0016, 0.0013, 0.0011, 0.0021, 0.0016, 0.0015, 0.0018,\n                       0.0017, 0.0017, 0.0016, 0.0015, 0.0026, 0.0018, 0.0017, 0.0016, 0.0014,\n                       0.0016, 0.0017, 0.0011, 0.0015, 0.0017, 0.0022, 0.0014, 0.0013, 0.0014,\n                       0.0021, 0.0015, 0.0017, 0.0010, 0.0013, 0.0017, 0.0015, 0.0016, 0.0011,\n                       0.0012, 0.0011, 0.0016, 0.0026, 0.0017, 0.0016, 0.0014, 0.0022, 0.0023,\n                       0.0014, 0.0015, 0.0023, 0.0012, 0.0017, 0.0025, 0.0012, 0.0028, 0.0010,\n                       0.0014, 0.0022, 0.0012, 0.0021, 0.0015, 0.0017, 0.0018, 0.0015, 0.0017,\n                       0.0019, 0.0018, 0.0016, 0.0012, 0.0020, 0.0015, 0.0011, 0.0026, 0.0011,\n                       0.0017, 0.0015, 0.0014, 0.0012, 0.0017, 0.0013, 0.0014, 0.0016, 0.0014,\n                       0.0022, 0.0016, 0.0017, 0.0012, 0.0013, 0.0020, 0.0014, 0.0013, 0.0013,\n                       0.0014, 0.0018, 0.0010, 0.0015, 0.0019, 0.0014, 0.0014, 0.0013, 0.0016,\n                       0.0015, 0.0012, 0.0018, 0.0015, 0.0018, 0.0018, 0.0016, 0.0014, 0.0010,\n                       0.0013, 0.0016, 0.0022, 0.0015, 0.0014, 0.0012, 0.0011, 0.0017, 0.0021,\n                       0.0015, 0.0020, 0.0023, 0.0022, 0.0013, 0.0013, 0.0013, 0.0013, 0.0013,\n                       0.0017, 0.0026, 0.0019, 0.0015, 0.0019, 0.0013, 0.0012, 0.0011, 0.0013,\n                       0.0020, 0.0010, 0.0013, 0.0016, 0.0018, 0.0015, 0.0015, 0.0013, 0.0017,\n                       0.0012, 0.0016, 0.0014, 0.0019, 0.0018, 0.0012, 0.0012, 0.0012, 0.0019,\n                       0.0011, 0.0011, 0.0020, 0.0017, 0.0021, 0.0017, 0.0013, 0.0011, 0.0011,\n                       0.0015, 0.0017, 0.0012, 0.0015, 0.0017, 0.0014, 0.0017, 0.0014, 0.0014,\n                       0.0025, 0.0015, 0.0024, 0.0014, 0.0015, 0.0017, 0.0014, 0.0018, 0.0011,\n                       0.0018, 0.0017, 0.0012, 0.0013, 0.0016, 0.0021, 0.0012, 0.0014, 0.0013,\n                       0.0011, 0.0010, 0.0022, 0.0013, 0.0019, 0.0027, 0.0018, 0.0017, 0.0013,\n                       0.0024, 0.0018, 0.0015, 0.0014, 0.0015, 0.0012, 0.0022, 0.0015, 0.0017,\n                       0.0017, 0.0014, 0.0013, 0.0013, 0.0015, 0.0015, 0.0018, 0.0017, 0.0015,\n                       0.0013, 0.0021, 0.0013, 0.0010, 0.0015, 0.0013, 0.0015, 0.0019, 0.0019,\n                       0.0022, 0.0013, 0.0013, 0.0015, 0.0012, 0.0017, 0.0015, 0.0015, 0.0013,\n                       0.0014, 0.0022, 0.0012, 0.0013, 0.0017, 0.0015, 0.0025, 0.0017, 0.0015,\n                       0.0011, 0.0027, 0.0015, 0.0017, 0.0012, 0.0011, 0.0011, 0.0020, 0.0014,\n                       0.0015, 0.0018, 0.0011, 0.0016, 0.0023, 0.0016, 0.0015, 0.0013, 0.0012,\n                       0.0013, 0.0012, 0.0018, 0.0014, 0.0018, 0.0018, 0.0012, 0.0014, 0.0020,\n                       0.0018, 0.0023, 0.0014, 0.0015, 0.0014, 0.0017, 0.0013, 0.0020, 0.0015,\n                       0.0018, 0.0016, 0.0028], dtype=torch.float64),\n                      zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n                      axis=0),\n               Parameter containing:\n               tensor([-0.0425, -0.0171, -0.0525, -0.0365, -0.0543, -0.0846, -0.0428, -0.0416,\n                       -0.0460, -0.0463, -0.0269, -0.0311, -0.0365, -0.0527, -0.0585, -0.0584,\n                       -0.0321, -0.0354, -0.0594, -0.0341, -0.0356,  0.0327, -0.0529, -0.0110,\n                       -0.0334, -0.0444, -0.0269, -0.0236, -0.0497, -0.0854, -0.0407, -0.0655,\n                       -0.0144, -0.0392, -0.0553, -0.0425, -0.0532, -0.0372, -0.0227, -0.0425,\n                       -0.0634, -0.0249, -0.0790, -0.0406, -0.0402, -0.0336, -0.0373, -0.0765,\n                       -0.0210, -0.0403, -0.0328, -0.0394, -0.0504, -0.0347, -0.0546, -0.0381,\n                       -0.0569, -0.0401, -0.0391, -0.0397, -0.0191, -0.0535, -0.0533, -0.0468,\n                       -0.0393, -0.0433, -0.0349, -0.0380, -0.0507, -0.0490, -0.0202, -0.0607,\n                       -0.0444, -0.0240, -0.0451, -0.0369, -0.0426, -0.0574, -0.0528, -0.0439,\n                       -0.0307, -0.0515, -0.0318, -0.0359, -0.0434, -0.0453, -0.0426, -0.0421,\n                       -0.0868, -0.0074, -0.0661, -0.0566, -0.0285, -0.0549, -0.0271,  0.0192,\n                       -0.0301, -0.0470, -0.0526, -0.0785, -0.0186, -0.0341, -0.0126, -0.0283,\n                       -0.0102, -0.0477, -0.0593, -0.0036, -0.0281, -0.0394, -0.0449, -0.0322,\n                       -0.0576, -0.0235, -0.0213, -0.0315, -0.0299, -0.0163, -0.0261, -0.0342,\n                       -0.0422, -0.0202, -0.0332, -0.0483, -0.0351, -0.0555, -0.0225, -0.0367,\n                       -0.0362, -0.0406, -0.0434, -0.0450, -0.0520, -0.0572, -0.0735, -0.0492,\n                       -0.0157, -0.0039, -0.0408, -0.0552, -0.0224, -0.0471, -0.0485, -0.0389,\n                       -0.0223, -0.0373, -0.0881, -0.0254, -0.0642, -0.0632, -0.0177, -0.0482,\n                       -0.0445, -0.0515, -0.0335, -0.0109, -0.0461, -0.0414, -0.0418, -0.0855,\n                       -0.0525, -0.0511, -0.0964, -0.0388, -0.0471, -0.0573, -0.0382, -0.0309,\n                       -0.0193, -0.0498, -0.0219, -0.0883, -0.0360, -0.0321, -0.0506, -0.0351,\n                       -0.0212, -0.0134, -0.0087, -0.0405, -0.0243, -0.0398, -0.0681, -0.0414,\n                       -0.0302, -0.0517, -0.0432, -0.0689, -0.0447, -0.0172, -0.0412, -0.0460,\n                       -0.0335, -0.0392, -0.0079, -0.0640, -0.0137, -0.0497, -0.0528, -0.0388,\n                       -0.0249, -0.0906, -0.0360, -0.0796, -0.0778, -0.0092, -0.0320, -0.0274,\n                       -0.0595, -0.0301, -0.0405, -0.0879, -0.0567, -0.0525, -0.0480, -0.0452,\n                       -0.0318, -0.0328, -0.0340, -0.0211, -0.0395, -0.0447, -0.0266, -0.0865,\n                       -0.0539, -0.0265, -0.0595, -0.0665, -0.0365, -0.0333, -0.0442, -0.0500,\n                       -0.0540, -0.0207, -0.0143, -0.0537, -0.0285, -0.0500, -0.0445, -0.0628,\n                       -0.0501, -0.0542, -0.0488, -0.0447, -0.0348, -0.0328, -0.0396, -0.0462,\n                       -0.0316, -0.0064, -0.0301, -0.0767, -0.0602, -0.0297, -0.0325, -0.0474,\n                        0.0079, -0.0152, -0.0477, -0.0148, -0.0568, -0.0707, -0.0790, -0.0138,\n                       -0.0488, -0.0367, -0.0298, -0.0442, -0.0435, -0.0312, -0.0755, -0.0438,\n                       -0.0141, -0.0777, -0.0319, -0.0438, -0.0696, -0.0587, -0.0385, -0.0845,\n                       -0.0809, -0.0206, -0.0652, -0.0415, -0.0443, -0.0837, -0.0379, -0.0311,\n                       -0.0383, -0.0392, -0.0666, -0.0801, -0.0058, -0.0143, -0.0404, -0.0224,\n                       -0.0344, -0.0778, -0.0521, -0.0482, -0.0543, -0.0424, -0.0467, -0.0385,\n                       -0.0373, -0.0200, -0.0446, -0.0225, -0.0373, -0.1100, -0.0431, -0.0756,\n                       -0.0263, -0.0386, -0.0310, -0.0106, -0.0067, -0.0762, -0.0684, -0.0296,\n                       -0.0342, -0.0514, -0.0126, -0.0307, -0.0019, -0.0502, -0.0504, -0.0772,\n                       -0.0281, -0.0379, -0.0538, -0.0427, -0.0666, -0.0189, -0.0648, -0.0208,\n                       -0.0365, -0.0356, -0.0236, -0.0209, -0.0543, -0.0537, -0.0324, -0.0707,\n                       -0.0336, -0.0407, -0.0614, -0.0243, -0.0413, -0.0427, -0.0449, -0.0470,\n                       -0.0562, -0.0353, -0.0529, -0.0236, -0.0595, -0.0407, -0.0161, -0.0273,\n                       -0.0307, -0.0321, -0.0800, -0.0461, -0.0854, -0.0502, -0.0287, -0.0922,\n                       -0.0747, -0.0059, -0.0518, -0.0438, -0.0562, -0.0607, -0.0357, -0.0307,\n                       -0.0537, -0.0416, -0.0505, -0.0716, -0.0210, -0.0564, -0.0372, -0.0609,\n                       -0.0128, -0.0102, -0.0155, -0.0691, -0.0572, -0.0429, -0.0271, -0.0277,\n                       -0.0593, -0.0415, -0.0814, -0.0147, -0.0378, -0.0574, -0.0465, -0.0548,\n                       -0.0307, -0.0575, -0.0638, -0.0172, -0.0532, -0.0250, -0.0397, -0.0380,\n                       -0.0390, -0.0451, -0.0498, -0.0480, -0.0207, -0.0418, -0.0543, -0.0529,\n                       -0.0512, -0.0166, -0.0213, -0.0380, -0.0510, -0.0430, -0.0509, -0.0480,\n                       -0.0134, -0.0372, -0.0605, -0.0475, -0.0042, -0.0939, -0.0263, -0.0508,\n                       -0.0256, -0.0417, -0.0518, -0.0688, -0.0447, -0.0461, -0.0470, -0.0217,\n                       -0.0385, -0.0137, -0.0575, -0.0590, -0.1159, -0.0477, -0.0328, -0.0524,\n                       -0.0329, -0.0593, -0.0276, -0.0431, -0.0512, -0.0392, -0.0416, -0.0826,\n                       -0.0303, -0.0389, -0.0351, -0.0189, -0.0409, -0.0169, -0.0405, -0.0425,\n                       -0.0334, -0.0895, -0.0273, -0.0247, -0.0420, -0.0501, -0.0099, -0.0367,\n                       -0.0279, -0.0494, -0.0370, -0.0536, -0.0598, -0.0402, -0.0326, -0.0525,\n                       -0.0343, -0.0438, -0.0490, -0.0453, -0.0515, -0.0421, -0.0243, -0.0368,\n                       -0.0472, -0.0481,  0.0101, -0.0729, -0.0198, -0.0506, -0.0397, -0.0520,\n                       -0.0210, -0.0713, -0.0572, -0.0079, -0.0723, -0.0423, -0.0826, -0.0175,\n                       -0.1130, -0.0341, -0.0672, -0.0536, -0.0426, -0.0447, -0.0682, -0.0513,\n                       -0.0582, -0.0156, -0.0743, -0.0266, -0.0393, -0.0137, -0.0437, -0.0335,\n                       -0.0294, -0.0188, -0.0323, -0.0282, -0.1046,  0.0422, -0.0488, -0.0708,\n                       -0.0231, -0.0419, -0.0362, -0.0763, -0.0254, -0.0255, -0.0416, -0.0432,\n                       -0.0345, -0.0499, -0.0910, -0.0448, -0.0135, -0.0735, -0.0351, -0.0688,\n                        0.0058, -0.0388, -0.0551, -0.0826, -0.0765, -0.0509, -0.0320, -0.0636,\n                       -0.0287,  0.0073, -0.0532, -0.0549, -0.1391, -0.0177, -0.0781, -0.0363,\n                       -0.0292, -0.0455, -0.0476, -0.0329, -0.0445, -0.0742, -0.0461, -0.0248,\n                       -0.0095, -0.0368, -0.0378, -0.0624, -0.0509, -0.0333, -0.0286, -0.0420,\n                       -0.0356, -0.0648, -0.0439, -0.0594, -0.0637, -0.0460, -0.0366, -0.0393,\n                       -0.0577, -0.0498, -0.0408, -0.0501, -0.0769, -0.0340, -0.0600, -0.0424,\n                       -0.0560, -0.0221, -0.0502, -0.0471, -0.0225, -0.0231, -0.0330, -0.0416,\n                       -0.0006, -0.0672, -0.0377, -0.0169, -0.0893, -0.0420, -0.0593, -0.0809,\n                       -0.0364, -0.0352, -0.0652, -0.0339, -0.0345, -0.1004, -0.0947, -0.0882,\n                       -0.1055, -0.0955, -0.0194, -0.0337, -0.0584, -0.0904, -0.0490, -0.0834,\n                       -0.0536, -0.0553, -0.0445, -0.0214, -0.0372, -0.0355, -0.0172, -0.0638,\n                       -0.0466, -0.0684, -0.0546, -0.0223, -0.0629, -0.0041, -0.0668, -0.0383,\n                       -0.0309, -0.0317, -0.0277, -0.0447, -0.0694, -0.0549, -0.0277, -0.0273,\n                       -0.0490, -0.0657, -0.0377, -0.0528, -0.0545, -0.0311, -0.0735, -0.0030,\n                       -0.0387, -0.0491, -0.0659, -0.0567, -0.0624, -0.0751, -0.0857, -0.0281,\n                       -0.0596, -0.0457, -0.0530, -0.0458, -0.0200, -0.0314, -0.0384, -0.0866,\n                       -0.0380, -0.0413, -0.0440, -0.0371, -0.0253, -0.1077, -0.0396, -0.0532,\n                       -0.0249, -0.0813, -0.0317,  0.0022, -0.0247, -0.0502, -0.0612, -0.0341,\n                       -0.0217, -0.0509, -0.0403, -0.0168, -0.0494, -0.0325, -0.0550, -0.0067,\n                       -0.0270, -0.0427, -0.0432, -0.0306, -0.0529, -0.0383, -0.0216, -0.0415,\n                       -0.0241, -0.0231, -0.0566, -0.0250, -0.0397, -0.0924, -0.0683, -0.0645,\n                       -0.0244, -0.0046, -0.0399, -0.0324, -0.0592, -0.0405, -0.0246, -0.0874,\n                       -0.0599, -0.0395, -0.0310, -0.0491, -0.0490, -0.0609, -0.0368, -0.0620,\n                       -0.0384, -0.0255, -0.0174, -0.0600, -0.0490, -0.0273, -0.0125, -0.0280,\n                       -0.0584, -0.0537, -0.0234, -0.0447, -0.0287, -0.0356, -0.0882, -0.0669,\n                       -0.0429, -0.0447, -0.0503, -0.0646, -0.0473, -0.0567, -0.0272, -0.0292,\n                       -0.0789, -0.0221, -0.0379, -0.0408, -0.0588, -0.0111, -0.0606, -0.0274,\n                       -0.0383, -0.0489, -0.0519, -0.0404, -0.0352, -0.0391, -0.0299, -0.0471],\n                      requires_grad=True))),\n             ('features.3.2.block.5.scale', tensor(0.0810)),\n             ('features.3.2.block.5.zero_point', tensor(63)),\n             ('features.3.2.block.5._packed_params.dtype', torch.qint8),\n             ('features.3.2.block.5._packed_params._packed_params',\n              (tensor([[-0.0106,  0.0018,  0.0779,  ..., -0.0478, -0.0442,  0.0336],\n                       [ 0.0840, -0.1978, -0.0325,  ..., -0.0271,  0.0542, -0.0217],\n                       [-0.0398,  0.0017, -0.0149,  ...,  0.0464,  0.0315,  0.0580],\n                       ...,\n                       [-0.0707, -0.0118, -0.0034,  ...,  0.0101, -0.0219, -0.0371],\n                       [-0.0205,  0.2148,  0.0341,  ..., -0.0648,  0.0341,  0.0409],\n                       [ 0.0376, -0.0213,  0.1242,  ..., -0.0339, -0.0477, -0.0615]],\n                      size=(192, 768), dtype=torch.qint8,\n                      quantization_scheme=torch.per_channel_affine,\n                      scale=tensor([0.0018, 0.0027, 0.0017, 0.0024, 0.0018, 0.0014, 0.0022, 0.0019, 0.0024,\n                       0.0017, 0.0018, 0.0014, 0.0032, 0.0021, 0.0015, 0.0014, 0.0016, 0.0016,\n                       0.0023, 0.0009, 0.0022, 0.0021, 0.0030, 0.0016, 0.0016, 0.0019, 0.0021,\n                       0.0015, 0.0015, 0.0017, 0.0015, 0.0018, 0.0016, 0.0019, 0.0015, 0.0020,\n                       0.0017, 0.0017, 0.0030, 0.0022, 0.0022, 0.0016, 0.0020, 0.0013, 0.0016,\n                       0.0022, 0.0020, 0.0016, 0.0018, 0.0019, 0.0019, 0.0015, 0.0015, 0.0019,\n                       0.0014, 0.0033, 0.0008, 0.0018, 0.0023, 0.0016, 0.0022, 0.0019, 0.0016,\n                       0.0018, 0.0024, 0.0018, 0.0012, 0.0016, 0.0019, 0.0019, 0.0032, 0.0019,\n                       0.0022, 0.0016, 0.0019, 0.0018, 0.0020, 0.0014, 0.0020, 0.0012, 0.0020,\n                       0.0017, 0.0020, 0.0016, 0.0015, 0.0018, 0.0017, 0.0025, 0.0020, 0.0034,\n                       0.0018, 0.0019, 0.0022, 0.0016, 0.0032, 0.0017, 0.0018, 0.0038, 0.0018,\n                       0.0016, 0.0018, 0.0032, 0.0018, 0.0015, 0.0017, 0.0025, 0.0019, 0.0046,\n                       0.0017, 0.0018, 0.0015, 0.0021, 0.0018, 0.0017, 0.0015, 0.0017, 0.0016,\n                       0.0014, 0.0018, 0.0015, 0.0016, 0.0018, 0.0024, 0.0021, 0.0017, 0.0016,\n                       0.0019, 0.0040, 0.0019, 0.0016, 0.0025, 0.0028, 0.0018, 0.0015, 0.0035,\n                       0.0017, 0.0021, 0.0022, 0.0032, 0.0017, 0.0017, 0.0022, 0.0013, 0.0019,\n                       0.0018, 0.0036, 0.0018, 0.0020, 0.0021, 0.0014, 0.0016, 0.0022, 0.0017,\n                       0.0020, 0.0016, 0.0016, 0.0015, 0.0033, 0.0015, 0.0014, 0.0022, 0.0014,\n                       0.0018, 0.0014, 0.0017, 0.0014, 0.0017, 0.0016, 0.0018, 0.0014, 0.0016,\n                       0.0019, 0.0016, 0.0039, 0.0017, 0.0016, 0.0014, 0.0015, 0.0020, 0.0016,\n                       0.0017, 0.0016, 0.0015, 0.0020, 0.0042, 0.0017, 0.0015, 0.0016, 0.0015,\n                       0.0017, 0.0034, 0.0013], dtype=torch.float64),\n                      zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n                      axis=0),\n               Parameter containing:\n               tensor([-2.5314e-02,  1.4655e-01,  1.6231e-02, -5.4467e-04, -2.8546e-02,\n                        2.7484e-02,  7.1062e-02, -2.1444e-02,  1.6755e-01, -9.3700e-03,\n                        1.2441e-02, -3.3202e-02, -3.8879e-02, -1.0720e-03, -1.4287e-01,\n                       -1.3092e-02, -1.5880e-02, -5.3940e-03, -7.8362e-03, -5.4484e-03,\n                        2.5012e-02,  1.5150e-02, -1.5423e-02, -2.1522e-02,  2.1791e-02,\n                       -1.1041e-02,  1.6287e-02,  1.1157e-01,  3.4343e-02,  2.2793e-02,\n                       -1.3176e-02, -7.2611e-03, -5.3384e-03,  7.0685e-03, -2.1704e-02,\n                       -4.3026e-03, -6.9141e-03, -1.1056e-02, -1.9560e-03,  5.3866e-03,\n                       -3.4563e-02, -1.9979e-02, -2.4604e-02, -1.1907e-01,  9.9709e-02,\n                       -5.9414e-04, -5.6720e-03, -1.1968e-02,  1.7509e-03,  6.1557e-03,\n                        5.1225e-02,  5.1815e-05,  6.4286e-03,  1.9075e-03,  2.7527e-02,\n                       -1.1921e-01, -1.4971e-02,  3.4513e-02,  1.5453e-02, -2.2913e-02,\n                       -1.8862e-03,  1.7757e-02, -3.4597e-02, -2.4940e-05, -3.3014e-02,\n                        1.2258e-03,  7.6241e-03, -3.1022e-03,  8.6880e-04, -1.6786e-02,\n                        2.8387e-01,  2.6133e-02, -3.3134e-02, -6.7928e-03, -1.0105e-02,\n                       -2.5364e-02,  2.7364e-02,  1.1398e-02,  7.8245e-04,  1.0407e-01,\n                       -1.2255e-01, -2.1758e-02,  5.3054e-03, -4.3157e-02, -2.3372e-02,\n                        3.3105e-02, -2.7773e-02,  1.0635e-02,  1.6218e-02,  1.4139e-01,\n                       -1.5775e-02, -1.1280e-02,  1.2160e-01,  8.9377e-03, -3.2882e-03,\n                        1.9241e-02, -1.0158e-02,  1.5001e-01, -1.1049e-02,  1.0156e-01,\n                       -2.6928e-02,  1.8181e-01, -6.3945e-04, -1.2707e-02, -6.6736e-04,\n                        3.9878e-03, -7.0313e-03, -4.0230e-02, -4.8627e-03, -1.3959e-02,\n                       -1.0680e-02,  5.2400e-04,  9.8755e-03,  1.8654e-02,  1.1007e-01,\n                       -2.5473e-02,  1.1241e-03,  1.0276e-01,  2.6366e-03, -5.9963e-03,\n                       -1.2559e-01, -4.5386e-03, -1.8686e-03, -1.3732e-02, -9.7704e-03,\n                        2.6457e-02,  5.7495e-03,  1.3668e-02,  1.8742e-01, -1.8531e-02,\n                       -1.2096e-02, -1.1978e-02,  2.8596e-02,  1.3698e-02, -4.0637e-02,\n                       -6.7687e-03, -1.7110e-02, -1.1279e-02,  4.3565e-02, -1.0199e-02,\n                        1.2799e-01,  6.2202e-03, -4.7383e-03, -3.4705e-04,  2.1272e-02,\n                       -2.1121e-01,  1.2059e-02, -9.0414e-03,  7.6668e-03,  3.8287e-02,\n                       -1.7760e-02,  1.4126e-01,  3.0693e-03, -2.9145e-03,  1.8254e-03,\n                        4.4308e-02, -1.6222e-02,  1.4063e-01, -1.2786e-02, -8.5792e-03,\n                        2.8454e-03,  2.9988e-02, -1.5076e-02, -2.8032e-02,  9.9152e-03,\n                       -1.6141e-02,  1.0024e-01,  2.3993e-02,  1.4175e-02, -2.5546e-02,\n                        1.9096e-02, -8.2653e-02,  6.0372e-03,  1.0729e-01,  2.2336e-02,\n                        3.0379e-02,  2.5051e-02, -1.1907e-02,  1.1722e-01,  1.4430e-02,\n                       -1.5842e-02, -4.1128e-03,  1.8595e-02,  1.7141e-02,  8.0723e-02,\n                        4.2632e-03, -2.3696e-02, -6.4135e-02,  7.0373e-03, -2.4349e-02,\n                        1.3575e-01, -2.7396e-02], requires_grad=True))),\n             ('features.4.0.weight',\n              tensor([ 4.7104e-02, -3.1982e-05,  1.0997e-01,  4.5181e-02,  9.6073e-02,\n                       5.3982e-02,  2.8141e-02,  5.7653e-02,  2.1784e-05,  1.1365e-01,\n                       5.7266e-02,  1.1968e-01,  6.3784e-02,  1.2431e-01, -8.5026e-04,\n                       9.8360e-02,  7.4685e-02,  8.2736e-02,  1.2116e-01,  1.3331e-01,\n                       6.2596e-02,  5.0625e-02,  8.2880e-02,  1.0054e-01,  5.5071e-02,\n                       7.3344e-02,  2.2186e-01, -2.3088e-04,  5.4510e-02,  4.3502e-02,\n                       6.4303e-02,  5.4122e-02,  9.6145e-02,  3.5238e-02,  1.2048e-01,\n                       7.0735e-02,  5.2717e-02,  1.4130e-01,  7.8453e-02,  1.4798e-01,\n                       2.0968e-01,  7.9901e-02,  1.4269e-01, -1.8079e-05,  2.8004e-05,\n                       1.1037e-01,  5.8128e-02,  5.1027e-02,  6.0646e-02,  9.7686e-02,\n                       4.2124e-02,  7.0982e-02,  2.0893e-01,  6.8349e-02,  3.3919e-02,\n                      -1.7868e-05,  1.8349e-01,  6.6566e-02,  4.8781e-02,  9.6492e-02,\n                       8.1325e-02,  1.0991e-01,  5.5243e-02,  5.4392e-02,  6.5234e-02,\n                       1.0280e-01,  1.0226e-01,  1.2511e-01,  3.2522e-02,  8.7786e-02,\n                      -4.4158e-05,  4.9872e-02,  5.7658e-02,  3.4106e-02,  4.0302e-02,\n                       9.6446e-02,  4.9162e-02,  1.0267e-01,  9.8264e-02, -1.5920e-02,\n                      -5.2916e-04,  5.5133e-02,  8.7420e-02,  6.0514e-02,  6.8254e-02,\n                       8.4386e-02,  1.2457e-01,  7.0108e-02,  7.5019e-02,  1.4213e-03,\n                       7.0340e-02,  6.8032e-02, -1.1679e-04,  1.1647e-01,  1.7069e-01,\n                       5.0536e-02,  9.6077e-02,  3.9674e-05,  7.3775e-02, -1.0836e-05,\n                       6.0605e-02,  1.4417e-05,  4.2891e-02,  6.7857e-02,  1.3557e-01,\n                       1.9411e-01,  5.6233e-02, -1.9368e-05,  3.6055e-02,  7.5356e-02,\n                       1.5356e-01,  4.2240e-02,  3.9303e-02,  4.1924e-02, -4.8994e-04,\n                       1.8190e-01,  6.9376e-02, -4.5672e-04,  3.6385e-02,  6.9943e-02,\n                       6.1266e-05,  6.4478e-02,  1.1760e-01,  1.3565e-01,  8.5371e-02,\n                       8.7789e-02,  7.8372e-02,  1.0198e-01,  4.3002e-05,  7.4209e-02,\n                       8.3480e-02,  3.0843e-02,  6.0672e-02,  5.0969e-02,  1.0115e-01,\n                       6.2806e-02,  1.1498e-01,  1.3627e-01,  5.0898e-02,  1.2008e-01,\n                      -2.9777e-04,  6.0313e-02,  1.5832e-01,  4.7734e-02,  9.3475e-02,\n                       8.7996e-06,  1.0987e-01,  7.2565e-02,  6.2978e-02,  7.6695e-02,\n                       4.5190e-02,  8.1503e-05,  1.4118e-01,  7.1144e-02,  5.1833e-02,\n                       3.9172e-02,  6.2448e-02, -1.6458e-05,  6.1821e-02,  1.3721e-01,\n                       1.3931e-01,  6.0276e-02,  1.0106e-01,  1.0876e-01,  1.3627e-01,\n                       4.3509e-02,  1.5427e-01,  3.1580e-02,  9.1133e-02,  5.3625e-02,\n                       9.9781e-02,  1.0053e-02,  6.5643e-02,  9.9112e-06,  8.5701e-02,\n                       2.1427e-01,  4.1824e-02,  5.2360e-02, -6.2259e-05,  5.5783e-02,\n                       6.9279e-02,  7.7687e-02,  9.3514e-02,  5.7405e-02, -5.5155e-06,\n                       1.5886e-01,  3.8094e-02,  5.9270e-02,  1.5491e-01,  6.4247e-02,\n                       1.4275e-05,  5.0633e-02])),\n             ('features.4.0.bias',\n              tensor([-1.5128e-03,  5.7371e-06, -6.9440e-03,  8.8456e-03,  9.8063e-04,\n                      -5.9083e-04, -3.2181e-03, -3.7424e-03, -8.7128e-06, -6.6723e-03,\n                      -1.6324e-03, -1.7709e-03, -3.5667e-03, -8.9442e-03,  2.6360e-05,\n                      -8.9050e-03,  1.0104e-03, -1.6105e-04, -4.1681e-03,  1.1794e-02,\n                      -2.4900e-03, -8.5562e-04, -5.8117e-03,  1.0269e-02,  2.2187e-03,\n                      -3.0033e-03, -2.2885e-02,  1.5111e-05, -3.6303e-03, -1.4496e-03,\n                      -1.8159e-03, -2.1462e-03, -4.8679e-03, -3.1745e-03, -5.3077e-03,\n                      -1.5570e-03, -2.9141e-03, -5.6406e-03, -2.7842e-03, -7.0209e-03,\n                       1.4877e-02,  1.2692e-03, -1.2267e-03,  4.7939e-06, -4.4331e-06,\n                      -5.4906e-03, -3.4103e-03, -3.0547e-03, -2.9173e-03, -1.9693e-03,\n                      -4.8219e-03,  6.9847e-05,  6.9934e-03, -3.2773e-03, -2.5432e-03,\n                       3.4661e-06, -2.7047e-02, -2.6839e-03, -4.8273e-03, -3.2361e-03,\n                      -4.1884e-03,  2.7444e-03,  8.3996e-04,  3.5703e-04, -1.8166e-04,\n                      -8.1037e-03, -2.2273e-03, -9.0464e-03, -6.8866e-04, -6.5667e-03,\n                       1.6021e-05, -2.7504e-03,  3.2999e-03, -3.1986e-03, -8.3399e-04,\n                      -3.0651e-03, -1.3183e-03, -1.9160e-03, -6.1851e-03,  1.3385e-03,\n                       2.2826e-05, -3.8201e-03, -3.5330e-03, -4.6088e-03, -6.4662e-04,\n                      -1.2339e-03, -7.8529e-03, -3.0330e-03, -4.9976e-03,  1.6062e-02,\n                      -2.0794e-03, -2.5285e-03,  8.6248e-06, -5.0502e-03, -1.1520e-02,\n                      -1.7292e-03, -5.4645e-03, -1.0103e-05, -4.4995e-03,  5.7418e-06,\n                      -1.9269e-02, -6.2806e-06,  3.9096e-04,  4.6393e-04, -2.6565e-03,\n                      -6.0376e-03, -2.4653e-03,  1.0244e-05, -8.4284e-04, -8.3868e-04,\n                       1.0193e-04, -5.3890e-03,  1.1308e-04, -1.2976e-03,  5.0799e-05,\n                      -3.5454e-03, -3.5409e-03,  6.9905e-05, -1.3018e-03, -7.4071e-04,\n                      -1.8392e-06, -1.7825e-03, -5.2489e-03, -1.5560e-03, -6.3213e-03,\n                      -3.2575e-03,  3.4783e-03, -4.4343e-03, -1.2085e-05, -9.6446e-04,\n                      -1.0338e-03, -1.7890e-03,  5.3598e-04, -2.7918e-03, -2.9653e-03,\n                       1.3084e-03, -6.5508e-03, -7.2259e-03, -8.9784e-03, -2.4822e-03,\n                       3.1247e-05, -1.7374e-03, -1.0010e-02, -1.0794e-03, -1.0902e-03,\n                      -8.6282e-06, -3.7371e-03, -3.4833e-04, -2.1033e-03, -2.2837e-03,\n                      -2.6797e-03, -1.4192e-05, -7.0819e-03, -4.8489e-03,  3.9601e-04,\n                      -1.0748e-03, -2.6387e-03,  3.7966e-06, -4.5175e-03, -8.7790e-03,\n                      -5.6430e-03, -4.7775e-03, -3.1157e-03, -3.2478e-03, -6.1264e-03,\n                      -2.0504e-03, -3.5730e-02, -1.6255e-03, -3.0277e-03, -1.6344e-03,\n                      -7.4596e-03, -1.1887e-03,  1.8651e-03, -4.9218e-07, -2.5409e-03,\n                       4.3984e-02, -1.6167e-03,  4.6118e-04,  1.3158e-06, -1.9963e-03,\n                      -1.2776e-03, -2.6221e-04, -5.9654e-03, -1.1765e-05, -2.4782e-06,\n                      -9.4025e-03, -2.9415e-03,  4.2584e-04, -6.2048e-03, -3.8197e-03,\n                      -6.4919e-06, -2.2715e-03])),\n             ('features.4.1.weight',\n              tensor([[[[ 0.0120,  0.0281],\n                        [ 0.0174,  0.0308]],\n              \n                       [[-0.0067,  0.0107],\n                        [ 0.0027,  0.0027]],\n              \n                       [[ 0.0388,  0.0468],\n                        [ 0.0414,  0.0374]],\n              \n                       ...,\n              \n                       [[ 0.0762,  0.0669],\n                        [ 0.0428,  0.0495]],\n              \n                       [[ 0.0094,  0.0107],\n                        [ 0.0053, -0.0080]],\n              \n                       [[-0.0308, -0.0428],\n                        [-0.0174, -0.0227]]],\n              \n              \n                      [[[-0.0094,  0.0094],\n                        [ 0.0070,  0.0211]],\n              \n                       [[ 0.0141,  0.0070],\n                        [-0.0258, -0.0070]],\n              \n                       [[ 0.0070,  0.0070],\n                        [ 0.0117,  0.0094]],\n              \n                       ...,\n              \n                       [[-0.0117, -0.0141],\n                        [-0.0070,  0.0000]],\n              \n                       [[-0.0305, -0.0187],\n                        [ 0.0000,  0.0234]],\n              \n                       [[-0.0187,  0.0047],\n                        [-0.0516, -0.0305]]],\n              \n              \n                      [[[ 0.0406,  0.0431],\n                        [ 0.0234,  0.0271]],\n              \n                       [[ 0.0049, -0.0012],\n                        [-0.0062, -0.0025]],\n              \n                       [[ 0.0603,  0.0505],\n                        [ 0.0640,  0.0603]],\n              \n                       ...,\n              \n                       [[ 0.0086,  0.0283],\n                        [ 0.0283,  0.0480]],\n              \n                       [[-0.0172,  0.0037],\n                        [ 0.0172,  0.0098]],\n              \n                       [[ 0.0197, -0.0135],\n                        [ 0.0086, -0.0086]]],\n              \n              \n                      ...,\n              \n              \n                      [[[ 0.0493,  0.0621],\n                        [ 0.0529,  0.0712]],\n              \n                       [[-0.0073,  0.0091],\n                        [ 0.0055,  0.0018]],\n              \n                       [[ 0.0694,  0.0913],\n                        [ 0.0694,  0.0840]],\n              \n                       ...,\n              \n                       [[-0.0694, -0.0676],\n                        [-0.1022, -0.1114]],\n              \n                       [[ 0.0164, -0.0055],\n                        [ 0.0037, -0.0201]],\n              \n                       [[-0.0602, -0.0347],\n                        [-0.0566, -0.0621]]],\n              \n              \n                      [[[ 0.0440,  0.0642],\n                        [-0.0238, -0.0037]],\n              \n                       [[-0.0092,  0.0128],\n                        [-0.0183,  0.0257]],\n              \n                       [[ 0.0532,  0.0000],\n                        [ 0.0220,  0.0018]],\n              \n                       ...,\n              \n                       [[-0.1632, -0.2347],\n                        [-0.0202, -0.0532]],\n              \n                       [[ 0.0293, -0.0092],\n                        [ 0.0055, -0.0073]],\n              \n                       [[-0.0312,  0.0220],\n                        [-0.0348, -0.0037]]],\n              \n              \n                      [[[ 0.0078,  0.0431],\n                        [-0.0078,  0.0215]],\n              \n                       [[ 0.0259,  0.0017],\n                        [-0.0078, -0.0052]],\n              \n                       [[ 0.0017, -0.0103],\n                        [ 0.0060, -0.0043]],\n              \n                       ...,\n              \n                       [[ 0.0052, -0.0034],\n                        [-0.0086,  0.0069]],\n              \n                       [[ 0.0138, -0.0095],\n                        [-0.0060, -0.0198]],\n              \n                       [[ 0.0069,  0.0026],\n                        [ 0.0078,  0.0095]]]], size=(384, 192, 2, 2), dtype=torch.qint8,\n                     quantization_scheme=torch.per_channel_affine,\n                     scale=tensor([0.0013, 0.0023, 0.0012, 0.0017, 0.0011, 0.0011, 0.0020, 0.0012, 0.0014,\n                      0.0018, 0.0017, 0.0019, 0.0011, 0.0009, 0.0015, 0.0013, 0.0013, 0.0013,\n                      0.0023, 0.0013, 0.0011, 0.0013, 0.0011, 0.0012, 0.0019, 0.0013, 0.0018,\n                      0.0016, 0.0011, 0.0014, 0.0018, 0.0014, 0.0014, 0.0010, 0.0010, 0.0019,\n                      0.0016, 0.0015, 0.0010, 0.0011, 0.0014, 0.0015, 0.0018, 0.0011, 0.0016,\n                      0.0012, 0.0016, 0.0012, 0.0015, 0.0011, 0.0014, 0.0018, 0.0014, 0.0008,\n                      0.0013, 0.0020, 0.0009, 0.0013, 0.0015, 0.0014, 0.0015, 0.0007, 0.0013,\n                      0.0011, 0.0016, 0.0011, 0.0016, 0.0017, 0.0014, 0.0017, 0.0014, 0.0015,\n                      0.0010, 0.0017, 0.0011, 0.0012, 0.0016, 0.0013, 0.0010, 0.0008, 0.0012,\n                      0.0015, 0.0011, 0.0019, 0.0008, 0.0024, 0.0013, 0.0010, 0.0017, 0.0020,\n                      0.0013, 0.0012, 0.0012, 0.0014, 0.0010, 0.0013, 0.0017, 0.0009, 0.0013,\n                      0.0007, 0.0012, 0.0015, 0.0014, 0.0015, 0.0019, 0.0014, 0.0019, 0.0010,\n                      0.0016, 0.0014, 0.0010, 0.0010, 0.0014, 0.0016, 0.0011, 0.0008, 0.0011,\n                      0.0012, 0.0018, 0.0011, 0.0014, 0.0014, 0.0015, 0.0011, 0.0017, 0.0010,\n                      0.0028, 0.0010, 0.0013, 0.0028, 0.0016, 0.0014, 0.0011, 0.0019, 0.0012,\n                      0.0014, 0.0015, 0.0018, 0.0015, 0.0018, 0.0011, 0.0013, 0.0033, 0.0016,\n                      0.0020, 0.0014, 0.0009, 0.0012, 0.0012, 0.0014, 0.0007, 0.0011, 0.0015,\n                      0.0008, 0.0010, 0.0013, 0.0013, 0.0011, 0.0012, 0.0013, 0.0015, 0.0014,\n                      0.0013, 0.0017, 0.0012, 0.0011, 0.0013, 0.0014, 0.0010, 0.0016, 0.0014,\n                      0.0012, 0.0011, 0.0009, 0.0010, 0.0016, 0.0018, 0.0025, 0.0008, 0.0019,\n                      0.0020, 0.0010, 0.0008, 0.0012, 0.0017, 0.0012, 0.0016, 0.0010, 0.0015,\n                      0.0012, 0.0015, 0.0017, 0.0013, 0.0021, 0.0014, 0.0015, 0.0014, 0.0022,\n                      0.0009, 0.0012, 0.0016, 0.0016, 0.0017, 0.0016, 0.0016, 0.0018, 0.0016,\n                      0.0013, 0.0014, 0.0013, 0.0014, 0.0011, 0.0014, 0.0014, 0.0018, 0.0013,\n                      0.0038, 0.0017, 0.0012, 0.0017, 0.0018, 0.0024, 0.0016, 0.0018, 0.0011,\n                      0.0018, 0.0010, 0.0015, 0.0014, 0.0007, 0.0015, 0.0013, 0.0012, 0.0013,\n                      0.0010, 0.0014, 0.0014, 0.0012, 0.0007, 0.0017, 0.0029, 0.0012, 0.0021,\n                      0.0011, 0.0010, 0.0012, 0.0013, 0.0017, 0.0014, 0.0013, 0.0021, 0.0018,\n                      0.0019, 0.0022, 0.0011, 0.0014, 0.0019, 0.0011, 0.0023, 0.0010, 0.0011,\n                      0.0009, 0.0015, 0.0011, 0.0006, 0.0017, 0.0013, 0.0012, 0.0022, 0.0012,\n                      0.0015, 0.0018, 0.0012, 0.0013, 0.0014, 0.0011, 0.0014, 0.0012, 0.0012,\n                      0.0011, 0.0019, 0.0016, 0.0012, 0.0018, 0.0015, 0.0013, 0.0021, 0.0009,\n                      0.0012, 0.0012, 0.0017, 0.0010, 0.0015, 0.0034, 0.0011, 0.0015, 0.0013,\n                      0.0020, 0.0012, 0.0012, 0.0012, 0.0014, 0.0021, 0.0014, 0.0015, 0.0017,\n                      0.0014, 0.0014, 0.0015, 0.0015, 0.0018, 0.0010, 0.0010, 0.0011, 0.0013,\n                      0.0018, 0.0012, 0.0017, 0.0019, 0.0017, 0.0026, 0.0012, 0.0013, 0.0018,\n                      0.0012, 0.0013, 0.0022, 0.0011, 0.0018, 0.0020, 0.0014, 0.0014, 0.0013,\n                      0.0010, 0.0015, 0.0014, 0.0014, 0.0018, 0.0024, 0.0012, 0.0020, 0.0013,\n                      0.0022, 0.0017, 0.0007, 0.0013, 0.0016, 0.0017, 0.0013, 0.0014, 0.0016,\n                      0.0012, 0.0010, 0.0007, 0.0011, 0.0024, 0.0012, 0.0010, 0.0012, 0.0011,\n                      0.0024, 0.0014, 0.0011, 0.0013, 0.0020, 0.0013, 0.0006, 0.0011, 0.0010,\n                      0.0015, 0.0014, 0.0012, 0.0010, 0.0014, 0.0010, 0.0020, 0.0010, 0.0017,\n                      0.0016, 0.0011, 0.0018, 0.0018, 0.0018, 0.0009], dtype=torch.float64),\n                     zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n                     axis=0)),\n             ('features.4.1.bias',\n              Parameter containing:\n              tensor([ 9.5501e-04, -2.3966e-02,  7.5645e-03,  3.7212e-03, -4.9345e-03,\n                       6.2153e-03,  5.2916e-03, -3.0879e-03, -5.4858e-03, -3.4078e-04,\n                      -8.7533e-05,  1.1242e-03,  4.4371e-03, -3.8733e-03,  4.7135e-03,\n                      -8.1355e-03,  7.0964e-03, -8.2947e-03,  2.5342e-03,  1.4122e-03,\n                       8.7938e-04,  5.1845e-03,  1.5864e-02,  6.7397e-03,  1.6528e-03,\n                       1.4652e-02, -1.8500e-03, -2.6179e-03, -7.1042e-03,  4.7148e-03,\n                      -1.4415e-02,  2.7835e-03,  4.6448e-03,  9.2975e-03, -2.0480e-04,\n                      -4.3572e-04, -1.6028e-03,  8.0725e-04,  4.8077e-03,  2.8829e-03,\n                       1.0676e-03, -6.6583e-03, -2.9316e-03, -1.1353e-04,  1.6212e-03,\n                       5.4236e-03, -9.8364e-05, -2.9110e-03, -3.8872e-03, -6.0394e-04,\n                      -9.0961e-03, -7.5080e-03, -4.3525e-03, -2.8051e-04,  1.8786e-03,\n                      -5.7264e-03, -8.7538e-04,  1.6692e-03, -7.5258e-03, -3.8642e-03,\n                       7.0680e-05,  9.8411e-03,  4.0555e-03,  1.5803e-03, -5.9506e-03,\n                       9.2921e-02,  2.3096e-03,  4.6317e-03, -2.9917e-04,  4.3918e-03,\n                      -6.3219e-03, -1.7398e-03,  1.4595e-03, -2.1626e-03,  1.8227e-03,\n                       2.3588e-03,  2.8616e-04,  8.8240e-03,  4.3317e-03, -8.5350e-04,\n                      -3.6777e-03, -8.7810e-03, -2.0837e-03,  1.5531e-03,  9.1461e-03,\n                       4.4952e-03, -3.8883e-04,  4.2405e-03, -5.3886e-03, -1.5456e-03,\n                       1.9837e-02,  3.6850e-03,  1.2133e-02,  2.4787e-03, -1.9240e-03,\n                      -2.6593e-03, -1.7176e-03,  6.9335e-03,  9.9946e-05, -1.2071e-02,\n                       1.3311e-03, -2.1116e-03,  1.4336e-03, -6.4888e-03,  3.7482e-03,\n                       2.6813e-03,  2.9642e-03, -3.1778e-03, -4.2337e-03,  8.1247e-03,\n                      -2.0560e-02,  1.1271e-03,  1.2070e-03,  2.8053e-03, -9.8702e-04,\n                       1.5746e-03,  1.3010e-03,  9.1008e-03, -4.1556e-03, -3.8087e-04,\n                       5.8708e-03,  2.2039e-03,  4.5411e-03,  1.6062e-03,  7.2919e-03,\n                      -5.9318e-02, -6.2178e-03,  7.8483e-03,  6.0578e-03, -2.6736e-03,\n                      -4.8416e-04,  5.9720e-03, -1.9473e-03,  2.4443e-03,  2.4241e-03,\n                       2.1407e-03,  3.9384e-03,  1.0933e-02,  1.9654e-03,  5.4571e-03,\n                      -3.5391e-03, -4.1650e-03,  1.0297e-02, -9.8273e-03,  5.7245e-03,\n                      -7.9206e-03, -5.6947e-03,  2.9327e-03, -7.1338e-03,  2.3436e-03,\n                      -7.0165e-03,  5.4360e-03, -1.7574e-03,  9.7583e-03,  1.2045e-03,\n                       4.7537e-03,  9.6948e-02, -4.5152e-03, -4.1679e-03, -3.3798e-04,\n                      -1.0405e-02,  4.3328e-04,  5.1628e-03,  1.0333e-03,  1.3366e-03,\n                      -1.3411e-02,  1.1075e-03,  7.3431e-03, -4.0659e-03,  5.0371e-04,\n                      -4.7882e-03, -5.2293e-03, -1.9483e-04, -2.6186e-03, -5.7543e-04,\n                       4.4659e-03, -1.1789e-03,  3.7324e-03, -3.0605e-03,  8.1916e-05,\n                       2.1774e-02, -5.3598e-03,  1.1412e-02,  1.0816e-02, -1.4128e-02,\n                      -8.8071e-03, -2.6767e-03,  2.0789e-03, -1.3213e-03, -1.0961e-02,\n                      -2.4499e-03, -3.0399e-03, -1.1724e-03,  1.3329e-03, -8.6850e-04,\n                      -2.3008e-03,  6.5724e-03,  8.5171e-03,  1.5714e-03, -3.8878e-03,\n                      -6.0179e-03, -3.5303e-03, -1.5990e-03,  3.6422e-03,  4.9613e-03,\n                      -1.3423e-03,  2.3659e-05, -3.9465e-03, -9.5609e-03,  6.7443e-03,\n                       8.5703e-04,  2.4789e-03,  1.7142e-03,  1.8202e-03, -5.5349e-03,\n                      -9.7154e-03,  5.9615e-03, -2.0610e-03, -4.7438e-03, -1.6581e-03,\n                      -1.3480e-02, -2.4359e-03, -1.3500e-03,  4.2794e-04,  7.2792e-03,\n                      -3.2997e-02,  1.0379e-02, -5.7132e-03,  2.2617e-03,  2.0366e-03,\n                       1.2356e-02,  2.3449e-03, -5.7359e-03,  3.8503e-04, -1.1418e-03,\n                       2.7972e-04,  4.2029e-03,  8.2761e-03,  2.2085e-02,  2.4961e-03,\n                      -1.4586e-03,  3.2053e-03,  3.5926e-03, -1.1389e-02,  1.1152e-03,\n                       7.3363e-02,  2.2396e-03,  1.2130e-02, -6.3478e-03,  3.1250e-03,\n                      -1.3007e-03,  3.0025e-03, -1.7498e-03,  8.0947e-04, -2.8554e-03,\n                       5.3891e-03, -1.9222e-03, -4.8820e-04, -5.4449e-03, -2.6138e-03,\n                      -5.5122e-03,  5.3355e-03,  1.0045e-03, -2.6561e-03, -1.1932e-01,\n                      -3.4330e-03,  6.3165e-04,  1.0707e-02, -2.5891e-04,  3.9488e-04,\n                      -2.9197e-03, -2.0188e-03,  2.9785e-03,  4.4889e-03, -2.1951e-02,\n                       1.4155e-03,  6.7694e-03,  7.7378e-03, -1.5391e-02, -7.4318e-04,\n                      -1.7895e-02,  9.5527e-04,  2.3400e-03,  2.1743e-03, -5.2923e-03,\n                       2.6685e-03, -1.2706e-03, -3.7780e-03,  1.3250e-03,  1.3070e-04,\n                       3.4082e-03, -4.8284e-03,  1.1592e-02, -1.6065e-04,  1.5429e-03,\n                      -2.0636e-03, -9.2872e-04,  1.2529e-03,  4.2829e-03,  3.3602e-03,\n                      -1.5326e-02, -9.4004e-05, -7.8751e-03,  1.5063e-03, -7.1135e-05,\n                       9.1529e-04, -2.5780e-03,  4.0216e-03,  3.6040e-03, -2.6917e-03,\n                      -1.7881e-03, -9.5634e-03,  1.0189e-03,  4.9608e-03, -9.8068e-03,\n                       5.9586e-03, -1.8876e-03, -8.6548e-03, -2.0118e-03, -9.4922e-05,\n                       9.2733e-06, -4.9244e-03,  6.0604e-04,  6.7945e-03, -2.1346e-03,\n                       8.4689e-06, -6.6467e-04, -4.9526e-03,  1.0811e-04,  9.2449e-03,\n                      -1.6822e-03, -2.0427e-03,  4.9382e-03,  5.9332e-03,  8.8730e-03,\n                       5.2798e-03,  2.0028e-04,  9.0252e-04, -7.2804e-03, -1.1783e-03,\n                      -2.8825e-03, -2.1890e-03, -2.7520e-04,  4.5555e-05, -1.1340e-01,\n                      -1.1494e-03,  4.0316e-03,  3.1418e-03,  1.2432e-02,  1.7689e-03,\n                      -3.7472e-03, -7.4857e-03,  1.2877e-03,  1.6007e-01,  2.2555e-03,\n                       3.5948e-02,  3.5620e-03,  7.3171e-03,  1.1586e-03,  5.5018e-03,\n                       1.6377e-03, -1.8288e-03,  9.3512e-04, -5.6861e-03,  1.9014e-03,\n                       1.7705e-03, -8.2443e-04,  2.5638e-03,  3.7351e-04,  3.5324e-03,\n                      -4.4499e-04,  1.0713e-02,  5.0335e-03,  6.9548e-03,  1.1740e-03,\n                       1.3224e-02, -3.1928e-03,  1.0911e-02, -4.6747e-03, -1.6758e-03,\n                      -8.1397e-03,  1.3012e-07, -2.1076e-03, -1.0119e-02],\n                     requires_grad=True)),\n             ('features.4.1.scale', tensor(0.0050)),\n             ('features.4.1.zero_point', tensor(59)),\n             ('features.5.0.layer_scale',\n              tensor([[[-0.0724]],\n              \n                      [[-0.0281]],\n              \n                      [[ 0.0474]],\n              \n                      [[ 0.0460]],\n              \n                      [[-0.0360]],\n              \n                      [[-0.0473]],\n              \n                      [[ 0.0431]],\n              \n                      [[ 0.0392]],\n              \n                      [[ 0.0375]],\n              \n                      [[ 0.0296]],\n              \n                      [[-0.0518]],\n              \n                      [[ 0.0360]],\n              \n                      [[ 0.0380]],\n              \n                      [[ 0.0629]],\n              \n                      [[ 0.0444]],\n              \n                      [[ 0.0498]],\n              \n                      [[-0.0551]],\n              \n                      [[-0.0590]],\n              \n                      [[ 0.0437]],\n              \n                      [[ 0.0574]],\n              \n                      [[-0.0395]],\n              \n                      [[-0.0374]],\n              \n                      [[-0.0461]],\n              \n                      [[-0.0347]],\n              \n                      [[ 0.0515]],\n              \n                      [[-0.0266]],\n              \n                      [[-0.0300]],\n              \n                      [[ 0.0311]],\n              \n                      [[-0.0685]],\n              \n                      [[ 0.0694]],\n              \n                      [[ 0.0281]],\n              \n                      [[-0.0452]],\n              \n                      [[-0.0447]],\n              \n                      [[-0.0515]],\n              \n                      [[ 0.0757]],\n              \n                      [[ 0.0359]],\n              \n                      [[-0.0483]],\n              \n                      [[-0.0450]],\n              \n                      [[ 0.0545]],\n              \n                      [[-0.0528]],\n              \n                      [[ 0.0368]],\n              \n                      [[ 0.0389]],\n              \n                      [[ 0.0486]],\n              \n                      [[-0.0293]],\n              \n                      [[-0.0381]],\n              \n                      [[ 0.0340]],\n              \n                      [[-0.0493]],\n              \n                      [[ 0.0466]],\n              \n                      [[-0.0423]],\n              \n                      [[-0.0321]],\n              \n                      [[-0.0500]],\n              \n                      [[ 0.0681]],\n              \n                      [[-0.0577]],\n              \n                      [[ 0.0676]],\n              \n                      [[-0.0503]],\n              \n                      [[ 0.0238]],\n              \n                      [[ 0.0651]],\n              \n                      [[-0.0713]],\n              \n                      [[-0.0318]],\n              \n                      [[-0.0628]],\n              \n                      [[ 0.0328]],\n              \n                      [[ 0.0204]],\n              \n                      [[-0.0354]],\n              \n                      [[ 0.0490]],\n              \n                      [[-0.0463]],\n              \n                      [[ 0.0351]],\n              \n                      [[-0.0388]],\n              \n                      [[ 0.0563]],\n              \n                      [[-0.0724]],\n              \n                      [[-0.0295]],\n              \n                      [[ 0.0359]],\n              \n                      [[-0.0579]],\n              \n                      [[ 0.0511]],\n              \n                      [[-0.0308]],\n              \n                      [[-0.0676]],\n              \n                      [[-0.0427]],\n              \n                      [[-0.0376]],\n              \n                      [[-0.0773]],\n              \n                      [[-0.0451]],\n              \n                      [[-0.0797]],\n              \n                      [[-0.0593]],\n              \n                      [[ 0.0377]],\n              \n                      [[ 0.0361]],\n              \n                      [[-0.0285]],\n              \n                      [[-0.0734]],\n              \n                      [[-0.0469]],\n              \n                      [[-0.0420]],\n              \n                      [[-0.0848]],\n              \n                      [[-0.0431]],\n              \n                      [[ 0.0301]],\n              \n                      [[ 0.0366]],\n              \n                      [[-0.0394]],\n              \n                      [[-0.0320]],\n              \n                      [[-0.0457]],\n              \n                      [[-0.0735]],\n              \n                      [[ 0.0476]],\n              \n                      [[ 0.0343]],\n              \n                      [[-0.0413]],\n              \n                      [[-0.0728]],\n              \n                      [[-0.0285]],\n              \n                      [[-0.0640]],\n              \n                      [[ 0.0364]],\n              \n                      [[-0.0768]],\n              \n                      [[ 0.0468]],\n              \n                      [[ 0.0368]],\n              \n                      [[-0.0526]],\n              \n                      [[-0.0548]],\n              \n                      [[-0.0489]],\n              \n                      [[-0.0777]],\n              \n                      [[-0.0743]],\n              \n                      [[-0.0548]],\n              \n                      [[-0.0401]],\n              \n                      [[ 0.0421]],\n              \n                      [[-0.0429]],\n              \n                      [[-0.0364]],\n              \n                      [[-0.0727]],\n              \n                      [[-0.0353]],\n              \n                      [[-0.0389]],\n              \n                      [[ 0.0540]],\n              \n                      [[ 0.0490]],\n              \n                      [[ 0.0812]],\n              \n                      [[ 0.0447]],\n              \n                      [[-0.0613]],\n              \n                      [[ 0.0393]],\n              \n                      [[ 0.0206]],\n              \n                      [[ 0.0515]],\n              \n                      [[-0.0288]],\n              \n                      [[ 0.0744]],\n              \n                      [[ 0.0637]],\n              \n                      [[ 0.0776]],\n              \n                      [[ 0.0849]],\n              \n                      [[ 0.0542]],\n              \n                      [[-0.0475]],\n              \n                      [[-0.0345]],\n              \n                      [[ 0.0494]],\n              \n                      [[ 0.0539]],\n              \n                      [[-0.0548]],\n              \n                      [[ 0.0321]],\n              \n                      [[-0.0826]],\n              \n                      [[-0.0380]],\n              \n                      [[-0.0348]],\n              \n                      [[ 0.0455]],\n              \n                      [[-0.0439]],\n              \n                      [[-0.0452]],\n              \n                      [[ 0.0405]],\n              \n                      [[ 0.0341]],\n              \n                      [[ 0.0252]],\n              \n                      [[ 0.0642]],\n              \n                      [[-0.0846]],\n              \n                      [[ 0.0272]],\n              \n                      [[-0.0334]],\n              \n                      [[ 0.0864]],\n              \n                      [[-0.0513]],\n              \n                      [[-0.0600]],\n              \n                      [[-0.0404]],\n              \n                      [[-0.0525]],\n              \n                      [[-0.1164]],\n              \n                      [[ 0.0403]],\n              \n                      [[ 0.0375]],\n              \n                      [[-0.0409]],\n              \n                      [[ 0.0420]],\n              \n                      [[ 0.0560]],\n              \n                      [[-0.0427]],\n              \n                      [[-0.0523]],\n              \n                      [[ 0.0435]],\n              \n                      [[ 0.0513]],\n              \n                      [[-0.0416]],\n              \n                      [[-0.0472]],\n              \n                      [[ 0.0731]],\n              \n                      [[-0.0625]],\n              \n                      [[-0.0628]],\n              \n                      [[ 0.0419]],\n              \n                      [[ 0.0542]],\n              \n                      [[ 0.0554]],\n              \n                      [[ 0.0401]],\n              \n                      [[ 0.0267]],\n              \n                      [[ 0.0478]],\n              \n                      [[-0.0379]],\n              \n                      [[-0.0647]],\n              \n                      [[ 0.0490]],\n              \n                      [[-0.0279]],\n              \n                      [[-0.0258]],\n              \n                      [[-0.0410]],\n              \n                      [[-0.0361]],\n              \n                      [[-0.0489]],\n              \n                      [[ 0.0358]],\n              \n                      [[ 0.0323]],\n              \n                      [[ 0.0801]],\n              \n                      [[-0.0373]],\n              \n                      [[-0.0273]],\n              \n                      [[ 0.0581]],\n              \n                      [[-0.0746]],\n              \n                      [[-0.0606]],\n              \n                      [[-0.0297]],\n              \n                      [[ 0.0571]],\n              \n                      [[ 0.0675]],\n              \n                      [[ 0.0635]],\n              \n                      [[-0.0209]],\n              \n                      [[ 0.0304]],\n              \n                      [[-0.0355]],\n              \n                      [[ 0.0272]],\n              \n                      [[ 0.0475]],\n              \n                      [[ 0.0611]],\n              \n                      [[ 0.0298]],\n              \n                      [[-0.0468]],\n              \n                      [[ 0.0361]],\n              \n                      [[ 0.0541]],\n              \n                      [[-0.0554]],\n              \n                      [[-0.0350]],\n              \n                      [[-0.0424]],\n              \n                      [[-0.0488]],\n              \n                      [[-0.0384]],\n              \n                      [[ 0.0259]],\n              \n                      [[ 0.0339]],\n              \n                      [[-0.0436]],\n              \n                      [[-0.0314]],\n              \n                      [[-0.0408]],\n              \n                      [[-0.0462]],\n              \n                      [[ 0.0725]],\n              \n                      [[ 0.0607]],\n              \n                      [[ 0.0374]],\n              \n                      [[-0.0337]],\n              \n                      [[ 0.0435]],\n              \n                      [[ 0.0514]],\n              \n                      [[-0.0511]],\n              \n                      [[-0.0490]],\n              \n                      [[-0.0387]],\n              \n                      [[ 0.0288]],\n              \n                      [[-0.0530]],\n              \n                      [[ 0.0274]],\n              \n                      [[-0.0332]],\n              \n                      [[-0.0420]],\n              \n                      [[-0.0478]],\n              \n                      [[-0.0536]],\n              \n                      [[ 0.0269]],\n              \n                      [[-0.0370]],\n              \n                      [[ 0.0442]],\n              \n                      [[ 0.0426]],\n              \n                      [[-0.0272]],\n              \n                      [[-0.0473]],\n              \n                      [[ 0.0629]],\n              \n                      [[ 0.0526]],\n              \n                      [[-0.0600]],\n              \n                      [[ 0.0305]],\n              \n                      [[-0.0423]],\n              \n                      [[ 0.0491]],\n              \n                      [[-0.0404]],\n              \n                      [[ 0.0404]],\n              \n                      [[-0.0339]],\n              \n                      [[ 0.0631]],\n              \n                      [[ 0.0352]],\n              \n                      [[ 0.0368]],\n              \n                      [[ 0.0447]],\n              \n                      [[ 0.0430]],\n              \n                      [[-0.0601]],\n              \n                      [[ 0.0555]],\n              \n                      [[ 0.0489]],\n              \n                      [[-0.0353]],\n              \n                      [[ 0.0539]],\n              \n                      [[ 0.0825]],\n              \n                      [[-0.0416]],\n              \n                      [[-0.0315]],\n              \n                      [[ 0.0445]],\n              \n                      [[ 0.0337]],\n              \n                      [[-0.0352]],\n              \n                      [[ 0.0524]],\n              \n                      [[-0.0397]],\n              \n                      [[-0.0619]],\n              \n                      [[ 0.0414]],\n              \n                      [[ 0.0442]],\n              \n                      [[-0.0562]],\n              \n                      [[ 0.0481]],\n              \n                      [[-0.0363]],\n              \n                      [[-0.0338]],\n              \n                      [[-0.0404]],\n              \n                      [[ 0.0443]],\n              \n                      [[-0.0388]],\n              \n                      [[ 0.0345]],\n              \n                      [[-0.0336]],\n              \n                      [[ 0.0566]],\n              \n                      [[ 0.0303]],\n              \n                      [[ 0.0484]],\n              \n                      [[ 0.0381]],\n              \n                      [[ 0.0435]],\n              \n                      [[ 0.0540]],\n              \n                      [[ 0.0525]],\n              \n                      [[-0.0383]],\n              \n                      [[ 0.0619]],\n              \n                      [[ 0.0555]],\n              \n                      [[-0.0625]],\n              \n                      [[-0.0616]],\n              \n                      [[-0.0434]],\n              \n                      [[-0.0578]],\n              \n                      [[ 0.0740]],\n              \n                      [[ 0.0416]],\n              \n                      [[ 0.0546]],\n              \n                      [[ 0.0465]],\n              \n                      [[ 0.0318]],\n              \n                      [[ 0.0420]],\n              \n                      [[-0.0455]],\n              \n                      [[-0.0662]],\n              \n                      [[ 0.0689]],\n              \n                      [[ 0.0500]],\n              \n                      [[ 0.0361]],\n              \n                      [[ 0.0338]],\n              \n                      [[ 0.0543]],\n              \n                      [[-0.0587]],\n              \n                      [[-0.0548]],\n              \n                      [[ 0.0541]],\n              \n                      [[ 0.0576]],\n              \n                      [[-0.0569]],\n              \n                      [[ 0.0313]],\n              \n                      [[-0.0765]],\n              \n                      [[-0.0454]],\n              \n                      [[-0.0266]],\n              \n                      [[-0.0628]],\n              \n                      [[-0.0492]],\n              \n                      [[-0.0466]],\n              \n                      [[ 0.0369]],\n              \n                      [[ 0.0362]],\n              \n                      [[-0.0502]],\n              \n                      [[-0.0525]],\n              \n                      [[-0.0324]],\n              \n                      [[ 0.0456]],\n              \n                      [[ 0.0416]],\n              \n                      [[-0.0452]],\n              \n                      [[-0.0372]],\n              \n                      [[ 0.0313]],\n              \n                      [[ 0.0435]],\n              \n                      [[ 0.0435]],\n              \n                      [[ 0.0704]],\n              \n                      [[ 0.0419]],\n              \n                      [[-0.0829]],\n              \n                      [[-0.0761]],\n              \n                      [[-0.0765]],\n              \n                      [[-0.0286]],\n              \n                      [[-0.0367]],\n              \n                      [[ 0.0331]],\n              \n                      [[ 0.0232]],\n              \n                      [[-0.0416]],\n              \n                      [[-0.0375]],\n              \n                      [[-0.0441]],\n              \n                      [[ 0.0429]],\n              \n                      [[ 0.0452]],\n              \n                      [[-0.0248]],\n              \n                      [[-0.0457]],\n              \n                      [[ 0.0377]],\n              \n                      [[-0.0320]],\n              \n                      [[-0.0496]],\n              \n                      [[ 0.0691]],\n              \n                      [[ 0.0376]],\n              \n                      [[ 0.0386]],\n              \n                      [[-0.0580]],\n              \n                      [[-0.0186]],\n              \n                      [[ 0.0626]],\n              \n                      [[ 0.0220]],\n              \n                      [[-0.0520]],\n              \n                      [[-0.0754]],\n              \n                      [[ 0.0506]],\n              \n                      [[-0.0811]],\n              \n                      [[ 0.0208]],\n              \n                      [[-0.0491]],\n              \n                      [[-0.0471]],\n              \n                      [[-0.0577]],\n              \n                      [[-0.0363]],\n              \n                      [[-0.0579]],\n              \n                      [[ 0.0604]],\n              \n                      [[-0.0398]],\n              \n                      [[ 0.0490]],\n              \n                      [[ 0.0533]],\n              \n                      [[ 0.0498]],\n              \n                      [[ 0.0405]],\n              \n                      [[-0.0695]],\n              \n                      [[ 0.0397]],\n              \n                      [[-0.0851]],\n              \n                      [[-0.0523]],\n              \n                      [[ 0.0278]],\n              \n                      [[-0.0551]],\n              \n                      [[-0.0404]],\n              \n                      [[ 0.0373]],\n              \n                      [[-0.0264]],\n              \n                      [[ 0.0539]],\n              \n                      [[-0.0441]],\n              \n                      [[ 0.0324]]])),\n             ('features.5.0.block.0.weight',\n              tensor([[[[ 0.0143,  0.0131,  0.0083,  ...,  0.0155,  0.0214,  0.0155],\n                        [ 0.0202,  0.0107,  0.0048,  ...,  0.0274,  0.0048,  0.0143],\n                        [ 0.0214,  0.0119,  0.0512,  ...,  0.0322,  0.0202,  0.0143],\n                        ...,\n                        [ 0.0119, -0.0036,  0.0476,  ...,  0.0167,  0.0024,  0.0167],\n                        [ 0.0107,  0.0167,  0.0155,  ...,  0.0226,  0.0179,  0.0083],\n                        [ 0.0024,  0.0262,  0.0107,  ...,  0.0167,  0.0167,  0.0357]]],\n              \n              \n                      [[[ 0.0019, -0.0095,  0.0038,  ..., -0.0057,  0.0019,  0.0000],\n                        [-0.0057, -0.0038,  0.0019,  ...,  0.0000,  0.0038,  0.0000],\n                        [-0.0038,  0.0038, -0.0190,  ...,  0.0076, -0.0038,  0.0019],\n                        ...,\n                        [-0.0038,  0.0000, -0.0323,  ..., -0.0304, -0.0095, -0.0057],\n                        [-0.0038, -0.0076,  0.0057,  ...,  0.0000,  0.0019,  0.0057],\n                        [ 0.0038, -0.0076,  0.0019,  ..., -0.0019,  0.0038, -0.0038]]],\n              \n              \n                      [[[ 0.0020,  0.0140,  0.0060,  ..., -0.0020,  0.0060, -0.0100],\n                        [ 0.0080, -0.0020, -0.0040,  ...,  0.0020,  0.0080,  0.0060],\n                        [ 0.0020, -0.0060, -0.0522,  ...,  0.0401,  0.0140,  0.0040],\n                        ...,\n                        [-0.0201, -0.0100, -0.0743,  ...,  0.0562,  0.0241,  0.0120],\n                        [-0.0060,  0.0000, -0.0060,  ...,  0.0000,  0.0060, -0.0020],\n                        [ 0.0040, -0.0080, -0.0020,  ...,  0.0120, -0.0020, -0.0060]]],\n              \n              \n                      ...,\n              \n              \n                      [[[ 0.0182, -0.0019,  0.0010,  ...,  0.0077,  0.0019,  0.0192],\n                        [ 0.0048,  0.0067,  0.0249,  ...,  0.0029,  0.0153,  0.0105],\n                        [ 0.0096,  0.0211,  0.0757,  ...,  0.0230,  0.0192,  0.0221],\n                        ...,\n                        [ 0.0211, -0.0010,  0.0086,  ..., -0.0067,  0.0086,  0.0125],\n                        [ 0.0038,  0.0010,  0.0096,  ..., -0.0019,  0.0029,  0.0058],\n                        [ 0.0105,  0.0019,  0.0182,  ...,  0.0019,  0.0038,  0.0125]]],\n              \n              \n                      [[[-0.0121, -0.0060, -0.0060,  ...,  0.0030, -0.0121, -0.0090],\n                        [-0.0030,  0.0000, -0.0060,  ..., -0.0151, -0.0060,  0.0030],\n                        [-0.0060, -0.0030, -0.0151,  ..., -0.0121, -0.0090, -0.0030],\n                        ...,\n                        [-0.0030, -0.0090, -0.0754,  ..., -0.0332, -0.0060, -0.0030],\n                        [-0.0090, -0.0181, -0.0060,  ...,  0.0121, -0.0121, -0.0060],\n                        [-0.0060, -0.0090, -0.0090,  ...,  0.0060,  0.0000, -0.0060]]],\n              \n              \n                      [[[ 0.0053,  0.0018,  0.0106,  ..., -0.0124, -0.0088, -0.0142],\n                        [ 0.0088,  0.0053,  0.0106,  ..., -0.0088, -0.0018, -0.0159],\n                        [ 0.0088,  0.0212,  0.0602,  ..., -0.0726, -0.0248, -0.0071],\n                        ...,\n                        [ 0.0177,  0.0248,  0.0761,  ..., -0.0619, -0.0212, -0.0142],\n                        [ 0.0088,  0.0035,  0.0106,  ..., -0.0124, -0.0053, -0.0053],\n                        [ 0.0053,  0.0124,  0.0177,  ..., -0.0106, -0.0018, -0.0142]]]],\n                     size=(384, 1, 7, 7), dtype=torch.qint8,\n                     quantization_scheme=torch.per_channel_affine,\n                     scale=tensor([1.1908e-03, 1.8981e-03, 2.0068e-03, 1.4480e-03, 2.7511e-03, 2.4704e-03,\n                      9.9464e-04, 2.8208e-03, 2.4325e-03, 1.8746e-03, 8.4975e-04, 2.6797e-03,\n                      3.1227e-03, 2.6110e-03, 2.9578e-03, 2.2530e-03, 2.9257e-03, 2.9935e-03,\n                      2.4402e-03, 2.2106e-03, 3.3384e-03, 1.6919e-03, 3.1608e-03, 2.7391e-03,\n                      1.6306e-03, 4.5206e-04, 1.7638e-03, 1.7854e-03, 1.4273e-03, 2.4384e-03,\n                      1.8519e-03, 1.7412e-03, 2.2667e-03, 2.8558e-03, 2.7922e-03, 2.4116e-03,\n                      2.3095e-03, 2.7348e-03, 3.3492e-03, 2.8655e-03, 2.1585e-03, 1.4963e-03,\n                      2.7277e-03, 2.6639e-03, 2.5747e-03, 3.1092e-03, 2.5883e-03, 3.0308e-03,\n                      2.4218e-03, 2.0978e-03, 2.7036e-03, 2.7274e-03, 1.7522e-03, 3.5827e-03,\n                      2.5101e-03, 1.2244e-03, 3.2069e-03, 2.5188e-03, 1.4209e-03, 3.1112e-03,\n                      1.9880e-03, 2.3149e-03, 1.6507e-03, 1.5228e-03, 2.6379e-03, 1.2265e-03,\n                      1.9292e-03, 2.9260e-03, 3.0666e-03, 4.4012e-04, 2.9155e-03, 2.3611e-03,\n                      1.9275e-03, 1.3331e-03, 1.5559e-03, 3.0947e-03, 2.5051e-03, 8.2854e-04,\n                      2.7027e-03, 2.9106e-03, 2.7443e-03, 1.7069e-03, 1.6476e-03, 1.6078e-03,\n                      3.2580e-03, 2.9014e-03, 2.3308e-03, 3.2717e-03, 2.1099e-03, 2.8924e-03,\n                      1.7303e-03, 2.9519e-03, 1.7183e-03, 2.2765e-03, 3.1281e-03, 2.9396e-03,\n                      2.6396e-03, 2.8636e-03, 3.3348e-03, 2.0252e-03, 2.4000e-03, 2.7683e-03,\n                      3.2627e-03, 1.6501e-03, 1.6514e-03, 2.5018e-03, 5.2654e-04, 3.3559e-03,\n                      2.5128e-03, 3.4869e-03, 2.5213e-03, 3.1082e-03, 2.7754e-03, 2.3120e-03,\n                      1.6185e-03, 3.3197e-03, 2.7305e-03, 2.8391e-03, 8.7931e-04, 3.1853e-03,\n                      2.7255e-03, 2.4428e-03, 2.6759e-03, 2.3507e-03, 2.1704e-03, 2.1104e-03,\n                      1.4304e-03, 3.2965e-03, 2.7356e-03, 9.6863e-04, 2.6417e-03, 2.7340e-03,\n                      3.4167e-03, 2.6121e-03, 1.7912e-03, 3.1189e-03, 2.5287e-03, 1.3663e-03,\n                      2.5416e-03, 1.7987e-03, 2.7465e-03, 2.7220e-03, 2.3745e-03, 1.1092e-03,\n                      2.1201e-03, 2.4744e-03, 2.2879e-03, 2.6793e-03, 2.9201e-03, 2.7753e-03,\n                      7.7187e-04, 3.0139e-03, 2.9874e-03, 3.2214e-03, 3.0454e-03, 2.5545e-03,\n                      1.1960e-03, 2.9892e-03, 2.5293e-03, 1.7595e-03, 2.8458e-03, 2.5104e-03,\n                      2.0803e-03, 2.3697e-03, 3.2373e-03, 2.0053e-03, 2.9063e-03, 2.5469e-03,\n                      3.2684e-03, 2.9301e-03, 3.0233e-03, 1.8508e-03, 7.6216e-04, 1.6147e-03,\n                      2.8229e-03, 1.5829e-03, 1.9802e-03, 2.0766e-03, 3.1288e-03, 2.1154e-03,\n                      1.6462e-03, 2.8480e-03, 3.4446e-03, 1.9772e-03, 2.2472e-03, 2.3140e-03,\n                      1.5808e-03, 3.4382e-03, 2.3064e-03, 2.2771e-03, 1.1458e-03, 3.1112e-03,\n                      2.5872e-03, 1.6747e-03, 2.9598e-03, 2.6912e-03, 1.9383e-03, 1.4456e-03,\n                      2.7650e-03, 2.6477e-03, 1.6651e-03, 2.6855e-03, 2.5435e-03, 2.6994e-03,\n                      2.4853e-03, 2.9049e-03, 2.5697e-03, 2.1575e-03, 2.3177e-03, 1.6889e-03,\n                      1.8204e-03, 8.0452e-04, 1.9457e-03, 1.8101e-03, 1.9998e-03, 2.8086e-03,\n                      1.7706e-03, 2.3686e-03, 3.2500e-03, 3.1129e-03, 1.7894e-03, 1.6975e-03,\n                      2.8981e-03, 1.1014e-03, 2.5438e-03, 2.6776e-03, 2.6912e-03, 1.4790e-03,\n                      3.0296e-03, 2.2326e-04, 3.0261e-03, 2.1308e-03, 3.0372e-03, 2.8231e-03,\n                      2.7947e-03, 6.8955e-04, 2.1111e-03, 2.6882e-03, 1.6002e-03, 1.8972e-03,\n                      2.3291e-03, 3.2344e-03, 2.2893e-03, 2.2795e-03, 3.0620e-03, 2.4803e-03,\n                      2.7010e-03, 2.2191e-03, 1.8542e-03, 2.2911e-03, 2.0139e-03, 1.9779e-03,\n                      9.5634e-04, 1.5104e-03, 1.2001e-03, 2.8684e-03, 2.5861e-03, 2.9920e-03,\n                      2.6817e-03, 3.2369e-03, 3.0367e-03, 3.1674e-03, 2.9295e-03, 2.3677e-03,\n                      8.3003e-04, 2.3034e-03, 2.8642e-03, 1.2279e-03, 2.5273e-03, 2.2241e-03,\n                      2.2178e-03, 2.0158e-03, 2.6592e-03, 1.7677e-03, 4.6265e-04, 2.4662e-03,\n                      1.9934e-03, 1.7284e-03, 2.2277e-03, 3.0156e-03, 1.1281e-03, 2.7144e-03,\n                      2.8981e-03, 2.3863e-03, 1.4621e-03, 3.2371e-03, 2.0553e-03, 8.4003e-04,\n                      2.7300e-03, 2.6132e-03, 2.3107e-03, 3.0928e-03, 2.6326e-03, 1.6696e-03,\n                      2.8419e-03, 1.0930e-03, 2.4956e-03, 1.4419e-03, 6.6420e-04, 4.8459e-04,\n                      2.7273e-03, 2.3715e-03, 3.2687e-03, 2.8771e-03, 2.0179e-03, 2.3529e-03,\n                      1.7921e-03, 2.2238e-03, 2.3471e-03, 2.6343e-03, 3.0990e-03, 7.4978e-04,\n                      3.1899e-03, 3.1132e-03, 1.2306e-03, 5.8319e-04, 2.4553e-03, 1.3743e-03,\n                      1.5925e-03, 1.8741e-03, 2.6115e-03, 1.6483e-03, 2.5145e-03, 2.5050e-03,\n                      3.0095e-03, 2.9893e-03, 2.3562e-03, 2.1065e-03, 2.9220e-03, 2.6958e-03,\n                      2.8028e-03, 2.6226e-03, 3.4332e-03, 3.2555e-03, 4.3569e-03, 2.2228e-03,\n                      2.2927e-03, 1.8415e-03, 1.4974e-03, 2.9378e-03, 1.6353e-03, 2.9018e-03,\n                      1.5692e-03, 2.2325e-03, 1.5948e-03, 2.4343e-03, 3.0123e-03, 1.6515e-03,\n                      2.1070e-03, 2.6618e-03, 1.7418e-03, 1.8532e-03, 3.1432e-03, 1.0421e-03,\n                      2.6496e-03, 1.4330e-03, 2.3867e-03, 3.1135e-03, 1.4012e-03, 3.1769e-03,\n                      1.4774e-03, 2.4824e-03, 3.0734e-03, 2.9900e-03, 3.0048e-03, 2.8946e-03,\n                      9.1211e-05, 1.7189e-03, 2.4397e-03, 2.6535e-03, 1.8420e-03, 2.4468e-03,\n                      3.2654e-03, 2.3636e-03, 3.1936e-03, 2.3019e-03, 2.0092e-03, 2.8074e-03,\n                      2.3526e-03, 2.9226e-03, 1.7159e-03, 9.5876e-04, 3.0152e-03, 1.7695e-03],\n                     dtype=torch.float64),\n                     zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n                     axis=0)),\n             ('features.5.0.block.0.bias',\n              Parameter containing:\n              tensor([-3.1252e-03, -1.7840e-03, -3.0520e-03, -2.9576e-03, -2.9211e-03,\n                      -6.4807e-04,  1.5272e-02, -1.5483e-03,  9.3851e-04,  7.8762e-05,\n                      -1.1286e-02,  5.8694e-03, -4.9952e-04,  5.2737e-03,  1.1560e-05,\n                      -1.3457e-02, -2.9283e-03,  1.7920e-03, -3.2967e-03, -5.8806e-03,\n                       4.3564e-03,  6.3418e-04, -3.0626e-04,  4.5296e-03, -3.7628e-03,\n                       1.3465e-02,  3.8492e-04, -1.0024e-05, -3.5140e-03,  4.5551e-03,\n                       1.7199e-02, -1.8418e-03,  1.4082e-03, -8.3048e-03, -1.3767e-03,\n                      -5.6227e-05, -8.3393e-04,  2.6995e-03, -6.9750e-03,  1.3807e-03,\n                       4.3574e-03, -2.2282e-03, -2.3357e-04, -8.3756e-04,  1.1143e-03,\n                      -2.2472e-03, -5.3346e-04,  8.5174e-05,  3.7289e-03,  8.3794e-04,\n                      -1.4571e-03, -8.0080e-03, -1.3853e-03,  4.7357e-03,  1.3034e-03,\n                      -1.0734e-03, -5.6164e-03,  1.6942e-03,  7.1453e-03, -4.1736e-03,\n                      -1.6811e-03, -7.8681e-03,  1.3457e-03, -6.3014e-03,  9.6515e-04,\n                       1.8319e-02, -3.5343e-03, -5.8728e-03, -2.1907e-03, -3.3552e-02,\n                      -8.0399e-04, -6.5889e-03, -3.5910e-03,  1.5621e-03,  1.3726e-02,\n                       4.4831e-03,  1.0835e-03,  7.8943e-03,  4.6302e-03, -2.4845e-03,\n                      -3.8523e-03, -2.7845e-03,  2.1265e-03, -5.6109e-04,  2.0720e-03,\n                      -2.0003e-03,  1.8954e-03, -3.4077e-03, -3.5789e-04,  6.0820e-04,\n                       5.2516e-03, -1.9668e-03, -1.1496e-03,  3.6891e-03, -1.2053e-03,\n                      -5.1778e-03, -4.8610e-03,  4.0213e-03, -3.7729e-03,  2.9499e-03,\n                       5.1353e-03, -2.1241e-04,  7.6084e-03,  3.7513e-03,  1.0207e-03,\n                       4.9655e-03,  1.2541e-03, -4.2291e-03, -2.0534e-04,  9.2437e-03,\n                       1.4777e-03, -6.9828e-03, -1.7189e-03, -1.1284e-03,  2.1310e-04,\n                      -7.6899e-04,  3.0460e-03,  1.0648e-02,  1.0516e-02, -9.8073e-04,\n                       1.3364e-03, -4.4918e-04,  8.3775e-03, -1.2682e-03, -2.1546e-03,\n                       8.7224e-03,  2.5772e-03, -3.8808e-03,  4.8802e-04,  6.8942e-03,\n                      -2.1701e-04,  1.7272e-03,  5.7867e-04, -1.3882e-03,  6.8727e-04,\n                      -3.2937e-03, -2.1948e-03,  4.3766e-04,  2.2048e-03, -1.3449e-03,\n                       5.7533e-04, -1.9788e-03, -3.1949e-03,  2.5533e-04,  9.2318e-04,\n                       3.8144e-03,  5.9651e-03, -2.2035e-04,  5.9625e-04,  4.7229e-03,\n                      -3.4954e-03, -5.1578e-04, -2.1180e-03, -6.6542e-03,  6.7747e-03,\n                       1.4053e-02, -2.0944e-05, -7.9532e-04,  9.6981e-04,  1.8583e-03,\n                      -2.7728e-03, -5.1677e-03, -8.7055e-05,  2.5883e-04,  6.5164e-03,\n                       1.6274e-03,  1.2178e-03, -9.8509e-03, -1.3148e-03, -1.9451e-03,\n                      -1.2712e-06,  4.5787e-03,  1.5737e-03, -1.4285e-02, -9.6740e-03,\n                       3.3378e-03, -4.5734e-03, -1.4848e-04, -1.2160e-03,  4.2738e-04,\n                       4.1905e-04, -5.4713e-03,  1.8201e-02, -1.8349e-02, -3.2461e-03,\n                       4.6307e-04,  1.3447e-03,  2.1090e-03,  2.0191e-03, -9.1145e-04,\n                       4.4830e-03,  2.7949e-03, -8.5294e-03, -1.1594e-03, -3.7638e-03,\n                       4.9078e-03, -1.4739e-02, -2.4720e-03,  9.6318e-04, -7.6483e-04,\n                      -3.5987e-03, -2.1932e-03, -6.1108e-03, -4.5704e-03, -2.8613e-03,\n                       2.9973e-03,  6.3154e-03, -1.0527e-02, -4.0927e-03,  2.4461e-02,\n                      -8.7694e-04,  7.3360e-03,  8.1937e-04, -4.2902e-04, -8.3729e-04,\n                      -2.2444e-05,  7.5580e-04,  6.0615e-04,  3.4114e-03, -2.0079e-04,\n                       1.5555e-03, -2.2759e-03, -2.9936e-03, -7.1170e-03,  2.5774e-03,\n                      -9.7048e-03,  1.3302e-03, -2.8154e-03, -1.2574e-04,  4.6938e-02,\n                      -5.7831e-03, -6.9944e-03, -1.0675e-02,  2.4904e-04, -1.8223e-03,\n                       2.6479e-06,  1.4135e-04, -6.9762e-04, -8.6020e-03, -8.3556e-04,\n                      -8.4036e-04,  8.3405e-03, -1.4508e-02,  5.4566e-03,  1.6576e-03,\n                      -4.2845e-04, -2.3575e-03, -3.8168e-03, -3.6521e-03, -1.0532e-03,\n                      -1.8903e-04,  2.7327e-04, -4.1204e-03, -6.2268e-03, -4.8452e-03,\n                      -8.6128e-03,  1.7298e-04, -2.9099e-04,  3.4886e-03, -3.2573e-04,\n                      -1.4186e-04, -2.3452e-03, -1.0807e-03, -9.3157e-04, -5.6120e-02,\n                       2.0296e-03,  6.5416e-04, -1.3733e-03, -2.5425e-03,  1.7274e-03,\n                       2.3824e-04,  2.0459e-03, -2.3743e-03, -1.1241e-03, -1.4889e-02,\n                       2.1796e-03, -8.1034e-05, -9.7806e-04, -4.3885e-03,  1.4005e-03,\n                       8.7246e-03,  8.6585e-04,  4.8532e-03, -3.1204e-03, -1.8327e-02,\n                       6.7531e-03,  3.4527e-03, -1.8000e-02,  1.7065e-03, -4.0487e-04,\n                       9.7197e-03, -1.7283e-03, -1.6131e-02,  1.5958e-03, -4.4349e-03,\n                       6.2816e-04,  4.8412e-04, -6.6063e-04, -4.4268e-04, -2.6048e-03,\n                       1.0638e-02, -4.8356e-04, -1.5632e-03, -1.8946e-03, -5.5063e-03,\n                      -1.9205e-03,  9.9773e-04,  9.1268e-03, -2.5975e-03, -2.6674e-03,\n                       2.8102e-04, -1.8641e-02,  1.3771e-03,  6.0121e-03, -1.8232e-03,\n                      -7.0881e-03, -1.9377e-03,  5.6653e-03,  1.9850e-04, -4.7801e-03,\n                       2.0089e-03, -4.3732e-03,  4.6919e-03,  1.2903e-03, -2.8495e-03,\n                      -4.8406e-03,  6.4400e-05,  7.3037e-04, -2.6458e-03, -1.3895e-02,\n                       9.2621e-03, -1.1252e-02, -1.3920e-03,  6.6889e-04, -1.8440e-02,\n                       1.1642e-03, -3.6368e-03, -1.5279e-04, -1.0392e-03,  1.7901e-03,\n                       1.3483e-03, -2.7062e-03,  2.3903e-03,  2.3428e-03, -4.0410e-03,\n                       3.2684e-04,  3.6842e-03, -1.9600e-03, -4.8576e-02, -5.5868e-03,\n                      -6.6782e-04,  2.8131e-03, -2.2986e-03, -8.6876e-02,  5.9148e-04,\n                       2.8741e-03, -4.6173e-03, -6.7936e-03, -1.1629e-02,  8.7364e-04,\n                       1.6888e-03, -1.6972e-03,  3.0278e-04, -1.3951e-03,  1.0355e-03,\n                       4.5120e-03,  4.9497e-02, -1.2824e-03,  2.3804e-03, -4.7921e-03,\n                      -4.0288e-03, -3.2838e-03, -1.3626e-03,  8.8973e-04,  1.4663e-03,\n                       5.0707e-03, -2.0564e-03,  2.0585e-03,  2.5408e-03,  1.2179e-03,\n                      -1.2113e-03, -2.6854e-03,  4.5787e-04,  9.0974e-04],\n                     requires_grad=True)),\n             ('features.5.0.block.0.scale', tensor(0.0017)),\n             ('features.5.0.block.0.zero_point', tensor(61)),\n             ('features.5.0.block.2.weight',\n              tensor([ 0.5156, -0.5628,  0.5284,  0.6598,  1.2204,  0.6718,  0.4194,  1.2298,\n                       0.7802,  0.7004,  0.5027,  1.1960,  1.2616,  1.1418,  1.7103,  0.6490,\n                       1.3005,  1.1503,  1.0184,  0.7564,  1.4965,  0.8221,  1.2375,  1.2698,\n                       0.5142,  0.4075,  0.6513,  0.7952,  0.7462,  1.1212,  0.6347,  0.6219,\n                       0.9417,  1.1068,  1.0358,  1.0024,  0.7928,  0.9293,  1.2663,  1.2597,\n                       0.9931,  0.8020,  1.0664,  1.0814,  0.9876,  1.4694,  1.1648,  1.0864,\n                       0.7236,  0.8142,  1.0235,  1.1520,  0.5259,  1.4948,  0.7432,  0.5877,\n                       1.4607,  0.7934,  0.6740,  1.3487,  0.7808,  1.2731,  0.6587,  0.6099,\n                       1.0300,  0.9509,  0.6067,  1.3491,  1.3850,  0.4486,  0.9974,  0.6748,\n                       1.3583,  0.5601,  0.6247,  1.3284,  0.9479,  0.7577,  1.1835,  1.2078,\n                       1.0885,  0.7334,  0.4857,  0.6298,  1.5889,  0.8719,  1.3705,  1.4496,\n                       0.6560,  1.3326,  0.5778,  1.3561,  0.6811,  0.7641,  1.4775,  1.1037,\n                       1.1175,  0.7133,  1.5515,  0.8249,  0.7720,  1.1881,  1.3870,  0.6174,\n                       0.9202,  0.6815,  0.4209,  1.6714,  1.0095,  1.1859,  0.7999,  1.0976,\n                       1.3258,  0.6263,  0.6672,  1.7045,  1.2329,  1.1103,  0.4563,  1.4026,\n                       1.1688,  0.6995,  0.7966,  1.1045,  0.8852,  1.2416,  0.6403,  1.3627,\n                       0.7457,  0.6117,  1.0873,  0.9105,  1.7194,  1.0273,  0.6053,  1.1583,\n                       0.7313,  0.4971,  0.9891,  0.6898,  1.2598,  1.0607,  0.8400,  0.4078,\n                       0.6587,  0.6978,  0.9474,  0.8057,  1.2608,  1.1288,  0.6192,  1.1073,\n                       1.8080,  1.3744,  1.5966, -0.5813,  0.6817,  1.1460,  0.9305,  0.5565,\n                       1.4388,  0.8228,  0.6592,  0.6516,  1.2208,  0.6884,  1.1914,  0.8918,\n                       1.4037,  1.0697,  1.1194, -0.4816,  0.5553,  0.5568,  1.0972,  0.6398,\n                       1.0644,  0.5927,  1.5399,  1.1643,  0.5510,  1.0827,  1.0807,  0.8100,\n                       1.1521,  1.1494,  0.5291,  1.3058,  0.9803,  0.7519,  0.6362,  1.4021,\n                       0.7973,  0.6783,  1.1987,  1.2640,  0.6184,  0.5787,  1.2517,  1.0888,\n                       0.5247,  0.9254,  0.7410,  1.1412,  0.8257,  1.3269,  0.8161,  0.6469,\n                       0.9029,  1.0815,  0.7754,  0.6367,  0.6906,  0.6882,  0.7861,  1.4163,\n                       0.6755,  1.0640,  1.3373,  1.4173,  0.6116,  0.7904,  1.1340,  0.5169,\n                       0.9956,  0.9452,  0.9404,  0.6839,  1.1210,  2.9646,  1.3760,  0.6132,\n                       1.2442,  0.9209,  1.1550,  0.3976,  0.6658,  0.9788,  0.7343,  0.8806,\n                       0.8914,  1.1997,  0.6455,  1.1578,  1.3365,  1.3501,  1.1366,  0.7059,\n                       0.9119,  0.7197,  0.6632,  0.6809,  0.4105,  0.6704,  1.8193,  0.9616,\n                       0.6798,  1.5681,  0.9150,  1.4694,  1.5917,  1.2142,  1.3019,  0.6820,\n                       0.8320,  0.8920,  1.3557,  0.5462,  0.8962,  0.9755,  0.7032,  0.5210,\n                       1.0836,  0.7635,  0.4920,  0.6423,  0.6885,  0.5156,  0.7073,  1.3298,\n                       0.5141,  1.3815,  1.3293,  0.9362,  0.4385,  0.9122,  0.6410,  0.6176,\n                       0.9885,  0.6400,  0.7733,  1.8126,  0.7660,  0.6252,  1.2163,  0.5031,\n                       0.8393,  0.5960,  0.4773,  0.4196,  0.8810,  0.8240,  1.5073,  1.0895,\n                       0.5795,  0.6274,  0.6048,  0.6717,  0.5842,  1.0536,  1.6995,  0.7416,\n                       1.4148,  1.1347,  0.4424,  0.6103,  1.0759,  0.5657,  1.0150,  0.6371,\n                       1.5867,  0.5597,  0.8304,  1.4208,  1.1283,  1.5049,  0.9910,  0.7825,\n                       1.2166,  1.0148,  0.8626,  1.0722,  1.3587,  1.4205,  0.6795,  0.6628,\n                       1.0745,  0.5614,  0.5697,  1.0913,  0.5609,  1.0844,  0.4805,  1.0459,\n                       0.8427,  1.0446,  1.3512,  0.5425,  0.6493,  0.8817,  0.5947,  0.7937,\n                       1.5321,  1.2201,  0.7852,  0.5578,  1.0818,  1.1281,  0.7689,  1.3024,\n                       0.4433,  0.8235,  1.6298,  1.2166,  1.8975,  1.0323,  2.8849,  0.6376,\n                       0.9660,  1.4708,  0.5387,  0.8362,  1.2408,  1.1483,  1.4720,  0.9099,\n                       0.6710,  1.2527,  0.9240,  0.9799,  0.6458,  0.4233,  0.9567,  0.6437])),\n             ('features.5.0.block.2.bias',\n              tensor([-1.5182e-01,  3.9215e-02, -1.5088e-02, -1.6909e-02,  1.0624e-01,\n                      -1.5068e-01, -2.6868e-01, -4.5173e-02, -1.0755e-01, -1.0786e-01,\n                       5.4065e-02, -4.3757e-01, -1.8330e-01, -3.7220e-01, -1.3918e-01,\n                       3.1285e-01,  3.4259e-02, -1.9678e-01,  7.6450e-02, -1.9455e-02,\n                      -3.7701e-01, -1.3633e-01, -6.5570e-02, -4.4819e-01,  1.0358e-02,\n                      -3.4393e-01, -1.8846e-01, -1.4961e-01, -5.2963e-01, -3.5243e-01,\n                      -9.6362e-01, -4.3686e-02, -1.4338e-01,  5.4411e-01, -7.4058e-03,\n                      -1.2401e-01, -6.1313e-02, -1.8790e-01,  3.2979e-01, -2.4088e-01,\n                      -2.7875e-01, -2.4316e-02, -8.1742e-02, -5.6591e-02, -1.6781e-01,\n                       8.0345e-02, -8.9013e-02, -1.1815e-01, -3.0182e-01, -8.6158e-02,\n                      -2.6517e-02,  3.0212e-01, -8.5212e-02, -4.5746e-01, -1.9282e-01,\n                      -9.0019e-02,  3.3271e-01, -1.0970e-01, -8.8376e-01,  2.6062e-01,\n                      -6.4812e-02,  4.8681e-01, -1.4859e-01,  2.7498e-02, -1.3976e-01,\n                       2.5510e-01, -2.0513e-02,  2.0631e-01,  4.4011e-04,  1.5825e-01,\n                      -5.7171e-02,  1.0512e-01,  2.1779e-01, -1.4572e-01, -4.2863e-01,\n                      -3.6257e-01, -1.2432e-01, -2.3639e-01, -3.6086e-01,  4.7164e-02,\n                       1.0925e-01,  1.7604e-02, -1.3291e-01, -1.9980e-01, -1.3368e-01,\n                      -1.4878e-01, -1.9374e-01,  8.4251e-02, -1.3333e-01, -2.0199e-01,\n                      -3.2191e-01, -3.0898e-02, -1.3602e-01, -1.9362e-01,  5.5075e-02,\n                       1.4226e-01,  8.9778e-02, -1.2527e-01,  1.7782e-01, -2.2566e-01,\n                      -2.5619e-01, -6.4982e-02, -5.4858e-01, -1.6375e-01, -1.9049e-01,\n                      -2.6898e-01, -3.3479e-02,  2.1923e-01, -5.6243e-02, -9.7928e-01,\n                      -4.3396e-01,  2.6249e-01,  3.7065e-02, -7.5462e-02, -9.7322e-02,\n                      -4.7197e-03, -2.8064e-01, -6.4308e-01, -1.9122e-01, -9.7296e-02,\n                      -1.0916e-01, -9.3783e-02, -4.2934e-01, -4.5314e-02, -4.0489e-02,\n                      -4.5710e-01, -2.2848e-01,  5.4221e-02, -6.5723e-02, -3.0532e-01,\n                      -9.8527e-02, -1.7874e-01, -1.5119e-01,  1.6150e-02, -1.0415e-01,\n                       1.6850e-01, -7.2470e-02, -1.4191e-01, -2.4037e-01, -9.6029e-02,\n                      -6.7126e-02, -4.7768e-02, -5.8259e-02, -1.2194e-01, -1.2147e-01,\n                      -3.0597e-01, -3.4883e-01, -2.9116e-02, -4.7668e-02, -3.5611e-01,\n                      -2.8451e-01, -2.1930e-02,  1.1160e-01,  4.1387e-01, -5.8679e-01,\n                       1.3745e+00,  3.9745e-02,  5.8670e-03, -1.4442e-01, -1.4110e-01,\n                       1.2763e-02,  9.9244e-02, -1.0249e-01, -9.4398e-02, -6.1474e-01,\n                      -1.5732e-01, -1.7523e-01,  3.7039e-01, -2.2916e-02, -1.2832e-02,\n                      -1.3036e-01,  2.3004e-01, -5.5508e-02,  2.6412e-01,  4.4937e-01,\n                      -2.6517e-01,  1.7800e-01, -1.3276e-01, -3.5946e-02, -1.4172e-01,\n                      -1.8405e-01,  2.2915e-01, -1.1924e+00,  6.1128e-01,  9.4851e-02,\n                      -1.4297e-01, -1.2548e-01, -2.5266e-01, -1.7825e-01, -8.7743e-02,\n                      -1.5176e-01, -2.8371e-01,  2.1334e-01, -1.0410e-01, -1.6390e-02,\n                      -6.6441e-01,  7.9115e-02, -1.2442e-01, -1.5494e-01, -6.4769e-02,\n                       1.6091e-02,  4.4092e-02,  1.0478e-01,  6.8371e-02,  4.9263e-02,\n                      -2.9604e-01, -3.3616e-01,  2.0779e-01,  2.5710e-02, -1.1915e+00,\n                      -4.7947e-02, -2.5854e-01, -1.4286e-01, -1.2493e-01, -6.6437e-02,\n                      -8.9147e-02, -2.0338e-01, -1.2366e-01, -1.9783e-01, -3.6234e-01,\n                      -1.7944e-01, -4.0695e-02,  1.4872e-01,  6.3163e-02, -1.2271e-01,\n                       6.9494e-01, -4.2438e-01, -1.9587e-02, -1.0779e-01,  4.8322e-01,\n                       3.2010e-01,  1.0288e-01,  5.3297e-01, -1.4343e-01, -1.8101e-03,\n                      -2.3688e-02, -1.3441e-01, -4.3978e-02,  5.5134e-01, -9.8443e-02,\n                      -1.7551e-01, -5.7648e-01,  3.6457e-01, -5.2239e-01, -1.9811e-01,\n                      -1.4863e+00, -1.3275e-01,  2.1193e-02,  1.1061e-01, -6.0887e-02,\n                      -1.0104e-01, -1.4350e-01, -9.7640e-02,  1.8932e-02,  1.3944e-01,\n                       2.7646e-01, -1.0103e-01, -6.7199e-02, -2.3327e-01, -3.5492e-02,\n                      -1.6440e-01,  3.4849e-02, -7.4392e-02, -7.2798e-02, -3.2840e-01,\n                      -1.5833e-01, -2.1261e-01, -2.8394e-01, -1.0537e-01, -1.7009e-01,\n                      -9.4591e-02, -9.1759e-02, -8.9063e-02, -1.2043e-01,  2.6314e-02,\n                      -1.5865e-01, -1.1219e-01, -4.2813e-02, -1.1377e-01, -1.7238e-01,\n                      -4.2809e-01, -1.2773e-01, -3.9002e-01,  4.8412e-02,  9.9082e-02,\n                      -1.0778e+00, -1.9265e-01,  2.2940e-01, -1.2356e-01, -7.0011e-02,\n                      -5.1980e-01,  9.4806e-02,  5.9619e-01, -2.2144e-01,  1.4796e-01,\n                      -8.8673e-02, -1.0139e-01, -9.5928e-02, -3.1620e-02, -1.1504e-01,\n                      -6.2081e-01, -6.7817e-02, -1.7276e-02,  1.2447e-02,  9.8669e-02,\n                      -5.8829e-02, -1.3230e-01, -4.1575e-01, -3.7139e-02, -2.2306e-03,\n                      -1.8614e-01,  1.6407e-01, -1.6169e-01, -4.5441e-01, -1.2062e-01,\n                       2.0150e-01, -3.6101e-04, -5.9238e-01, -1.2604e-01,  2.6017e-02,\n                      -3.5160e-01,  7.2643e-02, -2.4557e-01, -1.7635e-01,  7.1579e-02,\n                       4.7282e-01, -1.8499e-01, -1.3628e-01,  1.2774e-02,  5.5522e-01,\n                      -4.9250e-01,  5.1933e-01, -9.7190e-02, -6.3269e-02,  1.7429e+00,\n                      -1.0012e-01,  6.4625e-02, -5.5312e-02, -1.3571e-01, -1.5665e-01,\n                      -1.4198e-01, -1.6175e-02, -1.7186e-01, -1.9666e-01, -1.1708e+00,\n                      -8.9939e-02, -3.2894e-01, -7.6035e-02,  2.6211e-01,  9.8798e-02,\n                      -1.0682e-01, -2.0415e-01,  4.9944e-02, -1.1880e+00, -8.8641e-02,\n                      -2.4858e-01,  1.6230e-01,  3.3987e-01,  2.0819e-01, -1.7076e-01,\n                      -2.2164e-01, -7.0937e-02, -1.6899e-01, -8.4621e-02, -2.9307e-01,\n                      -2.7600e-01,  3.7737e-01, -3.1441e-02, -1.9102e-01,  1.9673e-01,\n                       4.1920e-02,  1.2361e-01,  1.1059e-02, -1.4286e-01, -1.7120e-01,\n                      -3.3058e-01, -5.9196e-02, -2.3230e-01, -1.6993e-01, -1.5068e-01,\n                      -1.0393e-01, -2.9036e-02, -4.8709e-02, -1.6786e-01])),\n             ('features.5.0.block.2.scale', tensor(0.1638)),\n             ('features.5.0.block.2.zero_point', tensor(40)),\n             ('features.5.0.block.3.scale', tensor(0.0758)),\n             ('features.5.0.block.3.zero_point', tensor(83)),\n             ('features.5.0.block.3._packed_params.dtype', torch.qint8),\n             ('features.5.0.block.3._packed_params._packed_params',\n              (tensor([[ 0.0214, -0.1388, -0.0619,  ...,  0.0256, -0.0235,  0.0128],\n                       [-0.0029,  0.1077, -0.0495,  ..., -0.0116,  0.0146,  0.0349],\n                       [ 0.1110, -0.0463,  0.0447,  ...,  0.0401,  0.0077,  0.0339],\n                       ...,\n                       [ 0.0041, -0.0150, -0.0123,  ...,  0.0027,  0.0710, -0.0055],\n                       [-0.0709,  0.0222, -0.0155,  ..., -0.0621,  0.0155, -0.0155],\n                       [ 0.0212, -0.0094,  0.0541,  ...,  0.0447,  0.0353,  0.0141]],\n                      size=(1536, 384), dtype=torch.qint8,\n                      quantization_scheme=torch.per_channel_affine,\n                      scale=tensor([0.0021, 0.0029, 0.0015,  ..., 0.0014, 0.0022, 0.0024],\n                      dtype=torch.float64),\n                      zero_point=tensor([0, 0, 0,  ..., 0, 0, 0]), axis=0),\n               Parameter containing:\n               tensor([-0.0325, -0.0174, -0.0568,  ..., -0.0414, -0.0368, -0.0549],\n                      requires_grad=True))),\n             ('features.5.0.block.5.scale', tensor(0.0445)),\n             ('features.5.0.block.5.zero_point', tensor(64)),\n             ('features.5.0.block.5._packed_params.dtype', torch.qint8),\n             ('features.5.0.block.5._packed_params._packed_params',\n              (tensor([[ 0.0162, -0.0529, -0.0103,  ...,  0.0427,  0.0206, -0.0015],\n                       [ 0.0000, -0.0558, -0.0194,  ...,  0.0049, -0.0534,  0.0170],\n                       [-0.0739,  0.0361,  0.0120,  ..., -0.0017,  0.0206,  0.0498],\n                       ...,\n                       [ 0.0276, -0.0436,  0.0291,  ...,  0.0320, -0.0160, -0.0029],\n                       [-0.0569, -0.0325, -0.0610,  ...,  0.0528,  0.0081, -0.0203],\n                       [ 0.0352,  0.0000, -0.1185,  ..., -0.0144,  0.0561, -0.0529]],\n                      size=(384, 1536), dtype=torch.qint8,\n                      quantization_scheme=torch.per_channel_affine,\n                      scale=tensor([0.0015, 0.0024, 0.0017, 0.0021, 0.0014, 0.0019, 0.0012, 0.0016, 0.0021,\n                       0.0021, 0.0014, 0.0015, 0.0019, 0.0033, 0.0018, 0.0015, 0.0015, 0.0015,\n                       0.0020, 0.0015, 0.0027, 0.0019, 0.0018, 0.0017, 0.0015, 0.0018, 0.0021,\n                       0.0022, 0.0024, 0.0019, 0.0021, 0.0016, 0.0015, 0.0016, 0.0022, 0.0016,\n                       0.0015, 0.0023, 0.0022, 0.0015, 0.0019, 0.0035, 0.0029, 0.0015, 0.0019,\n                       0.0029, 0.0023, 0.0016, 0.0017, 0.0015, 0.0026, 0.0040, 0.0018, 0.0029,\n                       0.0015, 0.0025, 0.0036, 0.0014, 0.0024, 0.0019, 0.0021, 0.0021, 0.0016,\n                       0.0015, 0.0019, 0.0033, 0.0016, 0.0024, 0.0022, 0.0021, 0.0017, 0.0015,\n                       0.0033, 0.0018, 0.0015, 0.0026, 0.0022, 0.0020, 0.0023, 0.0025, 0.0015,\n                       0.0015, 0.0015, 0.0035, 0.0024, 0.0022, 0.0026, 0.0015, 0.0017, 0.0016,\n                       0.0017, 0.0014, 0.0015, 0.0015, 0.0015, 0.0018, 0.0019, 0.0018, 0.0022,\n                       0.0016, 0.0020, 0.0018, 0.0022, 0.0016, 0.0027, 0.0015, 0.0015, 0.0025,\n                       0.0018, 0.0032, 0.0014, 0.0015, 0.0022, 0.0018, 0.0017, 0.0020, 0.0018,\n                       0.0026, 0.0017, 0.0017, 0.0015, 0.0017, 0.0017, 0.0035, 0.0028, 0.0028,\n                       0.0019, 0.0018, 0.0015, 0.0019, 0.0019, 0.0017, 0.0021, 0.0025, 0.0016,\n                       0.0020, 0.0018, 0.0015, 0.0020, 0.0022, 0.0017, 0.0022, 0.0035, 0.0015,\n                       0.0019, 0.0014, 0.0020, 0.0015, 0.0015, 0.0016, 0.0021, 0.0019, 0.0020,\n                       0.0015, 0.0023, 0.0020, 0.0046, 0.0021, 0.0022, 0.0016, 0.0017, 0.0016,\n                       0.0014, 0.0022, 0.0016, 0.0014, 0.0018, 0.0020, 0.0027, 0.0015, 0.0014,\n                       0.0016, 0.0017, 0.0015, 0.0017, 0.0019, 0.0019, 0.0017, 0.0029, 0.0022,\n                       0.0020, 0.0016, 0.0029, 0.0016, 0.0017, 0.0015, 0.0017, 0.0028, 0.0024,\n                       0.0032, 0.0023, 0.0019, 0.0018, 0.0015, 0.0019, 0.0023, 0.0015, 0.0019,\n                       0.0015, 0.0016, 0.0026, 0.0020, 0.0013, 0.0016, 0.0013, 0.0017, 0.0014,\n                       0.0014, 0.0016, 0.0017, 0.0013, 0.0026, 0.0018, 0.0015, 0.0016, 0.0017,\n                       0.0038, 0.0019, 0.0024, 0.0017, 0.0021, 0.0038, 0.0020, 0.0016, 0.0016,\n                       0.0019, 0.0017, 0.0015, 0.0018, 0.0024, 0.0018, 0.0014, 0.0015, 0.0015,\n                       0.0014, 0.0014, 0.0016, 0.0018, 0.0023, 0.0014, 0.0026, 0.0036, 0.0018,\n                       0.0014, 0.0015, 0.0030, 0.0018, 0.0016, 0.0019, 0.0015, 0.0014, 0.0020,\n                       0.0022, 0.0022, 0.0044, 0.0019, 0.0014, 0.0019, 0.0025, 0.0018, 0.0014,\n                       0.0014, 0.0024, 0.0020, 0.0029, 0.0021, 0.0017, 0.0014, 0.0024, 0.0018,\n                       0.0015, 0.0016, 0.0017, 0.0017, 0.0022, 0.0019, 0.0020, 0.0014, 0.0016,\n                       0.0014, 0.0016, 0.0024, 0.0015, 0.0017, 0.0014, 0.0019, 0.0016, 0.0017,\n                       0.0018, 0.0014, 0.0019, 0.0023, 0.0013, 0.0037, 0.0023, 0.0017, 0.0019,\n                       0.0023, 0.0019, 0.0019, 0.0016, 0.0020, 0.0021, 0.0027, 0.0014, 0.0016,\n                       0.0015, 0.0014, 0.0014, 0.0019, 0.0025, 0.0015, 0.0017, 0.0014, 0.0014,\n                       0.0017, 0.0015, 0.0019, 0.0025, 0.0021, 0.0034, 0.0015, 0.0014, 0.0017,\n                       0.0014, 0.0019, 0.0023, 0.0016, 0.0018, 0.0019, 0.0014, 0.0014, 0.0030,\n                       0.0019, 0.0022, 0.0017, 0.0016, 0.0016, 0.0021, 0.0018, 0.0020, 0.0016,\n                       0.0017, 0.0021, 0.0026, 0.0016, 0.0018, 0.0015, 0.0021, 0.0018, 0.0014,\n                       0.0022, 0.0015, 0.0034, 0.0017, 0.0028, 0.0020, 0.0022, 0.0030, 0.0027,\n                       0.0024, 0.0016, 0.0016, 0.0015, 0.0016, 0.0013, 0.0048, 0.0014, 0.0016,\n                       0.0021, 0.0016, 0.0018, 0.0018, 0.0019, 0.0016, 0.0025, 0.0015, 0.0018,\n                       0.0021, 0.0029, 0.0018, 0.0015, 0.0020, 0.0016], dtype=torch.float64),\n                      zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n                      axis=0),\n               Parameter containing:\n               tensor([ 7.4146e-03, -4.2783e-03,  1.9662e-02,  7.6429e-02,  3.8619e-02,\n                        2.2162e-02, -4.0717e-02, -2.7783e-03,  1.5546e-03,  2.1127e-02,\n                       -2.8176e-02,  3.8410e-03, -3.8768e-02,  1.1142e-02, -1.5324e-03,\n                        2.0924e-02, -1.5794e-01,  3.1393e-02,  8.7452e-03, -3.9100e-02,\n                       -3.2217e-02, -4.4854e-03, -1.1419e-01, -1.4592e-02,  3.3314e-02,\n                       -5.0200e-02, -2.7707e-03,  3.9579e-02, -7.5927e-02,  1.3942e-02,\n                       -4.5101e-02, -1.8877e-03,  4.2324e-03,  7.0000e-04,  4.2165e-02,\n                       -2.3408e-02,  1.1966e-02, -3.5850e-02, -2.5652e-02,  2.3238e-02,\n                        1.6776e-02, -2.1399e-03, -1.9376e-02, -1.1484e-03, -2.0728e-02,\n                        3.0888e-02, -7.7119e-03, -6.2534e-02, -2.2435e-02,  3.2386e-02,\n                       -4.3024e-03,  1.1664e-01,  3.1798e-02, -4.7139e-02, -1.5811e-02,\n                       -9.1828e-02, -1.3458e-02,  2.5442e-02, -5.7934e-02,  5.5355e-04,\n                       -1.6760e-03,  2.9201e-02, -5.4870e-02, -2.2267e-02,  4.5066e-02,\n                        6.5644e-02, -8.3322e-03,  1.4523e-02,  5.0136e-04,  6.2888e-03,\n                       -5.8544e-02,  7.6544e-03,  5.6340e-03,  1.4003e-02,  2.3021e-02,\n                        1.3159e-02,  2.2761e-02, -1.5383e-02,  2.0301e-02,  6.6437e-03,\n                        5.6567e-02,  4.3826e-02, -2.0049e-02,  6.9928e-03, -4.7548e-02,\n                        1.4572e-02, -1.2332e-02, -6.3982e-02, -1.6934e-03,  1.4043e-02,\n                        4.1437e-02,  4.2065e-02,  1.0029e-02, -2.7387e-02, -8.6457e-03,\n                        6.0336e-03,  1.6031e-02, -3.6171e-02, -6.8247e-03, -2.8834e-03,\n                        2.4590e-03, -7.8273e-03,  5.4483e-02, -1.7178e-02, -1.1092e-03,\n                       -4.0475e-02,  3.2878e-02,  6.3774e-03, -3.9648e-02, -5.6811e-02,\n                        3.4449e-02, -9.0210e-03,  7.5529e-03, -2.2516e-02,  5.0437e-02,\n                        3.0923e-02, -9.0840e-03,  1.2379e-02,  1.6185e-02,  6.7233e-02,\n                       -1.1103e-02,  7.3820e-04, -2.6888e-02, -6.8949e-04,  5.5206e-02,\n                        2.1518e-02,  3.9477e-02,  1.1595e-02, -2.5488e-02,  1.5972e-02,\n                       -8.2121e-03, -1.3125e-02, -2.1389e-02, -1.7418e-02,  2.2257e-02,\n                       -1.0389e-01,  1.0822e-02,  6.4148e-02, -4.8946e-03, -2.1111e-02,\n                        3.7388e-02, -1.8028e-02, -2.3019e-02, -5.3380e-03,  1.2799e-02,\n                       -1.4904e-02,  9.1653e-03,  2.4969e-03,  2.8481e-02,  8.4778e-03,\n                       -3.5987e-02, -3.5599e-02,  9.3953e-03,  1.1858e-01,  3.8368e-04,\n                        3.2590e-02, -1.2700e-01, -3.6527e-02, -4.3715e-03,  1.8650e-02,\n                       -4.6282e-02,  1.8148e-02, -5.9016e-03, -6.5114e-03, -1.6641e-02,\n                       -5.3130e-02, -8.2902e-04, -2.5910e-02, -8.0585e-03, -2.5304e-02,\n                        2.0055e-02,  2.2878e-02, -3.6063e-02, -4.1363e-02,  4.5562e-02,\n                        2.5534e-03,  2.7417e-02,  9.8595e-03, -1.9999e-02, -5.5715e-03,\n                       -4.0731e-02,  2.9999e-02,  1.6960e-02,  5.0298e-02, -3.6566e-02,\n                        8.2440e-02, -8.9511e-03,  9.2833e-04,  6.8617e-02,  1.1848e-04,\n                        9.5556e-03,  3.6411e-02,  2.8849e-02,  1.7972e-02,  6.7794e-03,\n                        1.6461e-02,  1.1255e-02, -1.7813e-02, -9.3035e-03,  5.1741e-03,\n                        2.5297e-02, -1.2085e-02,  7.6882e-02,  9.9501e-03,  1.4345e-02,\n                       -3.2620e-02,  7.8365e-03, -1.7458e-03,  2.8237e-02, -7.8708e-02,\n                       -3.9617e-02, -7.4422e-02,  2.6292e-04, -8.2234e-03,  4.4692e-03,\n                        4.2120e-02, -2.0682e-02, -3.1323e-03,  6.3277e-02,  7.2748e-02,\n                        9.7560e-03,  1.1115e-02,  2.1443e-02, -7.4476e-03,  3.7774e-02,\n                        8.1381e-02, -3.0709e-02, -2.1284e-03, -4.3335e-03,  1.9190e-02,\n                       -2.6332e-02,  3.3697e-03, -6.9859e-02, -1.2733e-02, -7.2649e-03,\n                       -1.6146e-03, -6.1156e-03,  7.2724e-02, -2.0027e-02, -1.9486e-02,\n                        6.7376e-03,  2.5750e-02,  4.0436e-02, -2.1309e-02,  5.9079e-02,\n                       -4.1265e-02, -1.7541e-02,  3.5167e-02,  2.3207e-02,  8.8843e-02,\n                       -5.5337e-04, -1.5764e-02,  7.9620e-03,  1.3387e-02,  6.9893e-02,\n                       -1.3202e-02, -4.0726e-02,  1.1399e-02,  2.9116e-02,  5.4927e-02,\n                       -9.8172e-03, -3.6442e-02, -2.6608e-03, -2.9941e-02,  6.5298e-02,\n                        7.0704e-03,  3.2241e-02, -1.6802e-02,  6.8627e-03, -3.3492e-02,\n                        2.8323e-02, -5.5983e-03, -7.8286e-02,  2.9561e-02,  3.4691e-02,\n                       -1.7646e-02,  3.8653e-02,  5.4074e-02,  6.2263e-02,  2.5761e-02,\n                       -5.1139e-02,  5.4815e-03,  4.7120e-03, -2.4347e-02,  2.7395e-02,\n                       -9.6269e-03, -2.3120e-02,  1.7840e-02, -5.3818e-02,  8.0761e-03,\n                       -1.5093e-01,  5.5214e-03,  1.0018e-01, -1.1917e-02, -5.9742e-04,\n                        3.0015e-02, -1.3895e-02, -3.4772e-02, -7.4623e-03,  2.1773e-02,\n                       -5.4027e-02,  1.5300e-03, -5.1158e-02,  1.8173e-02, -3.2744e-02,\n                       -1.2674e-02,  2.6036e-02, -2.6801e-02, -2.3682e-02, -1.1451e-02,\n                       -1.0297e-02, -8.5590e-03,  8.2983e-03, -5.6837e-02,  8.4248e-03,\n                        3.1484e-03,  4.2498e-02, -1.5292e-02, -8.3402e-03, -9.0075e-03,\n                       -8.5898e-03,  1.6991e-02, -8.8779e-03,  1.1385e-02,  4.5414e-02,\n                        2.4962e-02,  2.0011e-02, -1.2407e-03,  2.2659e-02,  4.7199e-03,\n                       -6.7689e-02, -4.7770e-02,  1.4027e-02, -3.3477e-02,  6.0013e-02,\n                       -7.4659e-03, -2.8456e-02, -2.6127e-02,  6.0214e-03, -1.2326e-02,\n                       -1.6307e-02,  4.8534e-02,  2.7421e-02,  2.1315e-02,  9.7227e-02,\n                        3.5916e-02,  4.1397e-02, -1.3295e-02, -2.1608e-01,  2.1989e-03,\n                        2.6925e-02, -1.6340e-02,  2.5467e-02, -2.8873e-02,  3.1153e-02,\n                       -2.0417e-02, -1.8608e-02,  6.1687e-02, -4.7870e-02, -8.6107e-03,\n                        1.4474e-02,  1.3671e-02,  4.4871e-02,  6.3391e-02, -5.1133e-02,\n                       -2.6005e-02,  2.7837e-02, -5.1238e-03,  2.6450e-02,  7.6138e-02,\n                        2.2314e-02,  1.6605e-02, -1.1607e-01, -1.2334e-02,  1.2006e-02,\n                       -5.5725e-02,  2.5838e-02, -1.7535e-02, -1.3379e-02,  2.7088e-02,\n                        2.6917e-02, -9.3482e-03, -3.6779e-02, -1.5327e-02],\n                      requires_grad=True))),\n             ('features.5.1.layer_scale',\n              tensor([[[-0.0655]],\n              \n                      [[ 0.0429]],\n              \n                      [[ 0.0575]],\n              \n                      [[ 0.0686]],\n              \n                      [[-0.0498]],\n              \n                      [[ 0.0615]],\n              \n                      [[-0.0479]],\n              \n                      [[-0.0515]],\n              \n                      [[-0.0499]],\n              \n                      [[-0.0504]],\n              \n                      [[-0.0528]],\n              \n                      [[ 0.0439]],\n              \n                      [[-0.0559]],\n              \n                      [[ 0.0851]],\n              \n                      [[ 0.0886]],\n              \n                      [[-0.0536]],\n              \n                      [[ 0.0844]],\n              \n                      [[-0.0797]],\n              \n                      [[-0.0640]],\n              \n                      [[-0.0628]],\n              \n                      [[-0.0711]],\n              \n                      [[ 0.0587]],\n              \n                      [[-0.0957]],\n              \n                      [[-0.0386]],\n              \n                      [[ 0.0543]],\n              \n                      [[-0.0333]],\n              \n                      [[-0.0543]],\n              \n                      [[-0.0572]],\n              \n                      [[-0.0565]],\n              \n                      [[-0.1100]],\n              \n                      [[-0.0422]],\n              \n                      [[ 0.0492]],\n              \n                      [[ 0.0817]],\n              \n                      [[-0.0802]],\n              \n                      [[ 0.0928]],\n              \n                      [[ 0.0512]],\n              \n                      [[ 0.0680]],\n              \n                      [[ 0.0536]],\n              \n                      [[ 0.0976]],\n              \n                      [[ 0.0755]],\n              \n                      [[-0.0530]],\n              \n                      [[-0.0619]],\n              \n                      [[ 0.0593]],\n              \n                      [[-0.0406]],\n              \n                      [[-0.0541]],\n              \n                      [[-0.0620]],\n              \n                      [[ 0.0482]],\n              \n                      [[ 0.0679]],\n              \n                      [[-0.0453]],\n              \n                      [[ 0.0629]],\n              \n                      [[ 0.0708]],\n              \n                      [[ 0.0802]],\n              \n                      [[-0.0589]],\n              \n                      [[-0.0885]],\n              \n                      [[ 0.0622]],\n              \n                      [[-0.0521]],\n              \n                      [[ 0.1098]],\n              \n                      [[ 0.0609]],\n              \n                      [[ 0.0432]],\n              \n                      [[-0.0658]],\n              \n                      [[ 0.0544]],\n              \n                      [[ 0.0311]],\n              \n                      [[ 0.0474]],\n              \n                      [[ 0.0634]],\n              \n                      [[-0.0612]],\n              \n                      [[-0.0442]],\n              \n                      [[-0.0414]],\n              \n                      [[ 0.0683]],\n              \n                      [[-0.0702]],\n              \n                      [[-0.0363]],\n              \n                      [[-0.0536]],\n              \n                      [[ 0.0508]],\n              \n                      [[ 0.0608]],\n              \n                      [[ 0.0362]],\n              \n                      [[-0.0680]],\n              \n                      [[-0.0906]],\n              \n                      [[ 0.0516]],\n              \n                      [[-0.0839]],\n              \n                      [[-0.0632]],\n              \n                      [[-0.0889]],\n              \n                      [[-0.0718]],\n              \n                      [[-0.0612]],\n              \n                      [[ 0.0621]],\n              \n                      [[-0.0595]],\n              \n                      [[-0.0766]],\n              \n                      [[ 0.0700]],\n              \n                      [[-0.0644]],\n              \n                      [[-0.0676]],\n              \n                      [[-0.0492]],\n              \n                      [[-0.0396]],\n              \n                      [[ 0.0614]],\n              \n                      [[-0.0684]],\n              \n                      [[-0.0541]],\n              \n                      [[-0.0717]],\n              \n                      [[-0.0865]],\n              \n                      [[-0.0784]],\n              \n                      [[-0.0374]],\n              \n                      [[ 0.0546]],\n              \n                      [[-0.0864]],\n              \n                      [[-0.0463]],\n              \n                      [[ 0.0626]],\n              \n                      [[-0.0469]],\n              \n                      [[-0.0828]],\n              \n                      [[ 0.0642]],\n              \n                      [[ 0.0657]],\n              \n                      [[-0.0607]],\n              \n                      [[ 0.0492]],\n              \n                      [[-0.0722]],\n              \n                      [[-0.0873]],\n              \n                      [[ 0.0778]],\n              \n                      [[ 0.0651]],\n              \n                      [[-0.0574]],\n              \n                      [[ 0.0538]],\n              \n                      [[ 0.0493]],\n              \n                      [[ 0.0408]],\n              \n                      [[ 0.0792]],\n              \n                      [[ 0.0460]],\n              \n                      [[ 0.0383]],\n              \n                      [[ 0.0524]],\n              \n                      [[ 0.0793]],\n              \n                      [[ 0.0736]],\n              \n                      [[ 0.0509]],\n              \n                      [[ 0.0554]],\n              \n                      [[-0.0728]],\n              \n                      [[ 0.0475]],\n              \n                      [[-0.0603]],\n              \n                      [[-0.0412]],\n              \n                      [[-0.0746]],\n              \n                      [[-0.0626]],\n              \n                      [[ 0.0666]],\n              \n                      [[ 0.0739]],\n              \n                      [[-0.0645]],\n              \n                      [[ 0.0713]],\n              \n                      [[-0.0475]],\n              \n                      [[-0.0696]],\n              \n                      [[-0.0950]],\n              \n                      [[ 0.0535]],\n              \n                      [[-0.0438]],\n              \n                      [[-0.0789]],\n              \n                      [[ 0.0675]],\n              \n                      [[-0.0498]],\n              \n                      [[ 0.0804]],\n              \n                      [[-0.0438]],\n              \n                      [[ 0.0649]],\n              \n                      [[ 0.0495]],\n              \n                      [[ 0.0483]],\n              \n                      [[-0.0444]],\n              \n                      [[-0.0715]],\n              \n                      [[-0.0792]],\n              \n                      [[-0.0326]],\n              \n                      [[-0.0565]],\n              \n                      [[-0.0717]],\n              \n                      [[ 0.0587]],\n              \n                      [[-0.0664]],\n              \n                      [[-0.0876]],\n              \n                      [[-0.0589]],\n              \n                      [[-0.3354]],\n              \n                      [[-0.0706]],\n              \n                      [[-0.0677]],\n              \n                      [[ 0.0442]],\n              \n                      [[ 0.0745]],\n              \n                      [[-0.0647]],\n              \n                      [[-0.0539]],\n              \n                      [[ 0.0538]],\n              \n                      [[ 0.0824]],\n              \n                      [[ 0.0579]],\n              \n                      [[ 0.0597]],\n              \n                      [[-0.0479]],\n              \n                      [[-0.0858]],\n              \n                      [[-0.0774]],\n              \n                      [[ 0.0652]],\n              \n                      [[-0.0449]],\n              \n                      [[ 0.0642]],\n              \n                      [[-0.0686]],\n              \n                      [[ 0.0499]],\n              \n                      [[ 0.0483]],\n              \n                      [[-0.0634]],\n              \n                      [[-0.0465]],\n              \n                      [[-0.0775]],\n              \n                      [[-0.0590]],\n              \n                      [[-0.0513]],\n              \n                      [[ 0.0334]],\n              \n                      [[-0.0703]],\n              \n                      [[-0.0640]],\n              \n                      [[ 0.0716]],\n              \n                      [[-0.0593]],\n              \n                      [[-0.0453]],\n              \n                      [[ 0.0963]],\n              \n                      [[-0.0561]],\n              \n                      [[-0.0371]],\n              \n                      [[ 0.0480]],\n              \n                      [[ 0.1096]],\n              \n                      [[ 0.0546]],\n              \n                      [[ 0.0445]],\n              \n                      [[-0.0644]],\n              \n                      [[ 0.0859]],\n              \n                      [[-0.0643]],\n              \n                      [[-0.0527]],\n              \n                      [[-0.0374]],\n              \n                      [[ 0.0501]],\n              \n                      [[ 0.0478]],\n              \n                      [[ 0.0554]],\n              \n                      [[-0.0595]],\n              \n                      [[ 0.0333]],\n              \n                      [[-0.0501]],\n              \n                      [[-0.0444]],\n              \n                      [[ 0.0579]],\n              \n                      [[ 0.0557]],\n              \n                      [[-0.0691]],\n              \n                      [[-0.0649]],\n              \n                      [[-0.0640]],\n              \n                      [[ 0.0482]],\n              \n                      [[ 0.0474]],\n              \n                      [[ 0.0465]],\n              \n                      [[ 0.0533]],\n              \n                      [[ 0.0502]],\n              \n                      [[ 0.0317]],\n              \n                      [[ 0.0575]],\n              \n                      [[ 0.0766]],\n              \n                      [[ 0.0802]],\n              \n                      [[ 0.0681]],\n              \n                      [[-0.0498]],\n              \n                      [[-0.0554]],\n              \n                      [[ 0.0526]],\n              \n                      [[ 0.0565]],\n              \n                      [[-0.0572]],\n              \n                      [[ 0.0596]],\n              \n                      [[ 0.0497]],\n              \n                      [[-0.0747]],\n              \n                      [[-0.0509]],\n              \n                      [[ 0.0514]],\n              \n                      [[-0.0550]],\n              \n                      [[ 0.0725]],\n              \n                      [[-0.0770]],\n              \n                      [[-0.0325]],\n              \n                      [[ 0.0475]],\n              \n                      [[ 0.0573]],\n              \n                      [[-0.0661]],\n              \n                      [[ 0.0349]],\n              \n                      [[-0.0669]],\n              \n                      [[ 0.0505]],\n              \n                      [[-0.0597]],\n              \n                      [[ 0.0574]],\n              \n                      [[-0.0378]],\n              \n                      [[ 0.0437]],\n              \n                      [[-0.0706]],\n              \n                      [[ 0.0519]],\n              \n                      [[ 0.0634]],\n              \n                      [[-0.0529]],\n              \n                      [[-0.0729]],\n              \n                      [[ 0.0506]],\n              \n                      [[-0.0493]],\n              \n                      [[ 0.0502]],\n              \n                      [[ 0.0494]],\n              \n                      [[ 0.0836]],\n              \n                      [[ 0.0738]],\n              \n                      [[ 0.0581]],\n              \n                      [[-0.0551]],\n              \n                      [[ 0.0578]],\n              \n                      [[ 0.0828]],\n              \n                      [[-0.0671]],\n              \n                      [[ 0.0691]],\n              \n                      [[ 0.0748]],\n              \n                      [[ 0.0455]],\n              \n                      [[-0.0418]],\n              \n                      [[-0.0580]],\n              \n                      [[-0.0498]],\n              \n                      [[-0.0629]],\n              \n                      [[-0.0653]],\n              \n                      [[ 0.0956]],\n              \n                      [[-0.0598]],\n              \n                      [[ 0.0463]],\n              \n                      [[ 0.0553]],\n              \n                      [[-0.0470]],\n              \n                      [[-0.0488]],\n              \n                      [[-0.0553]],\n              \n                      [[ 0.0561]],\n              \n                      [[ 0.0404]],\n              \n                      [[-0.0422]],\n              \n                      [[-0.0763]],\n              \n                      [[ 0.0365]],\n              \n                      [[-0.0570]],\n              \n                      [[-0.0589]],\n              \n                      [[-0.0734]],\n              \n                      [[ 0.0502]],\n              \n                      [[-0.0604]],\n              \n                      [[-0.0436]],\n              \n                      [[ 0.0735]],\n              \n                      [[-0.0733]],\n              \n                      [[ 0.0621]],\n              \n                      [[-0.0807]],\n              \n                      [[-0.0728]],\n              \n                      [[-0.0635]],\n              \n                      [[ 0.0667]],\n              \n                      [[-0.0426]],\n              \n                      [[-0.0579]],\n              \n                      [[ 0.0594]],\n              \n                      [[-0.0405]],\n              \n                      [[ 0.0514]],\n              \n                      [[-0.0502]],\n              \n                      [[-0.0702]],\n              \n                      [[ 0.0774]],\n              \n                      [[-0.0722]],\n              \n                      [[ 0.0606]],\n              \n                      [[-0.0408]],\n              \n                      [[-0.0572]],\n              \n                      [[-0.0592]],\n              \n                      [[-0.0503]],\n              \n                      [[-0.0509]],\n              \n                      [[-0.0876]],\n              \n                      [[-0.0901]],\n              \n                      [[ 0.0300]],\n              \n                      [[ 0.0748]],\n              \n                      [[ 0.0819]],\n              \n                      [[-0.0391]],\n              \n                      [[-0.0613]],\n              \n                      [[-0.0596]],\n              \n                      [[-0.0619]],\n              \n                      [[-0.0541]],\n              \n                      [[ 0.0535]],\n              \n                      [[-0.0363]],\n              \n                      [[-0.0616]],\n              \n                      [[ 0.0456]],\n              \n                      [[ 0.0764]],\n              \n                      [[ 0.0799]],\n              \n                      [[-0.0668]],\n              \n                      [[-0.0553]],\n              \n                      [[-0.0559]],\n              \n                      [[-0.0517]],\n              \n                      [[ 0.0616]],\n              \n                      [[-0.0827]],\n              \n                      [[-0.0543]],\n              \n                      [[ 0.0857]],\n              \n                      [[-0.0864]],\n              \n                      [[ 0.0736]],\n              \n                      [[-0.0490]],\n              \n                      [[ 0.0489]],\n              \n                      [[-0.0390]],\n              \n                      [[ 0.0506]],\n              \n                      [[ 0.0569]],\n              \n                      [[-0.0403]],\n              \n                      [[-0.0557]],\n              \n                      [[ 0.0444]],\n              \n                      [[-0.0591]],\n              \n                      [[ 0.0471]],\n              \n                      [[ 0.0512]],\n              \n                      [[ 0.0639]],\n              \n                      [[-0.0448]],\n              \n                      [[ 0.0580]],\n              \n                      [[-0.0719]],\n              \n                      [[-0.0525]],\n              \n                      [[-0.0669]],\n              \n                      [[ 0.0602]],\n              \n                      [[ 0.0204]],\n              \n                      [[ 0.0589]],\n              \n                      [[-0.0346]],\n              \n                      [[ 0.0544]],\n              \n                      [[ 0.0809]],\n              \n                      [[ 0.0624]],\n              \n                      [[ 0.0923]],\n              \n                      [[ 0.0533]],\n              \n                      [[-0.0496]],\n              \n                      [[-0.0511]],\n              \n                      [[ 0.0680]],\n              \n                      [[ 0.0488]],\n              \n                      [[ 0.0733]],\n              \n                      [[-0.0790]],\n              \n                      [[ 0.0594]],\n              \n                      [[ 0.0887]],\n              \n                      [[ 0.0764]],\n              \n                      [[ 0.0492]],\n              \n                      [[ 0.0557]],\n              \n                      [[-0.0879]],\n              \n                      [[ 0.0644]],\n              \n                      [[-0.0760]],\n              \n                      [[ 0.0785]],\n              \n                      [[ 0.0415]],\n              \n                      [[-0.0904]],\n              \n                      [[ 0.0551]],\n              \n                      [[-0.0652]],\n              \n                      [[-0.0547]],\n              \n                      [[-0.0486]],\n              \n                      [[ 0.0713]],\n              \n                      [[-0.0513]]])),\n             ('features.5.1.block.0.weight',\n              tensor([[[[ 0.0132,  0.0142,  0.0295,  ...,  0.0254,  0.0163,  0.0203],\n                        [ 0.0183,  0.0122,  0.0386,  ...,  0.0213,  0.0213,  0.0122],\n                        [ 0.0183,  0.0224,  0.0600,  ...,  0.0793,  0.0254,  0.0224],\n                        ...,\n                        [ 0.0132,  0.0203,  0.0488,  ...,  0.0559,  0.0152,  0.0163],\n                        [ 0.0020,  0.0102,  0.0152,  ...,  0.0183,  0.0041,  0.0122],\n                        [ 0.0142,  0.0112,  0.0234,  ...,  0.0224,  0.0203,  0.0224]]],\n              \n              \n                      [[[-0.0008, -0.0051,  0.0042,  ...,  0.0008, -0.0017,  0.0059],\n                        [-0.0042,  0.0059, -0.0025,  ..., -0.0042, -0.0135,  0.0000],\n                        [ 0.0025, -0.0042, -0.0152,  ..., -0.0744, -0.0110, -0.0051],\n                        ...,\n                        [-0.0110,  0.0017,  0.0321,  ...,  0.0000, -0.0118,  0.0025],\n                        [-0.0059,  0.0110,  0.0118,  ..., -0.0008, -0.0042,  0.0017],\n                        [ 0.0076,  0.0034,  0.0042,  ...,  0.0118, -0.0093, -0.0025]]],\n              \n              \n                      [[[ 0.0190,  0.0063,  0.0275,  ...,  0.0127,  0.0148,  0.0148],\n                        [ 0.0085,  0.0106,  0.0106,  ...,  0.0190,  0.0127,  0.0106],\n                        [ 0.0106,  0.0212,  0.0000,  ..., -0.0106,  0.0063,  0.0148],\n                        ...,\n                        [ 0.0127,  0.0127,  0.0085,  ...,  0.0000,  0.0106,  0.0169],\n                        [ 0.0042,  0.0106,  0.0190,  ...,  0.0127,  0.0042,  0.0085],\n                        [ 0.0127,  0.0063,  0.0169,  ...,  0.0063,  0.0190,  0.0106]]],\n              \n              \n                      ...,\n              \n              \n                      [[[-0.0161, -0.0101, -0.0101,  ..., -0.0121, -0.0141, -0.0081],\n                        [-0.0020, -0.0081, -0.0061,  ..., -0.0061, -0.0182, -0.0040],\n                        [-0.0121, -0.0040, -0.0343,  ..., -0.0504, -0.0202,  0.0000],\n                        ...,\n                        [-0.0222, -0.0081, -0.0363,  ..., -0.0484, -0.0040, -0.0182],\n                        [-0.0141, -0.0061, -0.0121,  ..., -0.0182, -0.0121, -0.0101],\n                        [-0.0020, -0.0121, -0.0061,  ..., -0.0040, -0.0081, -0.0141]]],\n              \n              \n                      [[[-0.0057, -0.0057,  0.0057,  ...,  0.0000, -0.0057, -0.0086],\n                        [-0.0029, -0.0029, -0.0086,  ..., -0.0029,  0.0029, -0.0029],\n                        [-0.0057, -0.0029, -0.0086,  ..., -0.0229, -0.0029, -0.0029],\n                        ...,\n                        [ 0.0000, -0.0086,  0.0086,  ..., -0.0687, -0.0229, -0.0057],\n                        [-0.0029, -0.0086, -0.0057,  ..., -0.0057, -0.0143,  0.0000],\n                        [ 0.0057,  0.0057,  0.0029,  ..., -0.0115,  0.0000, -0.0086]]],\n              \n              \n                      [[[ 0.0122,  0.0073,  0.0147,  ...,  0.0171,  0.0049,  0.0147],\n                        [ 0.0049, -0.0049,  0.0049,  ...,  0.0122, -0.0049,  0.0098],\n                        [ 0.0269,  0.0171,  0.0073,  ...,  0.0293,  0.0147,  0.0122],\n                        ...,\n                        [ 0.0073,  0.0098, -0.0024,  ...,  0.0245,  0.0122,  0.0122],\n                        [ 0.0073,  0.0073,  0.0073,  ...,  0.0171, -0.0049,  0.0220],\n                        [ 0.0196,  0.0098,  0.0245,  ...,  0.0245,  0.0122,  0.0147]]]],\n                     size=(384, 1, 7, 7), dtype=torch.qint8,\n                     quantization_scheme=torch.per_channel_affine,\n                     scale=tensor([0.0010, 0.0008, 0.0021, 0.0024, 0.0020, 0.0025, 0.0010, 0.0020, 0.0027,\n                      0.0026, 0.0013, 0.0026, 0.0028, 0.0008, 0.0024, 0.0026, 0.0027, 0.0027,\n                      0.0030, 0.0025, 0.0028, 0.0026, 0.0031, 0.0026, 0.0019, 0.0012, 0.0011,\n                      0.0005, 0.0024, 0.0029, 0.0016, 0.0023, 0.0027, 0.0027, 0.0011, 0.0030,\n                      0.0026, 0.0028, 0.0018, 0.0027, 0.0029, 0.0022, 0.0029, 0.0023, 0.0029,\n                      0.0032, 0.0026, 0.0030, 0.0025, 0.0027, 0.0031, 0.0013, 0.0017, 0.0010,\n                      0.0025, 0.0017, 0.0018, 0.0017, 0.0019, 0.0028, 0.0026, 0.0002, 0.0013,\n                      0.0029, 0.0031, 0.0012, 0.0019, 0.0027, 0.0030, 0.0012, 0.0030, 0.0020,\n                      0.0022, 0.0020, 0.0021, 0.0031, 0.0026, 0.0027, 0.0028, 0.0009, 0.0028,\n                      0.0028, 0.0011, 0.0015, 0.0017, 0.0019, 0.0025, 0.0017, 0.0018, 0.0022,\n                      0.0020, 0.0028, 0.0027, 0.0027, 0.0023, 0.0028, 0.0025, 0.0022, 0.0010,\n                      0.0027, 0.0013, 0.0028, 0.0030, 0.0026, 0.0019, 0.0024, 0.0019, 0.0013,\n                      0.0011, 0.0036, 0.0019, 0.0023, 0.0022, 0.0023, 0.0024, 0.0016, 0.0025,\n                      0.0028, 0.0011, 0.0032, 0.0014, 0.0026, 0.0027, 0.0021, 0.0013, 0.0024,\n                      0.0013, 0.0021, 0.0025, 0.0017, 0.0027, 0.0029, 0.0024, 0.0028, 0.0021,\n                      0.0032, 0.0016, 0.0018, 0.0007, 0.0024, 0.0029, 0.0030, 0.0023, 0.0020,\n                      0.0022, 0.0027, 0.0031, 0.0020, 0.0013, 0.0019, 0.0032, 0.0011, 0.0024,\n                      0.0009, 0.0026, 0.0023, 0.0007, 0.0030, 0.0025, 0.0012, 0.0030, 0.0024,\n                      0.0018, 0.0027, 0.0030, 0.0023, 0.0027, 0.0027, 0.0009, 0.0030, 0.0025,\n                      0.0009, 0.0015, 0.0014, 0.0026, 0.0023, 0.0027, 0.0021, 0.0016, 0.0029,\n                      0.0024, 0.0026, 0.0013, 0.0014, 0.0030, 0.0026, 0.0017, 0.0009, 0.0024,\n                      0.0014, 0.0028, 0.0031, 0.0025, 0.0023, 0.0028, 0.0023, 0.0021, 0.0020,\n                      0.0023, 0.0026, 0.0007, 0.0026, 0.0016, 0.0025, 0.0024, 0.0025, 0.0025,\n                      0.0016, 0.0027, 0.0021, 0.0005, 0.0030, 0.0024, 0.0024, 0.0023, 0.0025,\n                      0.0020, 0.0012, 0.0010, 0.0030, 0.0023, 0.0024, 0.0028, 0.0016, 0.0029,\n                      0.0020, 0.0026, 0.0017, 0.0029, 0.0026, 0.0026, 0.0017, 0.0019, 0.0029,\n                      0.0021, 0.0020, 0.0025, 0.0029, 0.0013, 0.0022, 0.0023, 0.0017, 0.0020,\n                      0.0025, 0.0024, 0.0011, 0.0028, 0.0019, 0.0017, 0.0025, 0.0019, 0.0024,\n                      0.0017, 0.0023, 0.0010, 0.0023, 0.0025, 0.0031, 0.0030, 0.0014, 0.0030,\n                      0.0033, 0.0027, 0.0028, 0.0010, 0.0025, 0.0027, 0.0013, 0.0028, 0.0027,\n                      0.0014, 0.0019, 0.0028, 0.0018, 0.0011, 0.0023, 0.0025, 0.0016, 0.0010,\n                      0.0028, 0.0009, 0.0018, 0.0022, 0.0016, 0.0019, 0.0028, 0.0019, 0.0009,\n                      0.0016, 0.0025, 0.0027, 0.0026, 0.0026, 0.0021, 0.0027, 0.0012, 0.0030,\n                      0.0016, 0.0018, 0.0012, 0.0014, 0.0012, 0.0027, 0.0022, 0.0022, 0.0023,\n                      0.0016, 0.0012, 0.0023, 0.0025, 0.0024, 0.0011, 0.0023, 0.0028, 0.0009,\n                      0.0011, 0.0025, 0.0016, 0.0019, 0.0023, 0.0026, 0.0027, 0.0025, 0.0022,\n                      0.0030, 0.0030, 0.0007, 0.0027, 0.0028, 0.0022, 0.0019, 0.0027, 0.0009,\n                      0.0011, 0.0026, 0.0013, 0.0020, 0.0022, 0.0026, 0.0021, 0.0020, 0.0030,\n                      0.0022, 0.0027, 0.0010, 0.0024, 0.0030, 0.0024, 0.0013, 0.0018, 0.0025,\n                      0.0024, 0.0028, 0.0008, 0.0022, 0.0021, 0.0030, 0.0013, 0.0010, 0.0009,\n                      0.0021, 0.0023, 0.0028, 0.0027, 0.0031, 0.0028, 0.0010, 0.0026, 0.0027,\n                      0.0027, 0.0021, 0.0029, 0.0021, 0.0025, 0.0017, 0.0020, 0.0020, 0.0026,\n                      0.0026, 0.0022, 0.0016, 0.0020, 0.0029, 0.0024], dtype=torch.float64),\n                     zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n                     axis=0)),\n             ('features.5.1.block.0.bias',\n              Parameter containing:\n              tensor([ 2.4213e-03, -2.6187e-04, -2.4560e-03, -1.9905e-03,  1.8177e-02,\n                       1.8347e-02,  4.5846e-03,  6.4553e-03,  7.7345e-04, -3.8819e-03,\n                       9.4348e-03,  7.3869e-03,  1.3674e-03,  7.7373e-03, -8.8701e-03,\n                       3.3150e-03, -3.3134e-04, -1.5769e-03, -1.5670e-03, -5.0083e-03,\n                      -5.2752e-03,  5.9483e-03,  1.3588e-02,  9.8281e-03,  9.2270e-03,\n                      -3.0177e-02, -1.1348e-02,  1.3845e-03,  2.1957e-03, -3.6731e-03,\n                      -7.2198e-03,  2.9646e-03, -1.4828e-03,  6.4601e-03, -1.3961e-03,\n                      -2.9112e-03, -6.9654e-03, -3.8272e-03,  3.8201e-03, -1.6448e-02,\n                      -3.0333e-03, -2.6902e-05,  3.0257e-03,  1.2731e-03,  5.8635e-03,\n                      -1.0767e-03,  9.0908e-04, -1.1718e-02,  3.4411e-03,  1.3446e-03,\n                       6.5087e-03,  2.4505e-03,  4.6232e-04, -1.8905e-03,  1.4146e-04,\n                       1.3626e-02,  4.4441e-03,  3.6924e-04,  1.6406e-02,  5.1886e-03,\n                       7.4564e-03, -5.6747e-02,  9.5356e-03, -7.9959e-03,  1.2474e-03,\n                      -3.8039e-02,  7.5060e-03, -2.1479e-03, -1.0199e-02,  1.7539e-02,\n                       7.3242e-03,  2.9333e-05,  5.8879e-04,  4.5058e-03, -1.4053e-02,\n                       3.7098e-03,  2.0565e-03,  2.4451e-03, -4.6635e-03, -8.9685e-04,\n                      -2.5562e-03, -9.6857e-03, -1.1639e-02, -6.0889e-03,  1.3666e-03,\n                       6.1760e-03, -1.4533e-03,  1.5616e-03, -5.7326e-03,  1.4018e-03,\n                      -8.9052e-03,  6.5670e-03,  1.5946e-03, -1.3278e-02, -9.3809e-04,\n                      -2.9273e-03, -6.2945e-03, -9.6302e-03, -3.5387e-03,  3.4601e-03,\n                       5.9717e-03, -1.0931e-02,  7.0138e-05,  8.7066e-03, -8.0425e-03,\n                       8.1820e-03, -1.7841e-03, -9.4143e-03,  4.9403e-03, -1.1706e-02,\n                       1.8531e-02, -6.1293e-03, -4.3054e-03,  7.9815e-03, -6.4195e-03,\n                      -4.7596e-03,  1.7406e-03, -2.3831e-03,  4.9605e-03, -6.0668e-03,\n                      -5.5365e-03, -7.1489e-03, -1.2180e-02, -4.5837e-03,  7.3546e-03,\n                       1.4150e-02, -1.2636e-02,  7.9412e-03,  3.6992e-03, -1.3825e-03,\n                      -1.3337e-03,  7.9343e-03,  4.8511e-04, -2.5742e-04,  5.1928e-03,\n                       5.6028e-03,  5.0482e-03, -5.2009e-03, -2.4275e-03,  2.5917e-03,\n                      -5.1625e-03,  1.8260e-03,  3.0996e-03,  1.7543e-03,  4.0518e-03,\n                       4.8967e-03,  3.8625e-04,  3.6198e-03, -2.4037e-03,  4.8356e-03,\n                      -5.2653e-03, -4.4685e-03, -1.0483e-02,  6.2049e-03, -8.8880e-03,\n                      -4.0422e-03,  4.5308e-04,  4.7965e-03,  8.3178e-03, -1.0278e-02,\n                      -2.1399e-02, -3.6715e-03, -4.9059e-03,  8.0247e-04, -1.2150e-02,\n                      -2.4496e-03, -1.9878e-02, -2.1479e-04, -2.4798e-03,  1.3614e-03,\n                       1.2452e-02, -5.3859e-03, -2.2374e-03, -7.1541e-03,  2.9131e-02,\n                       3.6329e-03, -6.5415e-04,  1.5787e-04, -2.0201e-02, -2.0442e-03,\n                      -6.6128e-03, -8.9913e-03,  1.3845e-02, -1.2533e-02, -9.5869e-04,\n                       8.2545e-03,  7.8567e-03, -1.5374e-03, -2.8816e-02, -5.1769e-03,\n                      -3.7324e-04,  6.4748e-03, -1.7948e-03,  6.9566e-03,  6.7421e-03,\n                       1.0120e-02,  8.7700e-03,  7.5745e-03,  1.4688e-02,  2.5496e-03,\n                       3.1960e-03,  9.1380e-04,  5.5611e-03, -6.4140e-03, -7.2407e-03,\n                      -9.8844e-03,  6.1680e-03,  4.2431e-03, -5.3061e-04, -1.0352e-02,\n                      -9.2029e-03, -3.8066e-03,  3.5782e-03, -6.2456e-04,  3.0141e-03,\n                       3.2928e-03,  1.0283e-03,  4.0117e-03,  4.3858e-03,  1.1605e-02,\n                      -6.4463e-03, -4.0577e-03,  3.3844e-03,  8.2759e-03,  1.9872e-03,\n                      -8.9035e-03, -6.6172e-03, -1.3288e-03, -7.4938e-03, -5.5801e-03,\n                      -2.3493e-02, -7.0634e-03,  1.4692e-02, -3.1418e-03,  9.4019e-03,\n                       2.9958e-03,  6.3383e-03, -1.6915e-03, -3.8004e-03,  4.4165e-03,\n                       7.2957e-03, -6.5340e-03,  1.3892e-02,  2.8358e-04, -6.5856e-03,\n                       1.8216e-02,  4.2965e-03, -5.1433e-03, -2.6716e-02, -6.5682e-03,\n                      -4.3750e-03, -1.7715e-03, -8.2648e-03,  1.0738e-02, -1.9142e-02,\n                       7.8132e-04, -3.6028e-03, -3.8391e-03,  5.2486e-03, -2.6176e-03,\n                      -1.1471e-03, -1.5197e-03,  4.4321e-04, -1.5079e-03,  3.7959e-02,\n                      -3.0999e-03,  3.8018e-03, -1.0412e-02, -2.1651e-04, -6.5489e-04,\n                      -8.7839e-03,  2.9006e-03, -3.5547e-03, -6.7755e-03,  7.0923e-03,\n                      -2.3750e-03, -4.7790e-03,  2.1921e-04,  7.9754e-04,  4.6814e-03,\n                      -9.0873e-03,  1.8874e-03,  3.7029e-03, -6.0147e-03, -7.5694e-03,\n                       4.7651e-03, -1.4154e-03,  8.4380e-03, -3.3598e-03, -1.6645e-03,\n                       9.3352e-03, -8.1833e-04,  7.5843e-03,  1.9027e-03,  5.5424e-03,\n                       1.2499e-02, -3.1118e-03,  6.9663e-03, -7.9346e-03,  3.3207e-03,\n                       8.2333e-03, -2.1062e-03,  2.2069e-03, -3.5292e-03, -3.0637e-03,\n                      -3.2145e-03,  8.5636e-03, -1.1374e-03,  1.2424e-02,  2.0452e-02,\n                      -2.6990e-03, -1.2859e-02,  8.1398e-03, -1.3114e-02, -1.1080e-03,\n                       8.7630e-04,  3.3974e-03,  8.3468e-04,  3.7543e-04, -5.7049e-03,\n                       2.2966e-03,  7.8609e-03,  5.5235e-03,  3.8406e-04, -2.0735e-03,\n                      -4.8218e-03, -2.2461e-03, -6.3050e-03,  7.3855e-03, -1.8141e-04,\n                      -1.0893e-02,  2.2924e-03, -1.7816e-03,  1.0169e-02, -6.2867e-03,\n                       8.5165e-03, -2.9417e-03, -8.4615e-03, -3.9408e-03, -2.1042e-03,\n                       3.7891e-03,  5.9734e-03, -6.3505e-04, -9.2728e-04,  2.4484e-02,\n                      -7.1663e-03,  3.7858e-03,  5.3459e-03, -1.2666e-03,  5.4740e-03,\n                      -2.4305e-03, -5.3098e-04, -3.6320e-03,  9.2147e-02,  7.6047e-03,\n                       3.3166e-02, -3.6868e-03, -1.0393e-02,  7.2406e-03,  1.9212e-03,\n                       2.8705e-03, -3.3743e-03, -8.5795e-03, -9.0773e-03,  1.7122e-03,\n                       2.5263e-03,  1.0229e-02,  3.8675e-03,  1.6122e-02, -5.4640e-03,\n                      -4.0449e-03, -5.8767e-03,  2.5182e-03,  4.4959e-03,  2.4448e-03,\n                       7.9011e-04,  2.8581e-03, -3.0073e-03, -7.0976e-04,  2.7159e-03,\n                       1.0375e-02, -4.2337e-03, -1.7013e-02, -3.4608e-03],\n                     requires_grad=True)),\n             ('features.5.1.block.0.scale', tensor(0.0030)),\n             ('features.5.1.block.0.zero_point', tensor(63)),\n             ('features.5.1.block.2.weight',\n              tensor([0.5251, 0.6527, 0.8241, 1.1418, 1.1377, 1.1116, 0.6580, 1.1901, 1.6563,\n                      1.2545, 0.5916, 1.1845, 1.3899, 0.9046, 1.4874, 0.8970, 1.1728, 0.8599,\n                      1.8912, 1.0150, 1.2292, 1.3627, 1.6095, 1.4101, 0.6816, 0.7263, 0.5935,\n                      0.6884, 1.0306, 1.0108, 0.8582, 0.9758, 1.3008, 1.2476, 0.8563, 1.3197,\n                      1.1120, 1.6669, 0.7697, 0.9690, 1.5442, 1.0924, 1.5301, 1.1310, 1.8908,\n                      1.5164, 1.4293, 1.1414, 1.0374, 1.2949, 1.8028, 1.2960, 0.6136, 0.6837,\n                      1.0868, 1.0094, 1.6609, 0.6332, 0.8343, 0.9916, 1.3441, 2.8896, 0.7922,\n                      1.1683, 1.5286, 1.0057, 0.8359, 1.2643, 1.4234, 0.7763, 1.2767, 0.6825,\n                      1.2898, 0.8852, 0.6642, 1.8307, 1.2573, 1.1450, 1.1942, 0.7923, 0.9834,\n                      1.3662, 0.6136, 0.7922, 0.6393, 0.9203, 1.2267, 0.5786, 0.7671, 1.1187,\n                      0.8504, 1.5475, 1.1840, 0.9406, 0.8365, 1.1717, 1.1468, 1.1653, 0.6668,\n                      1.1984, 0.7026, 1.1109, 1.8427, 0.9331, 0.9041, 0.8379, 0.7109, 1.0794,\n                      0.6107, 1.0244, 0.6851, 0.7854, 1.3095, 0.8538, 0.9966, 0.6944, 1.1626,\n                      1.5645, 0.5908, 1.0922, 0.5395, 1.0675, 0.8595, 1.1708, 0.7907, 1.5640,\n                      0.5906, 0.6468, 0.8407, 0.8541, 1.4893, 1.5430, 1.3172, 1.4161, 0.7715,\n                      1.3564, 0.7010, 0.7604, 0.7489, 1.2450, 1.3993, 1.6689, 1.0625, 0.8040,\n                      0.8761, 1.2015, 1.4690, 0.5852, 0.5547, 1.1087, 1.7992, 0.5734, 1.3089,\n                      0.6557, 1.3802, 1.2577, 1.2936, 1.2315, 1.2010, 0.6825, 1.2960, 0.7448,\n                      0.7305, 1.3691, 1.6465, 0.6939, 1.3381, 1.2528, 0.6107, 1.0945, 0.9733,\n                      0.6427, 0.7030, 0.6129, 1.1747, 1.1628, 1.5498, 0.8541, 0.7264, 1.7622,\n                      1.0744, 1.4044, 1.1134, 0.7707, 1.0608, 1.2179, 0.8068, 0.7987, 1.1957,\n                      0.8723, 1.1138, 0.7722, 0.7355, 1.1584, 1.1944, 1.2592, 0.7326, 0.8469,\n                      1.1765, 1.2829, 0.6750, 1.0608, 0.6538, 1.2710, 0.9167, 1.1717, 0.8236,\n                      0.6324, 1.3621, 1.1525, 0.5144, 1.3645, 1.0933, 0.9910, 0.8998, 1.2898,\n                      0.9506, 0.8469, 0.6646, 1.7529, 1.0880, 1.3186, 1.5739, 0.7298, 1.6588,\n                      1.3040, 1.1876, 0.7012, 1.2383, 1.8306, 1.2558, 0.6489, 1.0153, 1.2224,\n                      1.1211, 0.7445, 0.9497, 1.1318, 0.9245, 0.9424, 1.1714, 0.9995, 0.7796,\n                      1.2904, 1.4489, 1.1209, 1.5263, 0.8686, 0.8369, 1.0226, 0.8078, 1.0817,\n                      0.6979, 0.8025, 0.7935, 0.7685, 0.9264, 1.5221, 1.9265, 0.6427, 1.5808,\n                      1.4712, 1.4951, 1.1437, 0.8465, 1.2360, 1.4289, 0.5184, 1.6129, 1.4456,\n                      0.6810, 0.7158, 1.5778, 1.0488, 0.6829, 0.9369, 1.1426, 0.3675, 0.5246,\n                      0.9524, 0.6134, 1.2674, 1.1150, 0.7385, 0.6945, 1.6776, 0.8421, 0.7292,\n                      0.7282, 0.7814, 1.1719, 1.3785, 0.9824, 0.8356, 1.2639, 0.6960, 0.9845,\n                      0.8350, 0.6415, 0.7642, 0.6085, 0.6847, 1.2757, 0.8515, 0.9560, 0.7639,\n                      0.6268, 0.5673, 0.7586, 1.1386, 1.1868, 0.7234, 0.7588, 1.1124, 0.5988,\n                      0.6461, 1.0203, 0.7321, 1.1462, 1.0197, 1.8233, 1.0121, 1.2731, 1.2492,\n                      1.3852, 1.8880, 0.7545, 1.0584, 1.2210, 0.9800, 0.5392, 1.1547, 0.7672,\n                      0.6957, 1.0975, 0.7580, 0.9083, 0.8513, 1.3552, 1.0500, 0.7878, 1.2718,\n                      0.8870, 1.7178, 0.8838, 1.0213, 1.6116, 1.1003, 0.8325, 0.6479, 1.1300,\n                      1.2544, 0.9114, 1.2859, 0.6760, 1.2245, 1.7350, 0.6592, 0.8302, 0.8227,\n                      0.9484, 0.7904, 1.6062, 1.0760, 1.8470, 0.9411, 1.0229, 1.0215, 1.2798,\n                      1.2689, 0.7784, 1.5315, 0.6649, 1.1164, 0.6047, 1.0917, 0.9206, 1.0218,\n                      1.3304, 0.8093, 0.8079, 0.6929, 1.2012, 0.9734])),\n             ('features.5.1.block.2.bias',\n              tensor([-3.8584e-02,  1.6656e-01,  1.3718e-01,  2.0547e-01, -5.3206e-01,\n                      -4.3845e-01,  6.9388e-01, -1.3519e-01,  5.4457e-03,  1.3470e-01,\n                      -8.9274e-02, -1.1731e-01,  6.4487e-02, -4.1693e-03,  4.7539e-01,\n                       1.9249e-01,  9.7812e-02,  3.3270e-01,  6.0775e-02,  4.2134e-01,\n                       2.5739e-01,  6.9571e-02, -3.3317e-01, -2.9843e-01,  1.9460e-01,\n                       3.0908e-02,  4.0677e-01,  5.3140e-01,  3.1156e-01,  3.6825e-02,\n                       2.4363e-01,  1.8084e-02,  1.2940e-01, -1.6149e-01,  1.2995e-01,\n                       1.9815e-01,  3.3376e-01,  2.8127e-01,  3.1747e-02,  5.5393e-01,\n                       2.3099e-01,  7.8082e-02, -3.5571e-02,  8.1286e-02, -2.3146e-01,\n                       2.2120e-01,  9.4410e-02,  5.4942e-01, -3.5406e-02,  1.2510e-01,\n                      -2.0921e-01,  8.0960e-02,  1.1022e-01,  1.6884e-01,  1.4855e-01,\n                       5.4177e-01, -7.3106e-02,  3.1160e-01, -3.4307e-01,  2.6027e-02,\n                      -6.7926e-02, -8.0996e-01,  4.7542e-01,  5.4290e-01,  2.9676e-02,\n                       2.4151e-01,  1.3755e-01,  1.9959e-01,  6.1614e-01,  6.0495e-01,\n                      -1.2196e-01,  1.5490e-01,  9.4430e-02,  2.3532e-01,  6.1677e-01,\n                      -1.1032e-01, -5.6566e-02, -5.7584e-02,  2.2201e-01,  2.3590e-01,\n                       2.4220e-01,  3.3524e-01,  2.9803e-01,  4.1423e-01,  1.3029e-01,\n                      -1.0416e-01,  1.4847e-01, -7.3103e-02,  6.2203e-01,  9.8345e-03,\n                       4.1716e-01, -2.3502e-01,  1.7458e-02,  4.5860e-01,  1.1296e-01,\n                       2.1695e-01,  2.5274e-01,  4.8309e-01,  2.6959e-01, -3.3626e-02,\n                       4.2894e-03,  3.8692e-01,  1.5340e-02, -1.4196e-01,  3.1358e-01,\n                      -3.3667e-02,  3.7196e-01,  3.5755e-01,  2.9831e-01,  1.1574e+00,\n                      -1.1140e+00,  3.0111e-01,  2.5399e-01, -1.0262e-01,  3.4410e-01,\n                       1.5475e-01,  2.8016e-02,  2.3614e-01,  3.7080e-02,  2.6330e-01,\n                       8.1030e-02,  4.7647e-01,  6.5100e-01,  2.6402e-01,  9.9395e-02,\n                       5.4154e-02, -2.2452e-02,  1.2217e-01,  1.5755e-01,  2.0614e-01,\n                       1.5275e-01, -2.4951e-01,  1.5604e-01,  7.4852e-02, -3.8519e-02,\n                       2.4273e-01,  1.6874e-01,  2.7356e-01,  3.5833e-01,  1.2325e-01,\n                       4.6305e-01,  1.3320e-02, -2.8182e-02,  7.5136e-01,  1.5271e-01,\n                       6.6331e-02,  4.9495e-02,  8.9289e-02,  4.3208e-01, -8.8298e-02,\n                       4.1311e-01,  2.7000e-01,  4.5720e-01, -6.6448e-02,  4.2860e-01,\n                       1.4432e-01,  2.4251e-02, -7.8127e-02,  9.3576e-02,  4.8089e-01,\n                       9.0291e-01,  2.5359e-01,  4.1928e-01,  1.2362e-01,  6.5477e-01,\n                       2.5091e-01,  8.9848e-01,  3.6961e-02,  1.4319e-01,  8.3439e-02,\n                      -3.6154e-01,  4.0522e-01,  3.9555e-01,  2.2399e-01, -9.3713e-01,\n                       1.9257e-01,  1.2013e-01,  1.7528e-01,  5.4803e-01,  1.4418e-01,\n                       1.4786e-01,  4.2894e-01, -4.3627e-01,  7.3609e-01, -6.5136e-02,\n                      -2.9722e-01,  9.9898e-02, -3.6619e-02,  1.0110e+00,  2.1029e-01,\n                       2.0131e-01,  2.2213e-01,  1.2416e-01, -2.5936e-02, -1.7789e-01,\n                      -3.2603e-01,  2.1183e-01, -2.3370e-01, -4.1571e-01,  2.7936e-02,\n                      -1.2859e-01,  9.0279e-02, -2.0641e-01,  3.3082e-01,  2.8232e-01,\n                       4.2664e-01,  3.1258e-01, -3.7421e-02,  1.7970e-01,  6.6247e-01,\n                       2.8709e-01,  2.5778e-01,  5.7855e-02,  1.5469e-01, -7.0673e-02,\n                      -1.7726e-04,  1.6856e-01,  2.0931e-01,  3.4418e-01, -2.7970e-01,\n                      -1.4198e-01,  2.4420e-01, -1.6206e-02,  2.6585e-02, -3.1231e-02,\n                       4.1021e-01,  5.7364e-01,  1.5648e-01,  3.6641e-01,  3.5319e-01,\n                       9.1645e-01,  4.5153e-01, -3.5414e-01,  3.0242e-01, -2.1989e-01,\n                       3.8497e-01,  6.4198e-03,  5.1915e-01,  1.4336e-01, -6.6251e-02,\n                      -4.3367e-02,  2.3594e-01, -1.7259e-01,  1.0059e-02,  3.8591e-01,\n                      -5.3259e-01, -2.0077e-01,  2.7564e-01,  7.2466e-01,  3.1167e-01,\n                       3.4619e-01,  1.2518e-01,  4.6178e-01, -1.2514e-01,  3.3915e-01,\n                       7.5809e-02,  3.9957e-01,  2.3946e-01, -3.0822e-01,  1.5494e-01,\n                       1.3094e-01,  2.3708e-01,  8.8997e-04,  1.5595e-01,  1.3093e+00,\n                       1.7184e-01, -1.3924e-01,  4.6727e-01,  2.0583e-01,  1.2218e-01,\n                       4.8514e-01,  2.0354e-01,  2.9006e-01,  2.9969e-01, -2.9415e-01,\n                       1.3842e-01,  2.1439e-01, -1.7459e-01,  3.8127e-01, -1.3388e-01,\n                      -5.2620e-01, -1.4076e-02, -6.9433e-02,  2.6878e-01,  3.3751e-01,\n                      -2.5803e-01,  4.0369e-01,  2.1549e-01,  6.8948e-01,  2.3194e-01,\n                       1.1234e+00,  3.2542e-03,  1.4475e-01,  4.3446e-02, -9.3433e-02,\n                       3.1106e-03,  1.3766e-01, -1.1665e-01,  2.1953e-01,  7.9355e-01,\n                      -3.4255e-01,  4.8958e-01,  6.0907e-02,  1.7130e-01,  5.2440e-01,\n                       3.6530e-01, -2.3219e-02,  1.8750e-01, -8.0779e-02, -4.9853e-01,\n                       2.7302e-01, -4.5507e-01, -1.0562e-01,  5.0156e-01, -2.6253e-02,\n                       4.4853e-01,  3.6535e-03,  1.8536e-02,  4.7452e-02,  3.1643e-01,\n                       2.9660e-02, -5.2097e-02, -1.2229e-01,  1.3876e-01,  1.7737e-01,\n                       1.8732e-01,  2.5451e-01,  2.6115e-01, -1.6153e-01,  1.0675e-01,\n                       4.8725e-01, -5.2175e-03,  4.5738e-01,  7.8249e-02,  2.4262e-01,\n                       1.7709e-01,  2.8767e-01,  4.4294e-01,  3.0911e-01,  1.4292e-01,\n                       1.0231e-02, -1.3097e-01,  3.5530e-01,  1.4089e-01, -3.3036e-01,\n                       2.8483e-01, -1.3331e-01,  1.3975e-01, -1.7497e-01,  1.4909e-01,\n                       1.9495e-01,  2.1404e-01,  1.6669e-01,  1.9896e+00, -6.5008e-02,\n                      -8.9226e-02,  2.0006e-01,  3.0667e-01, -9.3847e-02,  3.9213e-01,\n                       2.5566e-01,  2.2436e-01,  4.9844e-01,  3.7237e-01,  5.4980e-02,\n                       2.5347e-01,  1.6140e-01,  1.7433e-03, -5.1179e-01,  2.7023e-01,\n                       4.3831e-01,  3.7966e-01,  6.7716e-01, -1.3978e-01,  1.8924e-01,\n                       1.2271e-01,  5.3974e-02,  2.0842e-01,  1.3789e-01, -2.1761e-02,\n                      -2.7282e-02,  4.9401e-01,  7.1072e-01,  1.8256e-01])),\n             ('features.5.1.block.2.scale', tensor(0.1205)),\n             ('features.5.1.block.2.zero_point', tensor(70)),\n             ('features.5.1.block.3.scale', tensor(0.0756)),\n             ('features.5.1.block.3.zero_point', tensor(85)),\n             ('features.5.1.block.3._packed_params.dtype', torch.qint8),\n             ('features.5.1.block.3._packed_params._packed_params',\n              (tensor([[-0.0369, -0.0128,  0.0786,  ..., -0.0369, -0.0754, -0.0193],\n                       [ 0.0459, -0.0474, -0.0535,  ..., -0.0168, -0.0092, -0.0428],\n                       [-0.0368, -0.0102,  0.0245,  ..., -0.0491,  0.0429, -0.0102],\n                       ...,\n                       [-0.0372, -0.0558,  0.0086,  ..., -0.0744,  0.0286,  0.0172],\n                       [-0.0171,  0.0086, -0.0228,  ..., -0.0428, -0.0969,  0.0014],\n                       [-0.0489, -0.0163, -0.0564,  ...,  0.0025, -0.0501,  0.0038]],\n                      size=(1536, 384), dtype=torch.qint8,\n                      quantization_scheme=torch.per_channel_affine,\n                      scale=tensor([0.0016, 0.0015, 0.0020,  ..., 0.0014, 0.0014, 0.0013],\n                      dtype=torch.float64),\n                      zero_point=tensor([0, 0, 0,  ..., 0, 0, 0]), axis=0),\n               Parameter containing:\n               tensor([-0.0358, -0.0524, -0.0502,  ..., -0.0465, -0.0399, -0.0386],\n                      requires_grad=True))),\n             ('features.5.1.block.5.scale', tensor(0.0400)),\n             ('features.5.1.block.5.zero_point', tensor(66)),\n             ('features.5.1.block.5._packed_params.dtype', torch.qint8),\n             ('features.5.1.block.5._packed_params._packed_params',\n              (tensor([[ 0.0296,  0.0515, -0.0031,  ...,  0.0125, -0.0172, -0.0016],\n                       [-0.0153,  0.0306, -0.0204,  ...,  0.0153,  0.0204, -0.0179],\n                       [ 0.0000,  0.0436,  0.0888,  ...,  0.0639, -0.0203,  0.0748],\n                       ...,\n                       [ 0.0035, -0.0945,  0.0683,  ...,  0.0333, -0.0088,  0.0910],\n                       [ 0.0766,  0.0416,  0.0233,  ...,  0.0233, -0.0516,  0.0733],\n                       [ 0.0609,  0.0203, -0.0111,  ..., -0.1163, -0.0221, -0.0018]],\n                      size=(384, 1536), dtype=torch.qint8,\n                      quantization_scheme=torch.per_channel_affine,\n                      scale=tensor([0.0016, 0.0026, 0.0016, 0.0022, 0.0016, 0.0014, 0.0020, 0.0014, 0.0021,\n                       0.0021, 0.0014, 0.0019, 0.0019, 0.0026, 0.0017, 0.0014, 0.0015, 0.0018,\n                       0.0019, 0.0017, 0.0021, 0.0020, 0.0034, 0.0016, 0.0018, 0.0019, 0.0023,\n                       0.0032, 0.0024, 0.0025, 0.0016, 0.0018, 0.0020, 0.0016, 0.0021, 0.0014,\n                       0.0016, 0.0020, 0.0021, 0.0016, 0.0016, 0.0039, 0.0019, 0.0019, 0.0019,\n                       0.0033, 0.0021, 0.0019, 0.0015, 0.0018, 0.0016, 0.0048, 0.0016, 0.0034,\n                       0.0017, 0.0025, 0.0034, 0.0016, 0.0018, 0.0018, 0.0018, 0.0020, 0.0020,\n                       0.0016, 0.0020, 0.0029, 0.0018, 0.0023, 0.0031, 0.0025, 0.0016, 0.0016,\n                       0.0029, 0.0020, 0.0016, 0.0018, 0.0022, 0.0023, 0.0015, 0.0025, 0.0018,\n                       0.0016, 0.0023, 0.0024, 0.0024, 0.0023, 0.0022, 0.0015, 0.0016, 0.0015,\n                       0.0015, 0.0018, 0.0017, 0.0017, 0.0015, 0.0022, 0.0015, 0.0019, 0.0019,\n                       0.0021, 0.0020, 0.0015, 0.0026, 0.0017, 0.0032, 0.0014, 0.0013, 0.0035,\n                       0.0016, 0.0034, 0.0015, 0.0016, 0.0028, 0.0018, 0.0018, 0.0027, 0.0018,\n                       0.0017, 0.0017, 0.0018, 0.0018, 0.0017, 0.0019, 0.0034, 0.0024, 0.0031,\n                       0.0025, 0.0018, 0.0017, 0.0021, 0.0025, 0.0016, 0.0028, 0.0028, 0.0016,\n                       0.0036, 0.0016, 0.0019, 0.0030, 0.0023, 0.0018, 0.0017, 0.0023, 0.0020,\n                       0.0021, 0.0019, 0.0016, 0.0016, 0.0017, 0.0027, 0.0022, 0.0015, 0.0014,\n                       0.0018, 0.0026, 0.0018, 0.0040, 0.0015, 0.0020, 0.0015, 0.0018, 0.0015,\n                       0.0014, 0.0021, 0.0024, 0.0018, 0.0022, 0.0016, 0.0016, 0.0017, 0.0017,\n                       0.0019, 0.0019, 0.0024, 0.0016, 0.0031, 0.0015, 0.0021, 0.0026, 0.0022,\n                       0.0026, 0.0018, 0.0039, 0.0020, 0.0015, 0.0024, 0.0027, 0.0023, 0.0021,\n                       0.0023, 0.0019, 0.0021, 0.0016, 0.0017, 0.0020, 0.0020, 0.0018, 0.0037,\n                       0.0014, 0.0014, 0.0022, 0.0017, 0.0016, 0.0013, 0.0016, 0.0018, 0.0014,\n                       0.0014, 0.0019, 0.0024, 0.0021, 0.0023, 0.0020, 0.0015, 0.0025, 0.0021,\n                       0.0026, 0.0054, 0.0018, 0.0025, 0.0035, 0.0036, 0.0018, 0.0021, 0.0030,\n                       0.0026, 0.0020, 0.0017, 0.0014, 0.0039, 0.0019, 0.0015, 0.0020, 0.0019,\n                       0.0014, 0.0018, 0.0015, 0.0025, 0.0017, 0.0021, 0.0018, 0.0016, 0.0018,\n                       0.0016, 0.0015, 0.0037, 0.0015, 0.0021, 0.0027, 0.0014, 0.0021, 0.0019,\n                       0.0023, 0.0017, 0.0024, 0.0018, 0.0015, 0.0014, 0.0017, 0.0018, 0.0016,\n                       0.0022, 0.0021, 0.0018, 0.0039, 0.0017, 0.0021, 0.0016, 0.0028, 0.0017,\n                       0.0020, 0.0016, 0.0015, 0.0019, 0.0020, 0.0018, 0.0015, 0.0014, 0.0021,\n                       0.0014, 0.0016, 0.0020, 0.0016, 0.0022, 0.0015, 0.0018, 0.0020, 0.0014,\n                       0.0021, 0.0016, 0.0028, 0.0017, 0.0016, 0.0023, 0.0015, 0.0019, 0.0021,\n                       0.0020, 0.0019, 0.0021, 0.0018, 0.0016, 0.0028, 0.0025, 0.0018, 0.0022,\n                       0.0014, 0.0016, 0.0015, 0.0025, 0.0019, 0.0014, 0.0020, 0.0020, 0.0023,\n                       0.0019, 0.0015, 0.0029, 0.0027, 0.0021, 0.0029, 0.0015, 0.0016, 0.0040,\n                       0.0024, 0.0025, 0.0020, 0.0023, 0.0016, 0.0019, 0.0018, 0.0016, 0.0022,\n                       0.0024, 0.0024, 0.0020, 0.0018, 0.0023, 0.0033, 0.0018, 0.0018, 0.0022,\n                       0.0018, 0.0018, 0.0026, 0.0015, 0.0023, 0.0028, 0.0023, 0.0016, 0.0021,\n                       0.0021, 0.0016, 0.0039, 0.0014, 0.0023, 0.0018, 0.0023, 0.0017, 0.0024,\n                       0.0028, 0.0019, 0.0019, 0.0014, 0.0019, 0.0014, 0.0031, 0.0016, 0.0021,\n                       0.0014, 0.0018, 0.0015, 0.0026, 0.0021, 0.0021, 0.0021, 0.0017, 0.0018,\n                       0.0017, 0.0029, 0.0029, 0.0018, 0.0017, 0.0018], dtype=torch.float64),\n                      zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n                      axis=0),\n               Parameter containing:\n               tensor([ 3.8465e-03,  1.8422e-02, -2.7806e-03,  3.7995e-02,  3.9414e-02,\n                       -4.3312e-03, -2.3598e-03,  1.8526e-02, -1.1203e-03, -1.8932e-03,\n                        6.4254e-03, -3.0761e-02, -7.5954e-03, -9.3154e-03, -1.3979e-02,\n                       -6.2330e-02,  1.0579e-01,  1.5299e-02,  1.6033e-02, -3.4020e-03,\n                       -2.9030e-02, -5.8050e-03, -4.2504e-02, -3.7274e-02, -2.2445e-03,\n                       -4.3635e-02,  3.5801e-02, -6.1263e-02, -6.4105e-02,  1.3925e-01,\n                       -5.6202e-03, -2.4517e-03, -1.3405e-02,  8.4239e-03,  2.7962e-02,\n                       -5.3714e-03, -4.3837e-03,  1.3990e-02,  3.2713e-04, -1.7826e-02,\n                       -4.4431e-03, -4.6457e-02,  6.1546e-03,  1.2613e-02, -1.3348e-02,\n                       -1.8009e-02,  1.7924e-02, -8.6579e-03, -3.9483e-02, -3.2300e-02,\n                       -5.0942e-03,  1.6939e-01,  2.1139e-02,  2.9358e-02,  9.6075e-03,\n                        8.2123e-04,  5.6410e-03, -1.4171e-02,  6.3933e-02,  5.5002e-03,\n                        6.8493e-02,  1.5238e-02,  5.5830e-03, -6.3361e-03,  1.2317e-02,\n                       -7.6423e-02, -2.7981e-04, -1.8721e-02,  1.2105e-02, -4.7824e-02,\n                        1.4389e-02, -4.0746e-03,  1.1281e-02, -1.4614e-02,  7.4141e-03,\n                       -6.0922e-03,  3.0863e-02,  1.6283e-02,  1.3420e-03,  1.6022e-02,\n                        3.2269e-02,  1.0525e-02, -1.1028e-02, -4.1468e-03, -6.5006e-03,\n                        1.6578e-02, -2.7784e-03, -2.1852e-02,  7.9979e-03,  2.6830e-02,\n                        1.4202e-01,  3.1274e-03,  5.3514e-04,  3.0151e-03,  2.3951e-02,\n                        1.3165e-02, -1.9983e-02,  1.6037e-02, -8.0385e-03, -1.1669e-02,\n                       -1.4748e-02, -8.7603e-03,  7.0202e-03,  1.2180e-02,  7.5905e-02,\n                       -1.8827e-02,  5.5146e-03,  2.7896e-02, -4.5140e-02,  2.4261e-02,\n                        7.2373e-02, -4.5932e-03,  1.6024e-02, -1.1278e-02, -5.1210e-02,\n                       -1.5910e-03,  2.6050e-02, -2.1971e-02, -1.1788e-02, -1.1737e-02,\n                       -3.3882e-02,  2.1034e-02,  3.1449e-02,  1.3581e-02,  6.7905e-02,\n                        4.8013e-02,  1.5475e-02,  3.4962e-02,  2.8745e-02,  1.6403e-03,\n                       -4.6786e-03, -3.4719e-02,  6.9961e-03, -2.8600e-02, -1.3113e-02,\n                        1.4376e-01,  6.2710e-03, -6.6552e-02, -1.9401e-03,  3.3205e-02,\n                        6.5038e-02,  4.4269e-02, -1.1443e-02, -4.4546e-02,  1.9151e-02,\n                        1.4596e-02,  1.4933e-02,  2.4277e-02, -1.6893e-02,  1.1614e-02,\n                       -3.6164e-02,  3.4975e-02,  1.1439e-02,  2.0891e-02, -7.1024e-03,\n                        4.0912e-02, -4.3552e-01,  9.7226e-02,  4.7709e-02,  5.3006e-03,\n                        1.2304e-01, -1.0197e-02, -1.2659e-02,  1.6697e-02, -2.0654e-02,\n                       -2.4086e-02,  6.4231e-03,  5.4034e-03, -1.1143e-02, -6.3549e-02,\n                        1.3218e-02, -6.5766e-03, -2.8052e-02,  6.1100e-03,  1.9150e-02,\n                        3.6075e-02, -1.6388e-02,  6.5211e-04, -1.5503e-02,  9.3037e-03,\n                       -4.2173e-02, -2.5309e-02, -4.2017e-02, -2.4962e-02, -2.4265e-02,\n                        1.7501e-03, -1.3791e-03, -3.7493e-02, -2.0286e-03, -2.0394e-03,\n                       -4.1785e-03,  5.8905e-03, -1.8645e-02, -5.5229e-03, -9.4895e-03,\n                       -8.7180e-03,  8.5196e-03, -4.8495e-02,  3.7664e-02,  8.3563e-03,\n                        2.4998e-02, -2.4352e-02, -1.9130e-02,  3.0299e-02, -1.4121e-02,\n                        2.8973e-02, -1.5700e-02, -4.7710e-03,  7.6951e-02, -7.5475e-02,\n                       -1.4632e-02,  4.9194e-02,  1.2456e-02,  3.2332e-04,  1.6182e-02,\n                       -5.3064e-02, -2.7222e-02,  5.6868e-03,  4.5853e-02, -4.6216e-02,\n                       -3.7333e-02, -8.5584e-03, -6.8864e-03,  1.1226e-02, -5.0745e-03,\n                        4.8440e-03, -9.6560e-03,  2.1502e-02,  3.6303e-02, -1.3043e-02,\n                        1.3940e-02,  2.4112e-03,  2.8739e-02,  3.4808e-02, -1.3263e-02,\n                       -1.3949e-03, -2.0529e-03, -1.5688e-02,  2.8878e-02, -3.3678e-02,\n                       -9.8552e-04, -2.8807e-02, -4.1128e-02,  2.1406e-02, -2.8473e-02,\n                       -8.0161e-02,  4.9637e-02,  5.3580e-02, -6.5106e-03, -3.5593e-02,\n                        1.3188e-02,  3.1211e-02, -2.5532e-03, -4.4192e-03, -1.3050e-02,\n                        3.8921e-03, -1.4829e-02,  2.1208e-02,  2.1770e-02,  4.7785e-02,\n                        5.7002e-04,  6.0988e-02, -1.3793e-03,  8.9982e-03,  2.1801e-02,\n                       -3.2806e-02,  4.1565e-03, -1.3786e-02, -2.0281e-02,  7.5462e-03,\n                        7.9195e-03,  1.7783e-03,  5.4460e-02,  3.2643e-02,  7.1860e-02,\n                        1.6895e-02,  2.1826e-02,  7.3357e-02,  3.8807e-02, -1.3899e-02,\n                       -2.3901e-02, -1.3329e-02, -1.4475e-03,  4.5220e-02, -1.1736e-03,\n                       -4.3023e-03, -1.0696e-02,  1.6412e-02,  4.8130e-03, -6.0100e-03,\n                       -9.1563e-02,  4.8299e-03,  4.3014e-02,  9.3709e-03, -3.3376e-02,\n                       -3.1264e-02,  4.1283e-02,  9.2377e-03,  1.2263e-02,  5.7671e-02,\n                       -4.5737e-02, -1.8931e-03, -6.7754e-03,  9.5087e-03, -3.4090e-02,\n                       -4.8894e-03,  9.3499e-03,  9.8616e-06,  1.5024e-03, -3.1335e-02,\n                       -3.7853e-02,  1.7492e-02,  1.0715e-02, -1.1144e-02,  3.1574e-02,\n                       -1.1462e-02,  3.2111e-02, -3.6415e-02, -8.0780e-03,  3.9928e-02,\n                        1.2922e-03, -2.9664e-02, -1.0808e-02, -1.2192e-03, -1.2280e-01,\n                       -3.9669e-02, -8.2418e-03,  7.9494e-03,  7.5415e-03, -2.1157e-02,\n                        5.1798e-03,  6.5927e-05, -1.1581e-02, -1.7084e-02, -2.6505e-02,\n                       -2.4745e-02, -2.5641e-03,  6.4924e-03, -1.1310e-02, -2.4488e-03,\n                       -3.9338e-03,  5.0997e-02, -6.8386e-03,  1.6138e-02, -6.0181e-03,\n                        8.3426e-03,  1.3477e-02,  1.1746e-03,  1.9438e-01, -2.4869e-02,\n                        1.6268e-03,  2.1992e-02, -3.2589e-02,  3.6001e-02,  1.4806e-02,\n                       -2.7266e-02, -5.8371e-03, -6.8681e-03, -1.8480e-02,  1.8654e-02,\n                        1.8047e-02,  4.0225e-03,  3.2172e-03, -2.3072e-02,  4.1164e-02,\n                        7.9146e-02,  9.8143e-03,  7.1515e-03, -4.9898e-02,  3.1114e-02,\n                        2.2675e-02, -3.1319e-02,  1.4411e-02, -2.5076e-02,  1.9536e-02,\n                        7.6370e-02,  1.3218e-02, -6.5799e-02,  7.5016e-03,  4.0440e-03,\n                        3.4808e-02, -6.3467e-03,  3.3382e-02, -2.0079e-02],\n                      requires_grad=True))),\n             ('features.5.2.layer_scale',\n              tensor([[[ 0.0586]],\n              \n                      [[-0.0531]],\n              \n                      [[-0.0546]],\n              \n                      [[ 0.1035]],\n              \n                      [[ 0.0677]],\n              \n                      [[ 0.0741]],\n              \n                      [[ 0.0468]],\n              \n                      [[-0.0733]],\n              \n                      [[ 0.0646]],\n              \n                      [[-0.0729]],\n              \n                      [[-0.0510]],\n              \n                      [[-0.0684]],\n              \n                      [[ 0.1035]],\n              \n                      [[-0.0869]],\n              \n                      [[ 0.0892]],\n              \n                      [[-0.0592]],\n              \n                      [[-0.1096]],\n              \n                      [[ 0.0834]],\n              \n                      [[-0.0646]],\n              \n                      [[-0.0625]],\n              \n                      [[ 0.1234]],\n              \n                      [[-0.0778]],\n              \n                      [[-0.1311]],\n              \n                      [[ 0.0521]],\n              \n                      [[ 0.0591]],\n              \n                      [[-0.0488]],\n              \n                      [[ 0.0766]],\n              \n                      [[-0.0784]],\n              \n                      [[-0.0451]],\n              \n                      [[-0.1573]],\n              \n                      [[ 0.0536]],\n              \n                      [[ 0.0661]],\n              \n                      [[ 0.0890]],\n              \n                      [[-0.0735]],\n              \n                      [[ 0.0876]],\n              \n                      [[ 0.0698]],\n              \n                      [[ 0.0903]],\n              \n                      [[ 0.0639]],\n              \n                      [[ 0.0892]],\n              \n                      [[ 0.0736]],\n              \n                      [[-0.0938]],\n              \n                      [[-0.0957]],\n              \n                      [[-0.0888]],\n              \n                      [[ 0.0801]],\n              \n                      [[-0.0746]],\n              \n                      [[ 0.0949]],\n              \n                      [[ 0.0214]],\n              \n                      [[-0.0811]],\n              \n                      [[ 0.0547]],\n              \n                      [[ 0.0950]],\n              \n                      [[-0.0935]],\n              \n                      [[-0.0925]],\n              \n                      [[ 0.0572]],\n              \n                      [[ 0.0867]],\n              \n                      [[ 0.0625]],\n              \n                      [[ 0.0719]],\n              \n                      [[ 0.1125]],\n              \n                      [[-0.0562]],\n              \n                      [[-0.0524]],\n              \n                      [[-0.0647]],\n              \n                      [[-0.0736]],\n              \n                      [[-0.0704]],\n              \n                      [[ 0.0590]],\n              \n                      [[ 0.0588]],\n              \n                      [[ 0.0691]],\n              \n                      [[-0.0534]],\n              \n                      [[ 0.0477]],\n              \n                      [[-0.1271]],\n              \n                      [[-0.0616]],\n              \n                      [[ 0.0438]],\n              \n                      [[-0.0713]],\n              \n                      [[-0.0506]],\n              \n                      [[ 0.1017]],\n              \n                      [[ 0.0499]],\n              \n                      [[-0.0636]],\n              \n                      [[ 0.0822]],\n              \n                      [[ 0.0596]],\n              \n                      [[-0.0591]],\n              \n                      [[-0.0765]],\n              \n                      [[ 0.0821]],\n              \n                      [[ 0.0767]],\n              \n                      [[-0.0833]],\n              \n                      [[-0.0869]],\n              \n                      [[-0.0476]],\n              \n                      [[-0.0876]],\n              \n                      [[-0.0729]],\n              \n                      [[-0.0519]],\n              \n                      [[-0.0680]],\n              \n                      [[-0.0479]],\n              \n                      [[ 0.0568]],\n              \n                      [[-0.0808]],\n              \n                      [[ 0.0863]],\n              \n                      [[-0.0785]],\n              \n                      [[-0.0890]],\n              \n                      [[ 0.0754]],\n              \n                      [[ 0.0830]],\n              \n                      [[-0.0829]],\n              \n                      [[-0.0818]],\n              \n                      [[-0.0752]],\n              \n                      [[ 0.0893]],\n              \n                      [[-0.0635]],\n              \n                      [[ 0.0624]],\n              \n                      [[-0.0506]],\n              \n                      [[ 0.0759]],\n              \n                      [[-0.0869]],\n              \n                      [[-0.0675]],\n              \n                      [[ 0.0485]],\n              \n                      [[ 0.1041]],\n              \n                      [[-0.0779]],\n              \n                      [[-0.0899]],\n              \n                      [[ 0.0744]],\n              \n                      [[-0.0705]],\n              \n                      [[-0.0373]],\n              \n                      [[-0.0578]],\n              \n                      [[ 0.0611]],\n              \n                      [[ 0.0702]],\n              \n                      [[ 0.0576]],\n              \n                      [[ 0.0451]],\n              \n                      [[ 0.0492]],\n              \n                      [[-0.0847]],\n              \n                      [[ 0.0641]],\n              \n                      [[ 0.0598]],\n              \n                      [[-0.0538]],\n              \n                      [[-0.0291]],\n              \n                      [[ 0.0992]],\n              \n                      [[ 0.0452]],\n              \n                      [[-0.0537]],\n              \n                      [[ 0.0569]],\n              \n                      [[-0.0571]],\n              \n                      [[-0.0478]],\n              \n                      [[-0.0213]],\n              \n                      [[ 0.0704]],\n              \n                      [[ 0.0501]],\n              \n                      [[ 0.1009]],\n              \n                      [[-0.0776]],\n              \n                      [[-0.1386]],\n              \n                      [[ 0.0559]],\n              \n                      [[ 0.0490]],\n              \n                      [[ 0.0422]],\n              \n                      [[ 0.0844]],\n              \n                      [[-0.0648]],\n              \n                      [[ 0.0792]],\n              \n                      [[-0.0866]],\n              \n                      [[ 0.0765]],\n              \n                      [[ 0.0549]],\n              \n                      [[-0.0536]],\n              \n                      [[-0.0595]],\n              \n                      [[-0.0630]],\n              \n                      [[-0.0699]],\n              \n                      [[ 0.0649]],\n              \n                      [[ 0.0693]],\n              \n                      [[ 0.0678]],\n              \n                      [[-0.0660]],\n              \n                      [[ 0.0672]],\n              \n                      [[ 0.0871]],\n              \n                      [[ 0.0684]],\n              \n                      [[ 0.5700]],\n              \n                      [[ 0.1016]],\n              \n                      [[ 0.0882]],\n              \n                      [[-0.0549]],\n              \n                      [[ 0.1161]],\n              \n                      [[-0.0610]],\n              \n                      [[ 0.0620]],\n              \n                      [[ 0.0205]],\n              \n                      [[-0.0705]],\n              \n                      [[-0.0697]],\n              \n                      [[ 0.0609]],\n              \n                      [[-0.0608]],\n              \n                      [[ 0.0779]],\n              \n                      [[-0.0764]],\n              \n                      [[ 0.0619]],\n              \n                      [[-0.0562]],\n              \n                      [[ 0.0659]],\n              \n                      [[-0.0786]],\n              \n                      [[-0.0646]],\n              \n                      [[ 0.0674]],\n              \n                      [[ 0.0633]],\n              \n                      [[-0.0557]],\n              \n                      [[-0.0806]],\n              \n                      [[ 0.0665]],\n              \n                      [[-0.0747]],\n              \n                      [[-0.0516]],\n              \n                      [[ 0.1016]],\n              \n                      [[ 0.0824]],\n              \n                      [[-0.0800]],\n              \n                      [[-0.1010]],\n              \n                      [[-0.0476]],\n              \n                      [[-0.0944]],\n              \n                      [[-0.0688]],\n              \n                      [[-0.0568]],\n              \n                      [[-0.0174]],\n              \n                      [[-0.0752]],\n              \n                      [[-0.0651]],\n              \n                      [[-0.0567]],\n              \n                      [[ 0.0619]],\n              \n                      [[ 0.0741]],\n              \n                      [[ 0.0590]],\n              \n                      [[-0.0669]],\n              \n                      [[ 0.0533]],\n              \n                      [[ 0.0737]],\n              \n                      [[-0.0784]],\n              \n                      [[-0.0719]],\n              \n                      [[ 0.0591]],\n              \n                      [[ 0.0624]],\n              \n                      [[ 0.0569]],\n              \n                      [[ 0.0791]],\n              \n                      [[ 0.0587]],\n              \n                      [[ 0.0548]],\n              \n                      [[ 0.0802]],\n              \n                      [[ 0.1066]],\n              \n                      [[ 0.0732]],\n              \n                      [[-0.0758]],\n              \n                      [[ 0.1014]],\n              \n                      [[ 0.0616]],\n              \n                      [[ 0.0707]],\n              \n                      [[-0.0692]],\n              \n                      [[ 0.0972]],\n              \n                      [[-0.0185]],\n              \n                      [[ 0.0767]],\n              \n                      [[ 0.1101]],\n              \n                      [[ 0.0927]],\n              \n                      [[ 0.0619]],\n              \n                      [[-0.0816]],\n              \n                      [[ 0.0523]],\n              \n                      [[-0.0583]],\n              \n                      [[ 0.0754]],\n              \n                      [[ 0.0749]],\n              \n                      [[ 0.0656]],\n              \n                      [[ 0.0768]],\n              \n                      [[-0.0506]],\n              \n                      [[ 0.0637]],\n              \n                      [[-0.0551]],\n              \n                      [[ 0.0805]],\n              \n                      [[ 0.0882]],\n              \n                      [[-0.0507]],\n              \n                      [[-0.0575]],\n              \n                      [[-0.0690]],\n              \n                      [[ 0.0928]],\n              \n                      [[ 0.0695]],\n              \n                      [[-0.0845]],\n              \n                      [[-0.0339]],\n              \n                      [[ 0.0683]],\n              \n                      [[ 0.0529]],\n              \n                      [[ 0.0613]],\n              \n                      [[-0.0576]],\n              \n                      [[-0.0800]],\n              \n                      [[-0.0602]],\n              \n                      [[ 0.1095]],\n              \n                      [[ 0.0661]],\n              \n                      [[ 0.0831]],\n              \n                      [[ 0.0529]],\n              \n                      [[ 0.0840]],\n              \n                      [[ 0.0545]],\n              \n                      [[-0.0641]],\n              \n                      [[ 0.0984]],\n              \n                      [[ 0.0877]],\n              \n                      [[-0.0614]],\n              \n                      [[ 0.0787]],\n              \n                      [[-0.0635]],\n              \n                      [[ 0.0814]],\n              \n                      [[-0.0749]],\n              \n                      [[-0.0926]],\n              \n                      [[-0.0755]],\n              \n                      [[ 0.0514]],\n              \n                      [[-0.0447]],\n              \n                      [[-0.0927]],\n              \n                      [[-0.0424]],\n              \n                      [[ 0.0643]],\n              \n                      [[-0.0617]],\n              \n                      [[ 0.1134]],\n              \n                      [[ 0.0642]],\n              \n                      [[-0.0475]],\n              \n                      [[ 0.0762]],\n              \n                      [[-0.0820]],\n              \n                      [[-0.0489]],\n              \n                      [[-0.0698]],\n              \n                      [[-0.0577]],\n              \n                      [[-0.0557]],\n              \n                      [[-0.0663]],\n              \n                      [[-0.0732]],\n              \n                      [[ 0.0447]],\n              \n                      [[ 0.0568]],\n              \n                      [[ 0.0575]],\n              \n                      [[ 0.1195]],\n              \n                      [[ 0.0511]],\n              \n                      [[-0.0832]],\n              \n                      [[ 0.0529]],\n              \n                      [[-0.0667]],\n              \n                      [[ 0.0734]],\n              \n                      [[ 0.0590]],\n              \n                      [[ 0.1164]],\n              \n                      [[-0.1324]],\n              \n                      [[ 0.0650]],\n              \n                      [[-0.0166]],\n              \n                      [[ 0.0577]],\n              \n                      [[ 0.0576]],\n              \n                      [[-0.0680]],\n              \n                      [[-0.0568]],\n              \n                      [[-0.0493]],\n              \n                      [[-0.0478]],\n              \n                      [[ 0.0638]],\n              \n                      [[-0.0688]],\n              \n                      [[ 0.0533]],\n              \n                      [[-0.0737]],\n              \n                      [[-0.0476]],\n              \n                      [[ 0.0577]],\n              \n                      [[ 0.0590]],\n              \n                      [[ 0.0597]],\n              \n                      [[ 0.0521]],\n              \n                      [[-0.1116]],\n              \n                      [[-0.0804]],\n              \n                      [[-0.0498]],\n              \n                      [[ 0.0713]],\n              \n                      [[-0.0987]],\n              \n                      [[ 0.0433]],\n              \n                      [[-0.0563]],\n              \n                      [[-0.0675]],\n              \n                      [[ 0.0841]],\n              \n                      [[ 0.0226]],\n              \n                      [[ 0.0745]],\n              \n                      [[ 0.0112]],\n              \n                      [[-0.0613]],\n              \n                      [[-0.0606]],\n              \n                      [[ 0.0698]],\n              \n                      [[ 0.1065]],\n              \n                      [[-0.0621]],\n              \n                      [[ 0.0366]],\n              \n                      [[-0.0848]],\n              \n                      [[ 0.0686]],\n              \n                      [[ 0.0783]],\n              \n                      [[-0.0808]],\n              \n                      [[ 0.0608]],\n              \n                      [[ 0.0805]],\n              \n                      [[-0.0795]],\n              \n                      [[ 0.0685]],\n              \n                      [[-0.0710]],\n              \n                      [[-0.0599]],\n              \n                      [[-0.0461]],\n              \n                      [[-0.0709]],\n              \n                      [[ 0.0322]],\n              \n                      [[ 0.0488]],\n              \n                      [[ 0.0840]],\n              \n                      [[ 0.0468]],\n              \n                      [[-0.0761]],\n              \n                      [[-0.0643]],\n              \n                      [[ 0.0561]],\n              \n                      [[ 0.0678]],\n              \n                      [[ 0.0505]],\n              \n                      [[-0.0518]],\n              \n                      [[-0.0595]],\n              \n                      [[-0.0687]],\n              \n                      [[-0.0749]],\n              \n                      [[ 0.0719]],\n              \n                      [[ 0.0222]],\n              \n                      [[ 0.0594]],\n              \n                      [[-0.0422]],\n              \n                      [[ 0.0702]],\n              \n                      [[-0.0658]],\n              \n                      [[-0.0972]],\n              \n                      [[-0.0831]],\n              \n                      [[ 0.0640]],\n              \n                      [[ 0.0612]],\n              \n                      [[ 0.0713]],\n              \n                      [[-0.0666]],\n              \n                      [[ 0.1084]],\n              \n                      [[ 0.0807]],\n              \n                      [[ 0.0863]],\n              \n                      [[ 0.0789]],\n              \n                      [[-0.1144]],\n              \n                      [[ 0.1006]],\n              \n                      [[ 0.0506]],\n              \n                      [[-0.0657]],\n              \n                      [[ 0.1079]],\n              \n                      [[-0.0794]],\n              \n                      [[-0.0709]],\n              \n                      [[-0.1088]],\n              \n                      [[ 0.0572]],\n              \n                      [[-0.1315]],\n              \n                      [[-0.0649]],\n              \n                      [[ 0.0879]],\n              \n                      [[-0.0637]],\n              \n                      [[ 0.0467]],\n              \n                      [[ 0.0808]],\n              \n                      [[-0.0756]]])),\n             ('features.5.2.block.0.weight',\n              tensor([[[[ 0.0157,  0.0231,  0.0199,  ...,  0.0136,  0.0210,  0.0241],\n                        [ 0.0210,  0.0136,  0.0168,  ...,  0.0241,  0.0231,  0.0105],\n                        [ 0.0294,  0.0115,  0.0587,  ...,  0.0493,  0.0231,  0.0262],\n                        ...,\n                        [ 0.0199,  0.0336,  0.0598,  ...,  0.0671,  0.0220,  0.0273],\n                        [ 0.0168,  0.0105,  0.0273,  ...,  0.0231,  0.0157,  0.0126],\n                        [ 0.0168,  0.0157,  0.0283,  ...,  0.0388,  0.0126,  0.0252]]],\n              \n              \n                      [[[-0.0080, -0.0016, -0.0080,  ..., -0.0096,  0.0016, -0.0128],\n                        [-0.0032, -0.0032, -0.0112,  ..., -0.0080, -0.0128,  0.0032],\n                        [-0.0096, -0.0112, -0.0399,  ..., -0.0478, -0.0080, -0.0128],\n                        ...,\n                        [ 0.0080,  0.0128,  0.0399,  ...,  0.0415,  0.0080,  0.0112],\n                        [ 0.0032,  0.0000,  0.0080,  ...,  0.0112, -0.0016, -0.0048],\n                        [-0.0032, -0.0048, -0.0064,  ...,  0.0048, -0.0048,  0.0032]]],\n              \n              \n                      [[[-0.0049,  0.0025,  0.0074,  ...,  0.0025,  0.0000, -0.0025],\n                        [ 0.0025,  0.0098,  0.0000,  ..., -0.0025,  0.0147,  0.0049],\n                        [-0.0049,  0.0074,  0.0172,  ...,  0.0294, -0.0025,  0.0049],\n                        ...,\n                        [ 0.0098,  0.0000,  0.0221,  ...,  0.0343,  0.0098,  0.0123],\n                        [ 0.0049,  0.0049,  0.0074,  ..., -0.0025,  0.0098,  0.0000],\n                        [ 0.0025,  0.0123,  0.0098,  ...,  0.0025,  0.0025,  0.0049]]],\n              \n              \n                      ...,\n              \n              \n                      [[[ 0.0066,  0.0111,  0.0066,  ...,  0.0066,  0.0000,  0.0133],\n                        [ 0.0133,  0.0000,  0.0088,  ...,  0.0022,  0.0111,  0.0044],\n                        [ 0.0044,  0.0133,  0.0243,  ...,  0.0199,  0.0066,  0.0111],\n                        ...,\n                        [ 0.0044,  0.0066,  0.0398,  ...,  0.0332,  0.0044,  0.0000],\n                        [ 0.0000,  0.0088,  0.0088,  ...,  0.0111,  0.0088,  0.0088],\n                        [ 0.0088,  0.0111,  0.0111,  ...,  0.0155,  0.0044,  0.0111]]],\n              \n              \n                      [[[ 0.0000,  0.0052, -0.0026,  ...,  0.0052, -0.0026,  0.0026],\n                        [ 0.0077,  0.0026,  0.0000,  ..., -0.0026,  0.0103,  0.0052],\n                        [ 0.0026,  0.0077, -0.0026,  ..., -0.0026,  0.0026,  0.0026],\n                        ...,\n                        [ 0.0026,  0.0155,  0.0799,  ...,  0.0696,  0.0232, -0.0052],\n                        [ 0.0052,  0.0155, -0.0206,  ..., -0.0155,  0.0026,  0.0077],\n                        [ 0.0103,  0.0026, -0.0206,  ..., -0.0129,  0.0103,  0.0000]]],\n              \n              \n                      [[[ 0.0122,  0.0087,  0.0227,  ...,  0.0192,  0.0035,  0.0157],\n                        [ 0.0139,  0.0017,  0.0157,  ...,  0.0296,  0.0017,  0.0070],\n                        [ 0.0087,  0.0139,  0.0697,  ...,  0.0750,  0.0070,  0.0209],\n                        ...,\n                        [-0.0174, -0.0209, -0.0715,  ..., -0.0837, -0.0157, -0.0244],\n                        [-0.0052, -0.0087, -0.0244,  ..., -0.0279, -0.0139, -0.0052],\n                        [-0.0070, -0.0192, -0.0157,  ..., -0.0192, -0.0139, -0.0070]]]],\n                     size=(384, 1, 7, 7), dtype=torch.qint8,\n                     quantization_scheme=torch.per_channel_affine,\n                     scale=tensor([0.0010, 0.0016, 0.0025, 0.0026, 0.0028, 0.0030, 0.0015, 0.0029, 0.0019,\n                      0.0015, 0.0017, 0.0022, 0.0025, 0.0008, 0.0013, 0.0018, 0.0028, 0.0024,\n                      0.0017, 0.0025, 0.0011, 0.0029, 0.0031, 0.0028, 0.0022, 0.0015, 0.0014,\n                      0.0015, 0.0024, 0.0029, 0.0027, 0.0026, 0.0020, 0.0009, 0.0009, 0.0030,\n                      0.0025, 0.0022, 0.0009, 0.0023, 0.0014, 0.0029, 0.0020, 0.0028, 0.0017,\n                      0.0010, 0.0018, 0.0028, 0.0025, 0.0019, 0.0021, 0.0031, 0.0018, 0.0008,\n                      0.0027, 0.0016, 0.0011, 0.0020, 0.0024, 0.0019, 0.0014, 0.0008, 0.0014,\n                      0.0026, 0.0015, 0.0016, 0.0023, 0.0022, 0.0023, 0.0006, 0.0016, 0.0023,\n                      0.0020, 0.0024, 0.0017, 0.0019, 0.0023, 0.0016, 0.0029, 0.0010, 0.0028,\n                      0.0015, 0.0015, 0.0020, 0.0011, 0.0014, 0.0026, 0.0015, 0.0021, 0.0027,\n                      0.0021, 0.0027, 0.0014, 0.0024, 0.0013, 0.0029, 0.0027, 0.0019, 0.0009,\n                      0.0017, 0.0016, 0.0031, 0.0016, 0.0029, 0.0012, 0.0025, 0.0023, 0.0020,\n                      0.0008, 0.0026, 0.0020, 0.0013, 0.0015, 0.0026, 0.0022, 0.0011, 0.0027,\n                      0.0028, 0.0017, 0.0020, 0.0015, 0.0014, 0.0023, 0.0009, 0.0031, 0.0017,\n                      0.0014, 0.0015, 0.0024, 0.0014, 0.0016, 0.0027, 0.0019, 0.0019, 0.0025,\n                      0.0029, 0.0022, 0.0014, 0.0016, 0.0022, 0.0031, 0.0013, 0.0026, 0.0018,\n                      0.0024, 0.0023, 0.0031, 0.0016, 0.0007, 0.0026, 0.0012, 0.0017, 0.0020,\n                      0.0017, 0.0009, 0.0024, 0.0010, 0.0013, 0.0029, 0.0024, 0.0028, 0.0022,\n                      0.0028, 0.0018, 0.0026, 0.0018, 0.0026, 0.0028, 0.0007, 0.0024, 0.0021,\n                      0.0018, 0.0023, 0.0013, 0.0030, 0.0015, 0.0021, 0.0025, 0.0008, 0.0024,\n                      0.0014, 0.0028, 0.0028, 0.0024, 0.0024, 0.0027, 0.0020, 0.0010, 0.0024,\n                      0.0023, 0.0017, 0.0014, 0.0025, 0.0013, 0.0029, 0.0017, 0.0020, 0.0021,\n                      0.0028, 0.0026, 0.0028, 0.0024, 0.0017, 0.0028, 0.0026, 0.0030, 0.0023,\n                      0.0024, 0.0026, 0.0029, 0.0009, 0.0027, 0.0017, 0.0015, 0.0014, 0.0025,\n                      0.0024, 0.0016, 0.0010, 0.0025, 0.0028, 0.0013, 0.0024, 0.0021, 0.0018,\n                      0.0018, 0.0027, 0.0016, 0.0020, 0.0016, 0.0014, 0.0022, 0.0019, 0.0029,\n                      0.0027, 0.0012, 0.0025, 0.0023, 0.0017, 0.0028, 0.0016, 0.0016, 0.0022,\n                      0.0028, 0.0023, 0.0015, 0.0029, 0.0026, 0.0025, 0.0013, 0.0023, 0.0016,\n                      0.0021, 0.0017, 0.0016, 0.0022, 0.0026, 0.0019, 0.0010, 0.0009, 0.0025,\n                      0.0012, 0.0019, 0.0026, 0.0009, 0.0021, 0.0024, 0.0013, 0.0027, 0.0029,\n                      0.0016, 0.0022, 0.0027, 0.0017, 0.0011, 0.0017, 0.0027, 0.0028, 0.0011,\n                      0.0019, 0.0014, 0.0021, 0.0021, 0.0011, 0.0019, 0.0021, 0.0024, 0.0009,\n                      0.0014, 0.0021, 0.0018, 0.0026, 0.0026, 0.0014, 0.0028, 0.0022, 0.0014,\n                      0.0025, 0.0015, 0.0019, 0.0020, 0.0009, 0.0026, 0.0029, 0.0024, 0.0023,\n                      0.0024, 0.0017, 0.0021, 0.0027, 0.0021, 0.0012, 0.0016, 0.0031, 0.0022,\n                      0.0008, 0.0021, 0.0013, 0.0016, 0.0026, 0.0019, 0.0026, 0.0030, 0.0027,\n                      0.0030, 0.0024, 0.0013, 0.0017, 0.0030, 0.0028, 0.0016, 0.0028, 0.0009,\n                      0.0009, 0.0021, 0.0022, 0.0026, 0.0022, 0.0017, 0.0019, 0.0024, 0.0032,\n                      0.0019, 0.0019, 0.0011, 0.0027, 0.0023, 0.0021, 0.0010, 0.0021, 0.0028,\n                      0.0009, 0.0013, 0.0010, 0.0021, 0.0016, 0.0020, 0.0008, 0.0019, 0.0010,\n                      0.0021, 0.0026, 0.0021, 0.0020, 0.0021, 0.0028, 0.0008, 0.0015, 0.0027,\n                      0.0026, 0.0023, 0.0019, 0.0023, 0.0021, 0.0010, 0.0024, 0.0010, 0.0027,\n                      0.0018, 0.0020, 0.0017, 0.0022, 0.0026, 0.0017], dtype=torch.float64),\n                     zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n                     axis=0)),\n             ('features.5.2.block.0.bias',\n              Parameter containing:\n              tensor([ 5.6374e-03, -3.2109e-03,  9.1714e-03,  8.3498e-04, -7.1896e-03,\n                       3.8336e-03,  7.8782e-03,  1.5313e-03,  8.8812e-04,  1.1500e-03,\n                       2.3066e-03, -1.7923e-03, -3.4643e-02, -1.2019e-02, -4.0414e-03,\n                       3.2799e-03, -1.8421e-03,  1.4711e-03,  1.0767e-02, -3.5383e-03,\n                       4.9004e-03,  1.0480e-02,  1.4596e-02, -9.8562e-04,  1.9380e-03,\n                       4.9581e-03, -8.4753e-05,  1.5990e-03,  8.9860e-03, -1.8916e-03,\n                      -1.1521e-02, -1.9600e-03, -5.6138e-03, -1.4729e-02, -4.8382e-03,\n                       7.6061e-04, -3.3008e-03, -1.0719e-02, -1.7303e-02, -6.6692e-03,\n                      -1.0043e-02, -3.1082e-03,  4.1581e-03, -7.6981e-03,  3.6364e-03,\n                      -9.3989e-03,  3.4503e-03, -2.0863e-03, -1.1170e-03, -2.7139e-03,\n                       9.6069e-03,  6.2444e-05,  4.9634e-03, -4.9922e-03,  1.4652e-03,\n                      -2.7606e-03,  1.3390e-02,  2.1215e-03,  2.1705e-02,  1.6026e-03,\n                      -2.3768e-03,  1.5336e-02, -1.6507e-03, -1.8462e-02, -2.2378e-02,\n                       1.4618e-02, -1.7309e-03, -2.7841e-04,  1.3963e-02, -9.5518e-03,\n                       7.2081e-04, -8.4001e-03,  1.8913e-02,  1.1249e-02, -6.2350e-03,\n                       1.1621e-02, -1.2532e-02, -7.4569e-03,  4.5148e-03,  1.1574e-02,\n                      -1.0159e-02, -3.8223e-03,  4.8759e-03,  3.5424e-03, -3.1045e-03,\n                       1.4507e-03,  5.6608e-04,  1.7420e-03, -9.1382e-03,  1.9148e-02,\n                       5.2225e-02, -7.2003e-03, -4.7016e-03, -1.9355e-02, -6.1765e-03,\n                       5.2928e-03, -6.3367e-03, -2.5767e-02, -4.6837e-03,  4.7590e-04,\n                      -8.4237e-03, -1.5382e-03, -8.1414e-03,  2.7172e-04,  1.2516e-02,\n                       1.5922e-04,  4.2807e-03,  2.8454e-03,  3.2472e-03,  1.0960e-02,\n                      -1.0013e-02, -1.6575e-02,  8.1393e-03,  1.5275e-03,  2.5611e-03,\n                      -3.2770e-03,  3.4895e-03,  3.6545e-03,  2.2645e-03,  4.4752e-03,\n                       6.3617e-03,  2.5293e-03, -4.6852e-04,  7.9349e-04,  2.4733e-03,\n                       4.6202e-04, -2.4450e-03,  3.4259e-03, -9.7625e-03, -8.2250e-03,\n                       1.3517e-03,  7.9527e-03, -4.9250e-04,  2.7674e-04,  9.4632e-03,\n                       2.0367e-02,  3.2158e-03,  4.1671e-02, -9.1539e-04,  2.0465e-03,\n                       1.7732e-04, -1.8215e-02,  5.9200e-03,  8.6226e-03,  6.5439e-03,\n                       1.0199e-02,  8.8360e-03, -1.3571e-02,  1.2653e-02,  1.3381e-03,\n                      -6.1670e-03, -3.4705e-03,  4.2567e-03, -2.1355e-02,  1.2147e-03,\n                       1.5201e-03,  8.4723e-03,  1.0736e-02,  4.2388e-03,  7.6938e-03,\n                       2.1848e-02,  1.2009e-03,  4.2904e-03,  3.5351e-03, -5.9692e-03,\n                      -1.7082e-03, -2.2004e-02,  5.4925e-04, -4.0522e-03,  9.6647e-03,\n                      -1.5031e-03, -5.3134e-03, -5.4001e-03,  2.2605e-02, -1.9183e-02,\n                       4.8877e-03, -3.7252e-03, -5.7908e-03,  3.6483e-03,  8.2091e-03,\n                       3.0424e-03, -1.1725e-02, -9.1366e-03, -3.7506e-03, -1.1547e-02,\n                      -1.1189e-02,  8.2686e-03, -2.5375e-03,  6.5876e-04, -6.9171e-03,\n                       5.9148e-04, -9.4639e-03,  3.2423e-03, -1.0099e-03,  1.0948e-03,\n                      -1.5905e-02,  5.7039e-04, -1.9454e-03,  2.2134e-02,  1.1193e-02,\n                       5.5474e-03,  7.5166e-03,  6.4358e-04, -4.8157e-03,  3.7249e-03,\n                       2.3581e-03,  1.9981e-04,  1.0917e-03,  1.4619e-02, -5.5464e-03,\n                      -6.0476e-03, -5.2206e-03,  1.8184e-03, -2.3262e-03,  2.8321e-03,\n                      -3.3156e-03,  1.2902e-02, -3.6958e-03, -4.0719e-03,  3.7022e-03,\n                       2.1454e-03,  1.3845e-03, -1.8682e-03, -3.0899e-04,  8.6900e-03,\n                      -1.6393e-02,  1.3701e-02, -7.2127e-04, -3.3954e-03, -5.6917e-03,\n                       2.5419e-03,  9.2154e-03,  7.6295e-03,  7.0865e-03, -4.5235e-03,\n                      -5.3382e-04, -2.2592e-03,  8.0197e-03,  2.1464e-03,  1.1466e-02,\n                       1.7868e-03, -4.3374e-03,  1.5490e-03,  8.1843e-03,  8.5245e-03,\n                      -8.4112e-03, -5.7124e-04, -3.1827e-03,  4.0690e-03,  3.7955e-03,\n                       3.5904e-03,  1.2533e-03, -3.2167e-03, -2.0495e-03, -1.5237e-05,\n                      -2.2024e-03, -1.8468e-03, -4.4886e-02, -5.3611e-02, -4.3251e-03,\n                      -1.9007e-02, -4.7414e-04,  4.1003e-03, -8.7630e-03, -4.0128e-03,\n                      -9.6423e-03, -2.2818e-02,  8.6203e-04,  3.3373e-03,  2.2441e-03,\n                       7.2595e-04, -1.6724e-03, -6.1990e-03,  1.9496e-03,  3.1160e-02,\n                      -3.7851e-03,  3.9714e-03, -1.9616e-02,  1.8068e-02,  2.7045e-03,\n                      -4.4170e-03,  1.1114e-02,  1.4943e-02, -8.3377e-03,  8.5370e-03,\n                       2.5386e-03,  4.0651e-03, -4.4154e-03,  4.6719e-03, -5.6494e-03,\n                       5.2149e-03, -1.1568e-02,  1.3810e-02, -3.2756e-03,  6.8754e-03,\n                      -5.4150e-03,  1.0349e-02,  4.7724e-04, -7.0998e-03,  9.0492e-03,\n                      -4.4943e-03,  4.8572e-04, -5.7174e-03, -2.1334e-04, -1.0233e-03,\n                      -1.2903e-02, -1.0693e-02, -6.1234e-03,  1.5575e-02,  1.7692e-03,\n                      -8.1183e-03,  4.3096e-04,  5.3392e-03, -9.3113e-03, -2.8580e-02,\n                       1.3444e-02, -2.3032e-03,  6.3452e-04, -6.6329e-03, -1.0572e-02,\n                      -3.3829e-03, -9.5271e-03, -3.2700e-03,  5.3303e-03,  9.8016e-03,\n                      -1.5302e-02, -1.4232e-03,  1.7573e-03, -3.1180e-03, -1.3027e-02,\n                      -1.4690e-03, -1.6963e-02, -1.8034e-02,  3.3808e-03,  1.7845e-03,\n                      -2.9947e-03,  2.1341e-02, -1.0926e-05,  3.9282e-03, -5.3511e-03,\n                       3.4978e-03,  8.0160e-04, -6.8116e-03, -2.7563e-03,  9.3009e-03,\n                      -1.6493e-02, -2.2528e-03,  7.3618e-03, -2.4809e-02,  1.4528e-03,\n                       6.0308e-03,  3.3868e-03,  4.8266e-03,  6.0688e-02, -4.7139e-03,\n                      -4.7713e-02,  9.9803e-03,  9.6626e-04,  1.1483e-03,  1.3953e-02,\n                      -1.2371e-03, -1.4411e-03, -2.2027e-02,  4.3834e-03, -5.6970e-03,\n                      -1.3405e-03, -7.0838e-04, -1.3252e-03,  7.6294e-03,  6.5147e-04,\n                      -6.6430e-04,  2.2848e-02,  8.3357e-03, -4.6371e-03, -2.6714e-03,\n                      -1.1935e-03, -2.3765e-02, -3.0510e-04, -3.6598e-03,  1.6635e-03,\n                       5.0536e-03,  7.2421e-03, -1.3437e-03, -2.1732e-03],\n                     requires_grad=True)),\n             ('features.5.2.block.0.scale', tensor(0.0041)),\n             ('features.5.2.block.0.zero_point', tensor(57)),\n             ('features.5.2.block.2.weight',\n              tensor([0.6177, 1.0539, 0.9457, 1.0971, 1.2336, 1.2016, 0.8136, 1.3991, 1.2978,\n                      1.2784, 0.7665, 0.9897, 1.4444, 0.9413, 0.6255, 0.8468, 0.9568, 0.8433,\n                      0.9188, 0.9638, 0.7471, 1.6488, 1.6122, 1.4607, 0.8871, 1.0903, 1.0046,\n                      1.2276, 1.0919, 0.7890, 1.0453, 1.1221, 0.7512, 0.5846, 0.8774, 1.3417,\n                      1.1440, 0.8943, 0.6668, 0.8442, 0.8855, 1.7158, 1.2208, 1.3050, 0.6668,\n                      0.8008, 1.0235, 1.1083, 0.9848, 1.1753, 0.7469, 0.9923, 0.7846, 0.8044,\n                      1.0113, 1.3848, 0.7828, 0.7833, 1.0980, 0.7150, 1.2138, 1.2499, 1.0845,\n                      0.9201, 0.7054, 1.2807, 1.0087, 1.4633, 0.7512, 0.5001, 0.9782, 0.9097,\n                      0.8393, 1.0721, 0.7466, 0.6818, 1.1539, 0.7571, 1.2652, 0.8625, 0.9678,\n                      1.1217, 1.1134, 0.9700, 0.6689, 0.7622, 0.9516, 0.6130, 0.9481, 1.1397,\n                      1.2238, 0.8770, 0.9882, 0.8534, 0.6094, 1.2519, 1.5378, 0.6922, 0.6984,\n                      1.2014, 0.8290, 1.2363, 0.9477, 1.2373, 0.9655, 0.9640, 0.8742, 1.3631,\n                      0.6912, 0.7760, 0.8889, 0.6255, 1.1123, 1.0521, 1.2603, 0.6732, 1.1688,\n                      1.0492, 0.7567, 0.6746, 0.7054, 0.8312, 0.9012, 0.8435, 2.4906, 1.2095,\n                      1.1249, 0.7292, 0.9970, 0.7948, 0.9591, 1.0382, 1.1592, 0.9162, 1.1439,\n                      1.4267, 0.9195, 1.0169, 0.8473, 1.1458, 1.4510, 0.5595, 1.3395, 0.9019,\n                      1.0022, 1.0605, 1.4976, 0.6795, 0.7067, 1.4763, 0.5830, 0.7309, 0.7551,\n                      0.7350, 0.7424, 1.1194, 0.5189, 0.5874, 1.5061, 1.0662, 1.0109, 0.8234,\n                      1.1227, 0.9583, 1.1965, 0.8179, 1.0578, 1.2399, 0.6924, 0.8315, 0.7775,\n                      0.7638, 0.8955, 0.8124, 1.3281, 1.1785, 0.6905, 1.0457, 0.7129, 0.9455,\n                      1.0462, 1.3805, 1.3989, 0.9620, 0.9091, 1.3230, 0.9850, 0.8374, 1.7360,\n                      1.0740, 0.8949, 0.7626, 1.0210, 1.1566, 1.1367, 1.1184, 0.8322, 0.9371,\n                      1.4952, 1.3710, 1.7958, 0.8992, 0.7944, 1.5022, 0.9902, 1.2894, 1.0054,\n                      0.9931, 1.1613, 1.0779, 0.6739, 1.3306, 1.2327, 1.0125, 0.8973, 1.1625,\n                      1.2462, 0.8285, 0.7858, 1.7234, 1.4225, 1.1923, 0.9070, 0.8900, 0.9128,\n                      1.1468, 1.1058, 1.1762, 0.7280, 1.3907, 0.9490, 1.0673, 0.9842, 1.0974,\n                      1.2937, 0.8964, 1.0748, 1.3408, 1.2182, 1.3726, 0.9286, 0.7792, 0.9247,\n                      1.4592, 0.8894, 0.7669, 1.1695, 1.6025, 1.1480, 0.8925, 0.9738, 0.6958,\n                      0.8924, 0.8625, 1.0593, 0.8116, 1.0035, 0.8162, 0.7303, 0.7851, 0.7629,\n                      0.6871, 0.7768, 1.1661, 0.7851, 1.0561, 1.0530, 0.7409, 1.5604, 1.0677,\n                      0.7800, 0.8975, 1.1619, 1.0645, 1.1724, 0.9748, 1.1756, 1.4150, 0.6864,\n                      0.7119, 0.9093, 0.8690, 0.8573, 0.6056, 0.7871, 0.9661, 1.0685, 0.7617,\n                      0.8302, 0.7983, 0.8966, 0.9547, 1.0210, 0.7952, 1.2421, 0.9950, 0.7334,\n                      1.2115, 0.7981, 0.9354, 0.7953, 0.6923, 0.8842, 1.1470, 1.0476, 0.9049,\n                      0.8439, 0.7840, 0.8613, 1.2644, 0.7621, 0.7659, 0.7386, 0.9988, 0.9244,\n                      0.7105, 0.9606, 0.6535, 0.8714, 1.2964, 1.0546, 0.9519, 1.4499, 1.0781,\n                      0.9939, 1.1428, 0.9897, 1.1348, 1.2687, 1.3955, 0.7433, 1.0445, 0.8626,\n                      0.7461, 0.8828, 1.2800, 1.3056, 0.9657, 1.2343, 0.7980, 0.9797, 1.2214,\n                      0.7892, 0.9620, 0.9082, 1.1409, 0.6995, 0.9005, 0.9330, 0.8671, 1.3080,\n                      0.6404, 0.8033, 0.9148, 0.7919, 1.3723, 1.0842, 0.7468, 0.7911, 1.0200,\n                      1.0986, 1.0249, 0.7863, 0.7347, 0.9109, 0.9699, 0.8388, 1.0136, 1.5167,\n                      0.8590, 0.8580, 0.8219, 0.7853, 1.4347, 0.6837, 1.3963, 0.6841, 1.2607,\n                      0.9514, 1.1374, 1.2357, 0.9520, 1.2012, 1.1595])),\n             ('features.5.2.block.2.bias',\n              tensor([ 4.1733e-02,  3.0997e-01, -1.7253e-01,  5.6638e-02,  2.6295e-01,\n                       2.3810e-02,  1.9035e-02,  1.0042e-01,  4.5816e-02,  1.1462e-01,\n                       3.9849e-02,  1.4247e-01,  1.3145e+00,  4.2758e-01,  2.2871e-01,\n                       2.7255e-02,  1.5484e-01,  2.6474e-01, -1.6674e-01,  3.5366e-01,\n                       2.8727e-01, -1.1885e-01, -1.2831e-01,  1.3346e-01,  1.0799e-01,\n                       1.0507e-01,  1.8064e-01,  9.5479e-02, -5.5026e-02, -6.0434e-01,\n                       3.5094e-01,  1.9474e-01,  3.9589e-01,  4.8809e-01,  8.7534e-01,\n                       6.0966e-02,  2.5048e-01,  3.1058e-01,  6.3830e-01,  3.4561e-01,\n                       2.9920e-01,  1.4338e-01, -5.1326e-02,  2.8152e-01,  6.7814e-02,\n                       1.9771e-01,  4.4479e-02,  2.0623e-01,  2.8908e-01,  2.5607e-01,\n                      -8.4976e-02,  1.7402e+00, -7.2587e-02,  3.7517e-01,  1.9452e-01,\n                       2.0455e-01,  1.4338e-01,  2.9495e-01,  2.1725e-01,  4.3278e-02,\n                       1.8560e-01, -4.8753e-01,  2.2861e-01,  4.8618e-01,  4.4811e-01,\n                       1.5167e-01,  2.6390e-01,  1.1212e-01, -1.7396e-01,  5.9563e-01,\n                       1.3898e-01,  3.8920e-01, -2.1069e-01,  5.1444e-02,  5.4588e-01,\n                       1.7430e-01,  3.1402e-01,  2.9734e-01, -3.4714e-02,  3.9265e-01,\n                       3.7949e-01,  2.3554e-01,  6.8014e-02,  3.0466e-02,  4.2881e-01,\n                       8.2723e-02,  1.2232e-01,  3.3705e-01,  4.4265e-01, -3.9748e-01,\n                       5.4561e-01,  1.8643e-01,  2.3335e-01,  4.4918e-01,  2.8357e-01,\n                      -1.2408e-01,  2.9822e-01,  5.3331e-01,  4.3080e-01,  1.3064e-01,\n                       1.3932e-01,  1.8572e-01,  2.4490e-01,  9.1949e-02,  5.3643e-01,\n                       2.5588e-01,  1.0059e-01,  7.2588e-02,  2.1390e-01,  3.0354e-01,\n                      -8.9840e-01,  6.1939e-01, -1.0701e-01,  3.4692e-01,  3.4744e-02,\n                       4.1275e-01,  2.5830e-02, -1.5381e-03,  8.1714e-02,  2.6292e-01,\n                       4.3766e-03,  3.5293e-02,  1.5257e-01,  3.0930e-01,  3.1904e-02,\n                       9.3673e-02,  1.8538e-01, -1.0761e-02,  4.4495e-01,  3.2460e-01,\n                       6.4120e-02, -1.0601e-01,  8.7199e-02,  1.0647e-01, -2.3896e-02,\n                      -9.0250e-01, -2.6970e-02,  4.1725e-01,  2.7809e-01,  8.0603e-02,\n                       1.0825e-01,  4.1035e-01,  5.3593e-03,  1.0981e-01,  8.8291e-02,\n                      -2.4247e-02, -2.5647e-01,  5.0434e-01,  2.4457e-01, -4.7373e-03,\n                       2.1822e-01,  4.3649e-01,  4.0805e-02,  5.6005e-01,  2.1799e-01,\n                       1.5337e-01,  2.4793e-02, -6.0853e-03,  1.0165e-01,  1.4473e-01,\n                       5.1851e-02,  1.3404e-01, -3.2907e-02,  4.8824e-02,  2.8383e-01,\n                       2.0301e-01,  6.2911e-01,  1.3809e-01,  8.5889e-02,  3.7414e-02,\n                       5.0029e-01,  2.6096e-01,  4.4902e-01,  1.5493e-01,  5.9982e-01,\n                       7.3695e-02,  1.9580e-01,  3.1857e-01,  2.5630e-01, -2.4518e-02,\n                       1.5268e-01,  3.6809e-01,  4.9997e-01,  4.3648e-01,  1.4893e-01,\n                       4.3572e-01,  1.8361e-01,  6.6125e-01,  1.1957e-02,  2.8552e-01,\n                       1.1706e-01,  5.6451e-01,  6.7358e-02,  1.9056e-01,  5.5351e-02,\n                       5.5096e-01,  1.2009e-01,  2.0939e-01, -6.3624e-01, -2.4275e-01,\n                      -3.4656e-02,  4.5441e-02,  4.1001e-02,  2.3395e-01,  2.8669e-01,\n                       9.2596e-02,  3.2922e-01,  2.5719e-01, -3.4490e-03,  6.4183e-02,\n                       3.8829e-01,  4.1107e-01,  1.0475e-01,  2.2231e-01,  7.2953e-02,\n                       2.0314e-01,  1.5133e-01,  1.5249e-01,  2.3612e-03, -1.5111e-01,\n                       3.2051e-01,  1.0328e-01,  1.7505e-01,  6.3208e-01, -1.1978e-02,\n                       4.2141e-01, -8.2524e-02,  1.7990e-01,  1.7821e-01,  3.6707e-01,\n                       9.6542e-02,  8.2541e-02, -6.1514e-02, -7.8318e-03,  2.0415e-01,\n                       1.3355e-01,  1.0942e-01, -1.2529e-01,  7.8120e-02, -3.8726e-02,\n                       9.6603e-02,  1.6338e-01,  2.0456e-01, -2.3286e-01, -6.5058e-02,\n                       4.9262e-01,  5.2020e-02,  2.3772e-01,  1.1233e-02,  8.1763e-02,\n                       2.5711e-01,  4.5418e-02,  2.7677e-01,  2.1288e-01,  3.0571e-01,\n                       1.6199e-01,  3.5486e-01,  8.4814e-01,  9.6053e-01,  2.1639e-01,\n                       4.2433e-01,  5.4906e-01,  1.7003e-01,  4.2192e-01, -2.7236e-02,\n                       2.4289e-01,  5.9376e-01,  5.5426e-01,  1.7922e-02,  1.4620e-01,\n                       1.8469e-01,  3.5883e-01,  2.6544e-01,  1.5878e-01,  1.4839e+00,\n                       2.0696e-01,  9.8134e-02,  7.0525e-01, -2.1426e-01,  2.6806e-01,\n                       2.6800e-01, -1.3615e-01, -2.3661e-01,  4.0908e-01,  8.5687e-02,\n                      -8.0868e-03,  1.4014e-01,  2.7598e-01, -5.3508e-02,  4.0589e-01,\n                      -3.2755e-03,  4.7959e-01, -1.0121e-01,  4.6937e-01, -8.0970e-02,\n                       3.5918e-01,  1.7730e-01,  2.8689e-01,  2.9940e-01,  3.6175e-01,\n                       7.5523e-02,  3.2949e-01,  2.5791e-01,  1.8361e-01,  1.2907e-01,\n                       4.8113e-01,  3.3794e-01,  2.2327e-01, -1.0581e-01,  1.5522e-02,\n                       2.5243e-01,  1.7860e-01,  5.1626e-03,  3.9246e-01,  4.4248e-01,\n                       2.6911e-01,  2.1735e-01,  2.5741e-01,  2.7534e-01,  3.7992e-01,\n                       1.9412e-01,  3.0543e-01,  1.5260e-01, -2.4882e-02,  6.1525e-03,\n                       4.8950e-01,  2.2327e-01,  1.4873e-01,  1.5620e-01,  3.8204e-01,\n                       2.4847e-01,  4.7539e-01,  5.3352e-01,  4.0825e-01,  7.7644e-02,\n                       1.7715e-01, -4.8141e-01,  1.7624e-01,  9.3443e-02,  1.9987e-01,\n                       2.3977e-01,  1.2441e-01,  4.0955e-01,  1.0505e-01, -2.0001e-01,\n                       5.0093e-01,  2.0058e-01,  2.9209e-01, -6.9874e-01,  2.3345e-01,\n                       6.9813e-02, -3.3806e-02,  7.4875e-02,  2.2042e+00,  4.4117e-01,\n                      -4.1326e-02, -1.4907e-01,  1.1057e-01,  2.2982e-01,  9.4246e-02,\n                       2.0859e-01,  1.6805e-01,  4.4792e-01,  1.5021e-01,  2.1259e-01,\n                       3.9302e-01,  5.2642e-01,  1.3029e-01, -1.1284e-01,  7.0919e-02,\n                       1.2806e-01, -4.1011e-01, -5.1024e-01,  2.2333e-01,  2.9792e-01,\n                       4.4121e-01,  8.1339e-01,  5.0982e-02,  1.7152e-01,  8.5913e-02,\n                      -4.0334e-02,  6.9745e-02,  9.3941e-02,  2.2300e-01])),\n             ('features.5.2.block.2.scale', tensor(0.1263)),\n             ('features.5.2.block.2.zero_point', tensor(53)),\n             ('features.5.2.block.3.scale', tensor(0.0931)),\n             ('features.5.2.block.3.zero_point', tensor(93)),\n             ('features.5.2.block.3._packed_params.dtype', torch.qint8),\n             ('features.5.2.block.3._packed_params._packed_params',\n              (tensor([[ 0.0291,  0.0000,  0.0276,  ..., -0.1120,  0.0175,  0.0116],\n                       [ 0.0677, -0.0329,  0.0522,  ..., -0.0039, -0.0097,  0.0232],\n                       [-0.1302,  0.0013,  0.0215,  ...,  0.1624,  0.0161, -0.0242],\n                       ...,\n                       [ 0.0088, -0.1313,  0.0204,  ...,  0.0350, -0.0204,  0.0146],\n                       [-0.0377,  0.0058, -0.0014,  ...,  0.0985, -0.0333, -0.0551],\n                       [ 0.0493, -0.0185,  0.0086,  ..., -0.0875,  0.0049, -0.0283]],\n                      size=(1536, 384), dtype=torch.qint8,\n                      quantization_scheme=torch.per_channel_affine,\n                      scale=tensor([0.0015, 0.0019, 0.0013,  ..., 0.0029, 0.0014, 0.0012],\n                      dtype=torch.float64),\n                      zero_point=tensor([0, 0, 0,  ..., 0, 0, 0]), axis=0),\n               Parameter containing:\n               tensor([-0.0418, -0.0774, -0.0476,  ..., -0.0331, -0.0409, -0.0380],\n                      requires_grad=True))),\n             ('features.5.2.block.5.scale', tensor(0.0475)),\n             ('features.5.2.block.5.zero_point', tensor(48)),\n             ('features.5.2.block.5._packed_params.dtype', torch.qint8),\n             ('features.5.2.block.5._packed_params._packed_params',\n              (tensor([[-0.0136, -0.0879, -0.0227,  ..., -0.0863,  0.0985, -0.0454],\n                       [ 0.0000, -0.0889, -0.0420,  ..., -0.0049,  0.0617,  0.0173],\n                       [-0.0183,  0.0667,  0.0267,  ..., -0.0083, -0.0017, -0.0100],\n                       ...,\n                       [-0.0412, -0.0059, -0.0255,  ..., -0.0059, -0.0137,  0.0726],\n                       [ 0.0661, -0.0543,  0.0000,  ...,  0.0085, -0.0678, -0.0560],\n                       [-0.0466, -0.0233,  0.0766,  ..., -0.0067, -0.0316, -0.0899]],\n                      size=(384, 1536), dtype=torch.qint8,\n                      quantization_scheme=torch.per_channel_affine,\n                      scale=tensor([0.0015, 0.0025, 0.0017, 0.0018, 0.0014, 0.0016, 0.0018, 0.0020, 0.0018,\n                       0.0029, 0.0017, 0.0017, 0.0018, 0.0027, 0.0015, 0.0017, 0.0017, 0.0017,\n                       0.0016, 0.0015, 0.0024, 0.0019, 0.0020, 0.0021, 0.0020, 0.0024, 0.0023,\n                       0.0032, 0.0021, 0.0026, 0.0023, 0.0028, 0.0023, 0.0017, 0.0025, 0.0016,\n                       0.0017, 0.0021, 0.0020, 0.0021, 0.0025, 0.0030, 0.0018, 0.0023, 0.0017,\n                       0.0021, 0.0041, 0.0022, 0.0021, 0.0019, 0.0018, 0.0034, 0.0015, 0.0025,\n                       0.0015, 0.0027, 0.0032, 0.0014, 0.0023, 0.0022, 0.0019, 0.0018, 0.0018,\n                       0.0014, 0.0015, 0.0023, 0.0020, 0.0022, 0.0014, 0.0019, 0.0016, 0.0016,\n                       0.0023, 0.0025, 0.0019, 0.0016, 0.0020, 0.0034, 0.0015, 0.0018, 0.0018,\n                       0.0018, 0.0023, 0.0037, 0.0020, 0.0021, 0.0022, 0.0017, 0.0021, 0.0014,\n                       0.0019, 0.0019, 0.0022, 0.0024, 0.0017, 0.0019, 0.0033, 0.0018, 0.0014,\n                       0.0032, 0.0015, 0.0016, 0.0026, 0.0016, 0.0027, 0.0016, 0.0019, 0.0028,\n                       0.0016, 0.0046, 0.0016, 0.0017, 0.0039, 0.0028, 0.0020, 0.0019, 0.0016,\n                       0.0015, 0.0020, 0.0024, 0.0015, 0.0021, 0.0017, 0.0041, 0.0024, 0.0025,\n                       0.0026, 0.0018, 0.0015, 0.0041, 0.0035, 0.0015, 0.0034, 0.0022, 0.0020,\n                       0.0023, 0.0018, 0.0017, 0.0038, 0.0016, 0.0018, 0.0018, 0.0024, 0.0017,\n                       0.0023, 0.0018, 0.0018, 0.0015, 0.0016, 0.0024, 0.0019, 0.0022, 0.0014,\n                       0.0020, 0.0018, 0.0017, 0.0038, 0.0022, 0.0019, 0.0015, 0.0018, 0.0017,\n                       0.0015, 0.0045, 0.0025, 0.0018, 0.0015, 0.0015, 0.0015, 0.0015, 0.0020,\n                       0.0016, 0.0019, 0.0015, 0.0016, 0.0036, 0.0015, 0.0028, 0.0017, 0.0024,\n                       0.0026, 0.0022, 0.0059, 0.0026, 0.0016, 0.0028, 0.0025, 0.0018, 0.0026,\n                       0.0027, 0.0031, 0.0036, 0.0016, 0.0025, 0.0016, 0.0045, 0.0018, 0.0029,\n                       0.0015, 0.0020, 0.0021, 0.0019, 0.0020, 0.0021, 0.0017, 0.0018, 0.0015,\n                       0.0016, 0.0018, 0.0035, 0.0019, 0.0022, 0.0023, 0.0017, 0.0023, 0.0019,\n                       0.0033, 0.0041, 0.0016, 0.0023, 0.0024, 0.0030, 0.0018, 0.0018, 0.0018,\n                       0.0018, 0.0022, 0.0020, 0.0015, 0.0021, 0.0019, 0.0021, 0.0025, 0.0015,\n                       0.0016, 0.0016, 0.0022, 0.0024, 0.0023, 0.0025, 0.0027, 0.0018, 0.0017,\n                       0.0018, 0.0019, 0.0030, 0.0020, 0.0032, 0.0024, 0.0015, 0.0023, 0.0025,\n                       0.0027, 0.0027, 0.0034, 0.0021, 0.0015, 0.0020, 0.0016, 0.0022, 0.0023,\n                       0.0021, 0.0016, 0.0017, 0.0041, 0.0018, 0.0030, 0.0017, 0.0028, 0.0019,\n                       0.0016, 0.0018, 0.0022, 0.0031, 0.0021, 0.0020, 0.0017, 0.0015, 0.0023,\n                       0.0013, 0.0018, 0.0029, 0.0014, 0.0041, 0.0020, 0.0022, 0.0021, 0.0015,\n                       0.0017, 0.0016, 0.0022, 0.0027, 0.0018, 0.0034, 0.0017, 0.0030, 0.0013,\n                       0.0030, 0.0019, 0.0022, 0.0021, 0.0023, 0.0018, 0.0018, 0.0018, 0.0021,\n                       0.0017, 0.0022, 0.0019, 0.0028, 0.0020, 0.0021, 0.0017, 0.0023, 0.0033,\n                       0.0023, 0.0016, 0.0016, 0.0040, 0.0021, 0.0029, 0.0015, 0.0019, 0.0024,\n                       0.0014, 0.0021, 0.0029, 0.0016, 0.0014, 0.0017, 0.0018, 0.0017, 0.0022,\n                       0.0024, 0.0021, 0.0020, 0.0023, 0.0025, 0.0030, 0.0024, 0.0020, 0.0017,\n                       0.0020, 0.0023, 0.0025, 0.0016, 0.0018, 0.0021, 0.0023, 0.0018, 0.0019,\n                       0.0022, 0.0019, 0.0025, 0.0015, 0.0024, 0.0027, 0.0014, 0.0022, 0.0024,\n                       0.0024, 0.0021, 0.0034, 0.0014, 0.0027, 0.0016, 0.0027, 0.0016, 0.0021,\n                       0.0017, 0.0018, 0.0018, 0.0020, 0.0017, 0.0017, 0.0019, 0.0017, 0.0030,\n                       0.0016, 0.0019, 0.0021, 0.0020, 0.0017, 0.0017], dtype=torch.float64),\n                      zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n                      axis=0),\n               Parameter containing:\n               tensor([-2.8552e-02, -5.4120e-03,  1.9179e-02,  6.1182e-02, -6.5619e-02,\n                        1.6197e-02,  1.6912e-02,  1.8499e-02,  1.6266e-02,  1.0645e-02,\n                        4.2285e-02, -8.4179e-03, -4.1298e-03,  1.5992e-02, -1.4713e-03,\n                       -7.4623e-02, -1.1381e-01, -1.6306e-03,  2.0154e-02,  4.0153e-03,\n                       -6.2978e-02,  7.1325e-03, -4.9258e-02,  2.3888e-02, -1.4458e-02,\n                       -6.1435e-02, -2.3162e-02, -3.1389e-02, -9.7439e-03,  1.2263e-01,\n                        5.3453e-02,  1.6776e-02,  1.8864e-02, -1.1390e-02,  3.8740e-02,\n                       -1.5458e-02, -3.2972e-02, -5.2351e-03,  3.7158e-02, -1.1852e-02,\n                        1.7780e-02, -3.7749e-02, -5.4643e-02,  4.8692e-03,  8.1713e-03,\n                        7.9519e-02,  9.7137e-04, -1.8626e-02,  1.8301e-02, -1.9638e-02,\n                       -1.0377e-02,  1.0829e-01, -1.4834e-02, -1.2560e-02,  5.4795e-03,\n                       -5.6988e-02,  4.9658e-02,  1.4253e-03, -7.2738e-02,  7.4404e-03,\n                       -5.3408e-02, -3.5066e-02,  2.9978e-02, -2.0077e-02, -1.4995e-02,\n                       -5.0187e-02,  2.9627e-02,  8.4638e-02, -2.2130e-03,  7.2661e-02,\n                       -1.8520e-04,  2.7197e-03, -1.6799e-02, -4.3901e-03,  2.6647e-02,\n                        1.0185e-02,  2.0836e-02,  1.2662e-02,  5.5393e-03,  1.1354e-02,\n                        8.6835e-03,  5.8598e-03,  2.6255e-02, -1.2179e-04, -2.4100e-02,\n                       -1.7127e-03,  1.1349e-02, -3.1014e-02,  2.2491e-02, -1.0448e-02,\n                       -2.9169e-02, -2.9910e-02, -1.0728e-02,  2.0531e-02, -3.2619e-02,\n                        4.9896e-03, -8.9456e-03, -5.6457e-02, -5.2164e-03,  1.2877e-02,\n                        3.1356e-02, -3.3062e-03, -4.1037e-03, -3.3615e-02, -7.3786e-03,\n                        1.0717e-02,  2.9458e-03, -1.5725e-02, -1.4619e-02,  4.4159e-03,\n                        1.4827e-01,  4.1695e-03, -4.9482e-03, -9.0429e-03, -5.5148e-02,\n                        2.5253e-02,  9.9103e-03, -3.5300e-02, -2.2951e-02, -1.0939e-03,\n                        7.8003e-04,  6.2893e-03, -2.9209e-02,  1.5095e-03,  3.3565e-02,\n                       -5.5471e-02,  2.1504e-02, -2.1118e-02,  2.1506e-02,  9.9623e-03,\n                       -6.5215e-03,  1.5150e-02,  1.1705e-02, -1.3304e-02, -1.1615e-02,\n                        1.7324e-01,  1.8965e-02,  1.0052e-01,  4.7386e-03, -1.2722e-02,\n                        4.5769e-03,  7.9029e-03,  2.2879e-02, -1.7340e-02,  1.4880e-03,\n                        1.2301e-02,  1.3862e-02,  3.5477e-02, -7.6121e-03,  1.7223e-02,\n                        4.7430e-02, -1.0962e-02,  1.6641e-02, -4.0847e-02,  1.2878e-02,\n                       -2.4744e-02,  8.4973e-01, -8.0013e-02, -5.6717e-02, -3.4769e-03,\n                        1.2382e-01, -3.5884e-02,  9.9083e-03, -1.8189e-03, -6.5587e-03,\n                        1.4844e-02, -1.2403e-02,  7.0795e-03, -1.5408e-02, -4.0252e-02,\n                       -1.8195e-02, -3.4861e-02, -5.9878e-02,  2.3461e-02, -4.7366e-02,\n                        3.4152e-02, -3.7337e-03,  1.2443e-03, -3.5658e-02, -6.6502e-03,\n                       -3.8457e-02,  3.8660e-02,  4.3746e-02,  1.7609e-02, -1.1639e-02,\n                        1.0606e-02, -7.3733e-03,  1.6741e-02, -3.3319e-02, -2.7332e-02,\n                        1.3340e-02,  2.0497e-03,  1.5395e-02,  6.2322e-03,  2.6463e-02,\n                       -2.9023e-03,  1.2138e-03, -6.8091e-02, -4.3838e-02, -5.3853e-03,\n                       -1.3290e-02,  1.0869e-02,  1.3455e-02, -7.1389e-03,  2.0802e-02,\n                       -1.6447e-02,  1.0597e-02, -2.2356e-02, -4.5847e-02, -1.3596e-02,\n                       -4.9673e-03, -2.5881e-02,  3.3113e-03,  1.6232e-02,  2.8009e-02,\n                        1.6770e-02, -4.0005e-02, -2.3418e-03,  4.4175e-02, -2.0755e-02,\n                       -3.5154e-02,  1.6712e-02, -2.0320e-02,  6.8228e-04,  6.3548e-03,\n                       -9.6352e-03, -3.5292e-02,  1.8604e-02, -2.2396e-02, -4.1987e-03,\n                       -7.0576e-03, -8.9252e-03, -1.6499e-02, -4.0046e-02, -9.9266e-04,\n                        1.0662e-02, -3.2367e-02,  2.4992e-02,  1.8785e-02, -3.0355e-02,\n                        4.5825e-03,  2.2982e-02, -1.9000e-02,  2.1561e-02,  1.9445e-02,\n                       -6.3876e-02, -2.8111e-02,  4.2899e-02,  7.8891e-03,  4.2467e-02,\n                       -2.0551e-02, -3.5110e-02, -2.7017e-03,  1.9486e-03,  9.0750e-02,\n                       -7.1416e-06, -6.2987e-03, -1.1629e-02,  2.6059e-03,  1.0874e-02,\n                        3.5437e-02, -2.5079e-02,  1.6470e-02, -1.3002e-02,  8.6723e-03,\n                       -3.5473e-02, -6.2040e-03, -4.1249e-02, -3.4425e-03, -3.1160e-02,\n                       -2.9975e-02,  1.2958e-03,  2.5435e-02,  7.7259e-02,  7.1393e-02,\n                        1.8033e-03, -1.3605e-02, -4.5180e-02,  4.1731e-02, -1.8682e-02,\n                       -5.2329e-02,  3.5617e-02, -1.4676e-03, -1.9493e-02, -1.0349e-02,\n                        2.1942e-02, -2.3830e-03,  5.5618e-03, -1.5509e-02,  2.0879e-02,\n                        1.2366e-01, -1.2388e-02,  1.3950e-02,  4.6703e-04,  3.0884e-02,\n                        3.1222e-02, -6.4900e-02, -1.9762e-03,  9.7742e-03,  6.7400e-02,\n                        2.1634e-02,  1.3475e-03, -4.5666e-03,  4.4896e-02, -8.4292e-03,\n                       -2.3289e-02,  3.4959e-03, -1.5490e-02,  1.5147e-02,  2.0967e-02,\n                       -3.5275e-02, -9.2003e-02,  2.4160e-03,  2.2704e-02, -1.4986e-02,\n                       -6.4621e-03,  1.5280e-02,  4.3852e-02, -4.4294e-03,  6.2514e-02,\n                        7.2598e-03, -2.8292e-02,  1.2125e-03, -2.4805e-02, -1.3600e-01,\n                       -6.8541e-03, -3.1301e-03,  1.3773e-02, -2.1773e-02, -6.3678e-02,\n                       -6.5915e-05,  8.0176e-03, -2.0753e-02, -5.7694e-03,  2.6725e-02,\n                       -1.4983e-02, -9.3019e-03, -1.4550e-02,  3.0142e-02,  1.2742e-02,\n                        3.7531e-03,  6.3798e-02, -6.5830e-04, -1.2017e-02,  6.7620e-02,\n                       -2.0136e-02, -2.5970e-03, -1.2734e-02, -1.4674e-01, -9.6502e-03,\n                       -2.6070e-02,  3.8439e-02, -2.6725e-02,  3.2333e-02,  1.2259e-02,\n                       -2.7404e-02, -1.8316e-02,  4.9702e-03,  1.9808e-02, -2.3987e-02,\n                        2.2417e-02, -1.1028e-02, -1.2815e-02,  1.3835e-02, -2.6576e-02,\n                        6.7716e-02, -6.3280e-02, -1.6877e-02,  4.2077e-02,  1.9887e-02,\n                        3.0907e-02,  4.4350e-02, -4.9946e-02,  1.0724e-03, -5.6836e-03,\n                       -6.1293e-02, -3.3347e-02, -3.6616e-02, -6.9467e-03,  7.2061e-02,\n                        2.1657e-02,  8.3941e-03,  2.9066e-02,  1.2849e-02],\n                      requires_grad=True))),\n             ('features.5.3.layer_scale',\n              tensor([[[ 0.0847]],\n              \n                      [[-0.0374]],\n              \n                      [[-0.0710]],\n              \n                      [[-0.0709]],\n              \n                      [[ 0.1024]],\n              \n                      [[-0.0987]],\n              \n                      [[-0.0461]],\n              \n                      [[-0.0948]],\n              \n                      [[-0.1252]],\n              \n                      [[ 0.0391]],\n              \n                      [[ 0.0598]],\n              \n                      [[ 0.1006]],\n              \n                      [[-0.0943]],\n              \n                      [[ 0.0987]],\n              \n                      [[ 0.1142]],\n              \n                      [[ 0.0810]],\n              \n                      [[ 0.0996]],\n              \n                      [[ 0.0963]],\n              \n                      [[ 0.1254]],\n              \n                      [[-0.0894]],\n              \n                      [[ 0.1737]],\n              \n                      [[ 0.0705]],\n              \n                      [[ 0.1223]],\n              \n                      [[-0.1151]],\n              \n                      [[-0.0603]],\n              \n                      [[-0.0309]],\n              \n                      [[-0.0504]],\n              \n                      [[-0.0430]],\n              \n                      [[-0.1387]],\n              \n                      [[ 0.1889]],\n              \n                      [[-0.0689]],\n              \n                      [[ 0.0579]],\n              \n                      [[-0.0888]],\n              \n                      [[ 0.1085]],\n              \n                      [[ 0.1104]],\n              \n                      [[ 0.0881]],\n              \n                      [[-0.0657]],\n              \n                      [[-0.1087]],\n              \n                      [[ 0.1307]],\n              \n                      [[ 0.1092]],\n              \n                      [[ 0.1053]],\n              \n                      [[ 0.0612]],\n              \n                      [[ 0.1009]],\n              \n                      [[-0.0966]],\n              \n                      [[ 0.1039]],\n              \n                      [[-0.1215]],\n              \n                      [[-0.0657]],\n              \n                      [[ 0.1197]],\n              \n                      [[-0.0636]],\n              \n                      [[ 0.0859]],\n              \n                      [[-0.1095]],\n              \n                      [[-0.1248]],\n              \n                      [[-0.0714]],\n              \n                      [[-0.0995]],\n              \n                      [[ 0.0775]],\n              \n                      [[ 0.0370]],\n              \n                      [[-0.1374]],\n              \n                      [[-0.0859]],\n              \n                      [[-0.0509]],\n              \n                      [[-0.1075]],\n              \n                      [[ 0.0495]],\n              \n                      [[-0.0745]],\n              \n                      [[ 0.0439]],\n              \n                      [[ 0.0931]],\n              \n                      [[-0.1177]],\n              \n                      [[-0.0650]],\n              \n                      [[ 0.0386]],\n              \n                      [[-0.1265]],\n              \n                      [[-0.1123]],\n              \n                      [[ 0.0378]],\n              \n                      [[-0.0743]],\n              \n                      [[ 0.0684]],\n              \n                      [[ 0.1226]],\n              \n                      [[-0.0330]],\n              \n                      [[-0.0947]],\n              \n                      [[-0.1247]],\n              \n                      [[-0.0847]],\n              \n                      [[ 0.1725]],\n              \n                      [[-0.0804]],\n              \n                      [[ 0.1086]],\n              \n                      [[-0.0970]],\n              \n                      [[-0.1044]],\n              \n                      [[-0.0773]],\n              \n                      [[ 0.0516]],\n              \n                      [[-0.1059]],\n              \n                      [[ 0.0887]],\n              \n                      [[-0.0947]],\n              \n                      [[-0.1105]],\n              \n                      [[-0.0492]],\n              \n                      [[-0.1225]],\n              \n                      [[ 0.0632]],\n              \n                      [[-0.1219]],\n              \n                      [[-0.0547]],\n              \n                      [[ 0.0793]],\n              \n                      [[-0.1127]],\n              \n                      [[-0.0995]],\n              \n                      [[ 0.0888]],\n              \n                      [[ 0.0992]],\n              \n                      [[ 0.1113]],\n              \n                      [[ 0.1085]],\n              \n                      [[-0.0869]],\n              \n                      [[ 0.0997]],\n              \n                      [[-0.0989]],\n              \n                      [[-0.0700]],\n              \n                      [[ 0.0529]],\n              \n                      [[ 0.0681]],\n              \n                      [[-0.0667]],\n              \n                      [[ 0.1127]],\n              \n                      [[-0.1040]],\n              \n                      [[ 0.1289]],\n              \n                      [[-0.1052]],\n              \n                      [[ 0.0778]],\n              \n                      [[-0.0884]],\n              \n                      [[ 0.0562]],\n              \n                      [[ 0.0562]],\n              \n                      [[ 0.1377]],\n              \n                      [[ 0.1249]],\n              \n                      [[ 0.0685]],\n              \n                      [[ 0.0579]],\n              \n                      [[-0.1090]],\n              \n                      [[ 0.0948]],\n              \n                      [[-0.0712]],\n              \n                      [[ 0.0833]],\n              \n                      [[ 0.1043]],\n              \n                      [[-0.0884]],\n              \n                      [[ 0.1211]],\n              \n                      [[-0.0353]],\n              \n                      [[ 0.1012]],\n              \n                      [[ 0.0834]],\n              \n                      [[-0.1183]],\n              \n                      [[-0.1366]],\n              \n                      [[-0.1139]],\n              \n                      [[ 0.1340]],\n              \n                      [[ 0.1046]],\n              \n                      [[ 0.0671]],\n              \n                      [[-0.1033]],\n              \n                      [[ 0.0723]],\n              \n                      [[ 0.0532]],\n              \n                      [[-0.1512]],\n              \n                      [[ 0.0650]],\n              \n                      [[ 0.1074]],\n              \n                      [[ 0.1340]],\n              \n                      [[-0.0867]],\n              \n                      [[-0.0563]],\n              \n                      [[ 0.0434]],\n              \n                      [[ 0.0645]],\n              \n                      [[-0.0730]],\n              \n                      [[ 0.0891]],\n              \n                      [[-0.1120]],\n              \n                      [[ 0.0948]],\n              \n                      [[ 0.0642]],\n              \n                      [[ 0.0936]],\n              \n                      [[-0.1260]],\n              \n                      [[ 0.0901]],\n              \n                      [[-0.1197]],\n              \n                      [[-0.0834]],\n              \n                      [[ 0.8361]],\n              \n                      [[ 0.0793]],\n              \n                      [[-0.0658]],\n              \n                      [[ 0.0707]],\n              \n                      [[ 0.1109]],\n              \n                      [[ 0.0821]],\n              \n                      [[ 0.0786]],\n              \n                      [[ 0.0913]],\n              \n                      [[-0.1532]],\n              \n                      [[-0.0839]],\n              \n                      [[-0.1216]],\n              \n                      [[-0.1077]],\n              \n                      [[ 0.1078]],\n              \n                      [[ 0.1088]],\n              \n                      [[ 0.1042]],\n              \n                      [[ 0.0760]],\n              \n                      [[-0.0989]],\n              \n                      [[-0.0825]],\n              \n                      [[ 0.0916]],\n              \n                      [[-0.0281]],\n              \n                      [[ 0.1149]],\n              \n                      [[ 0.0416]],\n              \n                      [[-0.1145]],\n              \n                      [[ 0.1184]],\n              \n                      [[-0.0359]],\n              \n                      [[-0.0694]],\n              \n                      [[-0.1366]],\n              \n                      [[ 0.1081]],\n              \n                      [[-0.0953]],\n              \n                      [[ 0.1056]],\n              \n                      [[-0.0339]],\n              \n                      [[-0.1260]],\n              \n                      [[ 0.0691]],\n              \n                      [[-0.0409]],\n              \n                      [[ 0.0826]],\n              \n                      [[-0.2143]],\n              \n                      [[ 0.0752]],\n              \n                      [[ 0.0387]],\n              \n                      [[-0.1082]],\n              \n                      [[ 0.1710]],\n              \n                      [[ 0.0696]],\n              \n                      [[ 0.0254]],\n              \n                      [[ 0.1060]],\n              \n                      [[-0.0986]],\n              \n                      [[ 0.0844]],\n              \n                      [[-0.1055]],\n              \n                      [[-0.0777]],\n              \n                      [[ 0.0890]],\n              \n                      [[-0.0809]],\n              \n                      [[-0.1163]],\n              \n                      [[-0.0773]],\n              \n                      [[ 0.0731]],\n              \n                      [[ 0.0814]],\n              \n                      [[-0.1131]],\n              \n                      [[-0.0543]],\n              \n                      [[-0.0762]],\n              \n                      [[ 0.0887]],\n              \n                      [[-0.0683]],\n              \n                      [[ 0.0690]],\n              \n                      [[-0.0985]],\n              \n                      [[-0.0700]],\n              \n                      [[-0.0468]],\n              \n                      [[-0.1129]],\n              \n                      [[-0.0971]],\n              \n                      [[ 0.0586]],\n              \n                      [[ 0.0378]],\n              \n                      [[-0.1083]],\n              \n                      [[ 0.0500]],\n              \n                      [[ 0.1113]],\n              \n                      [[-0.1054]],\n              \n                      [[ 0.0762]],\n              \n                      [[-0.0554]],\n              \n                      [[ 0.1121]],\n              \n                      [[ 0.0834]],\n              \n                      [[ 0.0777]],\n              \n                      [[ 0.0713]],\n              \n                      [[ 0.1299]],\n              \n                      [[-0.0896]],\n              \n                      [[ 0.0911]],\n              \n                      [[-0.0474]],\n              \n                      [[ 0.0577]],\n              \n                      [[ 0.0950]],\n              \n                      [[-0.0859]],\n              \n                      [[ 0.0775]],\n              \n                      [[ 0.0680]],\n              \n                      [[-0.1366]],\n              \n                      [[ 0.0791]],\n              \n                      [[-0.1038]],\n              \n                      [[-0.1035]],\n              \n                      [[ 0.0982]],\n              \n                      [[-0.1025]],\n              \n                      [[ 0.0648]],\n              \n                      [[ 0.0827]],\n              \n                      [[-0.0794]],\n              \n                      [[ 0.0440]],\n              \n                      [[ 0.0889]],\n              \n                      [[ 0.0479]],\n              \n                      [[-0.0884]],\n              \n                      [[ 0.1098]],\n              \n                      [[-0.0883]],\n              \n                      [[ 0.0746]],\n              \n                      [[-0.0966]],\n              \n                      [[ 0.1246]],\n              \n                      [[ 0.1121]],\n              \n                      [[-0.1131]],\n              \n                      [[-0.1093]],\n              \n                      [[-0.1062]],\n              \n                      [[ 0.0610]],\n              \n                      [[-0.0488]],\n              \n                      [[-0.1169]],\n              \n                      [[ 0.1051]],\n              \n                      [[ 0.0857]],\n              \n                      [[-0.1257]],\n              \n                      [[ 0.1046]],\n              \n                      [[-0.0850]],\n              \n                      [[-0.0521]],\n              \n                      [[-0.0834]],\n              \n                      [[-0.1051]],\n              \n                      [[ 0.0523]],\n              \n                      [[ 0.0983]],\n              \n                      [[ 0.0694]],\n              \n                      [[-0.0933]],\n              \n                      [[ 0.0929]],\n              \n                      [[ 0.1017]],\n              \n                      [[ 0.0429]],\n              \n                      [[-0.0988]],\n              \n                      [[-0.0997]],\n              \n                      [[ 0.0734]],\n              \n                      [[ 0.0619]],\n              \n                      [[-0.0952]],\n              \n                      [[ 0.0410]],\n              \n                      [[-0.1088]],\n              \n                      [[ 0.0928]],\n              \n                      [[-0.0855]],\n              \n                      [[-0.0885]],\n              \n                      [[ 0.1310]],\n              \n                      [[-0.1006]],\n              \n                      [[-0.1050]],\n              \n                      [[ 0.1104]],\n              \n                      [[-0.0776]],\n              \n                      [[-0.1127]],\n              \n                      [[ 0.0349]],\n              \n                      [[ 0.0534]],\n              \n                      [[-0.0604]],\n              \n                      [[-0.0941]],\n              \n                      [[-0.1081]],\n              \n                      [[ 0.1194]],\n              \n                      [[-0.0838]],\n              \n                      [[-0.0370]],\n              \n                      [[ 0.0617]],\n              \n                      [[-0.0926]],\n              \n                      [[ 0.0798]],\n              \n                      [[ 0.0621]],\n              \n                      [[-0.0736]],\n              \n                      [[-0.1200]],\n              \n                      [[ 0.0701]],\n              \n                      [[-0.1198]],\n              \n                      [[ 0.1098]],\n              \n                      [[ 0.0301]],\n              \n                      [[ 0.0797]],\n              \n                      [[ 0.1175]],\n              \n                      [[-0.0709]],\n              \n                      [[ 0.0539]],\n              \n                      [[-0.0456]],\n              \n                      [[ 0.0381]],\n              \n                      [[-0.0852]],\n              \n                      [[-0.0922]],\n              \n                      [[-0.1201]],\n              \n                      [[-0.1017]],\n              \n                      [[-0.0794]],\n              \n                      [[-0.0706]],\n              \n                      [[ 0.0918]],\n              \n                      [[-0.1150]],\n              \n                      [[-0.0738]],\n              \n                      [[ 0.0842]],\n              \n                      [[-0.1006]],\n              \n                      [[ 0.1049]],\n              \n                      [[-0.1203]],\n              \n                      [[ 0.1116]],\n              \n                      [[ 0.0548]],\n              \n                      [[-0.0895]],\n              \n                      [[-0.0355]],\n              \n                      [[ 0.0269]],\n              \n                      [[ 0.0987]],\n              \n                      [[ 0.0379]],\n              \n                      [[ 0.1026]],\n              \n                      [[-0.0511]],\n              \n                      [[ 0.1555]],\n              \n                      [[ 0.0664]],\n              \n                      [[ 0.1002]],\n              \n                      [[ 0.1115]],\n              \n                      [[ 0.0347]],\n              \n                      [[ 0.0761]],\n              \n                      [[ 0.0871]],\n              \n                      [[-0.0531]],\n              \n                      [[ 0.0780]],\n              \n                      [[-0.1204]],\n              \n                      [[-0.0318]],\n              \n                      [[ 0.0873]],\n              \n                      [[ 0.0244]],\n              \n                      [[-0.1325]],\n              \n                      [[ 0.0988]],\n              \n                      [[-0.1163]],\n              \n                      [[ 0.1230]],\n              \n                      [[-0.0233]],\n              \n                      [[-0.0700]],\n              \n                      [[ 0.1292]],\n              \n                      [[ 0.1006]],\n              \n                      [[-0.1159]],\n              \n                      [[ 0.1094]],\n              \n                      [[-0.1318]],\n              \n                      [[-0.0887]],\n              \n                      [[ 0.0940]],\n              \n                      [[ 0.1005]],\n              \n                      [[-0.0566]],\n              \n                      [[-0.1148]],\n              \n                      [[-0.1239]],\n              \n                      [[-0.0968]],\n              \n                      [[-0.1029]],\n              \n                      [[-0.0837]],\n              \n                      [[ 0.0586]],\n              \n                      [[-0.0941]],\n              \n                      [[ 0.1180]],\n              \n                      [[-0.1105]],\n              \n                      [[-0.0667]],\n              \n                      [[-0.0644]],\n              \n                      [[-0.1057]],\n              \n                      [[-0.0706]]])),\n             ('features.5.3.block.0.weight',\n              tensor([[[[-0.0170,  0.0017, -0.0068,  ...,  0.0000, -0.0068, -0.0068],\n                        [-0.0119, -0.0051,  0.0000,  ...,  0.0017,  0.0000, -0.0051],\n                        [ 0.0034, -0.0034,  0.0102,  ...,  0.0255,  0.0017,  0.0034],\n                        ...,\n                        [ 0.0017, -0.0068,  0.0238,  ...,  0.0017,  0.0034, -0.0204],\n                        [ 0.0017, -0.0017, -0.0017,  ...,  0.0000, -0.0051, -0.0034],\n                        [ 0.0068, -0.0051,  0.0000,  ..., -0.0051, -0.0051, -0.0051]]],\n              \n              \n                      [[[ 0.0068,  0.0017,  0.0085,  ..., -0.0017,  0.0051, -0.0017],\n                        [ 0.0017,  0.0017,  0.0085,  ..., -0.0017, -0.0051, -0.0051],\n                        [ 0.0085,  0.0152,  0.0322,  ..., -0.0186, -0.0068,  0.0000],\n                        ...,\n                        [ 0.0136,  0.0102,  0.0068,  ..., -0.0728, -0.0169, -0.0152],\n                        [ 0.0000, -0.0068,  0.0017,  ..., -0.0136, -0.0119, -0.0034],\n                        [ 0.0034,  0.0034,  0.0017,  ..., -0.0051, -0.0102, -0.0102]]],\n              \n              \n                      [[[ 0.0000, -0.0053, -0.0053,  ..., -0.0132, -0.0026,  0.0000],\n                        [ 0.0000,  0.0026, -0.0079,  ..., -0.0053,  0.0053, -0.0053],\n                        [-0.0106, -0.0026,  0.0000,  ...,  0.0000, -0.0053,  0.0026],\n                        ...,\n                        [-0.0053, -0.0026, -0.0106,  ..., -0.0159,  0.0053, -0.0053],\n                        [ 0.0026,  0.0000, -0.0026,  ..., -0.0079, -0.0026,  0.0000],\n                        [ 0.0000, -0.0106, -0.0026,  ..., -0.0053, -0.0026, -0.0079]]],\n              \n              \n                      ...,\n              \n              \n                      [[[ 0.0000, -0.0072, -0.0048,  ..., -0.0096, -0.0024, -0.0048],\n                        [-0.0048, -0.0024, -0.0120,  ..., -0.0024, -0.0096, -0.0024],\n                        [-0.0048,  0.0024,  0.0000,  ..., -0.0048,  0.0024, -0.0024],\n                        ...,\n                        [-0.0120, -0.0072, -0.0335,  ..., -0.0431, -0.0024, -0.0096],\n                        [ 0.0024, -0.0072, -0.0120,  ..., -0.0192, -0.0072, -0.0120],\n                        [-0.0096, -0.0072, -0.0072,  ..., -0.0096, -0.0048, -0.0048]]],\n              \n              \n                      [[[ 0.0000, -0.0027,  0.0000,  ..., -0.0027,  0.0000, -0.0027],\n                        [ 0.0027,  0.0027,  0.0054,  ...,  0.0054,  0.0000,  0.0054],\n                        [-0.0027,  0.0000, -0.0080,  ...,  0.0000,  0.0054, -0.0080],\n                        ...,\n                        [ 0.0000,  0.0107,  0.0456,  ...,  0.0322,  0.0000,  0.0080],\n                        [ 0.0054,  0.0054, -0.0027,  ...,  0.0000,  0.0080,  0.0027],\n                        [ 0.0027,  0.0027,  0.0027,  ...,  0.0080,  0.0027,  0.0027]]],\n              \n              \n                      [[[ 0.0000, -0.0013, -0.0148,  ...,  0.0027,  0.0027,  0.0201],\n                        [-0.0067,  0.0027, -0.0107,  ...,  0.0027,  0.0067,  0.0054],\n                        [-0.0134, -0.0134, -0.0188,  ...,  0.0228,  0.0040,  0.0121],\n                        ...,\n                        [-0.0161, -0.0269, -0.0846,  ...,  0.0994,  0.0094,  0.0201],\n                        [-0.0067, -0.0081,  0.0027,  ...,  0.0094,  0.0107,  0.0188],\n                        [-0.0134, -0.0040, -0.0027,  ...,  0.0201,  0.0081,  0.0121]]]],\n                     size=(384, 1, 7, 7), dtype=torch.qint8,\n                     quantization_scheme=torch.per_channel_affine,\n                     scale=tensor([1.6973e-03, 1.6938e-03, 2.6438e-03, 2.3087e-03, 2.6945e-03, 2.6064e-03,\n                      2.0516e-03, 2.5196e-03, 2.2730e-03, 2.2664e-03, 1.9579e-03, 2.8362e-03,\n                      2.2159e-03, 5.5824e-04, 1.2727e-03, 2.4371e-03, 2.1347e-03, 2.6978e-03,\n                      2.2809e-03, 2.7942e-03, 2.5845e-03, 2.0242e-03, 2.3327e-03, 2.3531e-03,\n                      2.1884e-03, 2.1872e-03, 1.8745e-03, 1.9421e-03, 2.6616e-03, 3.3358e-03,\n                      2.5103e-03, 2.2483e-03, 2.5655e-03, 1.8656e-03, 3.9331e-04, 2.6170e-03,\n                      2.1226e-03, 2.6897e-03, 1.2106e-03, 2.6112e-03, 1.6872e-03, 1.9778e-03,\n                      1.9762e-03, 2.3221e-03, 1.9540e-03, 1.5031e-03, 2.4940e-03, 2.6595e-03,\n                      2.4469e-03, 1.3889e-03, 2.2440e-03, 2.7641e-03, 2.4524e-03, 7.4334e-04,\n                      2.0267e-03, 1.1140e-03, 4.2240e-04, 2.4310e-03, 2.3573e-03, 2.3436e-03,\n                      1.9739e-03, 2.7355e-03, 1.8485e-03, 2.6560e-03, 2.4890e-03, 1.2022e-03,\n                      2.0600e-03, 2.4957e-03, 2.5211e-03, 8.0885e-04, 2.2875e-03, 2.4718e-03,\n                      1.2962e-03, 2.0356e-03, 2.3885e-03, 1.8859e-03, 2.6489e-03, 1.9428e-03,\n                      2.2875e-03, 1.8427e-03, 2.1963e-03, 1.5681e-03, 2.0916e-03, 2.1898e-03,\n                      1.3753e-03, 2.2123e-03, 2.2690e-03, 1.1979e-03, 1.9862e-03, 2.5954e-03,\n                      2.1648e-03, 2.1551e-03, 2.5549e-03, 1.9159e-03, 2.0511e-03, 1.9689e-03,\n                      2.1326e-03, 1.8207e-03, 1.4972e-03, 1.8203e-03, 2.4252e-03, 2.3163e-03,\n                      2.2209e-03, 2.5017e-03, 8.4881e-04, 1.9839e-03, 2.4706e-03, 1.5201e-03,\n                      1.1099e-03, 2.2670e-03, 2.6198e-03, 1.6593e-03, 2.2793e-03, 2.1490e-03,\n                      2.7095e-03, 2.1527e-03, 2.6204e-03, 2.7138e-03, 2.0973e-03, 1.9027e-03,\n                      1.9688e-03, 1.9649e-03, 2.7486e-03, 1.8317e-03, 1.9019e-03, 2.3312e-03,\n                      1.8621e-03, 2.0614e-03, 2.5862e-03, 2.6514e-03, 2.6826e-03, 2.0744e-03,\n                      2.1993e-03, 1.7418e-03, 2.3190e-03, 2.5915e-03, 2.5086e-03, 2.4923e-03,\n                      2.2254e-03, 1.9064e-03, 2.3681e-03, 2.1511e-03, 2.6375e-03, 1.9109e-03,\n                      2.0986e-03, 2.5751e-03, 2.9339e-03, 1.7039e-03, 1.2196e-03, 2.4091e-03,\n                      8.1636e-04, 2.9208e-04, 2.4093e-03, 2.2942e-03, 2.0031e-03, 2.3595e-03,\n                      4.3888e-05, 2.3553e-03, 2.3235e-03, 2.6895e-03, 2.7373e-03, 2.1495e-03,\n                      2.3625e-03, 2.8059e-03, 2.0056e-03, 2.2850e-03, 2.6256e-03, 2.9590e-03,\n                      1.0146e-03, 2.6813e-03, 2.5963e-03, 2.2833e-03, 1.9045e-03, 1.6070e-03,\n                      2.4377e-03, 2.1927e-03, 2.7542e-03, 2.3009e-03, 6.8067e-04, 2.5046e-03,\n                      1.8941e-03, 2.2837e-03, 9.2046e-04, 1.4689e-03, 2.6275e-03, 2.5639e-03,\n                      2.1367e-03, 8.3051e-04, 2.8011e-03, 1.8911e-03, 2.7095e-03, 2.1990e-03,\n                      2.5329e-03, 1.8547e-03, 2.7963e-03, 2.2332e-03, 2.5080e-03, 2.1990e-03,\n                      2.2759e-03, 2.2245e-03, 2.0402e-03, 2.6151e-03, 2.4865e-03, 2.4028e-03,\n                      2.7386e-03, 2.2698e-03, 2.5540e-03, 2.4432e-03, 2.0801e-03, 2.1777e-03,\n                      8.3130e-04, 2.4202e-03, 1.3924e-03, 2.6127e-03, 2.2878e-03, 2.6989e-03,\n                      2.3143e-03, 2.5596e-03, 1.7755e-03, 2.4947e-03, 2.1512e-03, 2.0407e-03,\n                      1.4713e-03, 2.2037e-03, 2.6319e-03, 2.3763e-03, 2.6306e-03, 2.4758e-03,\n                      2.4169e-03, 1.6861e-04, 2.3363e-03, 2.6535e-03, 2.1580e-03, 2.2214e-03,\n                      2.3584e-03, 2.4186e-03, 2.2619e-03, 2.6893e-03, 1.7488e-03, 2.5678e-03,\n                      2.4814e-03, 2.6589e-03, 2.5197e-03, 2.7231e-03, 2.2425e-03, 2.4395e-03,\n                      2.8334e-03, 2.4507e-03, 2.8426e-03, 2.1398e-03, 2.1051e-03, 2.5922e-03,\n                      2.0892e-03, 2.4331e-03, 1.8398e-03, 2.6747e-03, 2.5513e-03, 2.0977e-03,\n                      2.2012e-03, 1.3934e-03, 1.8200e-03, 1.5897e-03, 1.9981e-03, 2.7566e-03,\n                      8.0813e-04, 2.4299e-03, 2.2956e-03, 1.5023e-03, 2.8285e-03, 2.1244e-03,\n                      2.5337e-03, 2.3122e-03, 1.4338e-03, 1.6889e-03, 1.0465e-03, 2.6245e-03,\n                      2.5781e-03, 2.8458e-03, 2.1141e-03, 2.5631e-03, 2.3275e-03, 2.5495e-03,\n                      2.5482e-03, 1.8588e-03, 2.2467e-03, 1.5211e-03, 2.2419e-03, 2.0731e-03,\n                      2.4343e-03, 2.5924e-03, 2.2438e-03, 1.8890e-03, 2.5213e-03, 1.9159e-03,\n                      2.3665e-03, 2.3343e-03, 2.4264e-03, 1.9200e-03, 2.4558e-03, 2.4207e-03,\n                      2.4224e-03, 1.4815e-03, 2.7949e-03, 2.9378e-03, 2.2174e-03, 2.3884e-03,\n                      2.3987e-03, 2.5561e-03, 2.1616e-03, 2.1933e-03, 2.2735e-03, 2.2671e-03,\n                      2.1931e-03, 2.3305e-03, 1.9057e-03, 1.9965e-03, 2.7890e-03, 1.2574e-03,\n                      1.9374e-03, 1.9369e-03, 2.4232e-03, 2.7040e-03, 2.4168e-03, 2.6959e-03,\n                      2.7636e-03, 2.1149e-03, 2.3787e-03, 1.9865e-03, 2.8561e-03, 2.5402e-03,\n                      1.8181e-03, 2.2371e-03, 8.7212e-04, 7.9546e-04, 2.1422e-03, 2.0770e-03,\n                      2.8408e-03, 2.2261e-03, 9.5443e-04, 2.8679e-03, 2.0069e-03, 2.4919e-03,\n                      1.5615e-03, 2.5234e-03, 1.2921e-03, 2.8244e-03, 2.1439e-03, 2.1034e-03,\n                      1.0963e-03, 2.4547e-03, 1.9924e-03, 8.2118e-04, 1.5509e-03, 8.9033e-04,\n                      2.5376e-03, 1.7064e-03, 2.2146e-03, 1.7916e-03, 2.1287e-03, 6.6790e-04,\n                      1.8969e-03, 2.5615e-03, 2.0552e-03, 2.5158e-03, 2.8167e-03, 2.7606e-03,\n                      1.0117e-03, 2.5801e-03, 2.1680e-03, 1.8339e-03, 2.3785e-03, 2.0053e-03,\n                      2.3083e-03, 2.6504e-03, 2.3975e-03, 2.2827e-03, 2.5997e-03, 2.1054e-03,\n                      2.3514e-03, 1.6132e-03, 1.2683e-03, 2.3963e-03, 2.6822e-03, 1.3430e-03],\n                     dtype=torch.float64),\n                     zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n                     axis=0)),\n             ('features.5.3.block.0.bias',\n              Parameter containing:\n              tensor([ 5.7186e-03,  7.7344e-04,  1.3999e-03, -5.9331e-03, -3.5151e-04,\n                       6.0645e-03,  9.6679e-04,  3.1112e-03, -1.5731e-03, -1.1306e-03,\n                      -3.0602e-03, -5.7954e-03,  4.3500e-03, -4.3628e-03,  4.7206e-03,\n                      -1.0250e-03, -9.7614e-03, -9.0382e-03,  6.1353e-04,  4.8406e-03,\n                       5.0823e-03,  3.3151e-03,  6.4411e-03,  1.7692e-03, -5.0050e-03,\n                      -1.8670e-02, -7.9641e-04, -2.4060e-03, -4.4786e-03,  2.9781e-03,\n                       1.0848e-03, -3.3837e-03,  3.8390e-03,  7.3687e-03, -1.6367e-02,\n                       6.0879e-03, -8.2352e-04, -5.7254e-03, -1.5819e-03, -1.6479e-03,\n                       4.5858e-03, -2.6140e-03,  3.9371e-03,  1.3709e-03, -8.4562e-03,\n                       4.2848e-03, -3.1712e-04,  2.8850e-03, -7.9872e-03, -2.3230e-03,\n                      -7.0586e-03, -1.0570e-02,  2.8162e-04,  1.3983e-03,  7.9435e-04,\n                       2.7666e-04,  1.8495e-02,  2.8748e-03,  2.0861e-02, -6.1952e-03,\n                       2.4075e-03,  1.8285e-03, -2.5395e-03, -4.7754e-03, -5.2123e-03,\n                       8.0756e-03,  3.9517e-03, -1.0614e-02,  3.0396e-03, -6.0107e-03,\n                      -5.6310e-03, -1.4669e-03, -2.3689e-03, -1.5699e-03, -6.0731e-03,\n                       4.1113e-03, -3.1655e-03,  5.2073e-03,  9.8167e-04, -4.3950e-03,\n                      -6.7393e-03, -6.0113e-03, -5.3643e-03,  1.8817e-04,  7.7396e-03,\n                      -1.8421e-03,  8.4290e-05, -9.1254e-03,  4.3080e-03, -7.1684e-04,\n                       4.2539e-03, -8.9762e-03,  5.1617e-03,  3.4293e-03, -3.4809e-03,\n                       1.5265e-04,  3.4235e-03,  1.4995e-04, -1.7602e-03,  1.2982e-03,\n                       6.3846e-03,  3.8033e-03, -5.0573e-03, -7.4453e-04, -2.5165e-04,\n                      -1.4685e-03, -3.3724e-03,  1.7858e-02, -8.7239e-04, -3.5167e-02,\n                       8.6025e-03,  6.3423e-04, -1.0786e-03,  3.3734e-03,  1.0438e-03,\n                      -1.5511e-03, -1.8381e-03,  6.5786e-03, -7.5927e-03, -1.2017e-02,\n                      -2.1357e-03,  7.1413e-03, -8.4437e-03, -4.0693e-04, -5.7463e-04,\n                      -9.0461e-03, -2.4632e-03, -3.2409e-03,  3.8701e-03, -3.7987e-03,\n                       3.1431e-03,  2.2129e-03,  2.7329e-03, -1.0004e-03,  5.6601e-03,\n                      -8.0703e-04,  2.3087e-03, -1.5459e-02, -2.0042e-04,  1.0572e-04,\n                      -2.2806e-05,  5.7695e-03, -8.4731e-03, -4.7005e-03, -3.7024e-03,\n                       4.7233e-03,  6.3624e-03, -7.4310e-03, -4.6442e-03, -6.2610e-04,\n                      -1.1599e-02,  1.8678e-03,  1.3783e-04, -1.2944e-02,  2.5915e-03,\n                      -5.6573e-03,  1.2716e-03, -8.1894e-03,  1.4009e-03, -6.2871e-03,\n                       8.4991e-03, -1.7496e-03,  4.7018e-03, -2.2898e-03, -6.8199e-03,\n                       2.8614e-03,  3.4140e-03, -8.2604e-05, -6.5744e-03, -3.5625e-03,\n                      -8.7335e-03,  4.3604e-04,  5.0823e-03, -5.7115e-03, -6.6493e-03,\n                      -5.7394e-03,  4.1839e-03, -8.0088e-04, -7.8201e-04,  5.9482e-04,\n                      -1.7132e-03, -1.6031e-03,  2.3462e-04, -3.0456e-03,  3.3467e-03,\n                      -7.6304e-04, -4.7060e-03,  1.4773e-03, -7.9844e-03,  2.9161e-03,\n                      -5.1618e-03,  1.0242e-02, -7.0333e-03, -2.8801e-03, -1.5351e-03,\n                      -1.6831e-03, -2.9247e-03,  2.2434e-03,  3.6228e-03, -5.1649e-03,\n                      -4.4303e-03,  2.5542e-03,  2.1501e-03, -1.6825e-03, -1.4398e-03,\n                      -2.5195e-03, -1.3659e-03, -1.0763e-02, -4.7845e-03,  4.0396e-03,\n                       1.6836e-03, -1.7614e-03, -2.6182e-03,  2.3305e-03,  7.8087e-03,\n                       5.4506e-03, -7.4123e-03, -1.3488e-03, -8.5828e-03, -2.7309e-03,\n                       1.2917e-03,  9.9390e-04, -1.0024e-03, -1.1531e-02, -6.0548e-03,\n                      -8.6590e-03, -7.3150e-03,  3.2326e-03,  1.7905e-03,  8.8569e-02,\n                      -4.5473e-03,  8.9007e-04, -1.1694e-03, -7.3344e-03, -7.4075e-04,\n                      -1.8572e-03, -3.3259e-03,  2.1704e-03, -1.8060e-03, -8.2788e-03,\n                      -1.9521e-03, -5.6219e-04, -3.2172e-03, -4.3175e-03, -2.2702e-03,\n                       7.5199e-03, -4.2001e-03, -4.9116e-03, -1.6688e-03, -3.9086e-03,\n                      -2.7470e-03, -7.7885e-03,  2.1215e-04, -1.1771e-02, -2.2576e-02,\n                       2.1968e-03, -6.2298e-03, -2.5046e-03,  3.7566e-03, -9.1227e-03,\n                       1.9275e-03,  1.0013e-02,  6.0088e-03, -1.2425e-02,  3.0680e-03,\n                       2.7156e-03,  8.5208e-04, -1.4805e-02,  1.6196e-04, -5.6714e-03,\n                       5.4119e-04,  3.7878e-04, -3.6807e-04,  8.9437e-04, -3.2730e-03,\n                      -6.9893e-03, -5.4869e-03, -3.0062e-03,  3.5608e-03,  1.9127e-03,\n                       1.1966e-02,  5.0498e-04, -2.4908e-03,  3.8493e-04, -1.5993e-03,\n                      -3.9950e-03, -4.4430e-03,  9.7174e-04,  2.7839e-03,  4.1555e-03,\n                       1.2243e-02,  2.2291e-04, -9.0283e-03, -1.2149e-03,  6.1751e-04,\n                       1.3020e-03, -6.0127e-03, -1.1113e-03, -1.9849e-03,  4.5353e-03,\n                      -1.9412e-03, -7.2988e-03, -6.2427e-03,  1.4568e-03,  3.6053e-03,\n                       5.8027e-03, -1.8293e-03,  4.9744e-03, -2.7017e-04,  1.3507e-03,\n                      -2.1634e-03,  2.6463e-02, -9.7034e-03, -1.4958e-02, -6.1930e-03,\n                      -1.1259e-02,  2.4625e-04, -5.0982e-03,  1.0512e-04, -5.6617e-03,\n                       2.6528e-03,  5.8584e-03, -5.0479e-03,  3.5080e-03, -1.1585e-02,\n                      -6.0330e-03, -8.3459e-04, -5.4994e-03,  8.1585e-03, -4.4018e-03,\n                       3.8762e-04, -4.0007e-03, -1.3031e-02, -5.4751e-03, -2.7483e-03,\n                      -7.1326e-04,  1.5589e-03,  4.1762e-03, -1.8530e-04, -4.0673e-03,\n                       1.7448e-03, -6.4680e-03,  6.1912e-03, -8.5783e-04, -1.5289e-03,\n                      -4.0073e-03, -1.1823e-03, -1.9221e-04,  4.1768e-02,  2.4889e-03,\n                      -3.7916e-03, -5.3905e-03, -1.5540e-03,  8.9381e-02, -4.3132e-03,\n                       1.8240e-03,  4.2254e-03,  3.2911e-03, -5.7319e-03,  2.8447e-03,\n                      -2.3411e-03,  4.9407e-03, -1.0223e-02, -1.1323e-03, -3.5173e-03,\n                      -3.3620e-03, -2.0166e-02, -7.8818e-04,  2.4037e-03,  5.9933e-03,\n                       1.4474e-02, -6.8968e-03, -2.5222e-02,  5.3751e-03,  1.9764e-03,\n                      -6.6541e-03,  7.9677e-03,  2.9952e-03, -3.6131e-03, -9.8358e-04,\n                      -1.4936e-03,  7.2828e-04,  4.5399e-03,  1.4428e-03],\n                     requires_grad=True)),\n             ('features.5.3.block.0.scale', tensor(0.0031)),\n             ('features.5.3.block.0.zero_point', tensor(63)),\n             ('features.5.3.block.2.weight',\n              tensor([5.1165e-01, 8.8953e-01, 1.0415e+00, 8.9839e-01, 1.5201e+00, 1.0020e+00,\n                      7.0073e-01, 1.5283e+00, 8.2448e-01, 1.3011e+00, 6.7756e-01, 1.2578e+00,\n                      9.9931e-01, 7.3211e-01, 4.8250e-01, 1.0082e+00, 9.6278e-01, 8.6839e-01,\n                      9.5127e-01, 1.0374e+00, 4.7717e-01, 9.7415e-01, 1.1036e+00, 1.7056e+00,\n                      9.2962e-01, 1.1603e+00, 7.8551e-01, 1.0172e+00, 1.2436e+00, 6.9131e-01,\n                      1.1428e+00, 1.0449e+00, 8.0828e-01, 8.0467e-01, 5.3436e-01, 1.2252e+00,\n                      1.1345e+00, 1.2194e+00, 5.0952e-01, 7.9632e-01, 5.1570e-01, 7.8853e-01,\n                      7.6718e-01, 1.4009e+00, 6.2598e-01, 6.6933e-01, 9.5508e-01, 1.4038e+00,\n                      9.9383e-01, 6.1308e-01, 8.6811e-01, 1.1552e+00, 8.1700e-01, 4.9363e-01,\n                      9.2247e-01, 8.6976e-01, 7.5613e-01, 7.3629e-01, 1.5490e+00, 7.4475e-01,\n                      1.1922e+00, 1.0898e+00, 1.0411e+00, 9.3834e-01, 9.8947e-01, 1.1921e+00,\n                      9.6412e-01, 1.6928e+00, 1.3155e+00, 6.2405e-01, 1.1430e+00, 9.2191e-01,\n                      7.0668e-01, 1.0742e+00, 7.5950e-01, 5.8548e-01, 1.5664e+00, 6.6427e-01,\n                      1.2403e+00, 5.2700e-01, 1.1536e+00, 6.3541e-01, 9.9176e-01, 1.1761e+00,\n                      6.0488e-01, 1.1867e+00, 1.2193e+00, 4.6812e-01, 8.8225e-01, 1.4683e+00,\n                      1.2704e+00, 7.2209e-01, 9.8238e-01, 7.4474e-01, 5.9012e-01, 7.8342e-01,\n                      1.8274e+00, 9.1387e-01, 4.5417e-01, 7.5915e-01, 7.9207e-01, 1.4679e+00,\n                      1.5869e+00, 1.2492e+00, 7.8904e-01, 7.3594e-01, 8.9601e-01, 6.5183e-01,\n                      4.1060e-01, 8.0643e-01, 8.1483e-01, 8.1333e-01, 1.1341e+00, 8.6595e-01,\n                      1.4696e+00, 7.2766e-01, 1.4532e+00, 1.2873e+00, 8.4416e-01, 5.9571e-01,\n                      5.2429e-01, 1.0201e+00, 9.9621e-01, 1.0906e+00, 9.5938e-01, 1.3149e+00,\n                      9.6037e-01, 6.3063e-01, 9.3430e-01, 1.1925e+00, 1.2180e+00, 1.0520e+00,\n                      8.9108e-01, 6.9509e-01, 1.0454e+00, 1.2706e+00, 9.2493e-01, 1.2761e+00,\n                      8.5461e-01, 1.0014e+00, 1.4396e+00, 6.0982e-01, 1.1395e+00, 1.0181e+00,\n                      1.0062e+00, 1.1114e+00, 1.3968e+00, 6.5623e-01, 4.7733e-01, 1.7907e+00,\n                      6.4073e-01, 8.0261e-02, 9.6654e-01, 5.8164e-01, 6.0937e-01, 9.8420e-01,\n                      2.1105e-04, 7.8491e-01, 1.2127e+00, 1.0638e+00, 1.0240e+00, 7.3575e-01,\n                      7.4680e-01, 1.6006e+00, 1.2352e+00, 8.2955e-01, 1.2624e+00, 1.3696e+00,\n                      4.2647e-01, 1.0028e+00, 7.8774e-01, 8.8367e-01, 8.2947e-01, 6.5844e-01,\n                      1.1465e+00, 1.0162e+00, 1.2103e+00, 1.0020e+00, 5.9249e-01, 1.2386e+00,\n                      1.2143e+00, 1.8372e+00, 9.3130e-01, 8.9162e-01, 1.1680e+00, 1.3334e+00,\n                      9.7404e-01, 5.3304e-01, 1.9905e+00, 9.4665e-01, 1.0853e+00, 7.2003e-01,\n                      1.0075e+00, 1.1647e+00, 1.2029e+00, 1.2506e+00, 9.0791e-01, 9.1847e-01,\n                      1.7129e+00, 1.5510e+00, 1.2583e+00, 8.7409e-01, 6.9959e-01, 1.8167e+00,\n                      9.7389e-01, 1.0823e+00, 9.2964e-01, 9.6808e-01, 8.7045e-01, 7.9212e-01,\n                      5.8858e-01, 1.1431e+00, 6.3068e-01, 1.0988e+00, 9.4733e-01, 1.5506e+00,\n                      9.0053e-01, 1.1766e+00, 5.3352e-01, 1.4623e+00, 1.3731e+00, 1.1788e+00,\n                      9.0553e-01, 7.9730e-01, 1.0239e+00, 8.9743e-01, 1.2479e+00, 1.0638e+00,\n                      8.7295e-01, 2.6716e+00, 1.0847e+00, 9.9676e-01, 1.0556e+00, 9.6067e-01,\n                      1.5880e+00, 1.0974e+00, 1.0852e+00, 1.4669e+00, 1.0028e+00, 1.0267e+00,\n                      1.1603e+00, 1.0413e+00, 1.1031e+00, 1.3352e+00, 1.1143e+00, 1.1805e+00,\n                      1.4671e+00, 1.3424e+00, 1.3389e+00, 1.1673e+00, 9.9461e-01, 1.0383e+00,\n                      9.3690e-01, 9.0299e-01, 8.0754e-01, 1.1643e+00, 9.0189e-01, 8.8110e-01,\n                      1.1455e+00, 4.8811e-01, 6.5965e-01, 6.7562e-01, 6.8388e-01, 1.1986e+00,\n                      6.7536e-01, 1.4097e+00, 1.4261e+00, 6.1131e-01, 1.9084e+00, 7.0122e-01,\n                      8.8733e-01, 1.0031e+00, 8.4929e-01, 8.1369e-01, 7.2571e-01, 1.0074e+00,\n                      8.8983e-01, 1.5532e+00, 1.0018e+00, 7.5572e-01, 1.1055e+00, 1.2067e+00,\n                      1.0957e+00, 1.0894e+00, 8.4240e-01, 1.0538e+00, 1.0466e+00, 5.9879e-01,\n                      7.2008e-01, 7.9103e-01, 1.1469e+00, 9.0972e-01, 9.5430e-01, 7.7075e-01,\n                      1.4868e+00, 7.6912e-01, 8.4253e-01, 1.0794e+00, 9.1714e-01, 1.1359e+00,\n                      7.0362e-01, 4.4818e-01, 1.2227e+00, 1.3875e+00, 1.0939e+00, 8.9074e-01,\n                      8.8965e-01, 9.2707e-01, 8.8825e-01, 1.1955e+00, 6.5003e-01, 1.5713e+00,\n                      6.7869e-01, 9.7610e-01, 6.7231e-01, 6.5318e-01, 1.0445e+00, 7.6010e-01,\n                      1.4157e+00, 1.1364e+00, 1.0711e+00, 9.8689e-01, 1.3191e+00, 1.3298e+00,\n                      1.1238e+00, 1.4562e+00, 1.4415e+00, 1.0405e+00, 1.1384e+00, 1.4039e+00,\n                      8.3973e-01, 1.3716e+00, 4.9442e-01, 5.8018e-01, 8.0950e-01, 9.9801e-01,\n                      1.6184e+00, 1.1249e+00, 7.0562e-01, 1.3496e+00, 9.9198e-01, 1.5339e+00,\n                      6.7548e-01, 1.3003e+00, 1.0855e+00, 1.3854e+00, 1.0309e+00, 1.0557e+00,\n                      6.0071e-01, 9.0366e-01, 1.0900e+00, 5.7937e-01, 5.7308e-01, 1.2320e+00,\n                      8.7153e-01, 8.6911e-01, 8.7677e-01, 5.1548e-01, 8.6858e-01, 5.3171e-01,\n                      9.3712e-01, 9.7879e-01, 8.4301e-01, 8.3592e-01, 1.0194e+00, 9.3470e-01,\n                      5.8654e-01, 1.0520e+00, 1.2120e+00, 9.9830e-01, 7.4811e-01, 7.3741e-01,\n                      6.6079e-01, 1.5649e+00, 7.1123e-01, 1.2460e+00, 9.6911e-01, 9.3176e-01,\n                      8.3944e-01, 8.8665e-01, 8.6740e-01, 9.3459e-01, 1.1488e+00, 5.4278e-01])),\n             ('features.5.3.block.2.bias',\n              tensor([-1.1034e-01, -1.3711e-01, -9.5091e-02,  1.2338e-01, -1.8977e-01,\n                      -2.9652e-01, -4.1034e-02, -1.7799e-01,  3.7190e-02, -4.5611e-02,\n                       1.5833e-02,  2.1279e-01,  1.0701e-02,  3.2444e-01, -7.8328e-02,\n                      -4.5598e-01,  2.3223e-01,  4.9112e-01, -4.4592e-02, -2.4833e-01,\n                       2.1407e-02, -1.0958e-01, -5.2267e-01, -1.7709e-01,  1.4507e-01,\n                      -7.6734e-01,  2.0344e-02,  9.4934e-02,  3.0534e-01, -2.6053e+00,\n                      -5.3314e-01,  1.0201e-01, -9.9994e-02, -2.4016e-01, -8.6254e-02,\n                      -1.4885e-01, -6.2637e-02,  1.9504e-01, -1.7876e-01, -5.8527e-02,\n                      -1.0877e-01,  6.3561e-02, -6.9013e-02, -2.3371e-01,  4.8881e-02,\n                      -2.1508e-01,  1.4780e-02, -9.5022e-02,  2.1985e-01,  2.8443e-02,\n                       2.3078e-01, -8.6123e-01,  2.7206e-03,  1.0307e-03, -6.2792e-02,\n                      -1.8741e-02, -2.5288e-01, -7.6295e-02,  7.8230e-01,  1.0805e-01,\n                      -1.2708e-01,  2.1120e-02,  9.8271e-02,  1.3445e-01,  6.5475e-02,\n                       2.9381e-01, -1.1725e-01,  7.2345e-01, -9.5990e-02,  1.2199e-01,\n                       2.6426e-01,  3.7770e-02, -1.8691e-01,  2.5180e-02,  2.9448e-01,\n                       2.8497e-02,  1.3647e-01, -1.7103e-01, -8.2322e-02,  2.3991e-02,\n                       1.3664e-01,  1.3098e-01,  1.1030e-01, -6.1368e-02, -4.9634e-01,\n                       1.1408e-02, -4.0767e-03,  9.1367e-02, -1.4978e-01,  2.9312e-02,\n                      -4.3749e-02,  2.2252e-01, -1.3023e-01, -1.0743e-01,  9.0129e-02,\n                      -1.2699e-01, -2.8197e-01, -1.8309e-02,  6.3690e-02,  4.0171e-02,\n                      -2.0726e-01, -1.6993e-01,  1.4495e-01,  8.7817e-02,  8.7780e-03,\n                       6.1466e-02,  9.0775e-02,  5.0350e-02, -2.6629e-02,  2.1806e+00,\n                      -2.2507e+00, -1.9389e-02,  1.5628e-01, -1.0928e-01, -1.4631e-01,\n                      -1.4326e-01,  1.5325e-01, -3.6558e-01,  3.3566e-01,  3.2764e-01,\n                       7.0066e-02, -2.1839e-01,  2.6128e-01,  1.3054e-02,  8.5928e-02,\n                       5.5942e-01, -2.0456e-02, -4.0439e-03, -4.3226e-02,  8.5627e-02,\n                      -8.5578e-03, -5.5598e-02, -1.3891e-01, -3.0877e-03, -2.5890e-01,\n                      -5.9860e-01, -6.6211e-02, -1.3392e+00, -6.9169e-03,  2.1140e-02,\n                       1.4373e-02, -4.4971e-02,  2.4656e-01,  8.1357e-02,  6.4880e-02,\n                      -8.3384e-02, -2.8625e-01,  2.1606e-01,  1.2907e-01, -6.1264e-03,\n                       2.8825e-01,  1.7890e+00, -3.1685e-02,  2.5778e-01, -2.4586e-03,\n                       2.3244e-01,  1.9110e-05,  1.7826e-01, -1.1649e-01,  3.1687e-01,\n                      -2.4208e-01,  1.0180e-01,  3.8761e-02,  1.2905e-01,  3.5751e-01,\n                      -5.0146e-02, -9.5979e-02, -3.4975e-01,  9.6735e-02,  1.5157e-01,\n                       1.8839e-01, -6.6557e-02, -1.9509e-01,  1.0908e-01,  1.9764e-01,\n                       1.4800e-01, -2.5006e-01,  1.1498e-02,  1.1643e-01, -5.5505e-02,\n                      -2.9465e-02,  3.7400e-02,  1.3663e-01,  8.8804e-02, -4.3711e-01,\n                      -1.6472e-01,  1.2838e-01, -1.2944e-02,  5.0905e-01,  2.1455e-02,\n                       1.6395e-01, -2.2856e-01,  1.6690e-01,  1.0436e-01,  7.7724e-03,\n                       3.5983e-02,  1.2501e-01, -1.4746e-01, -2.1452e-01,  2.7286e-01,\n                       3.1517e-01, -4.6573e-01, -9.5180e-02, -9.0716e-02,  6.2098e-02,\n                       3.1026e-01, -2.5495e-02,  2.8994e-01,  2.1575e-03, -2.9712e-02,\n                      -1.4135e-02,  7.7131e-03,  5.2537e-02, -5.2055e-02, -3.1915e-01,\n                       9.5854e-02,  3.9534e-01, -7.7876e-03,  6.1174e-02, -3.2846e-01,\n                      -8.0114e-02, -7.7698e-02,  5.8379e-02,  3.1747e-01,  1.9874e-01,\n                       1.5670e-01,  5.8556e-02, -4.3986e-02, -3.5125e-02,  1.6626e+00,\n                       1.0389e-01, -1.5088e-01,  8.3158e-02,  1.6402e-01, -6.7675e-02,\n                       1.0528e-01,  1.6408e-01, -2.0432e-01, -3.0355e-01,  1.2272e-01,\n                       1.5909e-02, -4.1751e-02,  1.9206e-01,  3.5074e-01,  1.1338e-01,\n                      -4.0205e-01,  1.4780e-01,  2.7586e-01,  1.3910e-01,  1.4565e-01,\n                       3.4592e-02, -9.5663e-02,  1.0236e-02,  3.2831e-01,  3.3169e-01,\n                      -1.3388e-01,  7.5745e-02,  8.0318e-02, -1.8309e-01,  6.7951e-02,\n                      -7.5329e-02, -1.1865e-01, -7.2868e-02,  2.7874e-01, -3.7288e-02,\n                      -1.2393e-01, -1.3834e-01,  9.0248e-01, -9.9026e-02,  1.6848e-01,\n                      -1.0671e-02,  3.9329e-02,  2.1784e-02,  8.6521e-01,  6.7672e-02,\n                       1.2309e-01,  8.5048e-02,  1.6933e-01, -1.8655e-01, -7.7624e-02,\n                       7.9130e-01,  6.3946e-04,  1.1329e-01,  1.7387e-02,  5.6813e-02,\n                      -4.6268e-02,  1.7223e-01, -2.0040e-02, -1.0717e-01, -1.8827e-01,\n                      -8.1531e-01,  2.0024e-03,  1.1868e-01,  3.2305e-02, -7.0175e-02,\n                      -1.7113e-01,  3.2829e-01, -1.1839e-02,  1.3167e-02,  8.4005e-01,\n                       1.4009e-01,  7.8139e-02,  2.7754e-01, -3.6520e-02, -1.8017e-01,\n                      -1.5819e-01,  3.0153e-02, -1.4321e-01, -1.0643e-02, -2.9185e-02,\n                      -2.0072e-03,  1.4333e+00,  2.8097e-01, -1.1668e-02,  1.2371e-01,\n                       3.1400e-01, -8.6713e-02,  1.0852e-01, -3.3681e-02,  2.3223e-01,\n                      -1.5395e-01, -2.5403e-01,  8.7978e-02, -1.4491e-01, -1.3588e-01,\n                       1.3042e-01, -9.8945e-03,  3.6340e-02, -1.8821e-01,  1.1791e-01,\n                      -3.3080e-02,  1.1276e-01,  2.5616e-01,  2.9225e-01, -7.0890e-02,\n                      -3.7438e-03,  1.1691e-01, -1.6808e-01,  1.7722e-02,  1.1576e-01,\n                      -7.0813e-02, -1.2620e-01, -8.4387e-02,  5.3886e-02,  8.4136e-02,\n                      -5.6599e-03, -3.1533e-02, -5.6938e-02,  3.3101e-01, -9.5642e-02,\n                       6.3436e-02,  1.0006e-01,  1.3037e-01,  1.9693e+00,  2.2225e-01,\n                       8.9086e-02, -1.4197e-01,  5.6011e-03,  3.1107e-01, -8.2962e-02,\n                       8.4756e-02, -1.5706e-01,  2.3740e-01,  3.1273e-03, -2.0465e-02,\n                       7.1767e-01,  9.0200e-02,  5.9301e-02, -5.3759e-02, -1.9675e-01,\n                      -4.8029e-01,  6.0172e-01,  1.0001e+00, -4.7658e-01, -7.2748e-02,\n                       4.3098e-01, -2.2636e-01, -3.5164e-02,  1.0317e-01,  2.7789e-02,\n                       2.8577e-02, -6.9916e-02, -2.4684e-01, -3.3296e-02])),\n             ('features.5.3.block.2.scale', tensor(0.1969)),\n             ('features.5.3.block.2.zero_point', tensor(42)),\n             ('features.5.3.block.3.scale', tensor(0.0808)),\n             ('features.5.3.block.3.zero_point', tensor(82)),\n             ('features.5.3.block.3._packed_params.dtype', torch.qint8),\n             ('features.5.3.block.3._packed_params._packed_params',\n              (tensor([[-0.0884,  0.0198,  0.0701,  ..., -0.0487, -0.0731, -0.0061],\n                       [-0.0346,  0.0528, -0.0802,  ...,  0.0310,  0.0182, -0.0091],\n                       [ 0.0612, -0.0515,  0.0000,  ..., -0.0058, -0.0583,  0.0097],\n                       ...,\n                       [ 0.0177, -0.0013,  0.0013,  ...,  0.0127, -0.0165, -0.0456],\n                       [ 0.0599,  0.0178,  0.0064,  ...,  0.0306, -0.0255,  0.1299],\n                       [-0.0565, -0.0121,  0.0108,  ..., -0.0834, -0.0148,  0.0202]],\n                      size=(1536, 384), dtype=torch.qint8,\n                      quantization_scheme=torch.per_channel_affine,\n                      scale=tensor([0.0015, 0.0018, 0.0010,  ..., 0.0013, 0.0013, 0.0013],\n                      dtype=torch.float64),\n                      zero_point=tensor([0, 0, 0,  ..., 0, 0, 0]), axis=0),\n               Parameter containing:\n               tensor([-0.0497, -0.0673, -0.0365,  ..., -0.0461, -0.0565, -0.0463],\n                      requires_grad=True))),\n             ('features.5.3.block.5.scale', tensor(0.0466)),\n             ('features.5.3.block.5.zero_point', tensor(62)),\n             ('features.5.3.block.5._packed_params.dtype', torch.qint8),\n             ('features.5.3.block.5._packed_params._packed_params',\n              (tensor([[-0.0715,  0.0432, -0.0626,  ...,  0.0223, -0.0030,  0.0491],\n                       [-0.0328, -0.0819,  0.0382,  ...,  0.0246,  0.0027, -0.0136],\n                       [-0.0815,  0.0555,  0.0364,  ...,  0.0069, -0.0521,  0.0416],\n                       ...,\n                       [ 0.0263, -0.0511,  0.0540,  ..., -0.0044,  0.0204,  0.0088],\n                       [-0.0174, -0.0206, -0.0063,  ..., -0.0349, -0.0143,  0.1220],\n                       [-0.1173, -0.0844, -0.0469,  ..., -0.0250, -0.0016,  0.0219]],\n                      size=(384, 1536), dtype=torch.qint8,\n                      quantization_scheme=torch.per_channel_affine,\n                      scale=tensor([0.0015, 0.0027, 0.0017, 0.0019, 0.0021, 0.0019, 0.0019, 0.0018, 0.0016,\n                       0.0015, 0.0014, 0.0023, 0.0021, 0.0044, 0.0015, 0.0020, 0.0017, 0.0016,\n                       0.0019, 0.0016, 0.0025, 0.0014, 0.0024, 0.0017, 0.0014, 0.0020, 0.0023,\n                       0.0018, 0.0016, 0.0021, 0.0022, 0.0023, 0.0017, 0.0016, 0.0018, 0.0018,\n                       0.0014, 0.0017, 0.0017, 0.0016, 0.0019, 0.0016, 0.0018, 0.0029, 0.0023,\n                       0.0030, 0.0020, 0.0021, 0.0015, 0.0017, 0.0016, 0.0032, 0.0017, 0.0019,\n                       0.0014, 0.0029, 0.0046, 0.0019, 0.0017, 0.0020, 0.0020, 0.0022, 0.0021,\n                       0.0015, 0.0016, 0.0029, 0.0016, 0.0026, 0.0020, 0.0019, 0.0020, 0.0016,\n                       0.0025, 0.0020, 0.0015, 0.0024, 0.0021, 0.0030, 0.0014, 0.0017, 0.0016,\n                       0.0016, 0.0024, 0.0022, 0.0025, 0.0016, 0.0031, 0.0016, 0.0015, 0.0018,\n                       0.0013, 0.0021, 0.0018, 0.0022, 0.0016, 0.0023, 0.0026, 0.0022, 0.0018,\n                       0.0027, 0.0019, 0.0020, 0.0025, 0.0015, 0.0032, 0.0017, 0.0017, 0.0022,\n                       0.0015, 0.0059, 0.0018, 0.0018, 0.0025, 0.0015, 0.0016, 0.0015, 0.0019,\n                       0.0018, 0.0016, 0.0016, 0.0016, 0.0016, 0.0018, 0.0027, 0.0031, 0.0053,\n                       0.0025, 0.0016, 0.0016, 0.0022, 0.0021, 0.0017, 0.0025, 0.0037, 0.0018,\n                       0.0031, 0.0017, 0.0030, 0.0029, 0.0021, 0.0016, 0.0017, 0.0024, 0.0025,\n                       0.0018, 0.0022, 0.0025, 0.0017, 0.0015, 0.0030, 0.0024, 0.0015, 0.0018,\n                       0.0017, 0.0016, 0.0019, 0.0022, 0.0020, 0.0016, 0.0016, 0.0024, 0.0015,\n                       0.0016, 0.0022, 0.0016, 0.0019, 0.0018, 0.0015, 0.0016, 0.0017, 0.0016,\n                       0.0015, 0.0017, 0.0015, 0.0019, 0.0019, 0.0016, 0.0022, 0.0020, 0.0016,\n                       0.0025, 0.0021, 0.0031, 0.0017, 0.0016, 0.0019, 0.0020, 0.0015, 0.0017,\n                       0.0022, 0.0023, 0.0021, 0.0016, 0.0020, 0.0018, 0.0034, 0.0015, 0.0023,\n                       0.0017, 0.0016, 0.0019, 0.0014, 0.0015, 0.0025, 0.0014, 0.0015, 0.0013,\n                       0.0016, 0.0016, 0.0027, 0.0017, 0.0021, 0.0017, 0.0016, 0.0019, 0.0024,\n                       0.0020, 0.0018, 0.0015, 0.0022, 0.0019, 0.0020, 0.0020, 0.0017, 0.0024,\n                       0.0021, 0.0036, 0.0016, 0.0016, 0.0021, 0.0027, 0.0013, 0.0020, 0.0018,\n                       0.0016, 0.0016, 0.0016, 0.0023, 0.0042, 0.0015, 0.0020, 0.0021, 0.0015,\n                       0.0015, 0.0019, 0.0030, 0.0017, 0.0026, 0.0016, 0.0016, 0.0021, 0.0020,\n                       0.0023, 0.0015, 0.0025, 0.0018, 0.0017, 0.0016, 0.0017, 0.0020, 0.0015,\n                       0.0026, 0.0026, 0.0014, 0.0035, 0.0025, 0.0035, 0.0016, 0.0019, 0.0017,\n                       0.0017, 0.0018, 0.0016, 0.0026, 0.0021, 0.0021, 0.0014, 0.0015, 0.0027,\n                       0.0015, 0.0019, 0.0027, 0.0019, 0.0032, 0.0016, 0.0014, 0.0030, 0.0015,\n                       0.0014, 0.0018, 0.0022, 0.0023, 0.0015, 0.0025, 0.0015, 0.0014, 0.0017,\n                       0.0031, 0.0018, 0.0019, 0.0015, 0.0015, 0.0025, 0.0024, 0.0020, 0.0016,\n                       0.0018, 0.0018, 0.0013, 0.0017, 0.0015, 0.0017, 0.0022, 0.0020, 0.0020,\n                       0.0018, 0.0016, 0.0020, 0.0024, 0.0017, 0.0027, 0.0014, 0.0019, 0.0025,\n                       0.0015, 0.0015, 0.0018, 0.0021, 0.0016, 0.0018, 0.0019, 0.0020, 0.0017,\n                       0.0015, 0.0016, 0.0022, 0.0016, 0.0017, 0.0028, 0.0030, 0.0026, 0.0017,\n                       0.0016, 0.0018, 0.0022, 0.0018, 0.0029, 0.0018, 0.0021, 0.0014, 0.0014,\n                       0.0018, 0.0016, 0.0049, 0.0015, 0.0020, 0.0017, 0.0017, 0.0018, 0.0018,\n                       0.0023, 0.0016, 0.0016, 0.0016, 0.0035, 0.0014, 0.0030, 0.0020, 0.0020,\n                       0.0017, 0.0015, 0.0017, 0.0030, 0.0016, 0.0015, 0.0017, 0.0016, 0.0026,\n                       0.0015, 0.0018, 0.0016, 0.0015, 0.0016, 0.0016], dtype=torch.float64),\n                      zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n                      axis=0),\n               Parameter containing:\n               tensor([ 1.9523e-02,  2.0855e-02, -4.5170e-03, -7.0800e-02,  1.9980e-03,\n                       -5.5175e-02, -1.4606e-02,  1.8061e-02, -1.8490e-02, -3.9222e-02,\n                        5.7557e-03,  1.9076e-02, -1.3406e-01,  1.4751e-02, -8.3215e-03,\n                        5.0959e-02,  1.0850e-01, -1.5662e-02,  5.4437e-03,  1.5988e-02,\n                       -1.7568e-02,  1.1310e-02,  1.4061e-01, -4.4252e-02, -2.7051e-02,\n                       -5.0639e-02,  2.4576e-02, -3.5869e-02, -2.1121e-02, -1.4700e-01,\n                        3.5887e-02, -8.6342e-03,  2.0466e-02, -6.2069e-02, -2.7193e-02,\n                        1.0008e-03,  2.0552e-02, -2.7890e-02, -5.2889e-04,  9.3297e-03,\n                       -2.3501e-02,  2.0558e-03,  1.5421e-02,  1.6476e-02,  2.8392e-02,\n                       -6.0502e-02, -1.2663e-02,  1.1889e-02, -4.2717e-03, -3.9198e-02,\n                        3.4204e-03, -4.2607e-02,  6.0500e-03,  1.6834e-02,  2.1367e-02,\n                       -8.6084e-02, -2.0600e-02,  5.7945e-03, -6.2346e-02, -2.4524e-04,\n                        6.3402e-03, -6.1644e-02,  3.8048e-03, -1.6250e-02,  5.5824e-03,\n                       -2.8245e-02, -9.0714e-03, -6.3820e-03,  1.7757e-02,  2.8281e-02,\n                       -1.1738e-02,  4.3736e-03,  3.9498e-02,  8.0530e-03, -2.3388e-02,\n                       -5.3013e-03,  2.9839e-02,  1.2045e-03,  4.1742e-04, -1.0702e-02,\n                        7.9901e-03, -2.6268e-02,  4.9478e-02, -3.0787e-03,  1.6117e-02,\n                       -1.6176e-02, -2.3188e-02, -2.3736e-02, -8.7237e-03, -1.5639e-02,\n                        4.9920e-02, -3.4276e-02,  1.6562e-03, -2.7108e-02,  7.5462e-04,\n                        2.5077e-04, -3.1251e-03,  2.2211e-02, -3.5915e-02,  1.0229e-02,\n                       -6.0520e-03,  1.4503e-02, -2.6801e-02,  2.6320e-03,  8.0965e-03,\n                        1.2359e-02,  6.4639e-03,  6.7657e-04, -6.7246e-03,  2.6208e-02,\n                       -1.3159e-01,  1.2082e-02,  1.1586e-02,  5.7192e-03,  2.0987e-02,\n                       -2.0362e-02,  2.5716e-02, -2.4882e-03, -1.4780e-02,  2.1199e-02,\n                       -9.6379e-03, -1.3464e-02,  3.0105e-02,  1.2114e-02, -8.8955e-02,\n                       -3.4836e-02, -3.9557e-03, -4.3452e-02, -5.7627e-03,  1.8921e-03,\n                        1.1932e-02,  1.5407e-02,  5.1356e-03, -1.8607e-02,  1.3108e-02,\n                        9.7036e-02, -5.4970e-03,  7.9889e-02,  2.2718e-04,  1.3015e-02,\n                       -4.7233e-02,  1.3103e-02, -8.9721e-03,  1.1405e-02,  1.3491e-02,\n                       -3.2050e-02,  5.7894e-03, -2.3553e-02,  4.8707e-03, -4.7386e-02,\n                       -5.9598e-04, -3.0942e-02,  1.3112e-02,  1.0343e-02, -5.1066e-06,\n                       -2.9660e-02,  1.3258e+00, -9.1000e-02,  4.3214e-02, -3.0101e-02,\n                       -1.1456e-02,  2.2766e-02, -2.2869e-02,  2.0136e-03,  2.3799e-02,\n                       -3.5436e-02, -2.4003e-02, -2.2793e-02,  1.6867e-02,  1.2806e-02,\n                       -1.8472e-02,  3.4357e-02,  1.2319e-02,  3.8023e-03,  2.0646e-02,\n                       -2.0848e-02,  1.1662e-02, -1.9821e-02,  5.1752e-03,  1.1992e-02,\n                       -6.4286e-03,  3.5040e-02, -5.7403e-03,  3.6595e-03, -3.4966e-03,\n                        5.4521e-02,  1.8207e-02,  2.3863e-02,  1.1550e-02,  1.1652e-02,\n                        1.3396e-03,  1.5171e-02, -2.2503e-02, -1.0969e-02,  2.6609e-02,\n                        1.4337e-02,  4.6908e-03,  2.6824e-02,  3.2587e-02,  4.3603e-02,\n                        2.8766e-02,  2.6907e-02,  1.4298e-03,  1.8384e-02,  4.5725e-03,\n                       -2.8319e-02, -1.3790e-02,  7.2395e-03,  1.1074e-02,  4.1276e-02,\n                       -3.5321e-02, -1.2061e-01, -2.1850e-02, -4.3273e-02, -2.2389e-02,\n                        5.2868e-02,  1.3496e-02,  5.0996e-03, -2.7958e-03,  3.2860e-02,\n                       -1.5498e-02, -1.8051e-02,  1.1013e-02, -1.5245e-02, -1.4719e-02,\n                       -2.9796e-02, -6.4621e-03,  3.0743e-02, -1.4317e-02,  1.6564e-02,\n                        1.7046e-02,  3.4895e-02,  1.7281e-02,  3.9318e-02, -1.5078e-02,\n                        3.0040e-03, -1.0986e-02,  2.1096e-02, -2.2946e-02,  1.1125e-02,\n                        5.6065e-03, -1.6718e-02, -3.3079e-02,  3.8876e-02,  5.0418e-02,\n                        1.5950e-02,  2.0316e-02,  6.4976e-02, -6.3374e-02, -4.5942e-02,\n                        1.6104e-03, -7.6015e-03, -2.5377e-02, -7.8156e-03,  2.9606e-03,\n                        1.2283e-02, -5.4597e-02,  2.1711e-02,  2.4847e-02,  6.7888e-03,\n                        9.4588e-03, -2.4644e-02, -4.1367e-03, -3.3369e-03,  1.1532e-01,\n                        1.2875e-03,  5.2940e-03, -1.7882e-02, -3.8881e-02, -2.0074e-02,\n                        1.0187e-02,  1.2945e-03, -4.1604e-02, -1.5461e-03, -5.7696e-02,\n                       -1.5429e-02, -1.0056e-02, -4.2784e-03, -6.6231e-03, -1.0353e-02,\n                       -5.6725e-02, -2.2049e-02, -1.0944e-02, -8.1932e-03,  2.6645e-04,\n                       -3.5617e-03, -7.0048e-04, -2.3045e-03, -2.4259e-02, -6.9941e-03,\n                       -6.8287e-02,  2.0363e-02,  2.3765e-02, -1.2140e-02,  1.6707e-02,\n                       -1.7608e-02, -2.7813e-02, -2.9478e-02,  1.9180e-02,  5.0960e-02,\n                       -4.6668e-02,  3.2868e-03, -3.5880e-03,  9.1197e-03, -1.3397e-03,\n                       -9.5315e-03,  3.2746e-02, -2.0893e-03, -2.6864e-02, -4.2751e-03,\n                       -1.5060e-02,  7.0446e-02,  3.0752e-02, -1.9124e-02, -4.0990e-02,\n                        4.0241e-03, -1.0565e-02,  3.1479e-02,  6.5309e-03,  1.3611e-02,\n                       -8.0220e-03, -8.4600e-03, -6.9478e-03,  1.7366e-02,  7.1174e-02,\n                       -3.2725e-02,  1.5480e-02,  1.3669e-02,  3.2558e-02,  3.3782e-02,\n                       -1.8713e-02,  5.5166e-03,  1.0567e-02,  3.5231e-02, -9.1939e-03,\n                        2.7782e-02, -3.5310e-03,  3.0478e-02, -4.4781e-03, -1.4885e-02,\n                        6.9498e-03,  5.0025e-03, -1.9501e-02,  1.7673e-02, -1.7503e-01,\n                       -9.5664e-03,  3.2512e-02,  4.6275e-03,  2.4217e-01, -1.4821e-02,\n                        1.5818e-02, -1.1754e-02,  1.7382e-02, -2.2251e-02,  9.9919e-03,\n                        1.7410e-02,  7.1006e-03,  1.3304e-02,  5.4726e-02, -9.2118e-04,\n                       -2.4732e-03,  8.3892e-03, -1.2717e-02, -2.4358e-03, -4.9152e-02,\n                        6.5406e-02, -1.7116e-02, -1.6264e-02, -7.7598e-03,  3.2007e-02,\n                       -4.1762e-03,  2.1931e-02,  4.3648e-02, -1.1733e-02,  1.4534e-02,\n                       -5.3405e-02,  1.4123e-01, -5.9353e-02,  5.9231e-02, -5.0075e-02,\n                        1.2728e-02,  9.5573e-03, -3.8646e-02,  6.5036e-03],\n                      requires_grad=True))),\n             ('features.5.4.layer_scale',\n              tensor([[[-1.0440e-01]],\n              \n                      [[ 5.6372e-02]],\n              \n                      [[-1.1250e-01]],\n              \n                      [[-1.3126e-01]],\n              \n                      [[-1.4594e-01]],\n              \n                      [[-1.4125e-01]],\n              \n                      [[ 6.2999e-02]],\n              \n                      [[-1.5883e-01]],\n              \n                      [[-1.6774e-01]],\n              \n                      [[ 9.0535e-02]],\n              \n                      [[ 8.0224e-02]],\n              \n                      [[-1.5682e-01]],\n              \n                      [[ 1.4855e-01]],\n              \n                      [[-1.8164e-01]],\n              \n                      [[-1.5714e-01]],\n              \n                      [[ 1.1595e-01]],\n              \n                      [[ 1.5636e-01]],\n              \n                      [[-1.3757e-01]],\n              \n                      [[-1.5900e-01]],\n              \n                      [[-1.1847e-01]],\n              \n                      [[-2.1824e-01]],\n              \n                      [[ 1.1303e-01]],\n              \n                      [[ 1.8873e-01]],\n              \n                      [[ 1.6482e-01]],\n              \n                      [[-8.0741e-02]],\n              \n                      [[-4.8275e-02]],\n              \n                      [[-7.7314e-02]],\n              \n                      [[ 6.3188e-02]],\n              \n                      [[ 1.7155e-01]],\n              \n                      [[-2.5732e-01]],\n              \n                      [[-1.7046e-01]],\n              \n                      [[ 7.5859e-02]],\n              \n                      [[-1.4177e-01]],\n              \n                      [[-1.6859e-01]],\n              \n                      [[ 1.5418e-01]],\n              \n                      [[ 1.5190e-01]],\n              \n                      [[-1.1911e-01]],\n              \n                      [[-1.7786e-01]],\n              \n                      [[-1.8230e-01]],\n              \n                      [[ 1.5109e-01]],\n              \n                      [[ 1.4768e-01]],\n              \n                      [[-1.0048e-01]],\n              \n                      [[-1.6071e-01]],\n              \n                      [[ 1.6442e-01]],\n              \n                      [[ 1.7032e-01]],\n              \n                      [[-1.9296e-01]],\n              \n                      [[-6.1952e-04]],\n              \n                      [[ 1.5661e-01]],\n              \n                      [[ 8.7359e-02]],\n              \n                      [[-1.3678e-01]],\n              \n                      [[-1.6580e-01]],\n              \n                      [[-1.3404e-01]],\n              \n                      [[-9.9904e-02]],\n              \n                      [[ 1.7379e-01]],\n              \n                      [[ 1.0415e-01]],\n              \n                      [[-7.8006e-02]],\n              \n                      [[-1.8972e-01]],\n              \n                      [[ 1.1004e-01]],\n              \n                      [[-7.9264e-02]],\n              \n                      [[ 1.4408e-01]],\n              \n                      [[-7.5233e-02]],\n              \n                      [[ 1.3763e-01]],\n              \n                      [[-6.2793e-02]],\n              \n                      [[ 1.3447e-01]],\n              \n                      [[ 1.7515e-01]],\n              \n                      [[ 8.3929e-02]],\n              \n                      [[-5.3043e-02]],\n              \n                      [[ 1.7659e-01]],\n              \n                      [[ 1.4139e-01]],\n              \n                      [[-5.8681e-02]],\n              \n                      [[-1.2683e-01]],\n              \n                      [[ 9.0344e-02]],\n              \n                      [[-2.1458e-01]],\n              \n                      [[ 4.0183e-02]],\n              \n                      [[-1.2130e-01]],\n              \n                      [[ 1.6992e-01]],\n              \n                      [[-1.3360e-01]],\n              \n                      [[-1.1463e-02]],\n              \n                      [[-1.4812e-01]],\n              \n                      [[-1.3485e-01]],\n              \n                      [[-1.4841e-01]],\n              \n                      [[-1.5352e-01]],\n              \n                      [[ 1.2398e-01]],\n              \n                      [[ 8.9947e-04]],\n              \n                      [[ 1.4491e-01]],\n              \n                      [[-1.2418e-01]],\n              \n                      [[-9.0165e-02]],\n              \n                      [[ 1.3965e-01]],\n              \n                      [[-7.2043e-02]],\n              \n                      [[-1.5218e-01]],\n              \n                      [[-1.2519e-01]],\n              \n                      [[ 1.6910e-01]],\n              \n                      [[-9.9587e-02]],\n              \n                      [[ 1.3420e-01]],\n              \n                      [[-1.4637e-01]],\n              \n                      [[ 1.5737e-01]],\n              \n                      [[-1.2692e-01]],\n              \n                      [[-1.5019e-01]],\n              \n                      [[-1.5404e-01]],\n              \n                      [[ 2.0355e-01]],\n              \n                      [[ 1.2569e-01]],\n              \n                      [[-1.5549e-01]],\n              \n                      [[ 9.6268e-02]],\n              \n                      [[-1.1494e-01]],\n              \n                      [[ 8.2876e-02]],\n              \n                      [[-9.5973e-02]],\n              \n                      [[-8.8296e-02]],\n              \n                      [[ 1.8876e-01]],\n              \n                      [[-1.3823e-01]],\n              \n                      [[ 1.3415e-01]],\n              \n                      [[ 1.5764e-01]],\n              \n                      [[ 1.5907e-01]],\n              \n                      [[ 1.3666e-02]],\n              \n                      [[ 7.5348e-02]],\n              \n                      [[-1.1927e-01]],\n              \n                      [[ 1.6678e-01]],\n              \n                      [[-1.5803e-01]],\n              \n                      [[ 1.0569e-01]],\n              \n                      [[-7.1674e-02]],\n              \n                      [[ 1.6356e-01]],\n              \n                      [[ 1.2759e-01]],\n              \n                      [[ 1.0387e-01]],\n              \n                      [[-1.1798e-01]],\n              \n                      [[ 8.0651e-03]],\n              \n                      [[-2.2381e-01]],\n              \n                      [[ 6.3569e-02]],\n              \n                      [[-4.9731e-02]],\n              \n                      [[-1.5103e-01]],\n              \n                      [[-1.0697e-01]],\n              \n                      [[-3.0116e-04]],\n              \n                      [[ 1.0259e-02]],\n              \n                      [[-1.5819e-01]],\n              \n                      [[ 1.1085e-01]],\n              \n                      [[-1.8830e-01]],\n              \n                      [[ 1.0563e-01]],\n              \n                      [[ 1.8593e-01]],\n              \n                      [[-1.0140e-01]],\n              \n                      [[-7.7265e-02]],\n              \n                      [[ 6.8216e-04]],\n              \n                      [[-9.7449e-02]],\n              \n                      [[-1.6299e-01]],\n              \n                      [[-1.6527e-01]],\n              \n                      [[ 1.3669e-01]],\n              \n                      [[ 8.2659e-02]],\n              \n                      [[ 6.1451e-02]],\n              \n                      [[-1.4120e-01]],\n              \n                      [[-1.3535e-01]],\n              \n                      [[-1.1962e-01]],\n              \n                      [[-1.4600e-01]],\n              \n                      [[ 1.3482e-01]],\n              \n                      [[ 1.3035e-01]],\n              \n                      [[-1.2317e-01]],\n              \n                      [[-1.7314e-01]],\n              \n                      [[-1.4009e-01]],\n              \n                      [[-1.6935e-01]],\n              \n                      [[-1.6565e-01]],\n              \n                      [[-1.0446e+00]],\n              \n                      [[ 1.4148e-01]],\n              \n                      [[-1.1849e-01]],\n              \n                      [[-9.9104e-02]],\n              \n                      [[-1.6861e-01]],\n              \n                      [[-1.1948e-01]],\n              \n                      [[-1.2866e-01]],\n              \n                      [[-6.4012e-03]],\n              \n                      [[-1.9391e-01]],\n              \n                      [[-1.2949e-01]],\n              \n                      [[-1.6062e-01]],\n              \n                      [[-1.4987e-01]],\n              \n                      [[-1.4386e-01]],\n              \n                      [[ 1.5164e-01]],\n              \n                      [[ 1.3898e-01]],\n              \n                      [[-1.2495e-01]],\n              \n                      [[-1.3913e-01]],\n              \n                      [[-1.3645e-01]],\n              \n                      [[-1.4224e-01]],\n              \n                      [[-4.3859e-02]],\n              \n                      [[-1.5584e-01]],\n              \n                      [[-6.2082e-02]],\n              \n                      [[ 1.6093e-01]],\n              \n                      [[ 1.8869e-01]],\n              \n                      [[-6.7411e-02]],\n              \n                      [[-1.3944e-01]],\n              \n                      [[-1.8652e-01]],\n              \n                      [[-1.8169e-01]],\n              \n                      [[ 1.5936e-01]],\n              \n                      [[-1.8908e-01]],\n              \n                      [[-4.5395e-02]],\n              \n                      [[ 1.7083e-01]],\n              \n                      [[ 1.9165e-01]],\n              \n                      [[-8.7294e-02]],\n              \n                      [[ 1.2348e-03]],\n              \n                      [[ 6.8788e-02]],\n              \n                      [[-1.0453e-01]],\n              \n                      [[ 5.9254e-02]],\n              \n                      [[-1.5418e-01]],\n              \n                      [[-1.1910e-03]],\n              \n                      [[ 9.7003e-02]],\n              \n                      [[-4.8915e-02]],\n              \n                      [[ 1.6382e-01]],\n              \n                      [[ 1.5495e-01]],\n              \n                      [[-1.3576e-01]],\n              \n                      [[ 1.3675e-01]],\n              \n                      [[-1.0044e-01]],\n              \n                      [[-1.5317e-01]],\n              \n                      [[-1.1833e-01]],\n              \n                      [[-1.6185e-01]],\n              \n                      [[ 1.0652e-01]],\n              \n                      [[-9.4351e-02]],\n              \n                      [[ 1.2268e-01]],\n              \n                      [[ 1.6728e-01]],\n              \n                      [[-8.5664e-02]],\n              \n                      [[-1.1429e-01]],\n              \n                      [[ 1.5068e-01]],\n              \n                      [[ 1.1893e-01]],\n              \n                      [[ 9.7915e-02]],\n              \n                      [[ 1.8302e-01]],\n              \n                      [[ 9.9225e-02]],\n              \n                      [[ 7.8374e-04]],\n              \n                      [[ 1.5299e-01]],\n              \n                      [[-1.5348e-01]],\n              \n                      [[-9.9964e-02]],\n              \n                      [[-5.4772e-02]],\n              \n                      [[-1.6984e-01]],\n              \n                      [[ 7.1505e-02]],\n              \n                      [[ 1.4180e-01]],\n              \n                      [[ 2.0386e-01]],\n              \n                      [[-1.2449e-01]],\n              \n                      [[ 9.2957e-02]],\n              \n                      [[-1.4783e-01]],\n              \n                      [[-1.2541e-01]],\n              \n                      [[-1.5457e-01]],\n              \n                      [[ 1.1065e-01]],\n              \n                      [[-1.7499e-01]],\n              \n                      [[-1.3190e-01]],\n              \n                      [[-1.5833e-01]],\n              \n                      [[-8.5241e-02]],\n              \n                      [[ 8.0850e-02]],\n              \n                      [[-1.8869e-01]],\n              \n                      [[-1.6646e-01]],\n              \n                      [[ 1.1585e-01]],\n              \n                      [[-6.3707e-05]],\n              \n                      [[ 1.6774e-01]],\n              \n                      [[ 1.0123e-01]],\n              \n                      [[ 1.5380e-01]],\n              \n                      [[-1.6851e-01]],\n              \n                      [[ 1.6188e-01]],\n              \n                      [[ 1.5282e-01]],\n              \n                      [[-1.2520e-01]],\n              \n                      [[ 1.5427e-01]],\n              \n                      [[-1.0505e-01]],\n              \n                      [[-5.4591e-02]],\n              \n                      [[-1.5108e-01]],\n              \n                      [[-6.4382e-02]],\n              \n                      [[-1.1327e-01]],\n              \n                      [[-1.5160e-01]],\n              \n                      [[-1.4852e-01]],\n              \n                      [[-1.0873e-01]],\n              \n                      [[-1.6921e-01]],\n              \n                      [[-1.6642e-01]],\n              \n                      [[ 1.4770e-01]],\n              \n                      [[-1.6302e-01]],\n              \n                      [[ 1.8191e-01]],\n              \n                      [[-1.5124e-01]],\n              \n                      [[-1.1943e-01]],\n              \n                      [[ 7.1271e-02]],\n              \n                      [[-1.7564e-01]],\n              \n                      [[ 1.2064e-01]],\n              \n                      [[-1.1970e-01]],\n              \n                      [[ 1.7381e-01]],\n              \n                      [[-1.6363e-01]],\n              \n                      [[ 1.1827e-01]],\n              \n                      [[-6.7160e-02]],\n              \n                      [[-1.4162e-01]],\n              \n                      [[-1.7350e-01]],\n              \n                      [[-8.6958e-02]],\n              \n                      [[-1.3151e-01]],\n              \n                      [[-1.0699e-01]],\n              \n                      [[ 1.3595e-01]],\n              \n                      [[ 1.4381e-01]],\n              \n                      [[ 1.3815e-01]],\n              \n                      [[-5.9964e-02]],\n              \n                      [[ 1.3358e-01]],\n              \n                      [[-1.5058e-01]],\n              \n                      [[ 1.2968e-01]],\n              \n                      [[ 8.2130e-02]],\n              \n                      [[-1.4402e-01]],\n              \n                      [[ 5.5887e-02]],\n              \n                      [[ 1.4468e-01]],\n              \n                      [[-1.3322e-01]],\n              \n                      [[-1.1097e-01]],\n              \n                      [[-1.4305e-01]],\n              \n                      [[ 2.0617e-01]],\n              \n                      [[-1.4802e-01]],\n              \n                      [[ 1.5204e-03]],\n              \n                      [[ 1.4886e-01]],\n              \n                      [[ 1.0728e-01]],\n              \n                      [[-1.4415e-01]],\n              \n                      [[ 4.6770e-02]],\n              \n                      [[ 7.2798e-02]],\n              \n                      [[-8.0813e-02]],\n              \n                      [[-1.2968e-01]],\n              \n                      [[-1.4242e-01]],\n              \n                      [[-1.2090e-01]],\n              \n                      [[-1.4959e-01]],\n              \n                      [[-5.1058e-02]],\n              \n                      [[ 8.8502e-02]],\n              \n                      [[ 1.1075e-01]],\n              \n                      [[-1.0514e-01]],\n              \n                      [[-8.1350e-02]],\n              \n                      [[ 1.3261e-01]],\n              \n                      [[ 1.6116e-01]],\n              \n                      [[ 1.1705e-01]],\n              \n                      [[ 1.5519e-01]],\n              \n                      [[-1.7492e-01]],\n              \n                      [[-4.3262e-02]],\n              \n                      [[ 1.1099e-01]],\n              \n                      [[ 1.5777e-01]],\n              \n                      [[-1.0700e-01]],\n              \n                      [[-8.2049e-04]],\n              \n                      [[ 7.2213e-02]],\n              \n                      [[-4.1756e-03]],\n              \n                      [[ 1.1758e-01]],\n              \n                      [[-1.4981e-01]],\n              \n                      [[-1.4240e-01]],\n              \n                      [[ 1.6408e-01]],\n              \n                      [[-1.2810e-01]],\n              \n                      [[ 7.7823e-04]],\n              \n                      [[ 1.5980e-01]],\n              \n                      [[-1.6206e-01]],\n              \n                      [[-1.8547e-01]],\n              \n                      [[ 1.2095e-01]],\n              \n                      [[-1.4057e-01]],\n              \n                      [[-1.4459e-01]],\n              \n                      [[ 1.6304e-01]],\n              \n                      [[-1.4960e-01]],\n              \n                      [[-1.1007e-01]],\n              \n                      [[ 1.7948e-01]],\n              \n                      [[-5.2765e-02]],\n              \n                      [[ 5.2976e-02]],\n              \n                      [[ 1.9962e-01]],\n              \n                      [[ 5.0933e-02]],\n              \n                      [[-1.6944e-01]],\n              \n                      [[ 7.6190e-02]],\n              \n                      [[ 1.7844e-01]],\n              \n                      [[-9.7870e-02]],\n              \n                      [[ 1.5271e-01]],\n              \n                      [[ 1.2616e-01]],\n              \n                      [[-4.6412e-02]],\n              \n                      [[ 7.2317e-02]],\n              \n                      [[ 1.1320e-01]],\n              \n                      [[ 7.9420e-02]],\n              \n                      [[-1.3294e-01]],\n              \n                      [[-1.4926e-01]],\n              \n                      [[ 3.0757e-02]],\n              \n                      [[ 1.1799e-01]],\n              \n                      [[-4.4857e-02]],\n              \n                      [[-1.9255e-01]],\n              \n                      [[ 1.3367e-01]],\n              \n                      [[ 1.7382e-01]],\n              \n                      [[ 1.5918e-01]],\n              \n                      [[ 3.7945e-02]],\n              \n                      [[ 1.0027e-01]],\n              \n                      [[-1.8112e-01]],\n              \n                      [[ 1.3192e-01]],\n              \n                      [[ 1.2842e-01]],\n              \n                      [[-1.6070e-01]],\n              \n                      [[-1.7422e-01]],\n              \n                      [[ 1.2318e-01]],\n              \n                      [[-1.6199e-01]],\n              \n                      [[ 1.5350e-01]],\n              \n                      [[-8.1336e-02]],\n              \n                      [[ 1.5893e-01]],\n              \n                      [[ 1.6073e-01]],\n              \n                      [[-1.7569e-01]],\n              \n                      [[ 1.3954e-01]],\n              \n                      [[ 1.3280e-01]],\n              \n                      [[ 1.1180e-01]],\n              \n                      [[-1.6414e-01]],\n              \n                      [[ 1.6476e-01]],\n              \n                      [[-1.9303e-01]],\n              \n                      [[ 1.2002e-01]],\n              \n                      [[-7.8617e-02]],\n              \n                      [[ 1.7017e-01]],\n              \n                      [[ 1.1968e-01]]])),\n             ('features.5.4.block.0.weight',\n              tensor([[[[-0.0108, -0.0022, -0.0129,  ..., -0.0086,  0.0043, -0.0043],\n                        [-0.0022, -0.0086, -0.0108,  ..., -0.0086, -0.0043,  0.0065],\n                        [ 0.0000, -0.0065, -0.0216,  ..., -0.0173, -0.0065, -0.0086],\n                        ...,\n                        [-0.0065, -0.0065, -0.0302,  ..., -0.0302, -0.0086, -0.0086],\n                        [-0.0043, -0.0022, -0.0043,  ..., -0.0108, -0.0022,  0.0022],\n                        [-0.0022, -0.0129,  0.0022,  ..., -0.0065, -0.0043, -0.0065]]],\n              \n              \n                      [[[ 0.0053,  0.0088,  0.0106,  ...,  0.0123, -0.0053,  0.0088],\n                        [ 0.0000, -0.0018,  0.0070,  ...,  0.0159, -0.0053,  0.0088],\n                        [ 0.0106,  0.0141,  0.0211,  ...,  0.0317,  0.0088,  0.0106],\n                        ...,\n                        [-0.0141, -0.0159, -0.0617,  ..., -0.0705, -0.0088, -0.0088],\n                        [-0.0018,  0.0018,  0.0035,  ..., -0.0018, -0.0053, -0.0018],\n                        [-0.0106, -0.0070, -0.0018,  ..., -0.0106, -0.0035, -0.0088]]],\n              \n              \n                      [[[ 0.0027,  0.0000,  0.0054,  ...,  0.0000,  0.0027,  0.0054],\n                        [ 0.0000,  0.0108, -0.0054,  ..., -0.0027,  0.0054,  0.0081],\n                        [ 0.0027, -0.0027,  0.0000,  ..., -0.0054,  0.0000,  0.0027],\n                        ...,\n                        [-0.0054,  0.0108, -0.0216,  ..., -0.0189,  0.0054,  0.0000],\n                        [ 0.0027, -0.0027,  0.0027,  ..., -0.0027, -0.0054,  0.0027],\n                        [ 0.0054,  0.0000,  0.0000,  ..., -0.0027,  0.0054,  0.0054]]],\n              \n              \n                      ...,\n              \n              \n                      [[[ 0.0000,  0.0082, -0.0027,  ...,  0.0055, -0.0027,  0.0000],\n                        [ 0.0055, -0.0027,  0.0000,  ...,  0.0110,  0.0000,  0.0055],\n                        [ 0.0000,  0.0000,  0.0110,  ...,  0.0027, -0.0055, -0.0027],\n                        ...,\n                        [-0.0027, -0.0055, -0.0082,  ..., -0.0082,  0.0027, -0.0055],\n                        [ 0.0027, -0.0027,  0.0110,  ...,  0.0027, -0.0027, -0.0055],\n                        [ 0.0027, -0.0082,  0.0000,  ...,  0.0027,  0.0000,  0.0027]]],\n              \n              \n                      [[[ 0.0025,  0.0000, -0.0100,  ...,  0.0050, -0.0100,  0.0025],\n                        [ 0.0025, -0.0025, -0.0050,  ..., -0.0100,  0.0050,  0.0025],\n                        [-0.0100, -0.0025,  0.0000,  ...,  0.0075, -0.0025, -0.0100],\n                        ...,\n                        [ 0.0000, -0.0075,  0.0000,  ...,  0.0075, -0.0075, -0.0025],\n                        [-0.0050,  0.0050,  0.0000,  ...,  0.0075,  0.0025,  0.0000],\n                        [ 0.0000, -0.0151,  0.0050,  ...,  0.0050, -0.0025, -0.0125]]],\n              \n              \n                      [[[ 0.0115,  0.0000,  0.0115,  ...,  0.0023,  0.0046,  0.0092],\n                        [ 0.0115,  0.0092, -0.0023,  ...,  0.0069,  0.0069,  0.0000],\n                        [ 0.0046,  0.0069,  0.0254,  ...,  0.0185,  0.0046,  0.0069],\n                        ...,\n                        [ 0.0092,  0.0162,  0.0231,  ...,  0.0231,  0.0115,  0.0115],\n                        [ 0.0023,  0.0115, -0.0115,  ..., -0.0046,  0.0046,  0.0092],\n                        [ 0.0023,  0.0069,  0.0023,  ...,  0.0023,  0.0069,  0.0115]]]],\n                     size=(384, 1, 7, 7), dtype=torch.qint8,\n                     quantization_scheme=torch.per_channel_affine,\n                     scale=tensor([0.0022, 0.0018, 0.0027, 0.0023, 0.0026, 0.0024, 0.0028, 0.0025, 0.0014,\n                      0.0028, 0.0026, 0.0021, 0.0023, 0.0007, 0.0013, 0.0025, 0.0027, 0.0025,\n                      0.0010, 0.0027, 0.0012, 0.0026, 0.0018, 0.0019, 0.0027, 0.0021, 0.0026,\n                      0.0026, 0.0017, 0.0029, 0.0015, 0.0028, 0.0026, 0.0014, 0.0007, 0.0016,\n                      0.0025, 0.0024, 0.0016, 0.0017, 0.0015, 0.0024, 0.0022, 0.0018, 0.0008,\n                      0.0012, 0.0018, 0.0024, 0.0028, 0.0021, 0.0020, 0.0017, 0.0028, 0.0008,\n                      0.0028, 0.0016, 0.0005, 0.0022, 0.0028, 0.0009, 0.0026, 0.0029, 0.0026,\n                      0.0026, 0.0016, 0.0010, 0.0027, 0.0024, 0.0021, 0.0022, 0.0028, 0.0027,\n                      0.0013, 0.0028, 0.0014, 0.0013, 0.0022, 0.0015, 0.0027, 0.0015, 0.0025,\n                      0.0027, 0.0024, 0.0020, 0.0013, 0.0015, 0.0019, 0.0020, 0.0028, 0.0015,\n                      0.0027, 0.0012, 0.0028, 0.0026, 0.0024, 0.0028, 0.0021, 0.0019, 0.0007,\n                      0.0014, 0.0023, 0.0021, 0.0018, 0.0027, 0.0021, 0.0027, 0.0026, 0.0014,\n                      0.0014, 0.0018, 0.0024, 0.0013, 0.0018, 0.0027, 0.0025, 0.0011, 0.0009,\n                      0.0016, 0.0027, 0.0024, 0.0026, 0.0028, 0.0025, 0.0017, 0.0015, 0.0015,\n                      0.0026, 0.0025, 0.0026, 0.0016, 0.0015, 0.0014, 0.0014, 0.0013, 0.0027,\n                      0.0025, 0.0026, 0.0023, 0.0015, 0.0025, 0.0023, 0.0013, 0.0022, 0.0025,\n                      0.0028, 0.0027, 0.0014, 0.0027, 0.0014, 0.0017, 0.0008, 0.0024, 0.0018,\n                      0.0028, 0.0007, 0.0022, 0.0002, 0.0025, 0.0027, 0.0026, 0.0021, 0.0027,\n                      0.0026, 0.0019, 0.0014, 0.0024, 0.0010, 0.0014, 0.0005, 0.0025, 0.0021,\n                      0.0014, 0.0020, 0.0021, 0.0015, 0.0026, 0.0014, 0.0028, 0.0008, 0.0014,\n                      0.0027, 0.0013, 0.0012, 0.0014, 0.0025, 0.0022, 0.0027, 0.0009, 0.0022,\n                      0.0015, 0.0018, 0.0015, 0.0028, 0.0027, 0.0025, 0.0012, 0.0027, 0.0025,\n                      0.0020, 0.0019, 0.0016, 0.0019, 0.0024, 0.0018, 0.0028, 0.0021, 0.0026,\n                      0.0027, 0.0014, 0.0022, 0.0025, 0.0026, 0.0025, 0.0027, 0.0027, 0.0015,\n                      0.0020, 0.0018, 0.0016, 0.0022, 0.0027, 0.0027, 0.0018, 0.0027, 0.0008,\n                      0.0014, 0.0015, 0.0028, 0.0019, 0.0018, 0.0016, 0.0026, 0.0016, 0.0026,\n                      0.0020, 0.0029, 0.0027, 0.0022, 0.0018, 0.0027, 0.0017, 0.0017, 0.0027,\n                      0.0012, 0.0008, 0.0010, 0.0020, 0.0024, 0.0025, 0.0024, 0.0027, 0.0018,\n                      0.0028, 0.0026, 0.0016, 0.0028, 0.0025, 0.0012, 0.0018, 0.0015, 0.0014,\n                      0.0015, 0.0006, 0.0027, 0.0008, 0.0024, 0.0009, 0.0014, 0.0019, 0.0024,\n                      0.0026, 0.0027, 0.0012, 0.0021, 0.0022, 0.0024, 0.0026, 0.0014, 0.0021,\n                      0.0016, 0.0028, 0.0015, 0.0010, 0.0021, 0.0026, 0.0014, 0.0027, 0.0014,\n                      0.0021, 0.0025, 0.0023, 0.0023, 0.0023, 0.0017, 0.0017, 0.0025, 0.0020,\n                      0.0028, 0.0028, 0.0025, 0.0021, 0.0019, 0.0021, 0.0021, 0.0028, 0.0027,\n                      0.0025, 0.0024, 0.0028, 0.0023, 0.0017, 0.0010, 0.0022, 0.0025, 0.0011,\n                      0.0023, 0.0015, 0.0020, 0.0018, 0.0026, 0.0017, 0.0025, 0.0016, 0.0016,\n                      0.0027, 0.0019, 0.0014, 0.0025, 0.0012, 0.0024, 0.0025, 0.0024, 0.0013,\n                      0.0008, 0.0022, 0.0025, 0.0021, 0.0027, 0.0028, 0.0019, 0.0027, 0.0021,\n                      0.0028, 0.0013, 0.0012, 0.0016, 0.0013, 0.0027, 0.0009, 0.0026, 0.0027,\n                      0.0009, 0.0017, 0.0010, 0.0026, 0.0028, 0.0014, 0.0024, 0.0018, 0.0011,\n                      0.0030, 0.0028, 0.0010, 0.0017, 0.0022, 0.0018, 0.0008, 0.0015, 0.0024,\n                      0.0021, 0.0028, 0.0009, 0.0027, 0.0019, 0.0023, 0.0022, 0.0029, 0.0022,\n                      0.0014, 0.0020, 0.0017, 0.0027, 0.0025, 0.0023], dtype=torch.float64),\n                     zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n                     axis=0)),\n             ('features.5.4.block.0.bias',\n              Parameter containing:\n              tensor([ 5.3813e-03, -8.1568e-03,  5.6168e-03, -6.8792e-03, -1.4860e-02,\n                      -3.7472e-02, -1.4209e-03,  7.5784e-03, -1.8011e-02,  1.2719e-02,\n                       1.5442e-02, -7.0454e-03, -6.5829e-03,  9.8652e-03, -2.3392e-04,\n                       1.9682e-03,  5.7087e-03, -1.2516e-02, -4.9046e-04,  5.3479e-03,\n                      -2.1150e-03, -9.6958e-03,  5.8487e-03, -7.3272e-03,  6.4287e-03,\n                       2.9310e-02, -1.3219e-02,  6.6239e-03, -4.8674e-03, -6.0867e-03,\n                      -6.2584e-03, -6.9669e-03,  1.7821e-02, -1.7117e-02,  9.8120e-03,\n                       3.5304e-03, -7.2968e-03,  2.3665e-03, -3.0042e-03,  9.7309e-03,\n                       6.3328e-03, -1.2996e-03, -9.6359e-03, -1.2820e-02, -7.4238e-03,\n                       2.2043e-03, -4.7903e-03, -2.2946e-03,  2.6398e-03,  2.5476e-04,\n                      -6.5153e-03,  8.4909e-03, -1.3617e-03,  5.5930e-03, -1.6998e-03,\n                       1.0864e-03, -2.7126e-02,  1.0994e-02,  7.7484e-03,  6.1621e-04,\n                      -4.1611e-04,  1.8325e-02, -2.2621e-03,  5.7478e-03,  6.2247e-03,\n                       1.8202e-02,  9.9129e-03,  4.7413e-03,  2.7774e-02,  2.9425e-02,\n                       1.4945e-02, -2.9260e-03,  3.6498e-03,  6.4591e-03,  1.1333e-02,\n                       3.8677e-03, -4.7240e-03, -6.7764e-03, -6.1294e-03,  6.8533e-03,\n                       6.9689e-04,  3.4518e-03,  4.5958e-03,  8.2408e-05,  2.2126e-02,\n                      -1.8704e-04, -1.3541e-03, -4.3337e-03,  2.7772e-03, -2.7284e-03,\n                       7.2114e-03,  3.5588e-03, -6.8424e-03,  1.1088e-03, -3.1320e-03,\n                       3.3907e-03, -9.4691e-03,  7.3304e-04,  6.2191e-03, -2.1911e-03,\n                       2.0006e-03,  1.8774e-02,  2.4593e-02, -5.8029e-03, -1.1763e-02,\n                      -5.0117e-03, -1.2531e-02,  5.6283e-03,  3.9951e-03,  5.7691e-02,\n                       3.5340e-02, -3.8570e-03,  1.0248e-03, -8.2069e-03, -4.8477e-03,\n                      -7.0787e-03,  1.5819e-03,  1.2740e-03,  1.6320e-04,  1.5556e-02,\n                      -1.3978e-02,  2.1661e-03,  8.4044e-03,  6.5088e-03, -8.1418e-03,\n                      -2.7170e-02, -1.8035e-03,  7.9095e-03, -9.8955e-04,  3.1099e-03,\n                       3.8144e-03, -6.9695e-03, -2.8389e-03,  3.5303e-03, -2.2450e-03,\n                       2.3019e-02,  3.6198e-03, -1.4141e-02, -8.9390e-04,  8.0839e-04,\n                       2.1792e-03, -6.7934e-03,  2.7428e-03,  1.0692e-02,  6.5537e-03,\n                      -1.6569e-03,  2.1365e-04,  6.3157e-03, -3.6536e-03,  1.3239e-02,\n                       2.4118e-02,  2.7549e-03, -9.5546e-03, -6.4036e-03, -2.4832e-03,\n                       3.8148e-03,  2.9308e-02, -1.7360e-02, -6.2717e-03,  1.7773e-02,\n                      -8.8688e-03,  4.9660e-03, -3.3134e-03, -2.5599e-03, -2.8106e-03,\n                      -1.9583e-02, -4.8859e-03,  8.9152e-03,  2.2987e-03,  1.0750e-02,\n                      -3.8502e-03, -1.2379e-02,  2.5645e-03,  8.5866e-03,  3.9849e-03,\n                       2.2283e-02, -1.0899e-02,  1.6382e-03, -8.6479e-03,  6.8421e-03,\n                       2.0292e-02, -8.4558e-03, -1.2683e-02,  2.4370e-03,  4.2402e-03,\n                      -1.9779e-03,  8.8889e-03,  2.2142e-03, -2.9436e-03, -7.6205e-03,\n                      -1.6527e-03, -9.3781e-03, -1.6059e-02,  4.9098e-03,  1.7719e-02,\n                       1.1268e-03,  4.5272e-03, -2.3111e-02, -1.0815e-02, -1.5505e-03,\n                      -1.4342e-02,  1.4162e-03, -1.6417e-02, -3.0871e-04, -9.0441e-03,\n                       7.5719e-03, -3.3714e-03, -7.9211e-03,  1.6202e-04,  7.1034e-03,\n                      -6.5485e-03, -5.9732e-03, -1.0161e-02,  1.6816e-02, -7.3445e-04,\n                      -5.0978e-03,  1.3858e-02, -2.6457e-03, -1.6412e-03,  1.2137e-02,\n                      -3.5671e-03,  5.6266e-03, -5.3428e-04, -3.7545e-03, -2.8636e-03,\n                       1.1412e-02, -9.0036e-03,  9.8350e-04, -4.1585e-03,  2.2418e-03,\n                      -5.1304e-03,  1.6559e-02,  8.9503e-04, -7.0944e-03,  5.1284e-03,\n                       1.1845e-02,  3.3579e-03, -4.4089e-04,  4.6996e-03,  1.3259e-02,\n                      -1.8516e-03, -1.2089e-02,  2.1373e-03, -6.1495e-03,  2.6007e-02,\n                       2.1075e-03, -5.1056e-03, -4.4465e-03,  5.1484e-04, -1.0242e-02,\n                       5.0998e-03,  1.3938e-03,  3.9462e-04,  5.3263e-03, -4.4960e-02,\n                       7.2534e-03,  7.6715e-03,  5.2885e-03, -8.9843e-03,  4.9400e-03,\n                       1.4129e-02, -3.9333e-03,  2.0535e-02,  4.1835e-03,  4.9535e-03,\n                       4.4874e-04,  5.3841e-03, -2.9745e-04,  4.2157e-03, -4.6696e-03,\n                       8.1805e-03,  4.6550e-03, -4.4817e-03,  8.0739e-03,  3.0075e-02,\n                       1.2533e-02,  2.1495e-03,  3.5032e-03, -8.6826e-04,  2.3590e-03,\n                       3.2559e-02, -1.1696e-02, -1.6224e-02,  5.3513e-04, -8.8976e-05,\n                       1.2228e-02,  7.6625e-03, -4.8871e-03,  6.8151e-03,  5.7522e-03,\n                       2.3291e-03, -3.3081e-03, -6.6636e-03, -5.0096e-03,  1.0369e-02,\n                      -1.1179e-02,  6.5450e-04, -4.8026e-03,  2.7444e-03,  8.8289e-03,\n                      -6.0100e-03,  4.1955e-03, -1.1294e-02, -5.3229e-03,  5.1656e-03,\n                      -1.8913e-04,  1.3690e-02,  3.2304e-04, -7.5130e-03, -9.6372e-04,\n                      -5.4269e-03,  1.7650e-02, -3.5258e-03, -6.7098e-03,  5.0208e-03,\n                      -4.1555e-03, -2.4614e-03,  1.2237e-02, -4.5231e-03,  7.5165e-03,\n                       3.5583e-04, -5.9327e-04, -6.9803e-03,  7.0844e-03, -1.9915e-02,\n                      -9.5218e-03,  1.4190e-05, -2.3538e-03, -5.4070e-03, -8.3811e-03,\n                       1.3019e-02,  8.5020e-04, -1.1403e-02, -2.0891e-02, -1.0737e-02,\n                       2.6062e-03,  5.4201e-03, -4.6595e-03,  8.2231e-03, -2.3970e-04,\n                       6.1895e-03, -4.1156e-03, -8.2093e-03,  1.0638e-02,  1.1419e-02,\n                       6.8425e-03, -1.2845e-02, -4.8568e-03, -5.5232e-02, -2.5987e-03,\n                      -7.4759e-03,  7.6543e-04, -5.6620e-03, -8.0746e-02, -6.3228e-03,\n                      -6.2248e-02, -1.1335e-02, -3.7403e-03, -4.9320e-03, -1.6238e-02,\n                      -3.2885e-03,  1.6623e-04,  4.5981e-05,  2.8638e-03,  1.1233e-04,\n                       2.0829e-02, -1.2735e-02,  5.2867e-03, -6.3919e-04, -1.5171e-02,\n                       1.2036e-02,  2.1654e-02, -2.8690e-02, -9.4365e-04, -1.3017e-02,\n                      -2.1033e-03,  5.1818e-03, -7.7472e-03,  1.6041e-02, -2.1315e-03,\n                      -1.2918e-02, -5.8960e-03, -1.1717e-02, -2.6359e-03],\n                     requires_grad=True)),\n             ('features.5.4.block.0.scale', tensor(0.0047)),\n             ('features.5.4.block.0.zero_point', tensor(64)),\n             ('features.5.4.block.2.weight',\n              tensor([0.7513, 1.2190, 1.2296, 1.5784, 0.8334, 0.9401, 1.3731, 1.0792, 0.5865,\n                      1.9963, 1.2678, 1.0091, 1.0296, 0.9020, 0.6628, 0.8360, 1.2441, 1.0500,\n                      0.5826, 0.8769, 0.5676, 1.2740, 1.0487, 0.6847, 1.3683, 1.6019, 1.4243,\n                      1.8026, 0.6735, 0.7571, 0.9468, 1.5641, 0.9869, 0.6488, 0.7165, 0.7755,\n                      1.6120, 1.0353, 0.7125, 0.6852, 0.8469, 1.4598, 0.8648, 0.8176, 0.5985,\n                      0.8208, 1.1348, 1.0350, 1.1707, 1.0604, 0.9872, 1.0180, 1.0292, 0.6836,\n                      1.2994, 0.9726, 0.7866, 0.7691, 1.4583, 0.5765, 1.9497, 1.5486, 1.8241,\n                      0.8810, 0.5854, 1.2020, 1.6746, 1.4633, 0.8686, 1.3855, 1.2308, 1.2224,\n                      0.7914, 1.8808, 0.6309, 0.6706, 1.0242, 0.8036, 1.3560, 0.7178, 1.2046,\n                      1.0073, 1.0666, 1.5788, 0.6079, 1.1256, 0.9783, 0.6742, 1.4211, 0.5930,\n                      1.7782, 0.6485, 1.3896, 1.4563, 0.8969, 1.0151, 1.1388, 0.8124, 0.6503,\n                      0.8438, 0.7967, 0.7098, 0.7920, 1.5508, 1.3046, 1.3517, 1.0955, 0.7030,\n                      0.5990, 0.6597, 0.8389, 1.0955, 1.2941, 1.4159, 1.5645, 0.5125, 0.5826,\n                      0.8183, 1.1703, 0.8026, 0.9148, 1.3377, 1.0549, 1.2484, 0.8705, 0.7436,\n                      1.6410, 0.9368, 1.0507, 1.1719, 1.0157, 0.8002, 0.9112, 0.7857, 1.4571,\n                      1.5225, 1.0395, 1.6646, 1.0650, 1.7002, 0.9467, 0.6283, 0.9194, 1.6228,\n                      1.6690, 1.2872, 0.9552, 1.0253, 0.6065, 0.7597, 0.7310, 1.0543, 0.5917,\n                      1.0829, 0.6960, 1.0962, 0.0594, 1.2595, 1.5608, 1.2128, 0.7728, 1.0027,\n                      0.9143, 1.3168, 0.4765, 0.9342, 0.5685, 0.6218, 0.6277, 0.8365, 0.7494,\n                      0.6800, 0.7140, 1.1186, 0.7828, 1.2783, 0.7330, 1.6565, 0.6685, 0.5979,\n                      1.9774, 0.8784, 0.5841, 0.7290, 1.0593, 1.1336, 1.8289, 0.6554, 1.4062,\n                      1.1748, 1.2490, 0.8611, 1.1322, 1.9184, 1.0106, 1.0280, 1.3847, 1.6174,\n                      0.8569, 1.0222, 0.9079, 0.7408, 0.7845, 0.8962, 1.0254, 0.7979, 1.0201,\n                      1.0943, 0.6299, 0.9897, 1.6831, 1.4110, 1.2796, 1.2730, 1.1278, 0.8688,\n                      0.9878, 1.5398, 0.6323, 1.5780, 1.7984, 1.9663, 0.9007, 1.3665, 0.5396,\n                      0.7902, 0.9548, 1.3893, 0.6825, 0.8123, 0.9135, 1.1172, 0.7855, 1.4029,\n                      0.7728, 1.6444, 1.5810, 1.3109, 0.9449, 1.2390, 1.1728, 0.6990, 1.0867,\n                      0.6856, 0.6315, 0.7825, 0.8642, 1.9578, 1.1829, 1.6223, 1.5405, 0.9058,\n                      1.5890, 0.9281, 1.1996, 1.1915, 1.2004, 0.7888, 0.6318, 0.6144, 0.6331,\n                      0.8138, 0.5930, 1.1834, 0.8556, 1.1891, 0.7480, 0.7602, 0.7944, 1.3335,\n                      1.1030, 1.4981, 0.8531, 1.1233, 1.3158, 1.1423, 1.1814, 0.6476, 1.2583,\n                      0.6779, 1.6920, 0.8022, 0.6263, 1.6864, 1.0777, 0.8157, 1.7796, 0.6944,\n                      0.7825, 0.9193, 1.6685, 1.2040, 0.8824, 1.1166, 0.6535, 0.7911, 0.6748,\n                      1.8848, 1.1572, 1.2959, 0.7295, 0.6571, 0.9063, 0.9501, 1.8334, 1.4184,\n                      0.8341, 0.8700, 1.4109, 1.7441, 0.6202, 0.9302, 0.8124, 0.8791, 0.7333,\n                      0.8435, 0.6033, 1.5843, 1.5704, 1.8752, 1.3353, 0.8380, 0.8729, 0.8977,\n                      1.3313, 0.8691, 1.2382, 1.0225, 0.6314, 1.5563, 1.4122, 0.8160, 0.6416,\n                      0.6171, 0.8290, 1.1724, 0.9882, 1.9561, 1.5624, 1.1114, 1.7806, 0.9802,\n                      1.5134, 0.5694, 0.7626, 0.8267, 0.8506, 1.9006, 0.7279, 1.1374, 1.6573,\n                      0.7328, 0.6529, 1.3894, 0.9779, 1.8962, 0.6834, 0.9663, 0.5880, 0.6430,\n                      1.2004, 1.2040, 0.5110, 0.6599, 0.9482, 0.8030, 0.7180, 0.6334, 1.7044,\n                      1.3731, 1.2851, 0.5827, 0.9287, 1.1121, 0.9082, 1.9101, 1.4148, 1.3661,\n                      0.6088, 0.9020, 0.9691, 1.2971, 1.1376, 0.7023])),\n             ('features.5.4.block.2.bias',\n              tensor([ 5.6371e-01,  1.6848e-01,  1.5013e-01,  2.8835e-01,  1.5281e-01,\n                       9.2586e-01,  3.6107e-01, -1.1903e-01,  3.7512e-01,  3.3924e-01,\n                      -7.0506e-01,  2.0788e-01,  3.7349e-01,  2.8681e-01,  1.1526e-01,\n                      -1.7824e-02,  1.9441e-01,  3.5491e-01,  2.4595e-01,  1.2732e-02,\n                       1.9450e-01,  2.7991e-01, -1.2763e-01,  7.8975e-02, -5.3539e-02,\n                       1.2864e+00,  4.4702e-01, -2.3778e-01,  1.0851e-01,  2.6697e+00,\n                       4.0242e-01,  3.1740e-01, -1.8197e-01,  1.7584e-01, -2.7985e-01,\n                       7.0372e-02,  3.2088e-01,  7.5314e-02,  6.8622e-01,  1.1568e-01,\n                       2.0746e-02,  1.4803e-01,  2.8272e-01,  3.1827e-01,  3.3724e-01,\n                       2.8311e-01,  9.9539e-02,  7.0025e-02, -1.1466e-02,  9.6681e-02,\n                       2.1299e-01, -4.1137e-01,  2.8526e-01, -2.5311e-02,  1.8124e-01,\n                       1.3854e-01,  6.7405e-01, -1.7130e-01, -4.8180e-01,  7.6484e-02,\n                       1.3953e-01, -3.3920e-01,  8.7304e-02,  1.1523e-01,  8.3196e-02,\n                      -2.1385e-01, -1.4262e-01, -1.2255e-01, -4.0743e-01,  2.1592e+00,\n                      -2.7840e-01,  1.5309e-01,  5.8155e-01,  2.6367e-03, -5.4324e-03,\n                       1.4859e-01,  1.8908e-01,  3.8883e-01,  1.8110e-01,  1.6871e-01,\n                       3.1238e-02,  1.6929e-02, -1.1987e-01,  5.7991e-02,  1.6193e-01,\n                       7.8504e-02,  1.2418e-01,  3.7079e-01,  1.0582e-01,  1.8852e-01,\n                       1.6109e-01, -5.7092e-02,  1.9046e-01,  2.7760e-02,  2.5729e-01,\n                       3.1151e-01,  3.1230e-01, -3.4328e-02, -7.9403e-03,  1.7703e-01,\n                       1.7247e-01, -4.6216e-02, -3.2826e-01,  1.6200e-01,  3.2180e-01,\n                       2.0081e-01,  3.7293e-01, -8.1621e-02,  1.5201e-01, -1.3860e+00,\n                      -1.6057e+00,  8.6487e-02,  1.3537e-01,  3.3118e-01,  1.6628e-01,\n                       2.0395e-01,  1.2028e-01, -3.5493e-02,  8.0454e-01, -1.3992e-01,\n                       4.5853e-01,  2.9074e-01, -8.4330e-03, -3.6312e-02,  2.5271e-01,\n                       6.2811e-01, -1.1958e-01,  9.9073e-02,  1.9346e-01,  4.2799e-02,\n                      -1.8838e-01,  2.0637e-01,  5.6790e-03,  1.2260e-01,  1.3112e-01,\n                      -1.0290e+00,  9.3886e-02, -1.4504e+00,  3.1955e-01, -1.2871e-01,\n                       5.6456e-02,  2.8433e-01,  3.9696e-02, -2.1184e-01, -1.0018e-01,\n                       1.6842e-01,  9.0093e-03, -1.2728e-02,  2.9995e-01,  4.3291e-02,\n                      -4.5031e-01,  4.2846e-02,  2.5361e-01,  1.8090e-01,  2.7977e-01,\n                      -1.9361e-01,  2.5594e-02,  4.9275e-01,  3.0243e-01, -5.7274e-01,\n                       1.7802e-01,  1.8280e-02,  1.8217e-01,  1.7131e-01,  7.9909e-02,\n                       4.8190e-01,  2.9451e-01, -2.4770e-02,  9.2544e-02, -2.1356e-01,\n                       2.4115e-01,  4.7871e-01,  6.5348e-01, -1.5167e-01,  3.1411e-02,\n                       1.4072e-01,  2.7324e-01,  6.9320e-02,  1.7461e-01,  8.4934e-02,\n                      -1.1744e-01,  2.2846e-01,  5.3281e-01,  2.1502e-01, -1.6847e-01,\n                       2.7351e-01, -2.4030e-01,  1.2446e-01,  1.0141e-01,  2.8667e-01,\n                       5.3359e-02,  1.1186e+00,  4.7978e-01,  5.3419e-02, -3.1396e-01,\n                       1.0998e-01,  1.8192e-01, -5.6564e-01,  2.6492e-01,  8.0966e-02,\n                       3.6316e-01,  1.5304e-01,  5.9991e-01,  1.6939e-01,  2.0317e-01,\n                      -1.5481e-01,  2.5898e-01,  4.1099e-01,  1.6889e-01, -2.9896e-01,\n                       2.0239e-01,  2.6576e-01,  3.3134e-01, -3.3631e-01,  3.6262e-01,\n                       3.6158e-01, -5.5602e-02,  1.2616e-01,  6.0913e-01,  3.0176e-02,\n                       1.9386e-01, -4.5697e-02,  1.2854e-01,  1.6162e-01,  3.3309e-01,\n                       3.1489e-01,  2.6501e-01,  4.0970e-02,  7.6026e-02, -1.1451e-02,\n                       2.3009e-01, -4.0747e-01,  1.1661e-01,  3.4015e-01, -1.2285e-02,\n                      -6.1523e-02,  3.7092e-02,  7.2095e-02, -1.3814e-01, -7.6990e-02,\n                       1.1207e-01,  2.7247e-01,  1.0289e-02,  8.4603e-01, -8.6240e-02,\n                      -5.8868e-02,  2.1148e-01,  3.8694e-01,  1.2660e-01,  3.2494e-01,\n                       3.0627e-02,  7.2380e-03,  1.0000e-01,  1.6846e-01,  1.1416e+00,\n                      -1.6107e-02, -1.4354e-01, -5.3036e-02,  4.5391e-01,  2.6748e-01,\n                      -1.3896e-01,  2.9929e-01, -2.0669e-01,  1.5819e-02,  2.8561e-01,\n                      -5.3284e-02,  1.5963e-01,  1.4055e-01,  2.0643e-02,  2.1124e-01,\n                       1.2382e-02, -6.7575e-02,  1.4667e-01, -4.9807e-02,  1.7388e+00,\n                      -1.6714e-01, -2.3293e-02,  2.3977e-01,  2.4654e-01,  8.2275e-02,\n                       1.3276e+00,  3.2435e-01,  2.3781e-01, -1.3074e-02,  2.6030e-01,\n                      -1.5678e-01, -1.0637e-01,  4.0222e-01,  2.1657e-02, -1.5860e-01,\n                      -5.1576e-01,  1.8556e-02,  1.8980e-01,  3.6723e-01, -8.2649e-02,\n                       3.6998e-01,  2.1295e-01,  1.8709e-01, -7.3033e-03, -6.8515e-01,\n                       2.7889e-01,  1.4155e-01,  2.6449e-01,  1.8131e-01, -1.5962e-01,\n                       1.2591e-01, -2.6586e-01,  1.9828e-01,  2.6152e-01, -6.3185e-02,\n                       2.3943e-01, -8.1469e-01,  2.0484e-01, -7.1712e-02,  1.5756e-01,\n                       1.2209e-01,  7.9736e-02, -3.1111e-01,  2.9717e-01, -1.0380e-01,\n                       6.4549e-02,  2.1761e-01,  1.9753e-01, -6.3612e-02,  8.1825e-01,\n                       1.9691e-01,  9.8272e-02,  1.1211e-01,  3.1936e-01,  3.7157e-01,\n                      -2.9355e-01, -1.6932e-02,  4.7350e-01,  2.5025e-01,  4.0852e-01,\n                       3.7596e-02,  6.8798e-03,  2.5955e-01, -9.5323e-03, -3.1050e-02,\n                      -1.2688e-01,  6.8986e-02,  3.2167e-01, -1.5393e-02,  2.3108e-01,\n                      -4.8778e-02,  4.7142e-01,  1.2425e-01, -9.3192e-01,  2.6480e-01,\n                       4.0990e-01,  1.2602e-01,  1.8693e-01, -1.1227e+00,  4.8794e-01,\n                      -1.3134e-02,  2.9246e-01,  2.1508e-01,  6.5552e-01,  4.0777e-01,\n                       2.9636e-02,  2.6492e-01, -7.1725e-02,  1.5702e-01,  1.8995e-02,\n                      -4.5765e-01,  2.9949e-01, -7.7158e-02,  2.2871e-01,  4.5998e-01,\n                      -3.0742e-01,  6.3264e-02,  1.1720e+00,  1.4739e-02,  3.5369e-01,\n                       2.3572e-01,  4.3292e-01,  1.9170e-01, -2.9181e-02,  1.0663e-01,\n                       3.9645e-01,  2.8215e-01,  4.2761e-01,  3.4935e-01])),\n             ('features.5.4.block.2.scale', tensor(0.1738)),\n             ('features.5.4.block.2.zero_point', tensor(59)),\n             ('features.5.4.block.3.scale', tensor(0.0887)),\n             ('features.5.4.block.3.zero_point', tensor(86)),\n             ('features.5.4.block.3._packed_params.dtype', torch.qint8),\n             ('features.5.4.block.3._packed_params._packed_params',\n              (tensor([[ 0.0141,  0.0039,  0.0013,  ..., -0.0064,  0.0141, -0.0051],\n                       [-0.0567,  0.0181, -0.0761,  ..., -0.0348, -0.0438,  0.0838],\n                       [-0.0009,  0.0085,  0.0775,  ...,  0.0718, -0.0312, -0.0104],\n                       ...,\n                       [-0.0204, -0.0586, -0.0433,  ..., -0.0369, -0.0318, -0.0318],\n                       [-0.0417,  0.0360,  0.0043,  ...,  0.0130,  0.0043, -0.0389],\n                       [-0.0170,  0.0483,  0.0235,  ...,  0.0170,  0.1213, -0.0587]],\n                      size=(1536, 384), dtype=torch.qint8,\n                      quantization_scheme=torch.per_channel_affine,\n                      scale=tensor([0.0013, 0.0013, 0.0009,  ..., 0.0013, 0.0014, 0.0013],\n                      dtype=torch.float64),\n                      zero_point=tensor([0, 0, 0,  ..., 0, 0, 0]), axis=0),\n               Parameter containing:\n               tensor([-0.0380, -0.0529, -0.0325,  ..., -0.0314, -0.0378, -0.0560],\n                      requires_grad=True))),\n             ('features.5.4.block.5.scale', tensor(0.0428)),\n             ('features.5.4.block.5.zero_point', tensor(72)),\n             ('features.5.4.block.5._packed_params.dtype', torch.qint8),\n             ('features.5.4.block.5._packed_params._packed_params',\n              (tensor([[ 0.0582,  0.0274,  0.0017,  ..., -0.0154,  0.0445, -0.0428],\n                       [ 0.0217, -0.0772, -0.0410,  ..., -0.0121, -0.0024, -0.0434],\n                       [ 0.0325,  0.0598,  0.0885,  ...,  0.0403,  0.0390, -0.0286],\n                       ...,\n                       [ 0.0131,  0.0175,  0.1021,  ...,  0.0160, -0.0131, -0.0569],\n                       [-0.0057,  0.0944,  0.0208,  ..., -0.0208, -0.0283,  0.0623],\n                       [-0.0093,  0.0234,  0.0499,  ..., -0.0545,  0.1044, -0.0016]],\n                      size=(384, 1536), dtype=torch.qint8,\n                      quantization_scheme=torch.per_channel_affine,\n                      scale=tensor([0.0017, 0.0024, 0.0013, 0.0025, 0.0015, 0.0015, 0.0015, 0.0015, 0.0014,\n                       0.0025, 0.0016, 0.0018, 0.0019, 0.0022, 0.0020, 0.0017, 0.0020, 0.0015,\n                       0.0014, 0.0015, 0.0027, 0.0017, 0.0019, 0.0019, 0.0016, 0.0016, 0.0021,\n                       0.0020, 0.0017, 0.0028, 0.0022, 0.0022, 0.0018, 0.0020, 0.0020, 0.0018,\n                       0.0021, 0.0014, 0.0021, 0.0015, 0.0021, 0.0025, 0.0021, 0.0022, 0.0019,\n                       0.0027, 0.0011, 0.0022, 0.0017, 0.0016, 0.0018, 0.0033, 0.0018, 0.0019,\n                       0.0016, 0.0060, 0.0033, 0.0016, 0.0016, 0.0015, 0.0019, 0.0024, 0.0018,\n                       0.0014, 0.0017, 0.0031, 0.0019, 0.0022, 0.0020, 0.0019, 0.0020, 0.0016,\n                       0.0024, 0.0018, 0.0016, 0.0020, 0.0017, 0.0026, 0.0020, 0.0017, 0.0016,\n                       0.0016, 0.0018, 0.0010, 0.0020, 0.0020, 0.0021, 0.0016, 0.0015, 0.0014,\n                       0.0016, 0.0020, 0.0017, 0.0024, 0.0014, 0.0014, 0.0023, 0.0018, 0.0022,\n                       0.0038, 0.0020, 0.0022, 0.0036, 0.0015, 0.0024, 0.0019, 0.0017, 0.0021,\n                       0.0014, 0.0041, 0.0017, 0.0018, 0.0015, 0.0019, 0.0018, 0.0018, 0.0016,\n                       0.0018, 0.0017, 0.0016, 0.0015, 0.0015, 0.0015, 0.0016, 0.0031, 0.0041,\n                       0.0022, 0.0013, 0.0017, 0.0010, 0.0042, 0.0016, 0.0026, 0.0030, 0.0016,\n                       0.0035, 0.0014, 0.0017, 0.0009, 0.0018, 0.0025, 0.0016, 0.0020, 0.0019,\n                       0.0017, 0.0017, 0.0023, 0.0016, 0.0020, 0.0020, 0.0020, 0.0016, 0.0016,\n                       0.0022, 0.0015, 0.0024, 0.0029, 0.0021, 0.0018, 0.0014, 0.0017, 0.0014,\n                       0.0016, 0.0012, 0.0017, 0.0014, 0.0015, 0.0017, 0.0016, 0.0015, 0.0015,\n                       0.0019, 0.0019, 0.0016, 0.0020, 0.0021, 0.0019, 0.0014, 0.0018, 0.0019,\n                       0.0035, 0.0026, 0.0019, 0.0021, 0.0015, 0.0023, 0.0019, 0.0016, 0.0031,\n                       0.0021, 0.0009, 0.0061, 0.0016, 0.0019, 0.0018, 0.0009, 0.0017, 0.0051,\n                       0.0018, 0.0018, 0.0027, 0.0016, 0.0016, 0.0023, 0.0015, 0.0019, 0.0013,\n                       0.0016, 0.0016, 0.0023, 0.0020, 0.0021, 0.0017, 0.0017, 0.0022, 0.0022,\n                       0.0041, 0.0010, 0.0017, 0.0023, 0.0029, 0.0019, 0.0026, 0.0016, 0.0022,\n                       0.0029, 0.0031, 0.0018, 0.0017, 0.0017, 0.0026, 0.0018, 0.0020, 0.0016,\n                       0.0028, 0.0018, 0.0020, 0.0028, 0.0031, 0.0024, 0.0010, 0.0019, 0.0016,\n                       0.0017, 0.0016, 0.0023, 0.0016, 0.0035, 0.0037, 0.0019, 0.0017, 0.0016,\n                       0.0025, 0.0015, 0.0017, 0.0016, 0.0015, 0.0019, 0.0016, 0.0019, 0.0017,\n                       0.0022, 0.0025, 0.0016, 0.0037, 0.0025, 0.0033, 0.0015, 0.0019, 0.0018,\n                       0.0015, 0.0019, 0.0017, 0.0037, 0.0020, 0.0018, 0.0013, 0.0016, 0.0018,\n                       0.0014, 0.0018, 0.0023, 0.0015, 0.0054, 0.0015, 0.0016, 0.0017, 0.0016,\n                       0.0016, 0.0018, 0.0016, 0.0028, 0.0015, 0.0009, 0.0020, 0.0016, 0.0016,\n                       0.0018, 0.0016, 0.0016, 0.0016, 0.0016, 0.0020, 0.0021, 0.0016, 0.0016,\n                       0.0020, 0.0017, 0.0016, 0.0032, 0.0015, 0.0016, 0.0019, 0.0015, 0.0022,\n                       0.0015, 0.0015, 0.0020, 0.0012, 0.0018, 0.0013, 0.0014, 0.0017, 0.0021,\n                       0.0018, 0.0019, 0.0009, 0.0019, 0.0016, 0.0023, 0.0016, 0.0017, 0.0016,\n                       0.0023, 0.0013, 0.0024, 0.0016, 0.0018, 0.0055, 0.0032, 0.0018, 0.0014,\n                       0.0017, 0.0020, 0.0031, 0.0019, 0.0025, 0.0015, 0.0022, 0.0015, 0.0022,\n                       0.0023, 0.0016, 0.0024, 0.0015, 0.0021, 0.0023, 0.0017, 0.0019, 0.0016,\n                       0.0030, 0.0015, 0.0017, 0.0017, 0.0031, 0.0013, 0.0027, 0.0016, 0.0023,\n                       0.0021, 0.0014, 0.0017, 0.0018, 0.0017, 0.0016, 0.0026, 0.0019, 0.0033,\n                       0.0014, 0.0025, 0.0017, 0.0015, 0.0019, 0.0016], dtype=torch.float64),\n                      zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n                      axis=0),\n               Parameter containing:\n               tensor([ 1.9020e-02, -4.0896e-02,  1.6688e-02, -2.6324e-02,  6.1564e-03,\n                        5.5006e-04,  9.6524e-03, -1.2214e-03, -8.1719e-03, -1.4271e-02,\n                       -4.4894e-02,  1.0935e-02,  1.6081e-01,  2.3942e-02,  4.2596e-03,\n                        2.0695e-02,  4.1764e-02,  2.6715e-03, -2.3854e-03,  1.5998e-02,\n                        3.6972e-04, -1.4837e-02,  1.2748e-01, -9.4521e-03, -2.0045e-02,\n                       -8.1072e-02,  1.7906e-02,  3.2229e-02, -1.0891e-02, -8.1006e-02,\n                        5.0321e-02,  1.7903e-02,  7.1944e-03,  2.3306e-02,  9.3503e-04,\n                       -7.5091e-03,  6.2547e-02, -5.9664e-03, -8.7375e-03, -7.4196e-03,\n                        7.3939e-03, -1.5085e-02, -3.8740e-02,  5.5710e-03, -1.4606e-02,\n                       -3.8330e-02, -7.3327e-03,  2.2335e-03, -3.7270e-03,  1.1222e-02,\n                       -2.0926e-02,  1.3489e-01, -9.0749e-04, -2.2720e-02, -8.8366e-03,\n                        9.5552e-02, -4.5121e-02, -1.8041e-03, -2.3426e-02, -1.0363e-02,\n                       -1.5434e-02,  1.4824e-02, -2.4662e-02,  1.1265e-03, -8.7371e-03,\n                        2.5190e-02, -3.9278e-03,  1.8247e-02, -1.0531e-03, -8.6229e-02,\n                        1.5124e-02, -1.5960e-03, -1.1054e-02, -1.5981e-02,  2.2478e-03,\n                        1.8582e-02,  3.7749e-03,  3.6977e-03,  1.4552e-02, -1.8880e-02,\n                       -9.7037e-03,  1.4247e-02, -5.7982e-02,  8.7554e-03, -2.6087e-02,\n                        4.3310e-03, -2.8585e-02,  1.1553e-02,  2.0229e-02, -8.5677e-03,\n                       -2.7002e-02, -4.5930e-03, -1.1189e-02, -1.9140e-02, -2.7097e-03,\n                        3.3604e-03, -1.9976e-02, -1.1941e-02,  1.1735e-02, -8.5509e-03,\n                       -4.0681e-03, -6.4622e-03,  7.7795e-02,  1.0837e-02,  3.3068e-02,\n                        1.1161e-02, -1.2150e-02, -5.6612e-03,  9.1579e-03,  1.1444e-01,\n                        9.6582e-02,  6.7733e-03,  2.1030e-02,  1.0917e-02,  3.1475e-03,\n                        7.1528e-03, -1.3514e-02, -7.9586e-03,  1.9399e-02, -8.6868e-03,\n                        3.3657e-03, -2.1663e-04, -6.8384e-03,  5.3861e-03, -5.9975e-02,\n                       -1.6695e-01,  1.2733e-02,  2.0928e-02,  1.0046e-02, -4.9167e-03,\n                       -1.2395e-02, -1.1485e-02,  5.0731e-05,  5.9435e-03,  1.1004e-02,\n                       -1.4426e-01, -1.9367e-03, -8.7277e-02, -9.8953e-03,  1.5388e-02,\n                        1.1210e-02, -1.1274e-02, -2.1996e-03, -9.1632e-03,  2.8744e-02,\n                        2.3011e-02, -5.1758e-03,  1.6828e-02, -2.8231e-02, -1.8855e-02,\n                       -6.7989e-03,  1.3876e-02,  2.2683e-02,  3.0346e-02, -4.8065e-03,\n                       -8.6057e-04, -1.5019e+00, -6.8381e-02,  5.1457e-02,  3.5566e-03,\n                        4.3343e-02,  3.5162e-04,  2.8987e-02,  1.1916e-03,  2.9882e-02,\n                        4.9402e-04,  3.7652e-03, -1.7312e-02, -3.4830e-03, -2.2547e-03,\n                       -6.7887e-04, -2.8329e-02,  3.1685e-02,  1.3219e-02, -4.2574e-02,\n                       -4.9527e-02, -1.0323e-02,  2.8582e-02,  4.2309e-03, -1.9883e-02,\n                       -7.9534e-02,  3.1228e-02,  4.1194e-03,  8.8992e-03,  2.4266e-02,\n                       -7.4203e-03,  1.1670e-04, -2.1410e-02, -3.3431e-03,  5.4532e-02,\n                       -2.0928e-03, -5.7912e-03,  6.7665e-03,  1.3494e-02,  2.0892e-02,\n                        9.0242e-03,  1.7474e-02, -6.1726e-02,  5.7712e-03, -1.3095e-02,\n                       -1.9431e-02,  3.2281e-03, -6.0353e-03,  1.7054e-02,  4.7571e-04,\n                       -6.0839e-03,  9.2517e-03,  1.1349e-02, -3.9577e-02, -5.8816e-02,\n                       -6.7746e-03, -1.1503e-01,  5.6748e-03,  7.6172e-03,  1.4122e-02,\n                       -3.5184e-02, -5.2137e-04, -9.6767e-03, -5.1339e-03,  4.5740e-02,\n                        2.9927e-02, -7.8095e-03, -4.7804e-03,  7.0721e-03,  2.3764e-03,\n                       -1.1852e-02,  4.5037e-02, -1.2803e-02,  7.8993e-03,  4.3152e-03,\n                       -4.1101e-02, -1.7688e-02,  1.1072e-02,  4.4452e-02,  1.4622e-02,\n                        6.1115e-03,  5.9613e-03, -2.6522e-02, -3.4022e-02,  3.2068e-02,\n                        7.7631e-03,  1.0345e-02, -6.3011e-03,  1.0773e-02,  2.3944e-02,\n                        9.3298e-02,  8.1287e-03, -5.2038e-02, -2.6673e-02, -3.8103e-02,\n                       -4.1087e-04,  2.7272e-02,  2.1084e-03,  1.9790e-03,  6.9549e-03,\n                        2.1552e-02,  6.0602e-03,  2.5466e-02, -4.4019e-03,  3.3074e-04,\n                        2.7165e-02,  1.7819e-02,  1.1170e-02,  2.3330e-03, -1.0381e-01,\n                       -2.2257e-02, -1.5031e-02, -1.0199e-02,  1.3974e-02,  1.0497e-02,\n                       -3.4106e-02,  6.0320e-03, -1.4672e-02,  4.7278e-03,  1.1016e-01,\n                        5.3286e-06,  1.5138e-02,  6.5168e-03, -5.0413e-03,  8.7982e-03,\n                        7.9247e-02, -1.4483e-02, -4.6403e-03,  3.1841e-03,  7.8766e-03,\n                       -3.3551e-03,  6.1430e-03, -2.5259e-02,  2.6908e-02, -1.1270e-02,\n                       -9.8878e-02,  6.0911e-03,  8.7835e-03,  4.7309e-03, -7.0829e-03,\n                        1.4285e-02,  1.9702e-03, -7.9629e-03, -5.3425e-03,  3.0769e-02,\n                       -4.5127e-02, -3.5842e-04,  3.4123e-04,  2.0663e-02, -7.2391e-03,\n                       -2.9248e-02, -3.4058e-02,  2.4459e-02, -1.1755e-02,  3.0582e-02,\n                        2.9822e-03,  3.4092e-02,  2.0932e-03,  2.5656e-02,  7.7372e-02,\n                        9.1771e-03, -2.5023e-03,  2.4668e-03,  2.5800e-03,  5.1455e-02,\n                       -7.0409e-03,  8.0016e-03, -1.7029e-02,  7.8405e-03, -1.3608e-02,\n                       -6.7472e-02,  1.1151e-02,  1.8702e-02,  4.7354e-02, -1.7185e-02,\n                        1.3931e-02, -2.1290e-02,  1.4789e-02,  1.3478e-02, -7.4932e-03,\n                       -5.4932e-02,  2.3894e-03,  6.6941e-04, -6.3136e-02,  1.1619e-02,\n                        2.0882e-04, -3.2057e-02, -1.7652e-02,  1.0884e-02,  5.8970e-02,\n                        2.7167e-05, -1.0828e-02,  1.6907e-02,  1.8482e-01, -1.7380e-02,\n                        1.3083e-02,  9.5931e-03,  6.6683e-03,  7.1911e-02, -1.3409e-04,\n                       -9.0867e-02, -5.1721e-03,  1.1163e-02, -3.7356e-02,  3.1413e-02,\n                        3.6033e-02, -3.1603e-03, -3.6655e-03, -1.1271e-02,  2.8857e-02,\n                       -3.9083e-02,  9.3598e-03,  7.8622e-03,  8.3071e-03, -2.2880e-02,\n                       -1.7709e-02, -5.0136e-03, -4.8571e-02, -4.4898e-04,  1.9894e-03,\n                        3.1172e-02,  1.3515e-01, -3.5221e-02,  1.9432e-02, -9.8250e-03,\n                       -1.4645e-02, -3.2317e-03,  4.8340e-02,  5.1375e-04],\n                      requires_grad=True))),\n             ('features.5.5.layer_scale',\n              tensor([[[ 0.0767]],\n              \n                      [[ 0.0472]],\n              \n                      [[ 0.0819]],\n              \n                      [[-0.0919]],\n              \n                      [[-0.1072]],\n              \n                      [[-0.0944]],\n              \n                      [[-0.0411]],\n              \n                      [[-0.1167]],\n              \n                      [[ 0.1227]],\n              \n                      [[-0.0547]],\n              \n                      [[-0.0580]],\n              \n                      [[-0.1183]],\n              \n                      [[-0.1408]],\n              \n                      [[ 0.1240]],\n              \n                      [[ 0.1160]],\n              \n                      [[-0.0787]],\n              \n                      [[-0.1232]],\n              \n                      [[-0.1038]],\n              \n                      [[ 0.1197]],\n              \n                      [[-0.0893]],\n              \n                      [[ 0.1694]],\n              \n                      [[ 0.0872]],\n              \n                      [[ 0.1810]],\n              \n                      [[-0.1124]],\n              \n                      [[-0.0578]],\n              \n                      [[-0.0328]],\n              \n                      [[ 0.0534]],\n              \n                      [[ 0.0499]],\n              \n                      [[ 0.1374]],\n              \n                      [[-0.2618]],\n              \n                      [[-0.1187]],\n              \n                      [[-0.0586]],\n              \n                      [[ 0.1007]],\n              \n                      [[-0.1057]],\n              \n                      [[ 0.1089]],\n              \n                      [[-0.1172]],\n              \n                      [[-0.0800]],\n              \n                      [[-0.1468]],\n              \n                      [[-0.1186]],\n              \n                      [[ 0.1053]],\n              \n                      [[-0.1086]],\n              \n                      [[-0.0795]],\n              \n                      [[ 0.1319]],\n              \n                      [[-0.1142]],\n              \n                      [[ 0.1207]],\n              \n                      [[ 0.1329]],\n              \n                      [[-0.3184]],\n              \n                      [[ 0.1284]],\n              \n                      [[ 0.0664]],\n              \n                      [[ 0.1012]],\n              \n                      [[-0.1349]],\n              \n                      [[ 0.1980]],\n              \n                      [[-0.0726]],\n              \n                      [[-0.1105]],\n              \n                      [[ 0.0779]],\n              \n                      [[-0.0573]],\n              \n                      [[ 0.1324]],\n              \n                      [[ 0.0792]],\n              \n                      [[ 0.0484]],\n              \n                      [[-0.1020]],\n              \n                      [[ 0.0498]],\n              \n                      [[-0.1102]],\n              \n                      [[ 0.0434]],\n              \n                      [[ 0.0990]],\n              \n                      [[ 0.1192]],\n              \n                      [[ 0.0804]],\n              \n                      [[-0.0415]],\n              \n                      [[ 0.1401]],\n              \n                      [[-0.1464]],\n              \n                      [[-0.0389]],\n              \n                      [[-0.1072]],\n              \n                      [[-0.0708]],\n              \n                      [[-0.1712]],\n              \n                      [[-0.0373]],\n              \n                      [[-0.0922]],\n              \n                      [[-0.1186]],\n              \n                      [[ 0.0945]],\n              \n                      [[-0.2081]],\n              \n                      [[ 0.0952]],\n              \n                      [[ 0.1011]],\n              \n                      [[ 0.1099]],\n              \n                      [[ 0.1170]],\n              \n                      [[-0.0916]],\n              \n                      [[-0.0950]],\n              \n                      [[ 0.1058]],\n              \n                      [[ 0.0975]],\n              \n                      [[ 0.1141]],\n              \n                      [[-0.0983]],\n              \n                      [[ 0.0557]],\n              \n                      [[ 0.1122]],\n              \n                      [[ 0.0817]],\n              \n                      [[ 0.1256]],\n              \n                      [[-0.0766]],\n              \n                      [[ 0.0889]],\n              \n                      [[-0.1097]],\n              \n                      [[ 0.1084]],\n              \n                      [[-0.0631]],\n              \n                      [[-0.1263]],\n              \n                      [[-0.1130]],\n              \n                      [[-0.1077]],\n              \n                      [[-0.0882]],\n              \n                      [[ 0.1116]],\n              \n                      [[-0.1300]],\n              \n                      [[-0.0851]],\n              \n                      [[ 0.0659]],\n              \n                      [[ 0.0722]],\n              \n                      [[ 0.0665]],\n              \n                      [[-0.1394]],\n              \n                      [[ 0.0988]],\n              \n                      [[-0.2080]],\n              \n                      [[ 0.1201]],\n              \n                      [[-0.1118]],\n              \n                      [[ 0.1394]],\n              \n                      [[-0.0574]],\n              \n                      [[-0.1015]],\n              \n                      [[ 0.1390]],\n              \n                      [[ 0.1133]],\n              \n                      [[ 0.0913]],\n              \n                      [[ 0.0493]],\n              \n                      [[-0.1176]],\n              \n                      [[ 0.0923]],\n              \n                      [[-0.0765]],\n              \n                      [[-0.0851]],\n              \n                      [[-0.2060]],\n              \n                      [[-0.1350]],\n              \n                      [[ 0.1328]],\n              \n                      [[ 0.0397]],\n              \n                      [[-0.1054]],\n              \n                      [[-0.0815]],\n              \n                      [[ 0.2391]],\n              \n                      [[ 0.3346]],\n              \n                      [[-0.1230]],\n              \n                      [[-0.1443]],\n              \n                      [[-0.0919]],\n              \n                      [[ 0.0732]],\n              \n                      [[ 0.1370]],\n              \n                      [[ 0.0738]],\n              \n                      [[ 0.0453]],\n              \n                      [[-0.2569]],\n              \n                      [[-0.0664]],\n              \n                      [[-0.1095]],\n              \n                      [[-0.1207]],\n              \n                      [[-0.0939]],\n              \n                      [[ 0.0666]],\n              \n                      [[ 0.0483]],\n              \n                      [[ 0.1044]],\n              \n                      [[ 0.0914]],\n              \n                      [[-0.0897]],\n              \n                      [[-0.0978]],\n              \n                      [[-0.0798]],\n              \n                      [[-0.0848]],\n              \n                      [[ 0.0924]],\n              \n                      [[-0.1222]],\n              \n                      [[-0.1037]],\n              \n                      [[-0.1319]],\n              \n                      [[ 0.1341]],\n              \n                      [[ 1.1898]],\n              \n                      [[-0.1075]],\n              \n                      [[-0.0862]],\n              \n                      [[ 0.0717]],\n              \n                      [[ 0.1371]],\n              \n                      [[-0.0837]],\n              \n                      [[-0.0947]],\n              \n                      [[-0.0823]],\n              \n                      [[-0.1597]],\n              \n                      [[-0.0915]],\n              \n                      [[ 0.1239]],\n              \n                      [[ 0.1142]],\n              \n                      [[-0.1065]],\n              \n                      [[-0.1139]],\n              \n                      [[-0.1065]],\n              \n                      [[ 0.0824]],\n              \n                      [[ 0.1031]],\n              \n                      [[ 0.1177]],\n              \n                      [[ 0.1073]],\n              \n                      [[ 0.0324]],\n              \n                      [[ 0.1143]],\n              \n                      [[ 0.0492]],\n              \n                      [[ 0.1125]],\n              \n                      [[ 0.1367]],\n              \n                      [[ 0.0403]],\n              \n                      [[-0.0815]],\n              \n                      [[-0.1490]],\n              \n                      [[ 0.1287]],\n              \n                      [[-0.1091]],\n              \n                      [[ 0.1359]],\n              \n                      [[-0.0399]],\n              \n                      [[-0.1207]],\n              \n                      [[ 0.1471]],\n              \n                      [[-0.0625]],\n              \n                      [[ 0.0834]],\n              \n                      [[ 0.3018]],\n              \n                      [[-0.0810]],\n              \n                      [[-0.0440]],\n              \n                      [[ 0.1147]],\n              \n                      [[ 0.2848]],\n              \n                      [[-0.0670]],\n              \n                      [[ 0.0289]],\n              \n                      [[ 0.1361]],\n              \n                      [[ 0.1352]],\n              \n                      [[ 0.0869]],\n              \n                      [[ 0.1112]],\n              \n                      [[-0.0755]],\n              \n                      [[ 0.1157]],\n              \n                      [[ 0.0857]],\n              \n                      [[-0.1059]],\n              \n                      [[ 0.0791]],\n              \n                      [[-0.0686]],\n              \n                      [[ 0.0834]],\n              \n                      [[ 0.1197]],\n              \n                      [[ 0.0599]],\n              \n                      [[-0.0969]],\n              \n                      [[-0.1015]],\n              \n                      [[ 0.0884]],\n              \n                      [[-0.0685]],\n              \n                      [[ 0.1200]],\n              \n                      [[ 0.0531]],\n              \n                      [[ 0.3194]],\n              \n                      [[-0.1044]],\n              \n                      [[ 0.1390]],\n              \n                      [[-0.0661]],\n              \n                      [[ 0.0406]],\n              \n                      [[-0.1056]],\n              \n                      [[-0.0542]],\n              \n                      [[-0.1512]],\n              \n                      [[-0.1625]],\n              \n                      [[-0.0960]],\n              \n                      [[ 0.0674]],\n              \n                      [[-0.1096]],\n              \n                      [[-0.1096]],\n              \n                      [[-0.1258]],\n              \n                      [[ 0.0834]],\n              \n                      [[-0.1454]],\n              \n                      [[-0.1045]],\n              \n                      [[ 0.1153]],\n              \n                      [[-0.0554]],\n              \n                      [[ 0.0579]],\n              \n                      [[-0.1172]],\n              \n                      [[-0.0971]],\n              \n                      [[ 0.0807]],\n              \n                      [[-0.2716]],\n              \n                      [[-0.1248]],\n              \n                      [[ 0.0798]],\n              \n                      [[ 0.1020]],\n              \n                      [[ 0.1197]],\n              \n                      [[ 0.1438]],\n              \n                      [[ 0.1146]],\n              \n                      [[-0.0859]],\n              \n                      [[ 0.1089]],\n              \n                      [[-0.0835]],\n              \n                      [[-0.0450]],\n              \n                      [[ 0.0990]],\n              \n                      [[ 0.0475]],\n              \n                      [[-0.0877]],\n              \n                      [[ 0.1131]],\n              \n                      [[-0.1020]],\n              \n                      [[ 0.0805]],\n              \n                      [[-0.1355]],\n              \n                      [[ 0.1333]],\n              \n                      [[ 0.1114]],\n              \n                      [[ 0.1144]],\n              \n                      [[ 0.1063]],\n              \n                      [[ 0.1113]],\n              \n                      [[ 0.0946]],\n              \n                      [[ 0.0700]],\n              \n                      [[-0.1263]],\n              \n                      [[ 0.1503]],\n              \n                      [[-0.0865]],\n              \n                      [[ 0.1539]],\n              \n                      [[ 0.1290]],\n              \n                      [[-0.0829]],\n              \n                      [[ 0.0520]],\n              \n                      [[ 0.1075]],\n              \n                      [[-0.1118]],\n              \n                      [[-0.0433]],\n              \n                      [[ 0.1082]],\n              \n                      [[-0.0800]],\n              \n                      [[-0.0937]],\n              \n                      [[-0.1134]],\n              \n                      [[ 0.1075]],\n              \n                      [[-0.0380]],\n              \n                      [[-0.1619]],\n              \n                      [[-0.1032]],\n              \n                      [[ 0.0809]],\n              \n                      [[-0.0581]],\n              \n                      [[ 0.1324]],\n              \n                      [[ 0.0440]],\n              \n                      [[-0.1069]],\n              \n                      [[ 0.0928]],\n              \n                      [[-0.0887]],\n              \n                      [[-0.0964]],\n              \n                      [[-0.1475]],\n              \n                      [[ 0.1073]],\n              \n                      [[ 0.2861]],\n              \n                      [[ 0.1153]],\n              \n                      [[-0.0798]],\n              \n                      [[ 0.1077]],\n              \n                      [[-0.0381]],\n              \n                      [[ 0.0522]],\n              \n                      [[ 0.0578]],\n              \n                      [[ 0.0970]],\n              \n                      [[ 0.1036]],\n              \n                      [[-0.1653]],\n              \n                      [[ 0.0983]],\n              \n                      [[-0.0438]],\n              \n                      [[ 0.0673]],\n              \n                      [[-0.0885]],\n              \n                      [[-0.0790]],\n              \n                      [[-0.0601]],\n              \n                      [[ 0.0987]],\n              \n                      [[-0.1107]],\n              \n                      [[-0.0934]],\n              \n                      [[ 0.1157]],\n              \n                      [[ 0.1227]],\n              \n                      [[-0.0276]],\n              \n                      [[-0.0800]],\n              \n                      [[ 0.1109]],\n              \n                      [[-0.0765]],\n              \n                      [[-0.3035]],\n              \n                      [[ 0.0488]],\n              \n                      [[ 0.1722]],\n              \n                      [[-0.0889]],\n              \n                      [[-0.1116]],\n              \n                      [[ 0.1939]],\n              \n                      [[-0.1133]],\n              \n                      [[ 0.1318]],\n              \n                      [[-0.0936]],\n              \n                      [[ 0.1243]],\n              \n                      [[ 0.1172]],\n              \n                      [[-0.1388]],\n              \n                      [[-0.0902]],\n              \n                      [[-0.0976]],\n              \n                      [[-0.0983]],\n              \n                      [[ 0.1178]],\n              \n                      [[-0.1093]],\n              \n                      [[-0.0819]],\n              \n                      [[ 0.1190]],\n              \n                      [[-0.0388]],\n              \n                      [[-0.0318]],\n              \n                      [[-0.1749]],\n              \n                      [[-0.0413]],\n              \n                      [[ 0.1362]],\n              \n                      [[-0.0531]],\n              \n                      [[ 0.1304]],\n              \n                      [[-0.0817]],\n              \n                      [[-0.1015]],\n              \n                      [[-0.1698]],\n              \n                      [[ 0.0378]],\n              \n                      [[ 0.0725]],\n              \n                      [[-0.0913]],\n              \n                      [[-0.0548]],\n              \n                      [[ 0.0998]],\n              \n                      [[-0.1256]],\n              \n                      [[-0.0308]],\n              \n                      [[ 0.0815]],\n              \n                      [[ 0.0229]],\n              \n                      [[ 0.1293]],\n              \n                      [[-0.0957]],\n              \n                      [[-0.1147]],\n              \n                      [[ 0.1153]],\n              \n                      [[ 0.0318]],\n              \n                      [[ 0.0729]],\n              \n                      [[ 0.1361]],\n              \n                      [[-0.0965]],\n              \n                      [[-0.1051]],\n              \n                      [[-0.1024]],\n              \n                      [[ 0.1259]],\n              \n                      [[ 0.0851]],\n              \n                      [[ 0.1256]],\n              \n                      [[ 0.1222]],\n              \n                      [[-0.0625]],\n              \n                      [[ 0.1162]],\n              \n                      [[ 0.1254]],\n              \n                      [[ 0.1298]],\n              \n                      [[ 0.1111]],\n              \n                      [[-0.1087]],\n              \n                      [[ 0.0775]],\n              \n                      [[-0.1168]],\n              \n                      [[ 0.1211]],\n              \n                      [[-0.1292]],\n              \n                      [[-0.0875]],\n              \n                      [[-0.0592]],\n              \n                      [[ 0.1270]],\n              \n                      [[-0.0817]]])),\n             ('features.5.5.block.0.weight',\n              tensor([[[[-0.0019,  0.0019, -0.0135,  ...,  0.0058, -0.0019,  0.0097],\n                        [-0.0135,  0.0000, -0.0039,  ..., -0.0019, -0.0039, -0.0019],\n                        [ 0.0019, -0.0019, -0.0097,  ...,  0.0232,  0.0039,  0.0058],\n                        ...,\n                        [-0.0077,  0.0000, -0.0251,  ...,  0.0657,  0.0155,  0.0019],\n                        [-0.0039, -0.0058,  0.0058,  ..., -0.0077, -0.0077,  0.0019],\n                        [-0.0019,  0.0000, -0.0077,  ...,  0.0135,  0.0000,  0.0000]]],\n              \n              \n                      [[[ 0.0168,  0.0147,  0.0105,  ..., -0.0210, -0.0063, -0.0252],\n                        [ 0.0063,  0.0084,  0.0147,  ..., -0.0252, -0.0105, -0.0147],\n                        [ 0.0105,  0.0084,  0.0441,  ..., -0.0568, -0.0231, -0.0252],\n                        ...,\n                        [ 0.0168,  0.0105,  0.0610,  ..., -0.0862, -0.0231, -0.0336],\n                        [ 0.0084,  0.0084,  0.0147,  ..., -0.0147, -0.0147, -0.0105],\n                        [ 0.0168,  0.0000,  0.0063,  ..., -0.0105, -0.0084, -0.0168]]],\n              \n              \n                      [[[-0.0021, -0.0063, -0.0063,  ..., -0.0021,  0.0021, -0.0105],\n                        [-0.0063, -0.0084,  0.0063,  ..., -0.0168,  0.0084,  0.0063],\n                        [-0.0042, -0.0084, -0.0252,  ...,  0.0484, -0.0042,  0.0126],\n                        ...,\n                        [-0.0042, -0.0042, -0.0273,  ...,  0.0589,  0.0210,  0.0000],\n                        [-0.0021, -0.0126,  0.0021,  ..., -0.0105,  0.0021,  0.0063],\n                        [ 0.0021,  0.0042, -0.0084,  ...,  0.0084, -0.0084, -0.0021]]],\n              \n              \n                      ...,\n              \n              \n                      [[[ 0.0021,  0.0000,  0.0043,  ..., -0.0150, -0.0021,  0.0000],\n                        [ 0.0021,  0.0043,  0.0064,  ...,  0.0086,  0.0021,  0.0000],\n                        [-0.0043, -0.0064,  0.0150,  ..., -0.0300,  0.0000,  0.0043],\n                        ...,\n                        [ 0.0021, -0.0064,  0.0365,  ..., -0.0751, -0.0150, -0.0064],\n                        [-0.0021,  0.0064, -0.0021,  ...,  0.0107, -0.0021,  0.0000],\n                        [ 0.0043,  0.0043,  0.0021,  ..., -0.0172,  0.0021,  0.0043]]],\n              \n              \n                      [[[ 0.0080,  0.0032,  0.0000,  ..., -0.0032, -0.0016, -0.0080],\n                        [ 0.0048, -0.0097,  0.0000,  ...,  0.0000,  0.0032, -0.0064],\n                        [ 0.0000, -0.0032,  0.0080,  ...,  0.0274, -0.0129,  0.0032],\n                        ...,\n                        [ 0.0097,  0.0064,  0.0515,  ..., -0.1159, -0.0257, -0.0097],\n                        [ 0.0097,  0.0016,  0.0080,  ..., -0.0064, -0.0129, -0.0097],\n                        [ 0.0064,  0.0000,  0.0097,  ..., -0.0113, -0.0064, -0.0032]]],\n              \n              \n                      [[[ 0.0036, -0.0054, -0.0018,  ...,  0.0163,  0.0109,  0.0109],\n                        [ 0.0091, -0.0018, -0.0036,  ...,  0.0000, -0.0036,  0.0072],\n                        [-0.0072,  0.0000, -0.0326,  ...,  0.0435,  0.0091,  0.0072],\n                        ...,\n                        [-0.0091, -0.0109, -0.0561,  ...,  0.0887,  0.0417,  0.0091],\n                        [-0.0054,  0.0036,  0.0054,  ..., -0.0036,  0.0036,  0.0054],\n                        [-0.0091, -0.0127, -0.0181,  ...,  0.0163,  0.0072,  0.0091]]]],\n                     size=(384, 1, 7, 7), dtype=torch.qint8,\n                     quantization_scheme=torch.per_channel_affine,\n                     scale=tensor([0.0019, 0.0021, 0.0021, 0.0022, 0.0014, 0.0017, 0.0022, 0.0007, 0.0014,\n                      0.0022, 0.0022, 0.0013, 0.0016, 0.0006, 0.0015, 0.0019, 0.0014, 0.0018,\n                      0.0014, 0.0019, 0.0013, 0.0020, 0.0025, 0.0008, 0.0021, 0.0020, 0.0023,\n                      0.0023, 0.0012, 0.0030, 0.0011, 0.0021, 0.0019, 0.0016, 0.0011, 0.0011,\n                      0.0022, 0.0014, 0.0010, 0.0016, 0.0014, 0.0022, 0.0020, 0.0012, 0.0011,\n                      0.0009, 0.0024, 0.0016, 0.0020, 0.0020, 0.0021, 0.0026, 0.0019, 0.0009,\n                      0.0021, 0.0021, 0.0005, 0.0019, 0.0022, 0.0015, 0.0023, 0.0015, 0.0022,\n                      0.0015, 0.0017, 0.0013, 0.0023, 0.0016, 0.0019, 0.0020, 0.0015, 0.0020,\n                      0.0015, 0.0022, 0.0018, 0.0012, 0.0015, 0.0019, 0.0020, 0.0015, 0.0018,\n                      0.0021, 0.0022, 0.0028, 0.0012, 0.0020, 0.0019, 0.0016, 0.0020, 0.0015,\n                      0.0024, 0.0016, 0.0020, 0.0021, 0.0018, 0.0019, 0.0015, 0.0012, 0.0006,\n                      0.0010, 0.0014, 0.0004, 0.0027, 0.0022, 0.0023, 0.0020, 0.0018, 0.0015,\n                      0.0019, 0.0045, 0.0020, 0.0010, 0.0026, 0.0021, 0.0014, 0.0014, 0.0006,\n                      0.0020, 0.0021, 0.0011, 0.0017, 0.0021, 0.0019, 0.0021, 0.0016, 0.0029,\n                      0.0022, 0.0017, 0.0020, 0.0025, 0.0023, 0.0019, 0.0017, 0.0009, 0.0022,\n                      0.0023, 0.0018, 0.0022, 0.0020, 0.0022, 0.0015, 0.0014, 0.0016, 0.0021,\n                      0.0022, 0.0014, 0.0011, 0.0019, 0.0013, 0.0010, 0.0013, 0.0019, 0.0016,\n                      0.0018, 0.0012, 0.0013, 0.0003, 0.0022, 0.0023, 0.0020, 0.0017, 0.0018,\n                      0.0017, 0.0026, 0.0017, 0.0016, 0.0016, 0.0014, 0.0012, 0.0018, 0.0017,\n                      0.0013, 0.0011, 0.0015, 0.0013, 0.0021, 0.0015, 0.0021, 0.0009, 0.0014,\n                      0.0023, 0.0011, 0.0011, 0.0010, 0.0016, 0.0018, 0.0023, 0.0008, 0.0010,\n                      0.0013, 0.0026, 0.0021, 0.0019, 0.0023, 0.0015, 0.0018, 0.0022, 0.0023,\n                      0.0012, 0.0018, 0.0008, 0.0016, 0.0020, 0.0014, 0.0018, 0.0014, 0.0019,\n                      0.0020, 0.0016, 0.0018, 0.0021, 0.0022, 0.0014, 0.0018, 0.0020, 0.0011,\n                      0.0014, 0.0027, 0.0011, 0.0023, 0.0024, 0.0023, 0.0017, 0.0021, 0.0020,\n                      0.0024, 0.0014, 0.0022, 0.0015, 0.0020, 0.0009, 0.0017, 0.0016, 0.0021,\n                      0.0007, 0.0022, 0.0022, 0.0020, 0.0010, 0.0020, 0.0025, 0.0015, 0.0021,\n                      0.0011, 0.0008, 0.0018, 0.0018, 0.0023, 0.0017, 0.0021, 0.0022, 0.0015,\n                      0.0022, 0.0018, 0.0014, 0.0018, 0.0019, 0.0010, 0.0016, 0.0015, 0.0013,\n                      0.0010, 0.0005, 0.0016, 0.0018, 0.0018, 0.0014, 0.0019, 0.0015, 0.0019,\n                      0.0019, 0.0022, 0.0014, 0.0016, 0.0019, 0.0019, 0.0018, 0.0011, 0.0012,\n                      0.0015, 0.0017, 0.0018, 0.0011, 0.0022, 0.0020, 0.0019, 0.0022, 0.0019,\n                      0.0015, 0.0019, 0.0022, 0.0019, 0.0017, 0.0028, 0.0013, 0.0019, 0.0012,\n                      0.0022, 0.0023, 0.0021, 0.0016, 0.0015, 0.0018, 0.0013, 0.0023, 0.0021,\n                      0.0019, 0.0019, 0.0021, 0.0022, 0.0016, 0.0015, 0.0013, 0.0015, 0.0022,\n                      0.0018, 0.0015, 0.0021, 0.0022, 0.0023, 0.0022, 0.0019, 0.0013, 0.0018,\n                      0.0019, 0.0027, 0.0020, 0.0017, 0.0009, 0.0011, 0.0020, 0.0014, 0.0015,\n                      0.0007, 0.0015, 0.0018, 0.0011, 0.0022, 0.0023, 0.0017, 0.0023, 0.0016,\n                      0.0021, 0.0012, 0.0017, 0.0011, 0.0014, 0.0023, 0.0015, 0.0020, 0.0022,\n                      0.0011, 0.0012, 0.0012, 0.0019, 0.0022, 0.0011, 0.0017, 0.0012, 0.0012,\n                      0.0024, 0.0020, 0.0012, 0.0016, 0.0027, 0.0015, 0.0005, 0.0020, 0.0021,\n                      0.0020, 0.0021, 0.0006, 0.0016, 0.0015, 0.0015, 0.0021, 0.0019, 0.0021,\n                      0.0014, 0.0012, 0.0018, 0.0021, 0.0016, 0.0018], dtype=torch.float64),\n                     zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n                     axis=0)),\n             ('features.5.5.block.0.bias',\n              Parameter containing:\n              tensor([ 1.3361e-02, -1.5508e-02,  2.5772e-02,  1.5357e-02,  2.6099e-02,\n                      -1.2010e-02,  1.1549e-02,  8.9409e-03, -6.9299e-03,  6.7116e-03,\n                      -3.6246e-03, -1.4346e-02,  1.5870e-02,  1.6602e-02,  6.9188e-05,\n                       1.2393e-02, -1.3403e-03,  6.9725e-03,  1.5441e-02,  1.1201e-02,\n                      -6.9415e-04,  4.7568e-03,  2.0541e-02, -1.3426e-02, -3.5169e-03,\n                      -1.5260e-02, -4.3840e-03, -2.1708e-03,  2.0824e-03, -3.1018e-02,\n                      -3.3468e-03, -3.5021e-03,  5.7289e-03, -4.6359e-03,  2.1281e-03,\n                      -4.8630e-03, -1.4730e-02, -2.4078e-02, -6.9947e-03,  4.5359e-03,\n                       6.1470e-03, -8.0963e-03,  1.2460e-02,  3.1694e-03,  1.0922e-02,\n                       1.6564e-03,  4.4971e-03,  9.6300e-05,  9.9942e-03,  9.8702e-03,\n                      -1.1529e-03,  4.2106e-02,  5.8247e-03, -7.0472e-04, -1.1249e-02,\n                      -7.2475e-03,  2.4841e-02,  7.1477e-03, -4.3749e-04, -1.0556e-02,\n                       9.0319e-03, -6.6634e-05,  2.4124e-02, -5.4813e-03,  1.3756e-02,\n                      -1.8906e-03, -2.2240e-05, -1.0223e-02, -1.0626e-02,  1.3000e-02,\n                      -1.2535e-02,  3.3384e-03,  1.6348e-02, -4.7264e-03,  7.5225e-03,\n                      -1.1531e-03,  2.6257e-03, -5.2843e-03, -6.0695e-03,  5.9516e-03,\n                      -2.2171e-02, -1.0889e-02, -6.2197e-03,  3.5461e-03,  2.8007e-03,\n                       6.4945e-03,  2.3411e-02,  1.1006e-02,  6.1105e-03,  1.4932e-02,\n                       6.4795e-04, -1.8295e-03,  1.6263e-02, -2.0030e-02,  3.6058e-03,\n                       5.2038e-03,  1.1234e-02,  1.2060e-03,  1.4911e-02, -2.9453e-04,\n                       3.6860e-03, -2.1082e-02, -2.7490e-02, -2.8925e-03,  3.4031e-03,\n                       4.0415e-03,  2.8523e-02, -2.5486e-03,  8.8858e-03, -1.9445e-02,\n                      -1.9195e-02, -2.4128e-03, -2.7078e-03, -9.6425e-03,  3.7022e-03,\n                      -1.8342e-02, -1.2251e-02, -2.1281e-02,  2.9957e-04, -4.3585e-02,\n                       3.6812e-03,  1.5585e-02,  6.1858e-03,  5.8113e-03,  1.0755e-02,\n                      -1.2205e-02,  8.2254e-03,  5.6786e-03, -1.2870e-02, -8.7246e-04,\n                       7.0986e-03,  1.7834e-02,  1.0260e-02,  4.3587e-04, -1.6592e-02,\n                       1.1462e-02,  2.4073e-03,  1.6352e-02,  7.3834e-03,  7.9848e-04,\n                       7.0573e-03, -1.1040e-02, -2.7720e-03, -2.6700e-02,  1.0432e-02,\n                       1.2280e-02, -1.9259e-03, -1.4240e-02, -1.9734e-03, -1.7843e-03,\n                      -1.2746e-03,  3.8673e-03, -8.4373e-03, -3.7190e-03, -1.3032e-02,\n                      -1.0297e-02,  2.3559e-02, -7.1232e-03, -1.8178e-02, -2.4672e-03,\n                       1.3079e-02,  4.5405e-03, -7.1944e-04,  1.8364e-03, -1.1614e-03,\n                       2.7869e-03,  7.0856e-03, -1.1096e-02, -1.2614e-02, -7.5356e-03,\n                      -6.3084e-03, -5.9456e-04,  6.7273e-03,  1.6382e-02, -2.4018e-03,\n                       3.2450e-02,  8.4960e-03,  5.3463e-03, -6.3152e-03, -1.1178e-02,\n                      -1.9109e-02,  4.3522e-04,  3.0420e-02,  2.6727e-03, -1.2238e-02,\n                      -2.6593e-04,  3.3901e-03,  9.5123e-04, -1.2983e-03, -3.7570e-03,\n                       2.0253e-03, -3.7816e-02, -5.3138e-03, -6.7309e-03,  4.9813e-03,\n                       1.2286e-02, -9.7237e-03,  3.8864e-02,  6.6581e-04, -1.3117e-02,\n                      -2.1046e-03, -4.5783e-03, -2.1271e-02,  1.4084e-02,  7.1107e-03,\n                       4.4715e-04, -5.7038e-04,  7.3235e-03,  3.3037e-03,  3.2457e-02,\n                      -8.5250e-03, -3.8127e-03, -1.0715e-02, -5.6865e-03, -9.2457e-03,\n                       2.3203e-03,  1.5476e-02,  4.1304e-03, -1.3657e-02, -1.7597e-02,\n                       2.5212e-03, -1.1765e-03, -2.1124e-05, -6.8230e-03,  1.0684e-02,\n                       4.6127e-03,  1.8151e-02,  1.9478e-03,  5.8206e-04,  3.0661e-02,\n                      -9.0996e-05,  8.5662e-03,  9.1905e-03,  1.0665e-03,  1.6213e-03,\n                       2.0768e-03,  1.2257e-02,  1.0369e-03,  6.9821e-03, -1.8186e-03,\n                      -2.2726e-04, -5.2371e-03, -5.5388e-04, -2.7859e-02, -4.5864e-02,\n                      -1.4163e-02, -4.2300e-03, -1.5723e-02,  6.3319e-03,  5.2146e-04,\n                      -2.1018e-03, -7.0815e-03, -1.2494e-03,  1.4430e-02, -1.1649e-02,\n                      -3.8377e-03, -2.4704e-02,  3.0015e-03, -7.4884e-03, -5.3714e-03,\n                       1.8117e-03,  3.7436e-03,  1.6833e-02,  2.4365e-02, -2.5598e-02,\n                       4.5308e-03,  5.9097e-03,  7.9291e-03, -3.1987e-03,  3.9735e-03,\n                       6.4968e-03,  7.6513e-03,  1.3733e-03,  9.3624e-03,  1.1589e-02,\n                      -1.9755e-04,  3.4013e-03, -1.0675e-02, -2.2767e-03,  1.5089e-02,\n                      -4.3721e-02,  1.3850e-02, -1.0484e-03, -8.2607e-04,  3.7841e-03,\n                      -8.6846e-03, -3.7919e-04, -1.1781e-02, -1.1579e-02, -2.6417e-03,\n                       8.1472e-03,  1.6523e-03,  3.9214e-03, -6.8890e-03,  3.3717e-03,\n                      -3.5155e-03,  2.0072e-02,  1.4171e-02,  7.0560e-03, -2.2090e-03,\n                      -9.6282e-03, -2.5912e-04,  4.8618e-03, -4.9111e-03, -3.1320e-03,\n                      -5.3540e-03, -5.3099e-03, -2.5968e-02, -2.1096e-03,  1.2419e-02,\n                      -6.6845e-03,  2.7034e-02, -1.2896e-02, -6.5464e-03, -4.6711e-03,\n                       3.3387e-03,  1.1155e-02,  1.5432e-02,  1.8768e-03, -1.4621e-02,\n                      -2.7068e-03, -8.7340e-03, -1.7454e-03, -1.0307e-02,  7.7212e-04,\n                       1.9470e-02, -1.0612e-03, -9.0019e-03, -1.3730e-02,  5.1443e-03,\n                      -1.3973e-02, -3.6926e-03, -2.3322e-03,  1.5094e-02,  5.8407e-05,\n                      -2.6333e-03,  9.6532e-03, -2.5673e-03,  9.9867e-04,  1.2671e-02,\n                      -1.4313e-02, -5.2503e-03, -8.7375e-03, -2.9567e-02,  5.7390e-03,\n                      -4.5535e-02, -1.3278e-02,  9.6182e-03, -1.5602e-02, -1.2553e-02,\n                       6.8342e-03, -2.2697e-03, -1.9817e-02, -2.5269e-02, -3.8465e-03,\n                      -2.3369e-02,  1.5885e-03, -2.7065e-03, -6.9312e-03, -5.1482e-03,\n                      -9.1225e-03,  2.5122e-03, -7.6590e-03, -6.6331e-03,  7.9727e-03,\n                      -7.6339e-03,  1.6508e-02,  1.2723e-02, -5.7050e-03, -6.9940e-03,\n                       1.7270e-02,  1.2907e-02,  5.3425e-02, -6.3121e-03,  5.2046e-05,\n                      -7.4140e-03,  1.1582e-02,  2.8566e-03,  9.7875e-03,  6.7057e-03,\n                       1.9684e-02, -9.4050e-03, -1.5226e-03,  2.2004e-03],\n                     requires_grad=True)),\n             ('features.5.5.block.0.scale', tensor(0.0096)),\n             ('features.5.5.block.0.zero_point', tensor(81)),\n             ('features.5.5.block.2.weight',\n              tensor([1.1617, 1.2039, 0.9115, 1.6474, 0.8474, 0.7952, 1.4994, 0.4899, 0.6969,\n                      1.5537, 1.3566, 0.7162, 1.4190, 0.7977, 0.8422, 1.0384, 0.6634, 0.9546,\n                      0.7643, 0.9319, 0.7714, 1.0044, 1.0498, 0.4727, 1.2666, 1.7399, 1.5285,\n                      1.7876, 0.6038, 0.7369, 0.7544, 1.3142, 0.9505, 0.8099, 0.6467, 0.7786,\n                      1.4383, 0.4739, 0.6478, 0.8297, 0.7416, 1.3349, 1.2804, 0.7370, 0.7704,\n                      0.7874, 1.3115, 0.7540, 1.1572, 1.1693, 1.0058, 1.0727, 1.0212, 0.6688,\n                      1.1579, 1.6196, 0.4536, 0.9290, 1.4542, 0.7556, 1.7463, 1.0163, 1.7443,\n                      0.5573, 0.8862, 1.6700, 1.4656, 1.0883, 1.1155, 1.1906, 0.7522, 1.0946,\n                      0.8745, 1.7729, 0.9471, 0.6618, 0.9738, 1.0663, 1.0289, 0.7896, 0.8742,\n                      1.0923, 1.2668, 2.6350, 0.6602, 1.0600, 1.0929, 0.8198, 1.3442, 0.7311,\n                      1.2034, 0.8924, 1.2663, 1.1943, 0.8757, 0.9861, 0.7002, 0.9101, 0.4916,\n                      0.7583, 0.8091, 0.5974, 2.1805, 1.3283, 1.6193, 1.1064, 1.1417, 0.8661,\n                      0.9392, 0.9550, 0.9642, 0.8111, 1.1950, 1.3075, 1.0535, 0.6493, 0.4744,\n                      0.7887, 1.3333, 0.5895, 0.9415, 1.1484, 1.0243, 1.0580, 0.6429, 2.0245,\n                      1.6751, 0.4647, 1.0005, 1.1752, 1.0061, 0.9769, 1.2066, 0.6769, 1.2504,\n                      1.6336, 1.0758, 1.4510, 0.9405, 1.4674, 0.8372, 0.7089, 0.7791, 1.4763,\n                      1.4772, 0.8301, 0.8738, 0.9921, 0.7245, 0.7017, 0.9096, 0.9858, 0.7918,\n                      0.9170, 0.6866, 0.8964, 0.2348, 1.2915, 1.2880, 1.0755, 0.7408, 0.9392,\n                      0.8799, 1.3822, 0.8039, 0.8664, 0.8670, 0.7439, 0.6599, 0.8799, 0.8222,\n                      0.6232, 0.6492, 0.8974, 0.7663, 1.6020, 0.8507, 1.4927, 0.5924, 0.7309,\n                      2.0331, 0.9290, 0.6142, 0.6880, 0.7472, 1.2039, 1.6262, 0.6104, 0.8525,\n                      0.9562, 1.1551, 0.6512, 1.0349, 1.8238, 0.7972, 0.8058, 1.2182, 1.6748,\n                      0.7431, 1.0755, 0.6280, 0.8306, 0.9399, 0.7874, 0.8798, 0.6672, 0.9318,\n                      1.2018, 0.7453, 0.6954, 1.6829, 1.1428, 0.8857, 0.9060, 1.0820, 0.9517,\n                      0.6666, 1.6749, 0.4731, 2.0102, 1.6476, 1.8199, 0.7910, 1.1217, 0.9231,\n                      0.6198, 0.8249, 1.2855, 0.7698, 0.7096, 0.8270, 0.9279, 0.9237, 1.1973,\n                      0.4655, 1.4868, 1.4593, 1.2253, 1.0021, 1.1003, 1.2533, 0.7829, 1.0794,\n                      0.7286, 0.5733, 0.8968, 0.8644, 1.4979, 0.8903, 1.5198, 1.4256, 0.7122,\n                      1.4883, 0.9197, 0.8150, 0.8954, 1.0065, 0.8275, 0.8211, 0.8421, 0.7799,\n                      0.7518, 0.4722, 1.0228, 1.0211, 0.9360, 1.1929, 1.0959, 0.7615, 1.1657,\n                      1.0754, 1.3984, 0.9496, 0.8045, 1.1489, 1.2025, 0.9943, 0.6038, 0.8577,\n                      0.8130, 1.6909, 0.9879, 0.6256, 1.6285, 1.1583, 1.2406, 1.6182, 0.9032,\n                      0.7281, 0.9283, 1.7084, 1.2621, 0.8384, 0.9266, 0.5090, 0.9825, 0.4617,\n                      1.9980, 1.2665, 1.2874, 0.7850, 0.8317, 0.9746, 0.7440, 1.6678, 1.1546,\n                      0.9881, 0.8952, 1.2642, 1.6410, 0.7837, 1.0968, 0.7657, 0.8004, 0.9037,\n                      0.9482, 0.8313, 1.7961, 2.0930, 1.7588, 1.6207, 0.9398, 0.8350, 0.9979,\n                      0.8931, 1.6777, 1.6009, 0.8629, 0.4104, 0.8524, 1.2107, 0.7041, 0.7489,\n                      0.6101, 0.8594, 1.0286, 0.5804, 1.7640, 1.9554, 0.7872, 1.6111, 0.8789,\n                      1.2799, 0.6751, 0.9470, 0.4737, 0.8965, 1.6401, 0.6649, 1.0284, 1.4523,\n                      0.5993, 0.5384, 1.8905, 0.9773, 2.0403, 0.6584, 0.6624, 0.7566, 0.6984,\n                      1.9755, 1.0574, 0.5193, 0.9114, 1.3034, 0.7404, 0.4919, 1.1249, 1.7297,\n                      1.4478, 1.1962, 0.6770, 0.6513, 0.8453, 0.8483, 1.7510, 0.7833, 1.3240,\n                      0.7700, 0.7247, 1.1599, 1.2165, 1.0023, 0.8956])),\n             ('features.5.5.block.2.bias',\n              tensor([-2.8766e-02,  3.2147e-01, -3.1881e-01, -2.1758e-01, -2.4164e-01,\n                       2.6001e-01, -1.4476e-01, -2.4601e-02,  3.0289e-01, -5.1051e-02,\n                       2.5386e-01,  3.4388e-01,  4.0080e-01,  5.9012e-01,  1.5240e-01,\n                      -3.7091e-02,  1.4938e-01,  7.3397e-02, -8.4607e-03, -1.8411e-02,\n                       7.1307e-01,  7.2761e-02, -3.5975e-01,  1.5611e-01,  2.4133e-01,\n                       4.1709e-01,  1.9954e-01,  1.5542e-01,  1.1696e-01, -1.8834e+00,\n                       1.5079e-01,  1.9869e-01,  5.2255e-02,  2.5996e-01,  1.4087e-01,\n                       2.4365e-01,  4.6838e-01,  3.1587e-01,  2.0118e-01,  7.5849e-02,\n                       7.1347e-02,  2.9937e-01, -6.7802e-02,  1.2387e-01,  1.0289e-01,\n                       1.8677e-01,  1.2182e-01,  1.5021e-01,  2.4144e-02, -7.6074e-02,\n                       1.9595e-01,  5.9265e-03,  8.2993e-02,  2.2982e-01,  3.0401e-01,\n                       2.6295e-01,  5.2424e-02,  7.0412e-02,  2.0860e-01,  2.5441e-01,\n                      -8.7577e-02,  1.7347e-01, -3.3760e-01,  1.9560e-01,  4.4294e-02,\n                       2.0527e-01,  1.3748e-01,  2.6183e-01,  3.3442e-01,  4.1806e-01,\n                       2.4780e-01,  7.9162e-02,  3.6754e-01,  2.6326e-01,  9.5733e-02,\n                       1.3610e-01,  1.0410e-01,  3.2824e-01,  2.2869e-01,  1.0797e-01,\n                       4.4978e-01,  2.8251e-01,  2.6372e-01,  2.8236e-02,  1.1416e-01,\n                       1.2168e-02, -2.2249e-01, -7.8290e-03,  7.8474e-03, -5.6739e-02,\n                       1.5546e-01,  1.7288e-01, -1.7303e-01,  4.6777e-01,  1.0660e-01,\n                       1.5682e-01, -1.3334e-02,  2.2286e-01,  8.4036e-02,  1.7830e-01,\n                       1.3431e-01,  4.1577e-01,  4.0224e-01,  2.4960e-01,  3.9932e-02,\n                       5.3508e-02, -3.3251e-01,  4.0119e-01,  2.0428e-02, -2.7358e+00,\n                       1.2614e+00,  1.7040e-01,  1.9717e-01,  2.6689e-01,  3.7540e-02,\n                       2.2503e-01,  2.5177e-01,  8.9835e-01,  1.9307e-01,  6.3454e-01,\n                       9.3580e-02, -5.0800e-02,  1.4153e-02,  1.9624e-01,  1.5953e-01,\n                      -1.0048e+00, -8.1225e-02,  8.4417e-02,  3.0469e-01,  3.5408e-01,\n                       3.9441e-01, -1.4398e-01,  4.5404e-01,  1.5905e-01,  4.7451e-01,\n                       7.1269e-02,  7.3956e-02, -4.5829e-02,  2.1156e-01,  1.3072e-01,\n                       3.3458e-02,  3.2294e-01,  1.7928e-01,  5.7444e-01, -8.5325e-02,\n                      -3.2794e-02,  2.1881e-01,  3.8195e-01,  2.3450e-01,  1.6169e-01,\n                       2.0412e-01,  1.0105e-01,  2.7150e-01,  2.2242e-01,  2.9399e-01,\n                       3.1016e-01,  2.4669e-01,  2.4616e-01,  4.5663e-01,  1.8175e-01,\n                       1.4527e-01,  6.3914e-02,  6.7376e-02,  2.2489e-01,  2.2887e-01,\n                       1.9342e-01,  4.9237e-02,  3.1374e-01,  2.8099e-01,  2.6795e-01,\n                       2.3299e-01,  1.1356e-01,  1.6923e-01, -3.1315e-02,  1.7838e-01,\n                      -4.6847e-01,  4.5786e-02, -3.0951e-03,  2.1294e-01,  2.7862e-01,\n                       5.0441e-01,  1.9713e-01, -1.2150e-01,  2.6244e-01,  2.9619e-01,\n                       1.6735e-01,  9.0319e-02,  1.7283e-01,  2.0666e-01,  1.8844e-01,\n                       3.1634e-01,  1.3477e+00,  2.4382e-01,  3.0101e-01,  8.2541e-02,\n                       3.0506e-01,  2.7398e-01, -5.4091e-01,  1.4122e-01,  2.8606e-01,\n                       2.0238e-01,  2.0143e-01,  4.2297e-01, -3.4085e-02,  4.8384e-02,\n                       1.1147e-01,  1.2560e-01,  5.3952e-06,  1.0576e-01, -3.8253e-01,\n                       3.6406e-01,  1.8905e-01,  3.5855e-01,  2.3131e-01,  3.6016e-01,\n                       4.7116e-01, -6.2707e-02,  1.0173e-01,  3.1191e-01,  2.9254e-01,\n                       4.0959e-02,  2.1357e-01,  1.4123e-01,  2.2268e-01,  7.2570e-01,\n                       1.2405e+00, -1.6207e-01,  1.5625e-01,  1.9956e-01, -9.8813e-03,\n                       9.5262e-02, -1.2057e-02, -3.3183e-02,  1.2892e-01,  9.6712e-02,\n                       8.2748e-02, -1.1681e-01,  2.4396e-01,  1.9438e-01,  1.4644e-01,\n                       1.9722e-01,  2.3337e-01,  1.8445e-01,  7.6360e-01,  6.7756e-01,\n                       3.4012e-01,  1.8177e-01,  5.0111e-01,  2.0362e-01,  2.1901e-01,\n                       1.9969e-01,  1.8482e-01,  1.6913e-01, -7.2527e-02,  3.5708e-01,\n                       2.2809e-01,  5.9514e-01,  1.6680e-01,  2.8336e-01,  2.1445e-01,\n                       1.3598e-01,  1.4730e-01, -9.5570e-02, -2.4525e-01,  3.2978e-01,\n                       7.4008e-02,  3.4215e-01,  3.8375e-02,  1.8465e-01,  1.1524e-01,\n                       1.1608e-01, -4.3145e-02,  3.1255e-01,  3.0757e-01,  1.0024e-01,\n                       1.9349e-01,  9.9911e-02,  2.5413e-01,  1.5203e-01, -7.4841e-03,\n                       6.6557e-01, -7.0753e-02,  1.4417e-01,  2.1286e-01,  4.8601e-02,\n                       3.9980e-01,  1.4425e-01,  3.1023e-01,  2.7005e-01,  2.1421e-01,\n                       1.3824e-01,  2.5197e-01,  9.6378e-02,  4.9482e-01,  1.2448e-01,\n                       1.8807e-01,  5.9747e-03, -1.8624e-01, -1.0179e-02,  2.4109e-01,\n                       2.8251e-01,  1.4979e-01,  2.1809e-01,  2.4283e-01,  2.5476e-01,\n                       2.2424e-01,  2.3301e-01,  5.2408e-01,  1.7785e-01, -1.6197e-02,\n                       2.2180e-01, -2.1151e-02,  2.6775e-01,  2.6863e-01,  3.0049e-01,\n                       1.4972e-01,  1.1384e-02, -2.0800e-01,  8.2308e-02,  5.2102e-01,\n                       2.3455e-01,  3.2245e-01,  1.8945e-01,  2.1613e-01,  8.6756e-02,\n                       6.6453e-01,  1.9126e-01,  2.9232e-01,  2.1270e-01,  1.3753e-01,\n                       3.5242e-01,  1.8259e-01,  1.6453e-01,  2.3538e-01,  1.9949e-01,\n                       2.0683e-01,  6.5414e-02,  1.9062e-01,  5.5763e-02,  2.6791e-01,\n                       4.8639e-01,  1.9070e-01,  2.8354e-01,  4.5206e-01, -1.4768e-01,\n                       4.1285e-01,  2.1740e-01, -2.9368e-02, -1.4331e+00,  3.1250e-01,\n                       4.4680e-02,  2.5484e-01,  2.6146e-01, -4.7113e-01,  2.0279e-01,\n                       6.3105e-01,  1.5225e-01,  2.1797e-01,  3.3414e-01,  1.9128e-01,\n                       4.3528e-01,  7.0062e-02,  4.0626e-01,  3.0471e-01,  3.4646e-01,\n                       2.7374e-01, -2.6954e-02, -2.6120e-02,  2.2232e-01,  2.9040e-01,\n                      -9.0009e-02,  1.2367e-01, -3.9054e-01,  2.1143e-01,  1.6597e-01,\n                       2.8338e-01, -5.7108e-02,  1.7649e-01,  8.4891e-03,  1.1099e-01,\n                      -1.4543e-01,  2.8333e-01,  2.2175e-01,  9.3143e-02])),\n             ('features.5.5.block.2.scale', tensor(0.1582)),\n             ('features.5.5.block.2.zero_point', tensor(75)),\n             ('features.5.5.block.3.scale', tensor(0.0868)),\n             ('features.5.5.block.3.zero_point', tensor(82)),\n             ('features.5.5.block.3._packed_params.dtype', torch.qint8),\n             ('features.5.5.block.3._packed_params._packed_params',\n              (tensor([[ 0.0377, -0.0045,  0.0407,  ..., -0.0301,  0.0347, -0.0618],\n                       [-0.2229, -0.1042,  0.0417,  ...,  0.1937,  0.0417, -0.0583],\n                       [-0.0170, -0.0026,  0.0366,  ...,  0.0026,  0.0078,  0.0026],\n                       ...,\n                       [-0.0286, -0.0376, -0.0316,  ...,  0.0828, -0.0542, -0.0437],\n                       [-0.0508, -0.0285, -0.1016,  ..., -0.0447, -0.0427,  0.0346],\n                       [-0.0178,  0.0356, -0.0068,  ...,  0.0370, -0.0520, -0.0808]],\n                      size=(1536, 384), dtype=torch.qint8,\n                      quantization_scheme=torch.per_channel_affine,\n                      scale=tensor([0.0015, 0.0021, 0.0013,  ..., 0.0015, 0.0020, 0.0014],\n                      dtype=torch.float64),\n                      zero_point=tensor([0, 0, 0,  ..., 0, 0, 0]), axis=0),\n               Parameter containing:\n               tensor([-0.0184, -0.0372, -0.0278,  ..., -0.0425, -0.0743, -0.0481],\n                      requires_grad=True))),\n             ('features.5.5.block.5.scale', tensor(0.0630)),\n             ('features.5.5.block.5.zero_point', tensor(57)),\n             ('features.5.5.block.5._packed_params.dtype', torch.qint8),\n             ('features.5.5.block.5._packed_params._packed_params',\n              (tensor([[ 0.0619, -0.1565,  0.0052,  ...,  0.0275,  0.0017, -0.0344],\n                       [ 0.0134,  0.0269, -0.0034,  ..., -0.0437,  0.0370,  0.0571],\n                       [ 0.0341,  0.0279,  0.0357,  ..., -0.0202,  0.0326,  0.0435],\n                       ...,\n                       [ 0.0076,  0.1219, -0.0190,  ...,  0.0628,  0.0209,  0.0305],\n                       [-0.0079, -0.0347, -0.0709,  ..., -0.0047, -0.0488, -0.0016],\n                       [-0.0338, -0.0053, -0.0409,  ...,  0.0071,  0.0018,  0.0302]],\n                      size=(384, 1536), dtype=torch.qint8,\n                      quantization_scheme=torch.per_channel_affine,\n                      scale=tensor([0.0017, 0.0034, 0.0016, 0.0020, 0.0018, 0.0017, 0.0024, 0.0013, 0.0014,\n                       0.0018, 0.0017, 0.0018, 0.0025, 0.0017, 0.0015, 0.0015, 0.0018, 0.0016,\n                       0.0015, 0.0016, 0.0026, 0.0016, 0.0031, 0.0018, 0.0018, 0.0019, 0.0013,\n                       0.0016, 0.0016, 0.0047, 0.0023, 0.0016, 0.0014, 0.0015, 0.0022, 0.0015,\n                       0.0014, 0.0016, 0.0015, 0.0014, 0.0021, 0.0018, 0.0019, 0.0016, 0.0020,\n                       0.0027, 0.0034, 0.0024, 0.0015, 0.0013, 0.0015, 0.0030, 0.0020, 0.0015,\n                       0.0013, 0.0036, 0.0014, 0.0016, 0.0017, 0.0017, 0.0014, 0.0020, 0.0019,\n                       0.0015, 0.0014, 0.0027, 0.0020, 0.0020, 0.0013, 0.0015, 0.0017, 0.0017,\n                       0.0038, 0.0030, 0.0014, 0.0015, 0.0017, 0.0034, 0.0013, 0.0016, 0.0018,\n                       0.0017, 0.0016, 0.0021, 0.0020, 0.0017, 0.0019, 0.0016, 0.0016, 0.0015,\n                       0.0015, 0.0014, 0.0014, 0.0018, 0.0014, 0.0018, 0.0020, 0.0023, 0.0014,\n                       0.0027, 0.0018, 0.0015, 0.0027, 0.0019, 0.0028, 0.0016, 0.0017, 0.0025,\n                       0.0013, 0.0050, 0.0016, 0.0017, 0.0023, 0.0016, 0.0029, 0.0014, 0.0015,\n                       0.0023, 0.0018, 0.0019, 0.0018, 0.0016, 0.0013, 0.0018, 0.0020, 0.0042,\n                       0.0028, 0.0015, 0.0015, 0.0030, 0.0036, 0.0017, 0.0021, 0.0020, 0.0017,\n                       0.0026, 0.0015, 0.0020, 0.0024, 0.0014, 0.0020, 0.0017, 0.0015, 0.0017,\n                       0.0019, 0.0022, 0.0020, 0.0018, 0.0014, 0.0017, 0.0034, 0.0018, 0.0016,\n                       0.0015, 0.0018, 0.0021, 0.0047, 0.0016, 0.0017, 0.0019, 0.0017, 0.0015,\n                       0.0015, 0.0018, 0.0017, 0.0014, 0.0014, 0.0015, 0.0018, 0.0017, 0.0016,\n                       0.0015, 0.0016, 0.0015, 0.0016, 0.0016, 0.0016, 0.0019, 0.0018, 0.0016,\n                       0.0023, 0.0018, 0.0022, 0.0014, 0.0016, 0.0018, 0.0024, 0.0014, 0.0033,\n                       0.0022, 0.0017, 0.0029, 0.0022, 0.0016, 0.0020, 0.0033, 0.0015, 0.0025,\n                       0.0019, 0.0018, 0.0015, 0.0017, 0.0017, 0.0022, 0.0016, 0.0020, 0.0015,\n                       0.0016, 0.0014, 0.0026, 0.0015, 0.0023, 0.0018, 0.0015, 0.0018, 0.0027,\n                       0.0036, 0.0043, 0.0016, 0.0023, 0.0017, 0.0024, 0.0022, 0.0017, 0.0021,\n                       0.0022, 0.0024, 0.0014, 0.0014, 0.0018, 0.0033, 0.0018, 0.0022, 0.0016,\n                       0.0016, 0.0015, 0.0018, 0.0018, 0.0019, 0.0014, 0.0034, 0.0016, 0.0014,\n                       0.0016, 0.0018, 0.0018, 0.0014, 0.0017, 0.0018, 0.0016, 0.0027, 0.0019,\n                       0.0019, 0.0015, 0.0015, 0.0016, 0.0020, 0.0016, 0.0017, 0.0018, 0.0014,\n                       0.0015, 0.0018, 0.0019, 0.0026, 0.0018, 0.0028, 0.0016, 0.0020, 0.0019,\n                       0.0016, 0.0016, 0.0020, 0.0025, 0.0017, 0.0017, 0.0014, 0.0017, 0.0028,\n                       0.0015, 0.0015, 0.0018, 0.0014, 0.0019, 0.0024, 0.0018, 0.0017, 0.0013,\n                       0.0014, 0.0016, 0.0016, 0.0015, 0.0015, 0.0036, 0.0015, 0.0022, 0.0015,\n                       0.0021, 0.0021, 0.0015, 0.0015, 0.0016, 0.0021, 0.0017, 0.0019, 0.0017,\n                       0.0016, 0.0017, 0.0016, 0.0017, 0.0013, 0.0017, 0.0019, 0.0019, 0.0020,\n                       0.0016, 0.0014, 0.0023, 0.0048, 0.0017, 0.0016, 0.0014, 0.0015, 0.0016,\n                       0.0014, 0.0020, 0.0029, 0.0020, 0.0016, 0.0021, 0.0018, 0.0014, 0.0016,\n                       0.0013, 0.0014, 0.0021, 0.0014, 0.0022, 0.0022, 0.0023, 0.0028, 0.0015,\n                       0.0017, 0.0014, 0.0024, 0.0014, 0.0016, 0.0025, 0.0019, 0.0016, 0.0018,\n                       0.0017, 0.0016, 0.0039, 0.0015, 0.0019, 0.0016, 0.0015, 0.0017, 0.0017,\n                       0.0024, 0.0016, 0.0018, 0.0016, 0.0026, 0.0013, 0.0014, 0.0015, 0.0017,\n                       0.0021, 0.0015, 0.0014, 0.0021, 0.0019, 0.0015, 0.0031, 0.0015, 0.0016,\n                       0.0015, 0.0018, 0.0014, 0.0019, 0.0016, 0.0018], dtype=torch.float64),\n                      zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n                      axis=0),\n               Parameter containing:\n               tensor([-3.6678e-03, -1.3208e-02, -4.7020e-02, -1.4882e-02,  5.4378e-03,\n                       -2.3459e-02, -8.7962e-03,  2.8689e-02,  2.6539e-02,  8.6500e-03,\n                        2.5633e-03, -1.9254e-02, -1.8111e-01, -2.5252e-02, -6.7610e-03,\n                       -5.8208e-03, -3.9828e-02, -1.3373e-02, -1.9116e-03,  1.2631e-02,\n                       -6.4382e-03, -1.5257e-02,  4.3911e-02, -1.0101e-02, -2.4044e-02,\n                       -3.6476e-02,  2.2317e-02,  1.9601e-02, -2.0494e-02, -1.3453e-02,\n                        5.9468e-02, -1.7002e-02, -1.2170e-02,  4.8217e-02, -1.8997e-02,\n                        1.4120e-02,  5.7367e-02, -7.1456e-02,  3.3782e-03, -1.7300e-02,\n                        9.0549e-03, -1.3188e-02,  6.6161e-02, -1.0400e-02,  8.8761e-03,\n                        1.2290e-02,  9.6302e-02, -1.0040e-02, -3.4297e-03,  9.3067e-03,\n                        7.5395e-03,  6.0472e-02, -4.9556e-03, -4.1208e-02,  4.9970e-02,\n                        4.7401e-02,  1.6650e-02, -3.2723e-03, -9.3615e-03, -2.6604e-04,\n                       -1.6835e-02, -2.2317e-02, -7.5031e-03,  9.1798e-03,  2.6036e-02,\n                       -1.2734e-02,  1.7699e-02,  1.7396e-03, -1.1438e-02, -5.5683e-02,\n                       -1.8379e-03, -1.0718e-03, -3.4536e-02, -1.8070e-03, -8.9031e-03,\n                       -1.2782e-03,  1.4166e-04,  4.4814e-02, -7.4388e-03, -3.2479e-04,\n                        1.2307e-02,  2.3579e-02,  7.2979e-02,  5.4152e-03, -4.0047e-02,\n                       -1.3715e-02, -7.2290e-03,  2.6334e-02, -1.7047e-02,  8.4470e-03,\n                       -2.5921e-02, -7.2279e-02,  6.0255e-02, -4.6166e-02, -1.9261e-02,\n                        8.2032e-03, -2.1814e-02,  5.5129e-03,  2.5830e-02,  5.3649e-03,\n                        1.6042e-02,  1.0580e-02,  7.0334e-03, -2.5599e-02,  3.1252e-02,\n                        7.5578e-03, -1.1704e-02,  1.4083e-02, -6.8663e-03,  3.5066e-02,\n                        1.5168e-01, -8.4667e-03, -1.2217e-01, -5.5928e-03, -5.7340e-03,\n                       -3.6710e-02, -5.7691e-03, -2.2183e-02, -5.5866e-04,  3.3295e-02,\n                       -1.8493e-02,  1.2073e-02, -2.3314e-02,  1.5700e-02, -6.7804e-02,\n                       -2.2141e-01,  7.5537e-03,  1.8031e-02, -4.7564e-03, -5.0814e-04,\n                        3.0237e-02,  1.3573e-02,  1.8683e-03,  2.6562e-02,  8.0556e-03,\n                       -1.0734e-01,  3.3720e-03,  3.0370e-02,  3.3002e-02,  8.2910e-03,\n                        2.1788e-02, -4.3178e-03,  5.1103e-03,  5.6726e-03,  4.1882e-03,\n                       -3.3570e-02,  2.0030e-02,  1.0485e-03, -1.9788e-02,  2.9581e-04,\n                       -4.6190e-03, -1.7697e-02,  2.1273e-02,  5.3287e-02,  1.6553e-03,\n                        1.5905e-03,  1.6515e+00,  4.5343e-02,  1.5174e-02,  5.2319e-03,\n                       -3.5223e-02, -4.3688e-03,  3.0069e-02, -2.3393e-02, -1.5767e-02,\n                       -6.2236e-02, -2.6771e-03,  1.7373e-02,  1.0834e-02,  2.5365e-02,\n                        8.4555e-03,  2.8261e-02, -1.5155e-02, -5.8003e-03,  1.6370e-02,\n                        2.1342e-02, -9.4040e-03,  2.6139e-03, -3.3568e-02, -2.8350e-02,\n                        4.6028e-02,  5.3650e-02, -6.0197e-03, -2.6729e-02,  1.1657e-03,\n                        2.7182e-02, -1.0188e-02,  1.3173e-02,  2.2954e-02,  1.6566e-02,\n                       -1.8625e-02,  6.2338e-02,  2.7492e-02,  1.1428e-02, -4.1047e-03,\n                       -9.4573e-04, -6.9838e-03,  3.6966e-02,  2.3526e-03, -4.0614e-02,\n                        8.4008e-03,  1.3209e-03, -6.0175e-03,  6.2402e-03, -1.6454e-02,\n                       -5.6148e-03, -1.6939e-02, -2.8181e-04,  1.4689e-03, -3.7651e-02,\n                        3.9680e-03, -9.8218e-02,  2.2614e-04,  8.5429e-03,  1.0370e-02,\n                       -7.0042e-02, -1.9301e-02,  1.3351e-02,  1.3480e-02, -2.5795e-03,\n                        1.6745e-02, -2.3834e-03,  1.5578e-02, -1.2805e-03,  6.9123e-03,\n                       -2.6269e-02,  1.0525e-02, -1.5280e-02,  3.5158e-02, -1.5112e-02,\n                       -1.3886e-02,  1.0539e-02, -1.3856e-02,  5.2442e-02, -1.0442e-03,\n                        6.2290e-03, -1.7042e-02,  1.6472e-04, -1.4155e-02,  1.0073e-02,\n                        2.3201e-02, -1.8036e-02, -1.8049e-02,  1.9048e-02, -4.5167e-02,\n                        1.6144e-02, -1.3196e-02, -6.3652e-03, -2.9855e-02, -3.4307e-02,\n                        1.1763e-02, -2.5528e-02, -3.3136e-03, -9.1478e-03, -5.6450e-02,\n                       -7.6983e-03, -2.3756e-02,  2.9275e-02, -2.0759e-02, -1.8771e-03,\n                       -1.1347e-02,  2.0492e-02, -2.8172e-03, -3.5642e-02,  4.4916e-02,\n                        9.5300e-03, -1.6277e-02,  1.6606e-02, -3.1965e-03, -6.1651e-04,\n                        2.8689e-02,  3.1439e-03,  4.6736e-02,  2.1472e-03,  6.6062e-02,\n                       -1.4589e-02,  1.9691e-02,  1.8716e-02,  1.7602e-02, -6.0908e-03,\n                        4.3209e-02, -1.7441e-03, -3.9953e-02, -2.1503e-04, -1.1339e-02,\n                        1.7260e-02, -2.4578e-03,  5.2110e-03, -1.9934e-02,  9.4502e-03,\n                       -1.5345e-02,  1.6496e-02,  4.1060e-03, -3.4590e-02, -1.5932e-02,\n                       -7.2616e-04,  1.6947e-03,  1.0729e-02, -6.6884e-03, -1.1063e-02,\n                        2.3347e-02, -5.1074e-03,  3.4164e-02, -2.4819e-02, -1.6439e-03,\n                       -2.6329e-02,  1.5986e-02,  1.8847e-02,  9.1435e-03,  1.6254e-02,\n                       -1.5424e-02, -6.1749e-02,  4.4458e-03, -1.3840e-02,  3.0735e-02,\n                       -5.7890e-03,  1.0669e-02,  3.7961e-02, -5.0772e-02, -2.6375e-03,\n                       -1.3110e-02, -4.6507e-03, -1.5731e-02, -5.8352e-02, -2.0717e-02,\n                        2.9990e-02, -1.0468e-02,  7.7406e-03, -4.0810e-02,  4.3394e-03,\n                       -4.6808e-03,  1.1926e-02,  4.4518e-03, -2.3774e-02, -4.1913e-03,\n                       -5.0847e-02, -2.1319e-02,  5.3350e-03,  2.7545e-02, -2.4099e-02,\n                       -5.1892e-03,  1.7759e-02, -2.9980e-03,  3.2751e-03,  3.0556e-02,\n                       -1.8626e-02,  1.0735e-02,  1.9653e-03,  3.3522e-01,  1.0701e-03,\n                       -2.7684e-03,  2.1309e-02, -1.3147e-03, -7.3245e-02,  1.2711e-03,\n                        3.4178e-02,  9.9652e-06,  6.9259e-03,  2.0296e-02, -5.4374e-03,\n                        7.7735e-03,  1.7805e-02, -3.2388e-02,  1.6824e-03, -9.5688e-04,\n                       -3.3035e-02,  8.4474e-03, -2.9252e-03, -1.1237e-02, -6.5402e-03,\n                       -1.4110e-02, -1.0693e-02, -7.0882e-02, -1.2072e-02,  1.4315e-03,\n                        2.0378e-02,  1.0335e-01,  1.0920e-03,  2.2994e-02, -4.3524e-02,\n                        9.3041e-03,  1.6674e-02,  3.4357e-02, -9.6140e-03],\n                      requires_grad=True))),\n             ('features.5.6.layer_scale',\n              tensor([[[ 1.0908e-01]],\n              \n                      [[ 7.4461e-02]],\n              \n                      [[ 1.3679e-01]],\n              \n                      [[-1.9803e-01]],\n              \n                      [[-1.7877e-01]],\n              \n                      [[-1.7026e-01]],\n              \n                      [[-6.2630e-02]],\n              \n                      [[ 1.9035e-01]],\n              \n                      [[-1.8669e-01]],\n              \n                      [[ 1.0980e-01]],\n              \n                      [[ 7.7062e-02]],\n              \n                      [[ 2.1832e-01]],\n              \n                      [[-3.3714e-01]],\n              \n                      [[-2.6194e-01]],\n              \n                      [[ 1.6754e-01]],\n              \n                      [[ 1.2626e-01]],\n              \n                      [[-2.2832e-01]],\n              \n                      [[ 1.5924e-01]],\n              \n                      [[ 1.8325e-01]],\n              \n                      [[ 1.3928e-01]],\n              \n                      [[ 2.5598e-01]],\n              \n                      [[ 1.3671e-01]],\n              \n                      [[ 2.9285e-01]],\n              \n                      [[-1.6939e-01]],\n              \n                      [[ 7.7924e-02]],\n              \n                      [[ 5.8335e-02]],\n              \n                      [[ 9.2664e-02]],\n              \n                      [[-8.0210e-02]],\n              \n                      [[-2.0477e-01]],\n              \n                      [[ 3.8263e-01]],\n              \n                      [[ 2.5245e-01]],\n              \n                      [[ 7.9667e-02]],\n              \n                      [[ 1.6292e-01]],\n              \n                      [[ 1.8583e-01]],\n              \n                      [[-1.6260e-01]],\n              \n                      [[-1.9607e-01]],\n              \n                      [[-1.5855e-01]],\n              \n                      [[-2.2755e-01]],\n              \n                      [[-1.9108e-01]],\n              \n                      [[-1.6484e-01]],\n              \n                      [[ 1.8755e-01]],\n              \n                      [[-1.4976e-01]],\n              \n                      [[-2.6204e-01]],\n              \n                      [[ 1.9350e-01]],\n              \n                      [[ 2.1292e-01]],\n              \n                      [[ 2.5898e-01]],\n              \n                      [[-3.5881e-02]],\n              \n                      [[ 1.7367e-01]],\n              \n                      [[-1.0024e-01]],\n              \n                      [[ 1.8057e-01]],\n              \n                      [[ 2.3918e-01]],\n              \n                      [[ 2.2130e-01]],\n              \n                      [[-1.0793e-01]],\n              \n                      [[-2.3083e-01]],\n              \n                      [[ 1.0763e-01]],\n              \n                      [[-1.0942e-01]],\n              \n                      [[-2.2141e-01]],\n              \n                      [[ 1.1241e-01]],\n              \n                      [[ 9.3341e-02]],\n              \n                      [[ 1.4139e-01]],\n              \n                      [[ 8.7440e-02]],\n              \n                      [[ 2.5145e-01]],\n              \n                      [[ 6.5375e-02]],\n              \n                      [[-1.5741e-01]],\n              \n                      [[-1.9481e-01]],\n              \n                      [[-1.5199e-01]],\n              \n                      [[-5.9678e-02]],\n              \n                      [[-2.4315e-01]],\n              \n                      [[-2.1134e-01]],\n              \n                      [[ 6.6058e-02]],\n              \n                      [[ 1.9065e-01]],\n              \n                      [[ 1.0127e-01]],\n              \n                      [[-2.7201e-01]],\n              \n                      [[-4.7648e-02]],\n              \n                      [[-1.2773e-01]],\n              \n                      [[-1.8227e-01]],\n              \n                      [[ 1.5564e-01]],\n              \n                      [[ 4.8884e-03]],\n              \n                      [[ 1.9750e-01]],\n              \n                      [[ 1.4942e-01]],\n              \n                      [[ 1.6348e-01]],\n              \n                      [[ 1.8432e-01]],\n              \n                      [[ 1.5769e-01]],\n              \n                      [[-3.0524e-01]],\n              \n                      [[ 1.8221e-01]],\n              \n                      [[-1.4271e-01]],\n              \n                      [[ 2.6977e-01]],\n              \n                      [[ 1.4087e-01]],\n              \n                      [[ 7.7347e-02]],\n              \n                      [[ 1.5715e-01]],\n              \n                      [[ 1.8053e-01]],\n              \n                      [[-1.9745e-01]],\n              \n                      [[-1.4965e-01]],\n              \n                      [[ 1.5629e-01]],\n              \n                      [[-1.5224e-01]],\n              \n                      [[ 1.7398e-01]],\n              \n                      [[-1.2167e-01]],\n              \n                      [[-2.3768e-01]],\n              \n                      [[-1.6506e-01]],\n              \n                      [[-2.6723e-01]],\n              \n                      [[-1.3404e-01]],\n              \n                      [[-1.8912e-01]],\n              \n                      [[-2.4219e-01]],\n              \n                      [[-1.4213e-01]],\n              \n                      [[-1.1663e-01]],\n              \n                      [[ 1.0390e-01]],\n              \n                      [[-9.6243e-02]],\n              \n                      [[ 2.6972e-01]],\n              \n                      [[-1.4387e-01]],\n              \n                      [[-2.9835e-01]],\n              \n                      [[ 2.3414e-01]],\n              \n                      [[ 2.0452e-01]],\n              \n                      [[ 2.6949e-01]],\n              \n                      [[-7.7511e-02]],\n              \n                      [[-2.0597e-01]],\n              \n                      [[-2.1603e-01]],\n              \n                      [[-1.6425e-01]],\n              \n                      [[-2.6551e-01]],\n              \n                      [[-6.7328e-02]],\n              \n                      [[ 1.8614e-01]],\n              \n                      [[-1.3213e-01]],\n              \n                      [[ 1.0761e-01]],\n              \n                      [[ 1.2550e-01]],\n              \n                      [[-2.9093e-01]],\n              \n                      [[ 3.8475e-01]],\n              \n                      [[-1.7608e-01]],\n              \n                      [[ 5.7282e-02]],\n              \n                      [[-1.6709e-01]],\n              \n                      [[ 1.2734e-01]],\n              \n                      [[ 6.5650e-03]],\n              \n                      [[-3.4528e-03]],\n              \n                      [[-1.8132e-01]],\n              \n                      [[ 3.2735e-01]],\n              \n                      [[-2.2053e-01]],\n              \n                      [[-1.0997e-01]],\n              \n                      [[ 2.9105e-01]],\n              \n                      [[ 1.0696e-01]],\n              \n                      [[-8.1497e-02]],\n              \n                      [[ 4.8759e-04]],\n              \n                      [[ 1.1229e-01]],\n              \n                      [[-1.9452e-01]],\n              \n                      [[-1.7349e-01]],\n              \n                      [[-1.4400e-01]],\n              \n                      [[-1.0404e-01]],\n              \n                      [[-6.6795e-02]],\n              \n                      [[-1.6318e-01]],\n              \n                      [[-2.5787e-01]],\n              \n                      [[-1.2945e-01]],\n              \n                      [[-1.4694e-01]],\n              \n                      [[-1.6179e-01]],\n              \n                      [[-1.9507e-01]],\n              \n                      [[ 1.1428e-01]],\n              \n                      [[-1.9877e-01]],\n              \n                      [[-2.0153e-01]],\n              \n                      [[-2.0194e-01]],\n              \n                      [[-2.3365e-01]],\n              \n                      [[-1.4926e+00]],\n              \n                      [[ 2.2971e-01]],\n              \n                      [[ 1.4857e-01]],\n              \n                      [[-1.1351e-01]],\n              \n                      [[ 2.3028e-01]],\n              \n                      [[ 1.2749e-01]],\n              \n                      [[-1.5973e-01]],\n              \n                      [[-3.3588e-01]],\n              \n                      [[-2.4530e-01]],\n              \n                      [[ 1.6127e-01]],\n              \n                      [[-1.8538e-01]],\n              \n                      [[ 1.6118e-01]],\n              \n                      [[-1.7013e-01]],\n              \n                      [[ 1.8169e-01]],\n              \n                      [[ 1.4632e-01]],\n              \n                      [[-1.6363e-01]],\n              \n                      [[-1.6094e-01]],\n              \n                      [[-1.6787e-01]],\n              \n                      [[-1.8453e-01]],\n              \n                      [[ 6.6408e-02]],\n              \n                      [[-1.5786e-01]],\n              \n                      [[-6.7990e-02]],\n              \n                      [[ 1.8167e-01]],\n              \n                      [[ 2.2989e-01]],\n              \n                      [[ 8.0252e-02]],\n              \n                      [[ 2.4445e-01]],\n              \n                      [[-2.2061e-01]],\n              \n                      [[ 2.2875e-01]],\n              \n                      [[-1.6218e-01]],\n              \n                      [[ 2.5039e-01]],\n              \n                      [[-5.5661e-02]],\n              \n                      [[ 1.7973e-01]],\n              \n                      [[-2.8854e-01]],\n              \n                      [[ 2.2831e-01]],\n              \n                      [[ 3.0941e-01]],\n              \n                      [[-2.8135e-02]],\n              \n                      [[ 1.0937e-01]],\n              \n                      [[ 6.9826e-02]],\n              \n                      [[ 1.5617e-01]],\n              \n                      [[-1.0435e-02]],\n              \n                      [[-1.0136e-01]],\n              \n                      [[ 6.1893e-02]],\n              \n                      [[ 2.2620e-01]],\n              \n                      [[ 2.2091e-01]],\n              \n                      [[-1.7401e-01]],\n              \n                      [[-1.4919e-01]],\n              \n                      [[ 1.1209e-01]],\n              \n                      [[ 1.8573e-01]],\n              \n                      [[ 1.3022e-01]],\n              \n                      [[ 1.8329e-01]],\n              \n                      [[-1.1824e-01]],\n              \n                      [[ 9.6696e-02]],\n              \n                      [[-1.4796e-01]],\n              \n                      [[-2.5129e-01]],\n              \n                      [[-1.1071e-01]],\n              \n                      [[-1.9491e-01]],\n              \n                      [[ 1.5942e-01]],\n              \n                      [[ 1.4329e-01]],\n              \n                      [[-1.0810e-01]],\n              \n                      [[ 2.4792e-01]],\n              \n                      [[ 8.3096e-02]],\n              \n                      [[ 9.7147e-04]],\n              \n                      [[-1.6560e-01]],\n              \n                      [[-2.6445e-01]],\n              \n                      [[ 1.1785e-01]],\n              \n                      [[-6.1760e-02]],\n              \n                      [[-1.9282e-01]],\n              \n                      [[-7.5894e-02]],\n              \n                      [[-2.0824e-01]],\n              \n                      [[ 2.6643e-01]],\n              \n                      [[ 1.8848e-01]],\n              \n                      [[ 1.1316e-01]],\n              \n                      [[-1.6960e-01]],\n              \n                      [[ 1.7125e-01]],\n              \n                      [[ 2.6368e-01]],\n              \n                      [[ 1.2524e-01]],\n              \n                      [[ 2.1569e-01]],\n              \n                      [[-1.6279e-01]],\n              \n                      [[ 1.8960e-01]],\n              \n                      [[-1.0120e-01]],\n              \n                      [[-8.9056e-02]],\n              \n                      [[-2.6142e-01]],\n              \n                      [[ 2.8379e-01]],\n              \n                      [[ 1.2367e-01]],\n              \n                      [[-1.8737e-02]],\n              \n                      [[-1.8442e-01]],\n              \n                      [[ 1.1305e-01]],\n              \n                      [[-2.0980e-01]],\n              \n                      [[ 2.0265e-01]],\n              \n                      [[ 2.3829e-01]],\n              \n                      [[ 1.8537e-01]],\n              \n                      [[ 1.8916e-01]],\n              \n                      [[ 2.1952e-01]],\n              \n                      [[-1.2310e-01]],\n              \n                      [[ 6.1290e-02]],\n              \n                      [[ 1.7267e-01]],\n              \n                      [[ 6.5434e-02]],\n              \n                      [[-1.1204e-01]],\n              \n                      [[ 2.0035e-01]],\n              \n                      [[-1.7323e-01]],\n              \n                      [[ 1.2884e-01]],\n              \n                      [[ 2.3355e-01]],\n              \n                      [[ 1.9611e-01]],\n              \n                      [[ 1.4867e-01]],\n              \n                      [[ 1.7447e-01]],\n              \n                      [[-2.0358e-01]],\n              \n                      [[-1.7564e-01]],\n              \n                      [[-1.5895e-01]],\n              \n                      [[-1.5499e-01]],\n              \n                      [[-1.9608e-01]],\n              \n                      [[ 2.3861e-01]],\n              \n                      [[-1.2660e-01]],\n              \n                      [[-2.2819e-01]],\n              \n                      [[-2.1372e-01]],\n              \n                      [[-1.2479e-01]],\n              \n                      [[-7.5956e-02]],\n              \n                      [[ 2.0217e-01]],\n              \n                      [[ 1.9234e-01]],\n              \n                      [[-9.7637e-02]],\n              \n                      [[ 1.9459e-01]],\n              \n                      [[-1.3741e-01]],\n              \n                      [[-1.4396e-01]],\n              \n                      [[ 1.6647e-01]],\n              \n                      [[ 1.5382e-01]],\n              \n                      [[ 7.1940e-02]],\n              \n                      [[-2.4470e-01]],\n              \n                      [[ 1.7946e-01]],\n              \n                      [[-1.5796e-01]],\n              \n                      [[-8.3025e-02]],\n              \n                      [[ 2.3004e-01]],\n              \n                      [[-6.4424e-02]],\n              \n                      [[ 1.4657e-01]],\n              \n                      [[ 1.5352e-01]],\n              \n                      [[ 1.2591e-01]],\n              \n                      [[ 1.9274e-01]],\n              \n                      [[ 3.4833e-01]],\n              \n                      [[-1.5555e-01]],\n              \n                      [[-1.6483e-02]],\n              \n                      [[ 1.7092e-01]],\n              \n                      [[ 1.0396e-01]],\n              \n                      [[-1.6396e-01]],\n              \n                      [[-5.0407e-02]],\n              \n                      [[ 7.4172e-02]],\n              \n                      [[ 8.5538e-02]],\n              \n                      [[ 1.3867e-01]],\n              \n                      [[ 1.4370e-01]],\n              \n                      [[ 3.0979e-01]],\n              \n                      [[-2.4466e-01]],\n              \n                      [[-5.4811e-02]],\n              \n                      [[ 9.4708e-02]],\n              \n                      [[ 1.2176e-01]],\n              \n                      [[-1.1647e-01]],\n              \n                      [[-8.7825e-02]],\n              \n                      [[ 1.9898e-01]],\n              \n                      [[ 1.7239e-01]],\n              \n                      [[ 1.6463e-01]],\n              \n                      [[ 1.6017e-01]],\n              \n                      [[-1.9633e-01]],\n              \n                      [[ 5.2516e-02]],\n              \n                      [[ 1.0615e-01]],\n              \n                      [[ 1.5223e-01]],\n              \n                      [[ 1.6569e-01]],\n              \n                      [[ 2.1343e-02]],\n              \n                      [[-8.7056e-02]],\n              \n                      [[ 2.3871e-01]],\n              \n                      [[-1.2584e-01]],\n              \n                      [[ 1.5769e-01]],\n              \n                      [[ 3.1421e-01]],\n              \n                      [[ 1.9724e-01]],\n              \n                      [[-2.6542e-01]],\n              \n                      [[-2.6270e-03]],\n              \n                      [[ 1.9880e-01]],\n              \n                      [[-1.8175e-01]],\n              \n                      [[ 2.4497e-01]],\n              \n                      [[ 1.2945e-01]],\n              \n                      [[-1.5747e-01]],\n              \n                      [[ 1.4608e-01]],\n              \n                      [[-1.7899e-01]],\n              \n                      [[-1.5685e-01]],\n              \n                      [[ 1.7207e-01]],\n              \n                      [[-1.9291e-01]],\n              \n                      [[-5.2619e-02]],\n              \n                      [[-5.2517e-02]],\n              \n                      [[-2.7735e-01]],\n              \n                      [[-5.5405e-02]],\n              \n                      [[-2.0964e-01]],\n              \n                      [[-7.8802e-02]],\n              \n                      [[-2.0661e-01]],\n              \n                      [[ 1.8194e-01]],\n              \n                      [[-1.5283e-01]],\n              \n                      [[ 3.0798e-01]],\n              \n                      [[ 5.2619e-02]],\n              \n                      [[ 1.1132e-01]],\n              \n                      [[ 1.2386e-01]],\n              \n                      [[ 8.9782e-02]],\n              \n                      [[-1.9143e-01]],\n              \n                      [[ 1.6658e-01]],\n              \n                      [[ 9.5591e-02]],\n              \n                      [[-1.1342e-01]],\n              \n                      [[ 6.5440e-02]],\n              \n                      [[-2.1821e-01]],\n              \n                      [[-1.4759e-01]],\n              \n                      [[ 2.3327e-01]],\n              \n                      [[-1.6297e-01]],\n              \n                      [[-4.8542e-02]],\n              \n                      [[ 1.0409e-01]],\n              \n                      [[ 2.1354e-01]],\n              \n                      [[ 1.3717e-01]],\n              \n                      [[ 1.9418e-01]],\n              \n                      [[-1.6895e-01]],\n              \n                      [[ 1.7950e-01]],\n              \n                      [[ 1.2789e-01]],\n              \n                      [[-2.3730e-01]],\n              \n                      [[-2.3905e-01]],\n              \n                      [[ 9.0924e-02]],\n              \n                      [[ 1.8037e-01]],\n              \n                      [[ 1.9043e-01]],\n              \n                      [[-2.1367e-01]],\n              \n                      [[-1.5412e-01]],\n              \n                      [[ 2.2719e-01]],\n              \n                      [[ 1.9508e-01]],\n              \n                      [[-2.2021e-01]],\n              \n                      [[-1.8982e-01]],\n              \n                      [[-2.4610e-01]],\n              \n                      [[-1.6670e-01]],\n              \n                      [[ 7.9954e-02]],\n              \n                      [[-2.0341e-01]],\n              \n                      [[ 1.4850e-01]]])),\n             ('features.5.6.block.0.weight',\n              tensor([[[[ 0.0144,  0.0048,  0.0096,  ...,  0.0048,  0.0048,  0.0120],\n                        [ 0.0048,  0.0048, -0.0024,  ...,  0.0048,  0.0000, -0.0024],\n                        [ 0.0000,  0.0000,  0.0289,  ...,  0.0193, -0.0024,  0.0072],\n                        ...,\n                        [ 0.0072,  0.0000,  0.0144,  ...,  0.0072,  0.0120,  0.0000],\n                        [ 0.0024,  0.0048,  0.0024,  ..., -0.0120,  0.0072,  0.0072],\n                        [ 0.0072, -0.0120,  0.0048,  ...,  0.0024,  0.0048,  0.0072]]],\n              \n              \n                      [[[-0.0107, -0.0027,  0.0027,  ..., -0.0054,  0.0000, -0.0080],\n                        [-0.0054,  0.0000,  0.0000,  ..., -0.0134,  0.0054, -0.0054],\n                        [ 0.0000, -0.0080, -0.0294,  ..., -0.0482,  0.0027, -0.0080],\n                        ...,\n                        [-0.0054, -0.0107, -0.0321,  ..., -0.0535, -0.0027, -0.0054],\n                        [ 0.0000, -0.0054, -0.0054,  ..., -0.0107,  0.0054, -0.0054],\n                        [-0.0107, -0.0027,  0.0000,  ..., -0.0080, -0.0027, -0.0080]]],\n              \n              \n                      [[[-0.0059,  0.0039, -0.0098,  ..., -0.0138,  0.0079, -0.0138],\n                        [-0.0059,  0.0000, -0.0039,  ..., -0.0098, -0.0079,  0.0079],\n                        [-0.0039, -0.0098, -0.0275,  ..., -0.0256, -0.0059, -0.0059],\n                        ...,\n                        [-0.0098, -0.0039, -0.0433,  ..., -0.0374, -0.0079, -0.0138],\n                        [-0.0020, -0.0020, -0.0020,  ..., -0.0059, -0.0039, -0.0020],\n                        [-0.0020, -0.0118, -0.0020,  ..., -0.0098, -0.0098,  0.0039]]],\n              \n              \n                      ...,\n              \n              \n                      [[[-0.0129, -0.0077,  0.0052,  ..., -0.0129,  0.0026, -0.0077],\n                        [-0.0052, -0.0052,  0.0000,  ...,  0.0103, -0.0155,  0.0026],\n                        [ 0.0000, -0.0103, -0.0258,  ..., -0.0206,  0.0000, -0.0052],\n                        ...,\n                        [-0.0077,  0.0026, -0.0335,  ..., -0.0283,  0.0000,  0.0000],\n                        [-0.0103, -0.0052,  0.0077,  ..., -0.0026, -0.0103,  0.0026],\n                        [-0.0129, -0.0026, -0.0052,  ..., -0.0077, -0.0077, -0.0077]]],\n              \n              \n                      [[[ 0.0046, -0.0091,  0.0000,  ...,  0.0023,  0.0023, -0.0046],\n                        [ 0.0046,  0.0000,  0.0091,  ..., -0.0068,  0.0000,  0.0046],\n                        [-0.0046,  0.0046,  0.0000,  ..., -0.0023,  0.0000,  0.0046],\n                        ...,\n                        [ 0.0205, -0.0023,  0.0091,  ...,  0.0068,  0.0000,  0.0068],\n                        [-0.0046,  0.0091,  0.0000,  ...,  0.0000,  0.0046, -0.0023],\n                        [ 0.0091, -0.0068, -0.0023,  ...,  0.0000, -0.0046,  0.0023]]],\n              \n              \n                      [[[-0.0070, -0.0047,  0.0117,  ...,  0.0023,  0.0094, -0.0117],\n                        [-0.0047,  0.0047, -0.0164,  ..., -0.0094,  0.0000, -0.0047],\n                        [-0.0047,  0.0000,  0.0117,  ...,  0.0094, -0.0047, -0.0070],\n                        ...,\n                        [-0.0070, -0.0094,  0.0211,  ...,  0.0257, -0.0117,  0.0070],\n                        [ 0.0023,  0.0000, -0.0094,  ..., -0.0140, -0.0023, -0.0023],\n                        [-0.0070,  0.0000, -0.0047,  ...,  0.0117, -0.0070, -0.0047]]]],\n                     size=(384, 1, 7, 7), dtype=torch.qint8,\n                     quantization_scheme=torch.per_channel_affine,\n                     scale=tensor([0.0024, 0.0027, 0.0020, 0.0025, 0.0020, 0.0021, 0.0026, 0.0013, 0.0016,\n                      0.0025, 0.0023, 0.0020, 0.0017, 0.0006, 0.0019, 0.0020, 0.0026, 0.0017,\n                      0.0007, 0.0024, 0.0015, 0.0023, 0.0023, 0.0018, 0.0024, 0.0019, 0.0027,\n                      0.0026, 0.0012, 0.0027, 0.0012, 0.0026, 0.0018, 0.0018, 0.0007, 0.0009,\n                      0.0026, 0.0017, 0.0010, 0.0020, 0.0018, 0.0026, 0.0024, 0.0019, 0.0006,\n                      0.0011, 0.0013, 0.0017, 0.0024, 0.0024, 0.0022, 0.0011, 0.0023, 0.0007,\n                      0.0024, 0.0026, 0.0006, 0.0020, 0.0025, 0.0014, 0.0026, 0.0014, 0.0026,\n                      0.0017, 0.0016, 0.0012, 0.0028, 0.0016, 0.0018, 0.0019, 0.0020, 0.0024,\n                      0.0016, 0.0025, 0.0016, 0.0007, 0.0019, 0.0011, 0.0024, 0.0016, 0.0021,\n                      0.0026, 0.0027, 0.0018, 0.0011, 0.0019, 0.0013, 0.0018, 0.0025, 0.0016,\n                      0.0028, 0.0018, 0.0025, 0.0026, 0.0019, 0.0023, 0.0018, 0.0012, 0.0011,\n                      0.0013, 0.0017, 0.0006, 0.0016, 0.0025, 0.0025, 0.0024, 0.0022, 0.0015,\n                      0.0024, 0.0011, 0.0018, 0.0007, 0.0005, 0.0027, 0.0016, 0.0007, 0.0012,\n                      0.0013, 0.0024, 0.0016, 0.0019, 0.0024, 0.0020, 0.0010, 0.0028, 0.0021,\n                      0.0025, 0.0017, 0.0023, 0.0011, 0.0007, 0.0016, 0.0012, 0.0013, 0.0026,\n                      0.0028, 0.0023, 0.0021, 0.0011, 0.0027, 0.0020, 0.0012, 0.0021, 0.0026,\n                      0.0025, 0.0014, 0.0015, 0.0023, 0.0014, 0.0018, 0.0014, 0.0022, 0.0013,\n                      0.0021, 0.0010, 0.0015, 0.0003, 0.0025, 0.0024, 0.0025, 0.0013, 0.0021,\n                      0.0019, 0.0020, 0.0021, 0.0014, 0.0010, 0.0009, 0.0006, 0.0022, 0.0018,\n                      0.0015, 0.0014, 0.0014, 0.0016, 0.0023, 0.0014, 0.0026, 0.0006, 0.0008,\n                      0.0027, 0.0018, 0.0012, 0.0013, 0.0014, 0.0021, 0.0027, 0.0006, 0.0013,\n                      0.0015, 0.0019, 0.0014, 0.0023, 0.0027, 0.0017, 0.0011, 0.0026, 0.0025,\n                      0.0016, 0.0021, 0.0015, 0.0016, 0.0020, 0.0020, 0.0019, 0.0015, 0.0021,\n                      0.0026, 0.0020, 0.0017, 0.0026, 0.0018, 0.0016, 0.0018, 0.0024, 0.0014,\n                      0.0018, 0.0010, 0.0012, 0.0012, 0.0028, 0.0027, 0.0018, 0.0023, 0.0012,\n                      0.0015, 0.0024, 0.0024, 0.0013, 0.0008, 0.0013, 0.0021, 0.0020, 0.0026,\n                      0.0008, 0.0026, 0.0026, 0.0021, 0.0012, 0.0024, 0.0011, 0.0010, 0.0023,\n                      0.0009, 0.0007, 0.0026, 0.0021, 0.0025, 0.0022, 0.0026, 0.0025, 0.0020,\n                      0.0025, 0.0022, 0.0017, 0.0020, 0.0020, 0.0009, 0.0018, 0.0012, 0.0014,\n                      0.0012, 0.0008, 0.0012, 0.0009, 0.0020, 0.0011, 0.0023, 0.0021, 0.0024,\n                      0.0022, 0.0026, 0.0012, 0.0020, 0.0020, 0.0021, 0.0021, 0.0010, 0.0017,\n                      0.0013, 0.0023, 0.0014, 0.0009, 0.0025, 0.0023, 0.0017, 0.0027, 0.0023,\n                      0.0016, 0.0017, 0.0026, 0.0019, 0.0020, 0.0010, 0.0015, 0.0021, 0.0014,\n                      0.0026, 0.0023, 0.0023, 0.0018, 0.0015, 0.0006, 0.0015, 0.0027, 0.0023,\n                      0.0020, 0.0020, 0.0025, 0.0025, 0.0020, 0.0007, 0.0016, 0.0007, 0.0022,\n                      0.0019, 0.0018, 0.0025, 0.0010, 0.0027, 0.0010, 0.0021, 0.0006, 0.0011,\n                      0.0022, 0.0015, 0.0016, 0.0015, 0.0011, 0.0015, 0.0026, 0.0013, 0.0014,\n                      0.0012, 0.0018, 0.0022, 0.0012, 0.0027, 0.0026, 0.0018, 0.0027, 0.0015,\n                      0.0026, 0.0014, 0.0015, 0.0014, 0.0019, 0.0025, 0.0006, 0.0019, 0.0026,\n                      0.0007, 0.0016, 0.0010, 0.0021, 0.0023, 0.0017, 0.0017, 0.0014, 0.0010,\n                      0.0027, 0.0022, 0.0010, 0.0017, 0.0013, 0.0013, 0.0009, 0.0025, 0.0025,\n                      0.0024, 0.0024, 0.0008, 0.0020, 0.0016, 0.0007, 0.0024, 0.0024, 0.0025,\n                      0.0020, 0.0019, 0.0024, 0.0026, 0.0023, 0.0023], dtype=torch.float64),\n                     zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n                     axis=0)),\n             ('features.5.6.block.0.bias',\n              Parameter containing:\n              tensor([ 9.5135e-04, -1.6917e-03, -3.4124e-02, -7.0958e-03, -4.8007e-03,\n                      -4.0196e-02, -3.4580e-03, -3.8475e-03, -9.7614e-03,  1.7484e-02,\n                      -1.7898e-02,  8.8063e-03,  1.7306e-02, -1.0993e-02,  8.5564e-03,\n                      -5.6063e-03,  7.1514e-03, -2.4158e-03, -1.5583e-02,  1.0917e-03,\n                       1.2394e-03, -2.5507e-03,  1.5737e-02, -5.6967e-04, -2.1788e-03,\n                       3.1987e-02, -4.0104e-03, -3.5536e-03,  3.4141e-03,  4.2914e-02,\n                       1.2200e-02, -3.8078e-03, -6.1270e-03,  2.8629e-03, -1.6039e-02,\n                       3.6566e-03, -2.0339e-02,  1.8699e-02, -3.1507e-03, -4.0333e-03,\n                      -4.9913e-04, -7.9550e-04,  1.2041e-02,  6.1470e-03,  2.1987e-02,\n                       6.3520e-03, -1.3685e-02, -1.7572e-03,  2.6192e-03,  1.0104e-02,\n                       7.2633e-03, -5.4454e-03,  2.7751e-03,  5.8634e-03,  6.8828e-03,\n                      -1.3625e-02,  3.8562e-02,  5.1450e-04,  2.5122e-02,  8.1464e-03,\n                      -5.8952e-03,  1.3400e-02,  2.1945e-02, -1.7340e-02,  1.9620e-02,\n                       7.1336e-03, -1.4525e-02, -1.0642e-02,  1.9724e-02,  2.1996e-02,\n                      -8.4527e-03, -1.1745e-02,  9.5412e-03,  9.8493e-03,  2.2781e-02,\n                      -5.8323e-03, -4.4662e-03,  1.0823e-02, -8.0255e-03,  5.0697e-03,\n                       1.7102e-02,  1.7618e-02,  1.2475e-02, -3.4404e-03,  1.8601e-02,\n                      -9.8285e-03, -3.7735e-03, -4.3967e-03,  4.8213e-05,  5.7851e-03,\n                      -9.5515e-04,  3.1243e-02, -5.5819e-03,  6.9235e-03, -2.0507e-02,\n                      -2.1816e-02, -1.9519e-04,  6.4370e-03, -1.2897e-02, -7.9548e-03,\n                      -1.1666e-03,  4.8101e-03, -1.7932e-02,  1.3516e-02, -4.0552e-04,\n                      -1.6123e-02,  7.0889e-05, -4.3564e-03,  3.0250e-03,  4.4075e-03,\n                       5.7467e-02,  8.6427e-03, -8.1782e-02,  7.4540e-03,  2.8329e-02,\n                      -1.9014e-03, -1.1515e-02,  1.4319e-02, -8.7791e-03,  3.8580e-02,\n                      -1.3645e-02,  1.8070e-03,  1.1088e-02, -2.3236e-03,  4.7244e-02,\n                       1.6869e-03,  2.3183e-02,  1.0467e-02, -1.0766e-03, -6.5043e-03,\n                      -8.2429e-03, -7.3164e-04,  1.0016e-02,  1.5405e-02,  5.9717e-03,\n                       1.2376e-02,  9.0885e-03, -3.1109e-02,  6.8622e-03,  1.3756e-02,\n                       3.1276e-03, -1.0026e-02,  7.4707e-03, -7.0585e-03,  5.6354e-03,\n                      -4.1801e-03,  6.0533e-03,  3.7880e-03, -1.1683e-03, -2.7923e-03,\n                       1.5157e-05,  1.9230e-02, -2.2739e-02, -2.7081e-02, -2.4380e-02,\n                       1.2003e-02, -2.0202e-02, -1.9450e-03, -3.0758e-02,  1.8012e-02,\n                       4.7073e-02, -1.0040e-02, -1.5386e-02, -1.6729e-02, -8.7636e-03,\n                       2.7758e-02, -1.3357e-03, -2.0480e-02,  2.1409e-03,  2.2995e-03,\n                       1.6003e-03,  9.2884e-03, -1.5616e-02,  1.1261e-02,  4.0841e-03,\n                      -3.5091e-02,  5.6639e-03,  6.3657e-03, -6.4638e-03, -8.7630e-03,\n                      -5.0271e-02,  1.4257e-03, -5.7750e-03, -9.0489e-03, -1.1250e-02,\n                      -3.6393e-03, -3.9942e-03,  6.6420e-03, -5.6894e-03, -9.6050e-03,\n                      -1.8466e-02,  7.6263e-03,  2.5281e-02, -1.3578e-03,  1.4401e-02,\n                      -2.2466e-03,  8.6573e-03, -4.7037e-02,  1.3843e-02, -1.8471e-02,\n                      -1.7174e-02, -7.1573e-03, -1.8145e-02,  6.6274e-04, -7.4769e-03,\n                       3.5329e-03, -2.0450e-02, -7.6051e-04, -8.8754e-03, -3.3647e-02,\n                      -5.8305e-03,  2.1255e-03,  8.8511e-04, -1.1004e-02,  5.4624e-03,\n                       1.9796e-02,  1.2404e-03, -1.2184e-02,  1.1227e-02,  3.6110e-02,\n                      -6.3941e-03,  1.5223e-03,  1.8039e-02,  8.5794e-04, -4.3997e-02,\n                       1.6538e-02, -9.5486e-03, -6.7020e-03,  8.8338e-03, -3.7977e-03,\n                      -6.9697e-03,  1.8004e-03,  7.9382e-03, -1.3996e-02,  5.9175e-03,\n                      -3.6961e-04,  8.6243e-03,  1.6320e-02,  7.3422e-03,  1.6092e-02,\n                       8.2859e-03,  2.1481e-03,  6.7707e-03,  2.3893e-02, -1.5962e-02,\n                       8.1490e-03, -4.6168e-03,  2.1298e-02,  2.2869e-03, -2.7017e-02,\n                       3.7322e-03, -2.8982e-03,  6.0924e-03,  1.3490e-02, -4.2367e-03,\n                       1.2930e-03, -3.1410e-03,  4.1491e-03, -5.9575e-03,  1.2076e-02,\n                       1.3814e-02,  5.3806e-03, -2.1661e-02,  1.8330e-02, -2.3541e-02,\n                      -2.2895e-03,  6.2741e-04, -2.7044e-02,  9.1716e-03,  5.4353e-03,\n                      -1.2120e-03,  1.5161e-04, -9.4709e-03, -8.3120e-03,  7.1093e-02,\n                      -2.3343e-03,  7.7991e-03,  1.0790e-02, -3.5412e-03,  1.7302e-02,\n                       4.7866e-02, -9.9354e-03,  1.7575e-02, -7.5294e-03,  7.9180e-04,\n                       3.2292e-03, -6.9818e-03, -6.3145e-03,  8.2167e-03,  2.2458e-03,\n                      -1.9059e-02,  1.0134e-02,  7.7866e-03, -5.1583e-03, -1.0508e-02,\n                       5.6702e-03,  8.7821e-03,  4.6418e-03,  1.2449e-02, -1.0836e-02,\n                       2.3244e-03, -3.6447e-03,  2.5753e-02, -6.4599e-03, -5.1843e-03,\n                      -3.8739e-03, -1.6468e-02, -1.9925e-02,  3.0912e-03,  2.0666e-03,\n                      -3.0811e-03,  6.6650e-02, -3.3534e-03,  1.2899e-02,  3.7098e-02,\n                       2.1871e-02,  2.7147e-03,  8.6090e-03,  1.0780e-03, -1.2193e-02,\n                      -5.8610e-03, -9.8885e-03, -1.9393e-02,  3.7463e-02, -3.4514e-04,\n                       1.8903e-02,  1.7939e-03,  9.3016e-03,  1.7237e-02, -1.5716e-03,\n                      -8.4574e-03,  9.8492e-03,  8.9111e-03,  1.4489e-02, -4.8692e-03,\n                       1.0609e-02,  3.9515e-03,  2.2045e-03, -2.5999e-02, -6.6135e-03,\n                      -9.4171e-04, -4.0291e-03,  7.7603e-03,  3.4245e-03,  2.6835e-07,\n                       1.1877e-02, -1.6030e-02, -6.4494e-03, -6.7844e-02,  4.2257e-03,\n                       5.4953e-03,  2.6946e-03,  1.4463e-03,  5.6835e-02,  5.4328e-03,\n                      -7.3806e-02,  5.3913e-03, -2.6475e-03, -7.8206e-03,  1.5339e-03,\n                       1.3936e-02, -1.4913e-02, -5.9715e-04, -1.2251e-03,  5.5368e-03,\n                      -1.4920e-02, -6.8355e-03, -1.1753e-02, -1.5525e-02,  7.4166e-03,\n                      -1.1720e-02, -1.2667e-02,  5.5047e-02,  3.7356e-03, -1.9074e-03,\n                       4.7782e-03,  1.4430e-02, -1.3490e-02,  1.6018e-02,  2.7673e-03,\n                       5.2806e-04, -5.5488e-03, -1.6585e-02, -2.0082e-02],\n                     requires_grad=True)),\n             ('features.5.6.block.0.scale', tensor(0.0074)),\n             ('features.5.6.block.0.zero_point', tensor(59)),\n             ('features.5.6.block.2.weight',\n              tensor([1.1126, 2.3044, 1.0442, 1.8156, 0.9188, 0.9716, 1.4541, 0.6677, 0.7301,\n                      1.5330, 1.5636, 0.6943, 1.5644, 0.9164, 0.8724, 0.9727, 1.1874, 0.7859,\n                      0.6390, 1.0216, 0.8133, 0.9747, 1.6839, 0.7696, 1.1450, 2.2953, 1.5313,\n                      1.9941, 0.6514, 1.0297, 0.7799, 1.3179, 0.8384, 0.8288, 0.7768, 0.6323,\n                      1.6707, 0.7013, 0.7257, 0.8257, 0.9117, 1.4686, 1.6384, 0.7733, 0.6751,\n                      0.7380, 0.9294, 1.0189, 1.0937, 1.3374, 0.9600, 0.7368, 1.0302, 0.6933,\n                      1.1499, 1.9948, 0.7502, 0.9499, 1.6706, 0.7993, 1.8799, 0.6602, 1.8427,\n                      0.8308, 0.8192, 1.2158, 1.6304, 0.8823, 0.6925, 1.4557, 0.8753, 1.1293,\n                      0.7370, 1.8474, 0.7844, 0.7556, 0.8906, 0.8414, 1.0300, 0.8172, 0.8748,\n                      1.3088, 1.6373, 1.3217, 0.7484, 1.1267, 0.7117, 0.8493, 1.2870, 0.7877,\n                      1.6003, 0.7511, 1.1184, 1.3244, 0.8827, 0.9327, 1.0458, 0.8726, 0.7332,\n                      0.8442, 0.8464, 0.6371, 1.2956, 1.2933, 1.9831, 1.1532, 1.0450, 0.8146,\n                      1.1094, 0.7249, 0.8275, 0.5807, 0.6775, 1.2604, 0.8523, 0.6109, 0.7500,\n                      0.8327, 1.3757, 0.8645, 0.9216, 1.1214, 0.9649, 0.4468, 0.4975, 1.1387,\n                      1.8023, 0.7948, 1.1071, 0.8101, 0.8739, 0.7558, 0.6241, 0.8385, 1.3706,\n                      1.5198, 1.0535, 1.9930, 0.8522, 1.6415, 0.8968, 0.7094, 0.8498, 1.4826,\n                      1.5194, 0.8704, 0.8862, 1.0344, 0.7849, 0.9373, 0.9598, 1.0027, 0.7467,\n                      0.8900, 0.7482, 0.8009, 0.0961, 1.4402, 1.3609, 1.2619, 0.6026, 1.0031,\n                      0.7794, 1.4230, 0.8737, 0.7804, 0.6161, 0.6882, 0.6219, 0.9353, 0.8456,\n                      0.7561, 0.7200, 0.6381, 0.7488, 1.8449, 0.7436, 1.5354, 0.7431, 0.6256,\n                      2.7767, 0.9231, 0.6122, 0.6800, 0.7198, 0.9494, 1.6442, 0.6984, 0.8642,\n                      0.9136, 1.3000, 0.8822, 1.0017, 1.9120, 0.7715, 0.8072, 1.3144, 2.2734,\n                      0.7954, 0.9935, 0.8314, 0.7967, 1.0139, 1.0995, 0.8202, 0.7854, 1.0104,\n                      1.1743, 0.9578, 0.8663, 1.9931, 0.8663, 0.8173, 0.8065, 1.0495, 0.6995,\n                      1.0998, 0.9831, 0.7229, 0.5584, 1.6463, 2.0914, 0.9141, 1.1631, 0.7816,\n                      0.9128, 1.3247, 1.5868, 0.7198, 0.6360, 0.7246, 1.0033, 0.9876, 1.3276,\n                      0.6519, 1.4333, 1.4346, 1.0522, 0.6841, 1.2112, 0.8531, 0.6952, 1.0025,\n                      0.6526, 0.6764, 1.2816, 0.9340, 1.9024, 0.9079, 1.7548, 1.3852, 0.9581,\n                      1.3618, 0.8923, 0.7121, 0.8119, 0.9516, 0.5966, 0.7770, 0.7574, 0.7183,\n                      0.7380, 0.7142, 0.6302, 1.0266, 1.0586, 0.8225, 1.2559, 0.8567, 1.4108,\n                      0.9884, 1.3302, 0.7428, 0.9154, 1.8264, 0.9766, 0.9415, 0.6833, 0.8591,\n                      0.8237, 2.4606, 0.5926, 0.6501, 2.1680, 1.0961, 0.6792, 1.7186, 1.1010,\n                      0.7577, 0.8918, 1.8809, 0.8727, 0.8333, 0.8989, 0.7107, 0.9847, 0.6997,\n                      1.9320, 1.2664, 1.4792, 0.8634, 0.7874, 0.5004, 0.7261, 1.7219, 1.1965,\n                      0.9335, 0.9909, 1.3077, 1.8783, 0.8693, 0.7549, 0.7636, 0.6471, 2.1229,\n                      0.9144, 0.8256, 2.5300, 0.9113, 1.8912, 0.4964, 0.9415, 0.6410, 0.6264,\n                      0.8565, 0.5333, 1.0987, 0.8311, 0.6965, 0.8019, 1.4803, 0.7403, 0.8108,\n                      0.7322, 0.8036, 1.3152, 0.6570, 1.8447, 2.2931, 0.7070, 1.6251, 0.7799,\n                      1.3509, 0.7748, 0.9412, 0.7702, 0.7530, 1.7266, 0.9991, 0.9531, 1.5510,\n                      0.7286, 0.7499, 2.9120, 0.9379, 2.9969, 0.7323, 0.8100, 0.6040, 0.7525,\n                      2.1311, 1.0642, 0.6734, 0.7792, 0.9640, 0.8145, 0.7894, 1.3362, 1.5737,\n                      2.0082, 1.2360, 0.6449, 0.8665, 0.8367, 0.6503, 2.0660, 1.2322, 1.5409,\n                      0.8290, 0.9237, 1.2627, 1.2549, 0.9483, 0.9934])),\n             ('features.5.6.block.2.bias',\n              tensor([-4.2217e-01,  4.8584e-02,  6.5674e-01,  1.3192e-01, -7.6071e-03,\n                       4.6513e-01, -1.8077e-01, -1.4826e-01,  1.2659e-01, -5.1181e-01,\n                       1.1193e+00, -3.0683e-01, -5.7838e-01,  3.2806e-01, -1.0939e-01,\n                      -2.5274e-03, -2.7389e-01, -6.2869e-02,  1.8695e-01,  4.5965e-02,\n                      -1.0126e-01,  7.9790e-02,  5.2049e-03, -1.9916e-01,  8.7033e-02,\n                       1.2263e+00, -1.3929e-01, -2.1198e-01, -1.9695e-01,  1.2295e+00,\n                      -2.3673e-01,  2.3451e-01,  5.3887e-03, -8.9566e-01,  1.7263e-01,\n                      -2.3045e-02,  4.4599e-01, -2.7169e-01,  5.0590e-01, -1.3207e-01,\n                       3.1380e-02,  8.7704e-02, -2.5592e-01, -1.0586e-02, -5.7905e-02,\n                       1.7837e-01, -3.2038e-01, -1.3134e-02, -1.2181e-01, -1.6236e-01,\n                      -8.4040e-02, -2.2078e-01, -2.2844e-01, -3.8300e-01, -5.6592e-02,\n                       3.9931e-01, -6.1371e-01,  1.1763e-01, -5.4469e-01,  1.0184e-01,\n                       1.3048e-01, -3.7036e-01,  1.3910e-01,  1.5255e-01,  2.6951e-02,\n                       3.1282e-02,  1.5463e-02,  1.9332e-01, -2.7348e-01,  1.8008e+00,\n                       1.6016e-01,  7.8060e-02, -4.4561e-01,  6.4086e-02, -2.8759e-01,\n                      -1.3561e-01,  9.4259e-02,  1.7014e-02,  8.3118e-02, -2.6673e-01,\n                      -2.3245e-01, -2.5096e-01, -5.7117e-01,  1.6306e-02,  1.5404e-01,\n                       2.2297e-01,  5.3589e-02,  3.3519e-02, -6.6752e-02, -1.9220e-02,\n                      -1.3575e-01, -7.2745e-01,  7.7853e-02, -2.6777e-01,  2.9755e-01,\n                       3.6655e-02, -3.9978e-02, -1.2533e-01,  1.2374e-01, -5.5903e-02,\n                       8.5184e-02,  3.7677e-02,  3.9879e-01, -1.1187e-01, -1.0933e-01,\n                       2.5859e-01,  2.8469e-02, -1.1262e-02,  5.5629e-02, -5.1763e-02,\n                      -7.4576e-01, -1.4109e-02,  4.0608e-01, -3.7430e-02, -5.1805e-01,\n                       2.2324e-01, -1.1987e-02, -1.3177e-01,  7.4667e-01, -5.3683e-01,\n                       1.3681e-01, -7.1902e-02, -4.4111e-02,  1.0210e-01, -1.7280e+00,\n                       2.7082e-01, -4.0985e-01, -2.8407e-03,  1.8301e-02,  1.7371e-01,\n                       6.1550e-02,  1.6665e-02, -1.2382e-01, -2.0429e-01, -1.4548e-01,\n                      -9.5920e-01, -5.5690e-02, -1.4693e+00, -3.4164e-01, -4.2712e-01,\n                      -1.1821e-03,  9.3564e-02, -1.6964e-01,  1.3287e-01, -2.2391e-02,\n                       3.0443e-02,  1.5296e-01, -1.3558e-01,  1.9601e-01,  4.6441e-02,\n                      -2.7004e-02, -2.5389e-01,  2.0008e-01,  3.9378e-01,  3.0318e-01,\n                      -5.6930e-02, -1.6568e-01,  1.4465e-01,  5.5568e-01, -4.7913e-01,\n                      -5.6887e-01,  1.3408e-01, -4.8612e-02,  3.1336e-01,  9.3054e-02,\n                      -3.0862e-02, -3.2161e-02,  2.3470e-01, -1.5610e-01,  9.9284e-02,\n                      -4.9890e-02,  3.6824e-02,  5.4204e-01, -1.0901e-01, -8.8964e-02,\n                      -4.8796e-01, -2.3000e-03, -4.8511e-02,  3.0777e-01,  2.3928e-01,\n                      -3.3871e-01, -7.8390e-02,  2.5054e-01, -1.6257e-01, -3.3719e-03,\n                       8.5051e-02,  1.2459e-01, -1.4211e-01,  6.9006e-02,  9.8697e-02,\n                       2.8967e-01, -9.3701e-01, -3.4717e-01,  2.5566e-01, -2.0051e-01,\n                       2.9355e-02, -7.5888e-01, -1.6371e+00, -1.1585e-01,  3.9933e-01,\n                       1.4825e-01,  1.7057e-01,  3.2221e-01, -2.1508e-02, -3.1876e-02,\n                      -1.7651e-02,  2.4619e-01, -1.2663e-01,  1.7292e-02,  5.2022e-01,\n                       6.4920e-02, -2.9217e-02,  4.0837e-03,  1.0424e-01, -3.1429e-01,\n                      -1.3498e-01, -4.1959e-02,  4.7753e-02, -2.6162e-01, -2.0727e-01,\n                      -1.1063e-01,  7.5646e-02, -2.7549e-01, -1.7500e-01,  5.6224e-01,\n                      -1.8438e-01,  1.6748e-01,  1.4207e-01,  1.5062e-02, -9.3697e-02,\n                       1.8012e-01, -5.3950e-03, -8.4795e-02,  2.3029e-01,  5.1872e-02,\n                      -1.6460e-02, -1.1199e-01, -2.2558e-01, -2.8303e-01, -1.3710e-01,\n                      -1.6882e-01, -3.5947e-02, -4.8859e-02, -4.2224e-01,  2.1471e-02,\n                      -1.1434e-01,  5.9587e-02, -4.9141e-01,  8.5939e-02,  3.2153e-01,\n                      -1.0894e-01,  5.2046e-02, -3.1562e-02, -7.3856e-02,  1.9413e-01,\n                       1.0057e-02,  7.7669e-02,  1.3492e-01,  1.8893e-01, -1.4958e-01,\n                      -4.6610e-01, -1.5841e-01,  3.8310e-01, -3.5567e-02, -2.1150e+00,\n                       6.2659e-02, -1.2951e-02,  1.7097e-01, -1.0044e-01, -3.1266e-02,\n                      -1.4532e-01,  4.4352e-02,  2.4304e-01,  1.5193e-01,  2.8990e+00,\n                       3.6792e-03,  1.7566e-04, -3.3182e-02,  2.1089e-02, -1.1993e-01,\n                       1.5554e+00,  3.9937e-02, -4.6017e-02, -1.0698e-02,  3.5564e-02,\n                      -6.5200e-03,  1.3608e-01,  1.5639e-01, -1.5648e-01, -1.1130e-01,\n                      -5.7652e-01, -2.0860e-01, -2.4847e-01,  1.2431e-01, -1.6738e-02,\n                      -1.1451e-01, -4.9388e-02,  6.5820e-02, -1.6457e-01,  5.3450e-01,\n                       4.7987e-02,  1.6401e-02, -3.0886e-01,  2.7509e-02,  2.6682e-01,\n                       8.5686e-02,  4.1240e-01,  4.5973e-01,  3.0687e-02, -7.7764e-02,\n                       5.6934e-02, -4.0862e-01,  2.0690e-02,  1.6859e-03,  5.3680e-01,\n                      -1.4737e-01,  8.8453e-02,  1.9142e-01,  2.7044e-01,  9.6040e-02,\n                      -4.9730e-02,  6.4522e-02, -4.9760e-02, -2.7173e-01,  6.1629e-02,\n                      -5.2520e-01, -2.8573e-03, -1.6527e-01, -2.5393e-01,  5.7122e-04,\n                       4.3493e-02, -3.0691e-01, -5.6887e-01,  2.5820e-02, -5.7351e-02,\n                      -2.1252e-01, -3.6461e-02,  9.5018e-02,  5.5331e-02,  7.2529e-02,\n                      -6.1110e-02,  4.3811e-02,  1.4751e-01, -8.4865e-03,  1.3090e-01,\n                      -2.6969e-02,  2.3320e-01, -1.2150e-01, -1.2792e+00,  2.0765e-02,\n                       3.7340e-02, -5.9241e-01, -2.3097e-02,  2.5580e-01, -2.1512e-01,\n                      -1.4728e+00, -6.1874e-02,  4.2673e-02,  5.1925e-02,  9.5169e-02,\n                       1.1261e-01,  2.2456e-01,  3.6737e-01, -6.6657e-03,  2.8054e-01,\n                       5.0520e-01,  4.7442e-02,  2.4718e-01,  1.5539e-01, -2.9323e-01,\n                       4.6944e-01, -1.1019e-01, -8.4721e-01, -1.5723e-01, -8.2758e-02,\n                      -1.2557e-01,  1.0748e-01,  1.2803e-01, -1.9997e-01,  1.7925e-02,\n                       2.7149e-02,  7.9029e-02,  2.8624e-01,  2.7348e-01])),\n             ('features.5.6.block.2.scale', tensor(0.2462)),\n             ('features.5.6.block.2.zero_point', tensor(74)),\n             ('features.5.6.block.3.scale', tensor(0.0939)),\n             ('features.5.6.block.3.zero_point', tensor(83)),\n             ('features.5.6.block.3._packed_params.dtype', torch.qint8),\n             ('features.5.6.block.3._packed_params._packed_params',\n              (tensor([[ 0.0963, -0.0107,  0.0064,  ...,  0.0578, -0.0920, -0.1562],\n                       [-0.0219,  0.0231, -0.0058,  ..., -0.0081, -0.1443,  0.0012],\n                       [ 0.0054,  0.0094, -0.0148,  ..., -0.0027, -0.0364,  0.0580],\n                       ...,\n                       [-0.0293,  0.0354, -0.0139,  ..., -0.0123, -0.0108,  0.0031],\n                       [-0.0217, -0.0317, -0.0450,  ..., -0.0467, -0.0133,  0.0500],\n                       [ 0.0188, -0.0362, -0.0246,  ...,  0.0304, -0.0145, -0.0883]],\n                      size=(1536, 384), dtype=torch.qint8,\n                      quantization_scheme=torch.per_channel_affine,\n                      scale=tensor([0.0021, 0.0012, 0.0013,  ..., 0.0015, 0.0017, 0.0014],\n                      dtype=torch.float64),\n                      zero_point=tensor([0, 0, 0,  ..., 0, 0, 0]), axis=0),\n               Parameter containing:\n               tensor([-0.0736, -0.0366, -0.0561,  ..., -0.0436, -0.0389, -0.0334],\n                      requires_grad=True))),\n             ('features.5.6.block.5.scale', tensor(0.1104)),\n             ('features.5.6.block.5.zero_point', tensor(98)),\n             ('features.5.6.block.5._packed_params.dtype', torch.qint8),\n             ('features.5.6.block.5._packed_params._packed_params',\n              (tensor([[ 0.0129, -0.0097,  0.0178,  ..., -0.0824, -0.0468,  0.1405],\n                       [-0.0057, -0.0057,  0.1202,  ...,  0.0486,  0.0486,  0.0887],\n                       [-0.0608,  0.0512, -0.1055,  ..., -0.0672,  0.0368,  0.0544],\n                       ...,\n                       [-0.0830,  0.0277,  0.0026,  ..., -0.0198, -0.0290,  0.0171],\n                       [ 0.0503, -0.0015,  0.0222,  ...,  0.0873, -0.0148, -0.0030],\n                       [ 0.0143, -0.0643,  0.0343,  ...,  0.0400,  0.0343,  0.0114]],\n                      size=(384, 1536), dtype=torch.qint8,\n                      quantization_scheme=torch.per_channel_affine,\n                      scale=tensor([0.0016, 0.0029, 0.0016, 0.0025, 0.0018, 0.0023, 0.0015, 0.0017, 0.0017,\n                       0.0017, 0.0014, 0.0016, 0.0032, 0.0018, 0.0017, 0.0017, 0.0017, 0.0013,\n                       0.0016, 0.0016, 0.0018, 0.0015, 0.0019, 0.0019, 0.0015, 0.0019, 0.0020,\n                       0.0021, 0.0020, 0.0029, 0.0039, 0.0017, 0.0020, 0.0016, 0.0015, 0.0018,\n                       0.0015, 0.0014, 0.0016, 0.0016, 0.0024, 0.0017, 0.0019, 0.0017, 0.0016,\n                       0.0041, 0.0015, 0.0018, 0.0014, 0.0017, 0.0021, 0.0056, 0.0014, 0.0016,\n                       0.0017, 0.0039, 0.0015, 0.0018, 0.0017, 0.0017, 0.0017, 0.0017, 0.0016,\n                       0.0013, 0.0019, 0.0037, 0.0014, 0.0020, 0.0016, 0.0015, 0.0019, 0.0017,\n                       0.0022, 0.0034, 0.0016, 0.0021, 0.0017, 0.0011, 0.0014, 0.0016, 0.0016,\n                       0.0015, 0.0016, 0.0025, 0.0015, 0.0022, 0.0026, 0.0014, 0.0025, 0.0015,\n                       0.0015, 0.0016, 0.0017, 0.0016, 0.0017, 0.0020, 0.0018, 0.0019, 0.0013,\n                       0.0032, 0.0015, 0.0021, 0.0027, 0.0015, 0.0023, 0.0018, 0.0018, 0.0019,\n                       0.0017, 0.0046, 0.0017, 0.0021, 0.0022, 0.0016, 0.0022, 0.0016, 0.0016,\n                       0.0023, 0.0021, 0.0016, 0.0015, 0.0015, 0.0018, 0.0017, 0.0026, 0.0038,\n                       0.0022, 0.0015, 0.0016, 0.0011, 0.0011, 0.0014, 0.0017, 0.0029, 0.0014,\n                       0.0023, 0.0017, 0.0020, 0.0013, 0.0014, 0.0017, 0.0018, 0.0016, 0.0014,\n                       0.0014, 0.0015, 0.0028, 0.0018, 0.0019, 0.0024, 0.0022, 0.0016, 0.0017,\n                       0.0019, 0.0013, 0.0021, 0.0095, 0.0022, 0.0015, 0.0020, 0.0018, 0.0015,\n                       0.0015, 0.0021, 0.0019, 0.0015, 0.0017, 0.0015, 0.0017, 0.0016, 0.0016,\n                       0.0020, 0.0013, 0.0017, 0.0015, 0.0039, 0.0015, 0.0019, 0.0023, 0.0016,\n                       0.0023, 0.0024, 0.0016, 0.0018, 0.0014, 0.0018, 0.0028, 0.0015, 0.0041,\n                       0.0037, 0.0022, 0.0015, 0.0015, 0.0017, 0.0021, 0.0012, 0.0017, 0.0025,\n                       0.0017, 0.0016, 0.0018, 0.0017, 0.0016, 0.0018, 0.0015, 0.0019, 0.0013,\n                       0.0013, 0.0016, 0.0041, 0.0016, 0.0019, 0.0017, 0.0017, 0.0015, 0.0031,\n                       0.0016, 0.0012, 0.0015, 0.0031, 0.0016, 0.0018, 0.0019, 0.0014, 0.0016,\n                       0.0025, 0.0019, 0.0016, 0.0017, 0.0017, 0.0015, 0.0014, 0.0019, 0.0015,\n                       0.0017, 0.0017, 0.0015, 0.0031, 0.0024, 0.0014, 0.0015, 0.0017, 0.0016,\n                       0.0019, 0.0018, 0.0022, 0.0018, 0.0015, 0.0021, 0.0014, 0.0034, 0.0022,\n                       0.0017, 0.0019, 0.0016, 0.0018, 0.0015, 0.0016, 0.0021, 0.0017, 0.0019,\n                       0.0023, 0.0016, 0.0013, 0.0030, 0.0022, 0.0015, 0.0015, 0.0015, 0.0016,\n                       0.0014, 0.0018, 0.0015, 0.0022, 0.0026, 0.0022, 0.0015, 0.0017, 0.0016,\n                       0.0015, 0.0027, 0.0015, 0.0019, 0.0017, 0.0015, 0.0016, 0.0014, 0.0019,\n                       0.0016, 0.0015, 0.0015, 0.0016, 0.0015, 0.0012, 0.0017, 0.0017, 0.0017,\n                       0.0018, 0.0020, 0.0016, 0.0015, 0.0018, 0.0021, 0.0021, 0.0016, 0.0015,\n                       0.0014, 0.0014, 0.0014, 0.0019, 0.0015, 0.0018, 0.0018, 0.0017, 0.0027,\n                       0.0015, 0.0014, 0.0018, 0.0025, 0.0022, 0.0017, 0.0012, 0.0016, 0.0022,\n                       0.0016, 0.0022, 0.0011, 0.0024, 0.0018, 0.0032, 0.0016, 0.0016, 0.0017,\n                       0.0019, 0.0017, 0.0014, 0.0014, 0.0014, 0.0027, 0.0030, 0.0016, 0.0017,\n                       0.0015, 0.0014, 0.0030, 0.0016, 0.0015, 0.0027, 0.0030, 0.0016, 0.0016,\n                       0.0019, 0.0015, 0.0053, 0.0015, 0.0026, 0.0018, 0.0014, 0.0017, 0.0018,\n                       0.0027, 0.0013, 0.0015, 0.0015, 0.0030, 0.0014, 0.0014, 0.0018, 0.0018,\n                       0.0017, 0.0013, 0.0016, 0.0014, 0.0018, 0.0014, 0.0016, 0.0023, 0.0018,\n                       0.0017, 0.0028, 0.0015, 0.0013, 0.0015, 0.0014], dtype=torch.float64),\n                      zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n                      axis=0),\n               Parameter containing:\n               tensor([-3.9077e-02,  1.8356e-03, -2.3318e-02, -3.4404e-02, -1.2240e-02,\n                        1.6329e-02,  7.3164e-03,  1.3199e-02, -1.9398e-02, -1.4927e-02,\n                       -3.2861e-02,  6.2947e-03, -1.7545e-01,  3.2810e-02, -4.4477e-03,\n                        1.0237e-02,  2.5315e-02,  7.0430e-03, -2.4635e-03, -3.0287e-04,\n                       -1.7050e-02, -3.3916e-03,  6.2231e-02,  7.8234e-03,  9.7427e-04,\n                        4.8472e-02, -6.9843e-03, -1.9892e-02,  1.9003e-02,  3.5258e-02,\n                       -1.4196e-02,  1.3219e-02, -1.3469e-02, -1.3720e-02, -4.1289e-03,\n                        2.2484e-02,  4.1427e-02,  5.9547e-04, -1.0402e-02,  1.2377e-02,\n                       -3.2020e-02,  2.2868e-03, -6.2970e-02, -2.0503e-02,  5.7938e-03,\n                       -1.1466e-02,  1.7013e-01,  7.4264e-03,  1.4045e-02,  1.0500e-02,\n                       -2.5340e-02,  4.4858e-03, -1.3362e-02,  6.3965e-03,  9.1939e-03,\n                        5.3843e-03, -2.4233e-02, -9.9428e-03,  2.4547e-03, -1.0663e-02,\n                        1.3993e-02, -1.8791e-02,  2.3364e-02, -3.7402e-03, -6.3921e-03,\n                       -7.4766e-03, -1.0913e-02, -5.7223e-03, -8.6884e-03,  6.3585e-02,\n                        2.3315e-02,  1.8213e-02,  8.4001e-03,  9.7616e-03,  7.9557e-03,\n                        1.1066e-02,  1.4470e-02, -1.4767e-02, -2.9875e-03,  2.1335e-02,\n                        1.9548e-02,  9.2751e-03, -2.9632e-02, -2.2777e-02, -1.2796e-02,\n                       -1.1816e-02, -4.6040e-02, -8.0428e-03, -4.8532e-03, -2.4860e-03,\n                       -2.8818e-02,  3.8504e-03,  4.6780e-02, -2.3883e-02, -3.4200e-03,\n                        7.9701e-03, -5.8714e-03,  5.2171e-03, -9.1187e-03,  5.2823e-03,\n                        1.0973e-02, -4.0894e-03,  1.6480e-02, -2.6201e-02, -4.1623e-02,\n                       -8.0417e-03,  1.9094e-03,  1.4220e-02, -1.3005e-02,  1.9393e-02,\n                        3.0511e-02, -1.9775e-02,  2.3159e-02,  1.4720e-04,  6.8792e-03,\n                        1.4382e-03,  1.1281e-02,  2.9424e-02,  2.9134e-02,  2.0509e-03,\n                        1.1721e-02, -7.1342e-03,  1.4749e-02,  1.7412e-03,  1.2936e-01,\n                        2.6164e-01, -1.5975e-02, -4.2361e-04, -6.5434e-03, -3.9285e-02,\n                       -8.4636e-03, -7.5795e-03,  1.1160e-03,  2.4588e-02,  1.1954e-02,\n                       -1.1083e-01,  5.4625e-03, -6.5840e-02, -6.0598e-03, -2.3740e-02,\n                        7.6630e-03, -1.8776e-03, -1.8459e-02,  8.1361e-03,  5.6205e-03,\n                        2.0789e-02,  7.2455e-03,  1.2041e-02, -2.8485e-03, -2.5665e-03,\n                        3.2980e-02,  2.5610e-03,  2.4689e-03,  1.6214e-02,  5.6207e-03,\n                        8.1722e-03, -1.5399e+00, -4.5538e-02,  1.1740e-02, -1.6166e-02,\n                        4.1473e-03,  3.3782e-03,  1.2942e-03,  2.7566e-03, -6.2947e-03,\n                        9.7590e-03,  9.2975e-03, -1.1114e-02,  4.2122e-03, -4.2434e-03,\n                        6.9824e-03, -1.7466e-02,  4.1965e-02, -1.7213e-02, -2.1784e-02,\n                        6.4935e-02,  1.5991e-02,  7.6239e-03, -7.3276e-03, -5.2346e-03,\n                        7.9772e-02,  1.6729e-02, -2.2605e-03, -1.1247e-02, -9.7097e-03,\n                       -1.3297e-02, -2.5715e-02, -1.6500e-02,  1.5183e-02, -1.9748e-02,\n                        8.4776e-04,  2.5323e-02, -4.8242e-03, -5.0144e-03, -9.8676e-03,\n                        8.2817e-03, -3.5637e-02,  4.9491e-02, -3.6060e-02, -2.5950e-02,\n                       -1.2465e-02,  5.7898e-03, -7.0312e-03,  6.5361e-03,  5.0505e-03,\n                        6.1474e-03,  6.6553e-03, -2.9867e-03,  1.8598e-02,  2.2104e-02,\n                        9.1795e-03, -1.7708e-01, -3.0479e-03, -1.2622e-02,  7.2647e-03,\n                        2.7634e-02, -1.0244e-02,  3.5143e-03, -9.4819e-03,  1.2146e-02,\n                       -4.0602e-02, -3.8326e-02, -1.0089e-03, -8.2792e-03,  2.1193e-02,\n                        1.1856e-02, -2.5069e-03, -4.8401e-03,  1.2990e-02, -7.3207e-03,\n                       -4.1921e-02, -1.2219e-02,  3.2803e-03,  3.1708e-02, -7.7261e-03,\n                       -1.2375e-02, -1.5112e-03, -1.6451e-02, -1.2968e-02,  1.3990e-02,\n                        3.5968e-02,  5.5577e-03, -1.5821e-02, -1.8621e-02,  1.5398e-03,\n                        3.0837e-02, -7.3135e-03,  2.5206e-02,  4.0662e-02, -8.5005e-03,\n                       -6.4023e-03, -1.6221e-02, -3.9864e-03,  5.1610e-03, -4.2764e-03,\n                        3.3002e-03,  2.2952e-02,  2.8903e-03,  1.6601e-02,  9.3769e-03,\n                        2.7886e-03, -9.4053e-03,  2.9497e-03,  5.3232e-03,  5.9209e-02,\n                        5.9410e-03,  1.3526e-02, -1.8165e-02,  8.5889e-03,  1.7140e-02,\n                        3.3801e-02,  1.1862e-02,  2.0581e-02,  1.8160e-02,  1.1155e-01,\n                        1.8842e-03,  1.1718e-02, -1.8858e-02, -1.7123e-02,  1.4179e-02,\n                       -7.9974e-02, -9.0071e-04,  1.3363e-02, -1.2538e-02,  8.2199e-03,\n                       -5.7752e-03,  4.8597e-03,  1.9592e-02, -4.6823e-03,  1.0954e-02,\n                        4.1975e-02, -4.4675e-03,  4.2583e-03,  4.6556e-02, -1.0025e-02,\n                        8.3081e-05,  2.0052e-03,  8.3762e-03, -9.8890e-03, -2.3886e-02,\n                        2.8062e-02,  8.8615e-04, -1.4126e-02,  3.2385e-02, -1.2108e-02,\n                        1.9819e-03,  8.6018e-05,  2.9060e-02, -7.2526e-03,  2.7387e-02,\n                       -2.1552e-03,  1.7285e-02, -2.7822e-03,  2.0994e-02, -5.6055e-02,\n                        1.4186e-02, -4.4131e-03, -5.3902e-02,  2.7388e-02, -4.1360e-02,\n                        1.0340e-02,  9.3032e-03,  8.5047e-03,  1.3317e-02, -2.2244e-03,\n                        7.1975e-04,  1.6642e-02,  1.8796e-02,  1.0599e-02,  1.0877e-02,\n                        2.5531e-02, -9.6135e-03, -2.4616e-02, -1.3060e-02, -5.3672e-03,\n                        3.0884e-02, -2.2907e-03, -2.0483e-02,  4.0227e-02, -4.5227e-02,\n                       -2.1740e-02, -9.7021e-03,  2.5616e-03,  7.5040e-03, -1.4737e-02,\n                        2.4901e-03, -4.2033e-02, -2.2442e-02,  2.2622e-01,  2.2485e-02,\n                        1.2292e-02,  2.6039e-02,  9.3357e-03,  2.3662e-02,  1.4708e-02,\n                        7.8279e-02, -8.9923e-03,  6.4539e-03, -7.4592e-03, -1.4858e-02,\n                       -2.6396e-02, -8.6714e-03, -4.0644e-03,  7.5652e-03, -7.7958e-03,\n                       -1.3872e-02,  6.1959e-04,  3.7500e-03, -9.8846e-03,  7.1764e-02,\n                        3.0657e-02,  2.1614e-03, -3.4217e-02,  9.4768e-04,  2.9284e-03,\n                       -7.4315e-02,  2.9788e-03, -1.5549e-03, -6.6007e-03, -3.4716e-02,\n                       -1.3017e-02,  3.3050e-03, -1.9241e-02,  1.6180e-02],\n                      requires_grad=True))),\n             ('features.5.7.layer_scale',\n              tensor([[[-0.1041]],\n              \n                      [[ 0.0763]],\n              \n                      [[-0.1517]],\n              \n                      [[ 0.2530]],\n              \n                      [[-0.1968]],\n              \n                      [[ 0.1973]],\n              \n                      [[ 0.0672]],\n              \n                      [[-0.2327]],\n              \n                      [[ 0.1980]],\n              \n                      [[ 0.1206]],\n              \n                      [[ 0.0751]],\n              \n                      [[ 0.2552]],\n              \n                      [[ 0.4161]],\n              \n                      [[-0.3217]],\n              \n                      [[-0.1797]],\n              \n                      [[-0.1431]],\n              \n                      [[ 0.2850]],\n              \n                      [[ 0.1646]],\n              \n                      [[ 0.2151]],\n              \n                      [[-0.1339]],\n              \n                      [[ 0.2755]],\n              \n                      [[ 0.1459]],\n              \n                      [[ 0.3627]],\n              \n                      [[ 0.1791]],\n              \n                      [[ 0.0819]],\n              \n                      [[-0.0620]],\n              \n                      [[ 0.0924]],\n              \n                      [[ 0.0794]],\n              \n                      [[-0.2276]],\n              \n                      [[ 0.5267]],\n              \n                      [[ 0.2892]],\n              \n                      [[-0.0809]],\n              \n                      [[ 0.1694]],\n              \n                      [[ 0.2092]],\n              \n                      [[-0.1943]],\n              \n                      [[ 0.2089]],\n              \n                      [[-0.1776]],\n              \n                      [[-0.2474]],\n              \n                      [[ 0.2163]],\n              \n                      [[-0.1758]],\n              \n                      [[-0.2247]],\n              \n                      [[ 0.1660]],\n              \n                      [[-0.3319]],\n              \n                      [[-0.2119]],\n              \n                      [[-0.2553]],\n              \n                      [[-0.2738]],\n              \n                      [[ 0.0849]],\n              \n                      [[-0.2196]],\n              \n                      [[-0.1071]],\n              \n                      [[-0.2111]],\n              \n                      [[ 0.2726]],\n              \n                      [[-0.2511]],\n              \n                      [[-0.1191]],\n              \n                      [[ 0.2992]],\n              \n                      [[ 0.1301]],\n              \n                      [[-0.0976]],\n              \n                      [[ 0.2447]],\n              \n                      [[-0.1161]],\n              \n                      [[-0.0910]],\n              \n                      [[ 0.1639]],\n              \n                      [[-0.0866]],\n              \n                      [[-0.3526]],\n              \n                      [[ 0.0722]],\n              \n                      [[-0.1775]],\n              \n                      [[ 0.2282]],\n              \n                      [[-0.2378]],\n              \n                      [[-0.0718]],\n              \n                      [[ 0.2841]],\n              \n                      [[-0.2321]],\n              \n                      [[-0.0722]],\n              \n                      [[-0.2177]],\n              \n                      [[-0.1088]],\n              \n                      [[ 0.3358]],\n              \n                      [[-0.0556]],\n              \n                      [[ 0.1326]],\n              \n                      [[-0.1755]],\n              \n                      [[ 0.2061]],\n              \n                      [[-0.0290]],\n              \n                      [[ 0.2160]],\n              \n                      [[-0.1605]],\n              \n                      [[ 0.1793]],\n              \n                      [[ 0.2176]],\n              \n                      [[ 0.1834]],\n              \n                      [[ 0.4843]],\n              \n                      [[ 0.2157]],\n              \n                      [[ 0.1634]],\n              \n                      [[-0.3578]],\n              \n                      [[-0.1559]],\n              \n                      [[ 0.0825]],\n              \n                      [[-0.1691]],\n              \n                      [[ 0.2089]],\n              \n                      [[-0.2098]],\n              \n                      [[-0.1802]],\n              \n                      [[-0.1763]],\n              \n                      [[ 0.1610]],\n              \n                      [[-0.1928]],\n              \n                      [[ 0.1463]],\n              \n                      [[ 0.2886]],\n              \n                      [[ 0.1882]],\n              \n                      [[-0.2807]],\n              \n                      [[-0.1479]],\n              \n                      [[ 0.2084]],\n              \n                      [[-0.3485]],\n              \n                      [[-0.1554]],\n              \n                      [[-0.1491]],\n              \n                      [[ 0.1133]],\n              \n                      [[-0.1035]],\n              \n                      [[-0.3303]],\n              \n                      [[-0.1396]],\n              \n                      [[-0.4230]],\n              \n                      [[-0.3168]],\n              \n                      [[-0.2474]],\n              \n                      [[ 0.3809]],\n              \n                      [[ 0.0849]],\n              \n                      [[-0.3331]],\n              \n                      [[-0.2297]],\n              \n                      [[-0.1592]],\n              \n                      [[-0.4205]],\n              \n                      [[ 0.0693]],\n              \n                      [[ 0.1856]],\n              \n                      [[ 0.1414]],\n              \n                      [[ 0.1158]],\n              \n                      [[-0.1405]],\n              \n                      [[ 0.3547]],\n              \n                      [[-0.5268]],\n              \n                      [[-0.2318]],\n              \n                      [[ 0.0605]],\n              \n                      [[ 0.1992]],\n              \n                      [[-0.1494]],\n              \n                      [[-0.0213]],\n              \n                      [[ 0.0294]],\n              \n                      [[ 0.1882]],\n              \n                      [[-0.4668]],\n              \n                      [[ 0.2894]],\n              \n                      [[-0.1324]],\n              \n                      [[-0.3670]],\n              \n                      [[-0.1248]],\n              \n                      [[ 0.0817]],\n              \n                      [[-0.0253]],\n              \n                      [[ 0.1293]],\n              \n                      [[-0.2255]],\n              \n                      [[ 0.1794]],\n              \n                      [[ 0.1558]],\n              \n                      [[ 0.1127]],\n              \n                      [[ 0.0755]],\n              \n                      [[ 0.1883]],\n              \n                      [[-0.3727]],\n              \n                      [[ 0.1453]],\n              \n                      [[-0.1535]],\n              \n                      [[ 0.2143]],\n              \n                      [[ 0.3469]],\n              \n                      [[ 0.1255]],\n              \n                      [[ 0.2112]],\n              \n                      [[-0.2477]],\n              \n                      [[-0.2216]],\n              \n                      [[ 0.2687]],\n              \n                      [[ 1.8453]],\n              \n                      [[-0.2914]],\n              \n                      [[-0.1633]],\n              \n                      [[ 0.1218]],\n              \n                      [[ 0.2706]],\n              \n                      [[-0.1264]],\n              \n                      [[ 0.1807]],\n              \n                      [[ 0.3852]],\n              \n                      [[ 0.2828]],\n              \n                      [[-0.1748]],\n              \n                      [[ 0.1863]],\n              \n                      [[-0.1796]],\n              \n                      [[-0.1873]],\n              \n                      [[-0.1878]],\n              \n                      [[-0.1533]],\n              \n                      [[ 0.2026]],\n              \n                      [[ 0.1557]],\n              \n                      [[-0.2010]],\n              \n                      [[ 0.2124]],\n              \n                      [[ 0.0620]],\n              \n                      [[-0.1731]],\n              \n                      [[-0.0721]],\n              \n                      [[-0.1995]],\n              \n                      [[ 0.2547]],\n              \n                      [[ 0.0734]],\n              \n                      [[-0.3722]],\n              \n                      [[ 0.2488]],\n              \n                      [[ 0.2364]],\n              \n                      [[-0.1704]],\n              \n                      [[-0.2929]],\n              \n                      [[ 0.0557]],\n              \n                      [[ 0.1993]],\n              \n                      [[-0.3213]],\n              \n                      [[ 0.5241]],\n              \n                      [[ 0.4119]],\n              \n                      [[ 0.0377]],\n              \n                      [[ 0.1164]],\n              \n                      [[ 0.0621]],\n              \n                      [[ 0.1680]],\n              \n                      [[-0.0302]],\n              \n                      [[ 0.1006]],\n              \n                      [[ 0.0550]],\n              \n                      [[ 0.2695]],\n              \n                      [[-0.2730]],\n              \n                      [[ 0.2138]],\n              \n                      [[-0.1543]],\n              \n                      [[-0.1115]],\n              \n                      [[-0.2292]],\n              \n                      [[-0.1431]],\n              \n                      [[ 0.2105]],\n              \n                      [[ 0.1280]],\n              \n                      [[-0.1072]],\n              \n                      [[-0.1562]],\n              \n                      [[-0.2894]],\n              \n                      [[-0.1119]],\n              \n                      [[-0.3586]],\n              \n                      [[-0.1938]],\n              \n                      [[ 0.1615]],\n              \n                      [[-0.1140]],\n              \n                      [[ 0.3424]],\n              \n                      [[-0.1063]],\n              \n                      [[ 0.0315]],\n              \n                      [[ 0.1858]],\n              \n                      [[-0.4528]],\n              \n                      [[-0.1248]],\n              \n                      [[ 0.0635]],\n              \n                      [[-0.2360]],\n              \n                      [[ 0.0823]],\n              \n                      [[-0.2306]],\n              \n                      [[ 0.3205]],\n              \n                      [[ 0.2254]],\n              \n                      [[ 0.1220]],\n              \n                      [[ 0.2045]],\n              \n                      [[-0.1723]],\n              \n                      [[-0.3260]],\n              \n                      [[ 0.1372]],\n              \n                      [[-0.2433]],\n              \n                      [[-0.1846]],\n              \n                      [[ 0.2022]],\n              \n                      [[-0.1042]],\n              \n                      [[ 0.0951]],\n              \n                      [[ 0.3790]],\n              \n                      [[-0.3958]],\n              \n                      [[-0.1362]],\n              \n                      [[ 0.0281]],\n              \n                      [[-0.1900]],\n              \n                      [[-0.1071]],\n              \n                      [[ 0.3055]],\n              \n                      [[ 0.2449]],\n              \n                      [[-0.2850]],\n              \n                      [[-0.1962]],\n              \n                      [[ 0.2464]],\n              \n                      [[ 0.2838]],\n              \n                      [[-0.1205]],\n              \n                      [[-0.0632]],\n              \n                      [[ 0.1972]],\n              \n                      [[ 0.0624]],\n              \n                      [[-0.1246]],\n              \n                      [[ 0.2446]],\n              \n                      [[ 0.1997]],\n              \n                      [[ 0.1265]],\n              \n                      [[-0.2413]],\n              \n                      [[ 0.2157]],\n              \n                      [[ 0.1555]],\n              \n                      [[ 0.1932]],\n              \n                      [[ 0.2476]],\n              \n                      [[-0.1886]],\n              \n                      [[ 0.2056]],\n              \n                      [[-0.2022]],\n              \n                      [[-0.2082]],\n              \n                      [[-0.3241]],\n              \n                      [[ 0.1308]],\n              \n                      [[-0.2493]],\n              \n                      [[-0.2310]],\n              \n                      [[ 0.1414]],\n              \n                      [[-0.0756]],\n              \n                      [[ 0.3389]],\n              \n                      [[ 0.2436]],\n              \n                      [[ 0.1184]],\n              \n                      [[-0.2323]],\n              \n                      [[ 0.1530]],\n              \n                      [[-0.1479]],\n              \n                      [[ 0.2045]],\n              \n                      [[ 0.1611]],\n              \n                      [[-0.0756]],\n              \n                      [[ 0.2519]],\n              \n                      [[-0.2239]],\n              \n                      [[ 0.2059]],\n              \n                      [[-0.0857]],\n              \n                      [[-0.2459]],\n              \n                      [[-0.0666]],\n              \n                      [[ 0.1519]],\n              \n                      [[-0.1723]],\n              \n                      [[-0.1195]],\n              \n                      [[-0.2248]],\n              \n                      [[-0.3969]],\n              \n                      [[-0.1704]],\n              \n                      [[ 0.0435]],\n              \n                      [[-0.1784]],\n              \n                      [[-0.1136]],\n              \n                      [[-0.1710]],\n              \n                      [[-0.0517]],\n              \n                      [[ 0.0710]],\n              \n                      [[ 0.0931]],\n              \n                      [[-0.1465]],\n              \n                      [[-0.1666]],\n              \n                      [[-0.3973]],\n              \n                      [[ 0.4006]],\n              \n                      [[-0.0571]],\n              \n                      [[ 0.1031]],\n              \n                      [[ 0.1334]],\n              \n                      [[ 0.1202]],\n              \n                      [[-0.0869]],\n              \n                      [[-0.2191]],\n              \n                      [[ 0.1934]],\n              \n                      [[ 0.2018]],\n              \n                      [[-0.1787]],\n              \n                      [[-0.1965]],\n              \n                      [[ 0.0550]],\n              \n                      [[-0.1198]],\n              \n                      [[-0.1617]],\n              \n                      [[-0.2317]],\n              \n                      [[ 0.0493]],\n              \n                      [[-0.0861]],\n              \n                      [[ 0.2577]],\n              \n                      [[ 0.1393]],\n              \n                      [[-0.1713]],\n              \n                      [[ 0.3563]],\n              \n                      [[ 0.2143]],\n              \n                      [[-0.3774]],\n              \n                      [[ 0.0208]],\n              \n                      [[-0.2246]],\n              \n                      [[ 0.1847]],\n              \n                      [[-0.2650]],\n              \n                      [[ 0.1441]],\n              \n                      [[ 0.1790]],\n              \n                      [[ 0.1558]],\n              \n                      [[-0.1856]],\n              \n                      [[ 0.1506]],\n              \n                      [[ 0.2108]],\n              \n                      [[ 0.2145]],\n              \n                      [[ 0.0634]],\n              \n                      [[ 0.0508]],\n              \n                      [[ 0.3192]],\n              \n                      [[-0.0551]],\n              \n                      [[ 0.2228]],\n              \n                      [[ 0.0778]],\n              \n                      [[-0.2420]],\n              \n                      [[ 0.2946]],\n              \n                      [[ 0.1648]],\n              \n                      [[ 0.3580]],\n              \n                      [[ 0.0560]],\n              \n                      [[ 0.1803]],\n              \n                      [[-0.1270]],\n              \n                      [[ 0.1009]],\n              \n                      [[-0.2257]],\n              \n                      [[ 0.1761]],\n              \n                      [[ 0.1230]],\n              \n                      [[-0.1216]],\n              \n                      [[ 0.0732]],\n              \n                      [[-0.2444]],\n              \n                      [[ 0.1502]],\n              \n                      [[ 0.3445]],\n              \n                      [[-0.1723]],\n              \n                      [[ 0.0498]],\n              \n                      [[ 0.1109]],\n              \n                      [[ 0.2355]],\n              \n                      [[ 0.1513]],\n              \n                      [[ 0.3082]],\n              \n                      [[ 0.1798]],\n              \n                      [[-0.2059]],\n              \n                      [[ 0.1349]],\n              \n                      [[ 0.3077]],\n              \n                      [[-0.2688]],\n              \n                      [[ 0.0931]],\n              \n                      [[-0.1995]],\n              \n                      [[ 0.1913]],\n              \n                      [[ 0.2531]],\n              \n                      [[ 0.1507]],\n              \n                      [[ 0.2737]],\n              \n                      [[ 0.2712]],\n              \n                      [[ 0.2447]],\n              \n                      [[-0.2034]],\n              \n                      [[-0.2942]],\n              \n                      [[-0.1893]],\n              \n                      [[ 0.0834]],\n              \n                      [[-0.2283]],\n              \n                      [[-0.1590]]])),\n             ('features.5.7.block.0.weight',\n              tensor([[[[-0.0146, -0.0042,  0.0063,  ..., -0.0125, -0.0021, -0.0104],\n                        [ 0.0021, -0.0104, -0.0042,  ..., -0.0104, -0.0042, -0.0042],\n                        [-0.0125,  0.0021, -0.0292,  ..., -0.0459,  0.0042, -0.0167],\n                        ...,\n                        [-0.0125, -0.0167, -0.0501,  ..., -0.0543, -0.0021, -0.0167],\n                        [ 0.0084,  0.0042,  0.0063,  ..., -0.0042,  0.0021,  0.0084],\n                        [-0.0104, -0.0063, -0.0146,  ..., -0.0125,  0.0000, -0.0146]]],\n              \n              \n                      [[[ 0.0077,  0.0026,  0.0179,  ...,  0.0204,  0.0000,  0.0153],\n                        [ 0.0051, -0.0026,  0.0153,  ...,  0.0153, -0.0077,  0.0102],\n                        [ 0.0077,  0.0204,  0.0409,  ...,  0.0434,  0.0153,  0.0153],\n                        ...,\n                        [ 0.0153,  0.0128,  0.0179,  ...,  0.0026,  0.0230,  0.0102],\n                        [ 0.0077,  0.0026,  0.0230,  ...,  0.0306,  0.0077,  0.0077],\n                        [ 0.0153,  0.0102,  0.0179,  ...,  0.0179,  0.0051,  0.0230]]],\n              \n              \n                      [[[-0.0109,  0.0055, -0.0091,  ..., -0.0164,  0.0036, -0.0164],\n                        [-0.0109,  0.0018,  0.0036,  ..., -0.0073,  0.0036, -0.0109],\n                        [-0.0036,  0.0000, -0.0346,  ..., -0.0437,  0.0128, -0.0091],\n                        ...,\n                        [-0.0091, -0.0091, -0.0619,  ..., -0.0656,  0.0091, -0.0164],\n                        [-0.0073,  0.0055,  0.0073,  ..., -0.0055, -0.0091,  0.0036],\n                        [-0.0128, -0.0128, -0.0164,  ..., -0.0310,  0.0018, -0.0146]]],\n              \n              \n                      ...,\n              \n              \n                      [[[-0.0067,  0.0045, -0.0067,  ..., -0.0178,  0.0111, -0.0156],\n                        [-0.0156, -0.0045,  0.0089,  ..., -0.0067,  0.0022, -0.0111],\n                        [-0.0067,  0.0000, -0.0334,  ..., -0.0513, -0.0045, -0.0067],\n                        ...,\n                        [-0.0223, -0.0156, -0.0580,  ..., -0.0803,  0.0000, -0.0290],\n                        [ 0.0022,  0.0022, -0.0045,  ..., -0.0022,  0.0089, -0.0045],\n                        [-0.0089,  0.0045, -0.0178,  ..., -0.0111, -0.0067, -0.0045]]],\n              \n              \n                      [[[ 0.0019, -0.0019,  0.0000,  ...,  0.0000,  0.0000,  0.0058],\n                        [ 0.0039, -0.0058,  0.0058,  ...,  0.0058,  0.0019, -0.0096],\n                        [ 0.0058, -0.0116,  0.0058,  ...,  0.0058, -0.0096,  0.0077],\n                        ...,\n                        [ 0.0019,  0.0077,  0.0154,  ...,  0.0270, -0.0077,  0.0135],\n                        [-0.0058, -0.0019,  0.0000,  ..., -0.0096, -0.0039,  0.0039],\n                        [-0.0019,  0.0096,  0.0077,  ...,  0.0096,  0.0019,  0.0039]]],\n              \n              \n                      [[[-0.0031, -0.0015, -0.0031,  ..., -0.0046, -0.0107, -0.0061],\n                        [-0.0122, -0.0107, -0.0092,  ..., -0.0031, -0.0092, -0.0061],\n                        [ 0.0076, -0.0046, -0.0321,  ..., -0.0398, -0.0031,  0.0061],\n                        ...,\n                        [-0.0245, -0.0168, -0.0535,  ..., -0.0658, -0.0015, -0.0275],\n                        [ 0.0107, -0.0015,  0.0076,  ...,  0.0061,  0.0015,  0.0015],\n                        [-0.0184, -0.0153, -0.0046,  ..., -0.0199, -0.0092, -0.0107]]]],\n                     size=(384, 1, 7, 7), dtype=torch.qint8,\n                     quantization_scheme=torch.per_channel_affine,\n                     scale=tensor([0.0021, 0.0026, 0.0018, 0.0025, 0.0014, 0.0017, 0.0024, 0.0011, 0.0015,\n                      0.0025, 0.0022, 0.0014, 0.0020, 0.0006, 0.0016, 0.0018, 0.0022, 0.0015,\n                      0.0007, 0.0019, 0.0013, 0.0019, 0.0025, 0.0013, 0.0023, 0.0022, 0.0025,\n                      0.0024, 0.0011, 0.0028, 0.0013, 0.0022, 0.0018, 0.0014, 0.0008, 0.0007,\n                      0.0024, 0.0013, 0.0011, 0.0013, 0.0016, 0.0024, 0.0023, 0.0015, 0.0008,\n                      0.0010, 0.0014, 0.0017, 0.0021, 0.0024, 0.0013, 0.0012, 0.0020, 0.0009,\n                      0.0020, 0.0026, 0.0007, 0.0019, 0.0023, 0.0012, 0.0024, 0.0008, 0.0025,\n                      0.0013, 0.0015, 0.0010, 0.0025, 0.0019, 0.0018, 0.0019, 0.0006, 0.0021,\n                      0.0012, 0.0023, 0.0017, 0.0008, 0.0016, 0.0011, 0.0010, 0.0015, 0.0013,\n                      0.0024, 0.0025, 0.0010, 0.0009, 0.0019, 0.0014, 0.0014, 0.0022, 0.0015,\n                      0.0023, 0.0019, 0.0015, 0.0024, 0.0017, 0.0021, 0.0017, 0.0013, 0.0013,\n                      0.0012, 0.0015, 0.0010, 0.0011, 0.0022, 0.0024, 0.0022, 0.0019, 0.0010,\n                      0.0018, 0.0006, 0.0011, 0.0008, 0.0008, 0.0023, 0.0012, 0.0007, 0.0014,\n                      0.0019, 0.0022, 0.0013, 0.0016, 0.0021, 0.0017, 0.0007, 0.0034, 0.0013,\n                      0.0023, 0.0013, 0.0018, 0.0012, 0.0005, 0.0016, 0.0005, 0.0012, 0.0023,\n                      0.0025, 0.0018, 0.0021, 0.0010, 0.0026, 0.0019, 0.0011, 0.0017, 0.0024,\n                      0.0024, 0.0022, 0.0014, 0.0019, 0.0012, 0.0013, 0.0015, 0.0018, 0.0014,\n                      0.0011, 0.0010, 0.0015, 0.0001, 0.0024, 0.0022, 0.0023, 0.0020, 0.0017,\n                      0.0015, 0.0006, 0.0018, 0.0017, 0.0016, 0.0012, 0.0012, 0.0020, 0.0015,\n                      0.0016, 0.0017, 0.0009, 0.0013, 0.0023, 0.0017, 0.0024, 0.0008, 0.0008,\n                      0.0026, 0.0016, 0.0010, 0.0010, 0.0011, 0.0016, 0.0024, 0.0011, 0.0011,\n                      0.0010, 0.0008, 0.0014, 0.0021, 0.0025, 0.0016, 0.0009, 0.0022, 0.0023,\n                      0.0015, 0.0017, 0.0013, 0.0014, 0.0018, 0.0018, 0.0014, 0.0015, 0.0018,\n                      0.0021, 0.0018, 0.0014, 0.0025, 0.0012, 0.0017, 0.0019, 0.0021, 0.0010,\n                      0.0015, 0.0008, 0.0009, 0.0009, 0.0025, 0.0025, 0.0017, 0.0021, 0.0010,\n                      0.0017, 0.0023, 0.0024, 0.0012, 0.0016, 0.0007, 0.0018, 0.0018, 0.0024,\n                      0.0009, 0.0024, 0.0024, 0.0019, 0.0008, 0.0020, 0.0010, 0.0015, 0.0020,\n                      0.0012, 0.0008, 0.0024, 0.0013, 0.0024, 0.0019, 0.0025, 0.0023, 0.0017,\n                      0.0022, 0.0019, 0.0010, 0.0019, 0.0014, 0.0011, 0.0014, 0.0013, 0.0012,\n                      0.0013, 0.0009, 0.0007, 0.0009, 0.0017, 0.0011, 0.0021, 0.0018, 0.0022,\n                      0.0018, 0.0023, 0.0013, 0.0017, 0.0021, 0.0015, 0.0020, 0.0013, 0.0017,\n                      0.0011, 0.0022, 0.0016, 0.0008, 0.0023, 0.0021, 0.0019, 0.0025, 0.0019,\n                      0.0017, 0.0015, 0.0025, 0.0011, 0.0015, 0.0010, 0.0012, 0.0019, 0.0015,\n                      0.0025, 0.0024, 0.0022, 0.0015, 0.0015, 0.0006, 0.0018, 0.0025, 0.0020,\n                      0.0019, 0.0017, 0.0021, 0.0025, 0.0011, 0.0009, 0.0012, 0.0007, 0.0022,\n                      0.0017, 0.0014, 0.0026, 0.0010, 0.0024, 0.0009, 0.0017, 0.0017, 0.0006,\n                      0.0016, 0.0012, 0.0013, 0.0016, 0.0012, 0.0016, 0.0023, 0.0012, 0.0011,\n                      0.0011, 0.0015, 0.0023, 0.0011, 0.0025, 0.0026, 0.0017, 0.0025, 0.0013,\n                      0.0022, 0.0013, 0.0011, 0.0014, 0.0022, 0.0025, 0.0006, 0.0017, 0.0022,\n                      0.0007, 0.0015, 0.0014, 0.0020, 0.0021, 0.0015, 0.0016, 0.0011, 0.0011,\n                      0.0026, 0.0021, 0.0012, 0.0016, 0.0009, 0.0012, 0.0008, 0.0021, 0.0024,\n                      0.0023, 0.0022, 0.0008, 0.0018, 0.0015, 0.0015, 0.0023, 0.0022, 0.0023,\n                      0.0016, 0.0018, 0.0023, 0.0022, 0.0019, 0.0015], dtype=torch.float64),\n                     zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n                     axis=0)),\n             ('features.5.7.block.0.bias',\n              Parameter containing:\n              tensor([ 2.8919e-03,  1.2156e-02, -3.0748e-02, -6.1915e-03,  1.2803e-02,\n                       1.4736e-02,  1.8682e-02, -1.6372e-02, -5.9326e-03,  2.3490e-02,\n                       2.3470e-02, -4.2552e-03,  3.8998e-02,  3.8755e-03, -1.5545e-02,\n                       1.4415e-02,  1.2318e-03, -7.4929e-03,  1.3189e-02,  3.3550e-03,\n                       1.7711e-03, -4.0195e-03,  2.4597e-02, -8.7023e-03, -9.4992e-03,\n                      -3.3934e-02, -1.4919e-03, -1.5559e-02,  2.3000e-02,  4.1520e-02,\n                      -6.1181e-03, -7.0527e-03,  7.1551e-03,  2.1605e-02,  3.7213e-02,\n                       3.0599e-03,  3.0749e-02, -3.0092e-02, -3.2088e-03, -1.0922e-02,\n                       1.9497e-02, -1.6726e-02,  7.7456e-03,  4.8598e-03,  5.3507e-04,\n                      -2.2914e-02,  2.4873e-03, -1.0473e-02,  5.8438e-03, -1.0778e-02,\n                      -3.9315e-04, -1.1757e-03, -3.6465e-03, -2.0753e-02, -1.1945e-02,\n                       1.3201e-02, -3.1877e-02,  1.0663e-03, -1.5173e-03, -3.1018e-03,\n                       1.5642e-02,  2.6426e-02,  1.2112e-02, -1.2418e-02,  3.8769e-02,\n                      -9.5273e-03, -7.3454e-03, -9.0146e-03, -1.0089e-02, -4.5354e-02,\n                      -4.6570e-03,  6.7840e-03, -1.4159e-02,  8.0514e-03,  1.3774e-03,\n                       2.3679e-02, -6.9324e-03, -1.6292e-03, -3.3177e-02,  1.2343e-02,\n                      -2.9751e-03,  2.9923e-02,  1.5591e-02,  4.3861e-02,  7.3123e-03,\n                      -1.9121e-02,  6.2616e-04,  3.0212e-03,  9.1567e-03, -1.4119e-04,\n                       3.2766e-02,  1.4149e-02,  2.6433e-02,  4.0273e-03, -2.2374e-02,\n                       1.7117e-03, -7.7793e-03,  6.3249e-03,  1.1799e-02, -7.6019e-03,\n                      -9.1078e-03, -7.0253e-04,  2.5951e-02, -1.3839e-02,  9.6343e-03,\n                      -1.9739e-02, -1.3677e-02,  1.4988e-03,  5.3852e-03,  2.7904e-02,\n                      -2.6585e-02, -8.0456e-03,  3.9848e-02, -3.6016e-03, -3.2061e-02,\n                       2.3212e-03, -2.0838e-02, -1.5356e-02, -5.1921e-03, -1.9132e-02,\n                      -2.4211e-02,  1.5900e-03,  1.2645e-03,  3.2754e-02, -6.1422e-02,\n                      -1.8894e-02,  9.6562e-03, -2.5398e-02,  2.0306e-02,  1.0781e-02,\n                       2.7097e-02, -2.4297e-02,  1.7526e-02, -8.3942e-03, -6.6364e-03,\n                      -3.9235e-02,  8.5041e-03, -1.8573e-02,  1.7973e-02, -1.7693e-02,\n                      -2.7559e-03, -1.9980e-02,  4.2723e-03, -2.3807e-02,  1.8761e-02,\n                       1.4407e-02, -2.2725e-02,  3.1281e-02, -2.2586e-02,  1.8660e-03,\n                       4.4420e-03,  1.1496e-03, -5.2538e-03,  5.0191e-02,  1.2386e-02,\n                      -1.0442e-02,  1.2435e-01, -5.8862e-03,  4.1348e-02, -4.8742e-03,\n                       8.0847e-03, -1.6906e-02, -1.9115e-02,  1.0434e-02, -1.0431e-03,\n                      -3.9587e-02, -1.5723e-02,  3.5451e-02,  1.0844e-02, -8.0057e-03,\n                       2.9259e-03, -2.0852e-02, -1.9711e-02,  1.3904e-02, -2.0986e-02,\n                      -5.0424e-02, -5.6102e-03,  4.9415e-03, -3.3415e-02, -3.2284e-03,\n                       7.4170e-02, -4.2136e-02, -2.5502e-02, -1.3345e-03,  7.6414e-03,\n                       1.8610e-02,  5.0344e-03,  1.4268e-02, -1.0267e-02, -1.1894e-02,\n                       2.6424e-02, -4.5867e-03,  1.4400e-02,  5.9605e-03, -1.3381e-02,\n                       7.0805e-03,  1.5126e-03, -3.5961e-02, -1.6567e-02,  2.7497e-02,\n                      -1.7835e-02, -1.2733e-02,  2.5569e-02,  6.4923e-04, -1.2802e-02,\n                      -6.6447e-04, -1.4960e-02,  2.7366e-03,  1.0475e-03,  1.3299e-02,\n                       2.4086e-02, -5.1037e-02, -1.1561e-03, -1.9162e-02, -1.3892e-02,\n                      -3.4235e-02,  1.3414e-03,  4.3112e-03,  2.2775e-02, -5.3536e-03,\n                       2.7242e-03,  1.6178e-03, -1.0818e-02,  2.2230e-02,  2.6217e-02,\n                       5.3437e-03, -8.7976e-03,  1.0173e-02,  8.7791e-03, -1.6580e-02,\n                      -1.6995e-02, -1.1860e-02,  8.6195e-03, -5.4293e-03,  7.8545e-03,\n                       1.8690e-04, -1.4887e-02, -1.3200e-02, -4.4881e-02, -1.6678e-02,\n                       1.1804e-02, -2.7743e-02, -6.9627e-03, -3.5787e-02,  3.2035e-02,\n                       8.2414e-03, -2.9837e-02,  1.2159e-02, -3.6770e-02,  3.2891e-02,\n                      -7.3437e-03, -4.4198e-03, -4.8850e-03, -3.4570e-03, -9.5314e-03,\n                      -1.3755e-02, -1.0585e-02, -6.8939e-03, -2.5111e-02,  2.8692e-03,\n                       7.6646e-03, -5.3290e-03, -2.0988e-02, -6.9486e-03, -4.6489e-03,\n                       1.3497e-04,  7.1514e-03,  1.4561e-02, -2.4896e-02, -1.1955e-02,\n                       1.2565e-03,  1.1009e-03, -1.7633e-02,  6.1578e-03, -5.5150e-02,\n                       6.3806e-03, -2.4564e-02,  8.7544e-03,  6.3943e-03,  1.4074e-02,\n                       3.3355e-02, -3.5588e-04,  2.3485e-02, -6.8406e-03,  1.1565e-02,\n                      -1.2476e-02,  1.0411e-02, -2.0541e-02, -1.2590e-02,  6.6897e-04,\n                       8.7182e-04, -3.5892e-02, -7.3844e-03, -8.6342e-03,  1.0531e-02,\n                      -4.2910e-03, -9.7371e-03,  1.1361e-03,  6.6772e-03, -1.7278e-02,\n                       2.9145e-02,  1.9800e-02, -2.4822e-02,  3.7810e-02,  1.0363e-02,\n                      -1.6043e-03,  8.0574e-04,  3.7361e-02,  3.6630e-04,  1.2475e-02,\n                       6.7641e-04, -6.0291e-02,  2.4873e-03,  6.3461e-03, -3.8394e-02,\n                       4.1044e-02,  1.1538e-02,  3.7475e-03, -1.2821e-02,  2.2313e-02,\n                      -2.1593e-02,  1.8159e-02, -1.1459e-02, -1.7500e-02, -6.2557e-03,\n                       1.3817e-02, -5.2822e-03,  7.7256e-03, -1.2546e-02, -1.2511e-02,\n                      -7.2032e-04, -1.1317e-02,  7.3968e-03,  1.1247e-03, -1.6384e-02,\n                       1.1165e-02,  2.1759e-03, -8.5365e-05,  3.4266e-02,  2.0145e-03,\n                       6.4378e-03, -8.1695e-03, -9.4227e-03,  1.3327e-02,  3.6035e-03,\n                      -2.3827e-02, -2.6905e-02,  1.5460e-02,  7.7661e-03, -1.0666e-02,\n                       5.2374e-03,  2.4494e-02,  2.6110e-03,  2.3135e-02,  1.8760e-02,\n                      -5.5816e-02,  7.5580e-03,  3.5632e-03, -8.5412e-03,  1.1264e-02,\n                       1.7296e-02, -2.4064e-02, -2.7547e-02, -1.8860e-02, -4.2651e-02,\n                       2.7691e-02, -3.9342e-02,  1.1121e-02, -2.8013e-02,  2.0680e-02,\n                       1.5793e-02,  1.5981e-02,  5.3116e-02, -1.0584e-02,  1.9534e-02,\n                      -7.9585e-03,  4.0001e-02,  1.1154e-02, -2.1586e-02, -3.0264e-03,\n                       1.1924e-03, -9.2349e-03,  1.1167e-02,  3.8427e-02],\n                     requires_grad=True)),\n             ('features.5.7.block.0.scale', tensor(0.0147)),\n             ('features.5.7.block.0.zero_point', tensor(61)),\n             ('features.5.7.block.2.weight',\n              tensor([1.3274, 2.5233, 1.1608, 1.9023, 1.0176, 1.0002, 1.6902, 0.8299, 0.9765,\n                      1.5842, 1.7536, 0.7069, 1.4904, 0.9468, 1.0697, 1.2032, 1.2844, 0.9921,\n                      0.8373, 1.2047, 0.9173, 1.1489, 2.0564, 1.0416, 1.4442, 2.6469, 1.7674,\n                      1.9248, 0.8351, 1.4879, 0.9901, 1.5106, 1.0318, 0.9370, 0.8694, 0.7693,\n                      1.6876, 0.8341, 0.8408, 0.8664, 1.0648, 1.5205, 1.6331, 0.8628, 0.7035,\n                      0.9273, 1.0354, 1.1226, 1.3719, 1.1300, 0.8504, 1.0715, 1.2869, 0.7338,\n                      1.2755, 2.4453, 0.8618, 1.1486, 1.7293, 1.0264, 2.0845, 0.6032, 2.1958,\n                      1.0061, 0.9754, 1.3426, 1.8058, 0.9823, 0.9360, 1.3819, 0.8032, 1.3290,\n                      1.0003, 1.9244, 1.1301, 0.9180, 1.0406, 0.8777, 0.7167, 1.0399, 0.8810,\n                      1.3149, 1.7540, 0.3430, 0.8837, 1.2575, 0.9811, 1.0196, 1.5264, 1.0239,\n                      1.2104, 1.1307, 0.9439, 1.3535, 1.1451, 1.3093, 1.1918, 0.7980, 1.0129,\n                      0.9951, 0.9761, 0.8346, 0.9359, 1.3352, 2.1617, 1.4626, 1.3104, 0.7860,\n                      1.0165, 1.1179, 0.8272, 0.6713, 0.6747, 1.4201, 0.9784, 0.8166, 1.0125,\n                      0.5514, 1.5541, 1.0086, 1.1513, 1.4092, 1.1346, 0.6242, 0.6032, 1.4635,\n                      2.2569, 0.9842, 1.1455, 0.9869, 0.7849, 1.0695, 0.5195, 0.9416, 1.4181,\n                      1.7930, 1.1952, 2.0359, 0.9248, 1.5457, 1.0673, 0.8777, 1.0071, 1.7155,\n                      1.6844, 1.3102, 0.6240, 1.2591, 1.0133, 1.1690, 1.3956, 1.2252, 0.9628,\n                      0.7897, 0.8643, 1.0273, 0.1663, 1.4570, 1.5891, 1.4783, 1.1531, 1.1457,\n                      0.9306, 0.5655, 0.9859, 1.1091, 0.9423, 0.9037, 0.8881, 1.1575, 1.0402,\n                      0.9964, 0.9293, 0.7883, 0.7991, 1.9445, 1.0228, 1.5753, 0.8905, 0.7264,\n                      3.4035, 0.5901, 0.7796, 0.8271, 0.9322, 0.9084, 1.7449, 0.8557, 1.0312,\n                      1.3646, 0.5092, 1.0082, 1.2129, 2.0965, 0.8811, 0.8954, 1.5063, 2.6177,\n                      0.9457, 1.1010, 0.7667, 1.0004, 1.2299, 1.1538, 1.0131, 0.9188, 1.2043,\n                      1.2956, 1.1310, 0.9849, 1.7670, 0.9200, 1.0758, 1.0697, 1.4250, 0.7931,\n                      1.3361, 1.0676, 0.8413, 0.7170, 1.8800, 2.3655, 1.0401, 1.3919, 0.8105,\n                      1.1328, 1.4615, 1.5193, 0.8339, 0.8714, 0.6546, 1.0778, 0.9943, 1.5547,\n                      0.8866, 1.4324, 1.6129, 0.8660, 0.5734, 1.2727, 0.9999, 1.0188, 1.3005,\n                      0.7153, 0.6840, 1.3655, 0.8528, 1.9858, 0.9483, 1.7636, 1.5612, 1.1086,\n                      1.6839, 1.0745, 0.6844, 1.0506, 1.1383, 0.8446, 0.8885, 0.9837, 0.9020,\n                      0.8141, 0.9244, 0.6814, 0.9961, 1.1330, 0.8986, 1.2560, 0.9255, 1.3053,\n                      1.1515, 1.5479, 0.9851, 1.0098, 2.3047, 0.8261, 1.1660, 0.9636, 1.1055,\n                      1.0126, 2.3378, 0.8695, 0.8233, 2.0895, 1.3330, 1.0584, 1.8390, 1.0794,\n                      1.0480, 1.0461, 1.7996, 0.6832, 0.9945, 1.0046, 0.9441, 1.1711, 0.9908,\n                      2.1254, 1.5525, 1.5548, 1.0634, 1.0103, 0.6348, 1.0190, 2.0185, 1.3691,\n                      1.1907, 1.1710, 1.4224, 1.7865, 0.9235, 0.7452, 0.9132, 0.8547, 2.4326,\n                      1.2216, 0.9733, 2.6063, 1.0406, 2.1855, 0.7808, 1.1126, 1.0256, 0.6039,\n                      0.9447, 0.5676, 1.0428, 1.0479, 0.8382, 1.0738, 1.6491, 0.9346, 0.9982,\n                      0.9319, 1.0348, 1.5523, 0.8166, 1.9404, 2.6674, 0.8932, 1.7953, 0.9225,\n                      1.5845, 0.9860, 1.4362, 1.0198, 1.0227, 1.9291, 1.0830, 1.1052, 1.7851,\n                      0.7750, 0.9875, 2.6850, 1.2348, 3.0212, 0.9170, 1.0764, 0.6096, 1.0012,\n                      2.4630, 1.2747, 0.8060, 1.1090, 1.1711, 0.9328, 0.9293, 1.1546, 1.4633,\n                      1.8036, 1.3650, 0.7537, 1.1171, 0.9225, 1.0395, 2.1274, 1.4715, 1.6872,\n                      1.0265, 1.0494, 1.3258, 1.3927, 1.1344, 1.1072])),\n             ('features.5.7.block.2.bias',\n              tensor([ 4.2347e-01, -2.5497e-01,  5.8064e-01,  5.0967e-02, -1.0788e-01,\n                      -6.0649e-02,  1.0985e-01,  6.3817e-02,  2.4212e-01, -3.0900e-01,\n                      -8.2759e-01,  3.3953e-02, -9.6463e-01,  1.5875e-01,  2.6794e-01,\n                      -1.1371e-01,  2.4241e-01,  1.2209e-01, -5.9394e-03,  1.6111e-01,\n                       9.7695e-02,  2.8233e-01,  3.4808e-01,  3.2166e-01,  3.2774e-01,\n                      -8.0094e-01,  3.8430e-01, -7.1716e-02, -2.3477e-01,  1.6422e+00,\n                       3.0541e-01,  2.9613e-01,  1.1582e-01, -9.4816e-01, -1.7228e-01,\n                       1.1598e-01, -4.2705e-01,  4.2908e-01,  6.8517e-01,  5.1929e-01,\n                      -1.6310e-01,  1.9648e-01, -1.0240e-02, -2.8804e-03,  2.8031e-02,\n                       5.1700e-01, -3.6395e-01,  1.4815e-01, -6.2976e-02,  2.9180e-01,\n                      -2.5243e-02,  3.9829e-01,  3.3502e-01,  9.0661e-01,  3.5557e-01,\n                      -1.9299e-01,  5.1978e-01,  2.4613e-01,  7.4990e-02, -4.8509e-02,\n                      -1.4259e-01, -4.1086e-02,  6.1802e-01,  2.6704e-01, -6.6291e-02,\n                       2.5155e-01, -4.3988e-03,  8.1527e-02,  2.1873e-01, -5.7815e-03,\n                       1.3333e-01,  1.4563e-03,  5.0447e-02,  1.5373e-01, -8.2319e-02,\n                       2.7489e-01,  1.6446e-01, -6.3194e-02,  4.8143e-01,  1.5206e-01,\n                       8.2541e-02, -2.3972e-01, -5.1501e-01,  7.5895e-02,  7.0571e-01,\n                       3.2756e-01,  7.1698e-02,  6.9560e-02,  3.4356e-02,  4.5405e-02,\n                      -2.4981e-01, -2.6871e-01, -1.3741e-01,  2.1817e-01,  3.8622e-01,\n                      -1.8145e-01,  1.8228e-01,  4.5606e-02,  1.4691e-01,  2.6968e-01,\n                       1.5443e-01,  1.2558e-01, -3.6519e-01,  1.1509e-01, -2.5742e-02,\n                       4.2485e-01,  3.1954e-01,  8.8430e-02,  3.9180e-02,  1.0164e-01,\n                      -4.8276e-02,  1.9051e-01,  1.4808e-01,  5.0189e-02,  4.9332e-01,\n                       3.3550e-01,  2.4720e-01,  2.2032e+00,  6.4509e-01,  1.6293e-01,\n                       3.1283e-01,  8.7782e-03, -8.8822e-04, -1.7763e-01,  3.2402e+00,\n                       2.1179e+00,  4.0856e-01,  2.3433e-01, -3.4528e-02, -1.3243e-01,\n                      -3.5109e-01,  3.3570e-01,  8.2760e-02,  1.1204e-01,  2.6014e-01,\n                       1.5844e+00,  4.5645e-03, -4.8247e-01, -2.4954e-01,  4.8977e-01,\n                       5.4169e-02,  2.3113e-01,  1.1320e-01,  3.3716e-01, -2.6615e-01,\n                      -1.6032e-01,  2.1637e+00, -4.4063e-01,  1.8632e-01,  2.0079e-02,\n                       5.1567e-02, -3.2620e-02,  2.1283e-01, -4.2783e-01,  6.8916e-02,\n                       1.6937e-01,  1.3523e-01,  2.5591e-01, -5.7625e-01,  2.4520e-01,\n                       2.6153e-02,  2.9121e-01,  5.7831e-01,  2.5568e-02,  2.0685e-01,\n                       3.4384e-01,  3.8146e-01, -2.6266e-01,  2.4714e-01,  1.2262e-01,\n                       7.6413e-02,  4.5523e-01,  6.4669e-01, -7.3362e-02,  4.9462e-01,\n                      -1.3490e-01,  2.0646e-01,  1.0983e-01,  7.2625e-01, -2.8521e-02,\n                       2.3391e-01,  2.1520e+00,  5.8158e-03, -6.0655e-03,  1.0015e-01,\n                      -4.1238e-02,  4.3472e-02,  1.1299e-01,  1.3122e-01,  1.9905e-01,\n                      -1.0439e-02,  6.1794e-01,  3.1860e-03, -1.1991e-01,  1.3978e-01,\n                       1.7998e-02,  5.7995e-01, -1.2092e+00,  3.1879e-01, -2.3923e-01,\n                       2.7574e-01,  1.0985e-01, -3.3798e-01,  3.4262e-02,  2.3297e-01,\n                       4.7924e-02,  1.1857e-01,  1.8416e-01,  1.3175e-01, -2.5477e-01,\n                      -4.9960e-01,  1.9688e-01,  6.7372e-02,  3.4855e-01,  6.2270e-01,\n                       3.5494e-01, -2.0347e-02,  5.2839e-02, -2.2808e-02,  1.4829e-01,\n                      -1.9090e-01,  2.7708e-01,  1.9678e-01, -2.9751e-01, -2.2892e-01,\n                       3.2732e-02,  1.0906e-01, -7.5410e-02, -2.6922e-02,  1.6782e-01,\n                       5.8319e-01,  2.1625e-01, -4.5794e-02,  2.2907e-01, -6.6164e-02,\n                       1.6862e-02,  2.0393e-01,  1.5137e-01,  1.2026e+00,  2.3885e-01,\n                      -5.7016e-02,  4.5107e-01,  8.3293e-02,  1.1981e+00, -2.3212e-01,\n                      -7.5456e-02,  3.7246e-01, -1.3239e-01,  8.4414e-01, -8.4141e-02,\n                       1.6292e-01,  1.0411e-01,  5.8705e-02,  4.7083e-02,  8.2695e-01,\n                       1.5465e-01,  2.1758e-01, -5.9884e-02,  4.9101e-01,  1.3203e-01,\n                       3.6886e-01,  4.2944e-02,  5.3483e-01,  3.6677e-01,  1.6148e+00,\n                       8.2291e-02,  1.9680e-02,  2.3250e-01,  3.8006e-01,  1.6217e-01,\n                      -8.7073e-03,  5.5603e-02,  3.6230e-01, -6.4276e-02, -2.7017e+00,\n                       1.0746e-01,  1.8375e-01,  2.5427e-01,  3.3656e-02,  6.8091e-02,\n                       1.0305e+00,  1.6296e-01,  1.9919e-02, -1.1314e-01,  4.7992e-02,\n                       3.7143e-01, -1.0645e-01,  5.5535e-01,  2.3634e-01,  1.4057e-01,\n                      -6.0484e-01,  4.5413e-01,  2.9416e-01,  2.4231e-01, -1.6397e-01,\n                       1.8088e-01,  1.8035e-01, -1.0113e-01,  2.4780e-02,  5.2080e-01,\n                      -3.8109e-01, -4.0761e-03,  4.3039e-01, -2.8393e-01, -1.9445e-01,\n                       1.0560e-01, -5.0856e-02, -4.9086e-01,  1.7935e-01, -2.1225e-01,\n                       1.1141e-01,  2.7457e-02,  4.8398e-02,  4.8328e-02, -3.0430e-01,\n                      -1.9371e-01, -1.8081e-01, -5.4914e-01,  4.7688e-01, -1.2831e-01,\n                       3.2951e-01, -1.2144e-01,  2.6614e-01,  2.1987e-01,  1.9980e-01,\n                       5.6222e-01,  8.2323e-02, -9.3307e-02,  3.2505e-01,  1.8936e-01,\n                      -1.7894e-01,  3.5724e-01, -2.7868e-01,  2.3540e-01,  2.0965e-01,\n                      -1.6612e-01,  2.0011e-01, -1.5949e-01, -4.5713e-02,  6.8702e-02,\n                      -1.4802e-01,  2.6907e-01, -5.3389e-02, -7.2612e-02, -3.6327e-01,\n                       5.0092e-01,  3.2011e-01,  2.1582e-02,  2.9859e+00,  2.2769e-01,\n                      -1.4986e-01, -2.7969e-01,  9.3063e-02, -1.1321e-02, -9.0681e-02,\n                      -8.4747e-01, -1.6005e-02,  5.9846e-02,  9.7771e-02,  1.7480e-01,\n                       1.4853e-01,  3.7028e-01,  8.4143e-01,  3.3365e-01,  3.2089e-01,\n                      -4.0729e-01,  6.9678e-01, -1.0517e-01,  1.9255e-01, -4.4077e-01,\n                      -4.1037e-01, -8.2593e-02, -7.3555e-01,  1.0015e-01, -6.1799e-02,\n                       3.0380e-01, -1.0491e+00,  1.4734e-01,  3.7352e-01,  9.6625e-02,\n                      -8.3555e-03,  1.5332e-01, -3.7424e-02, -5.4676e-01])),\n             ('features.5.7.block.2.scale', tensor(0.2083)),\n             ('features.5.7.block.2.zero_point', tensor(74)),\n             ('features.5.7.block.3.scale', tensor(0.0992)),\n             ('features.5.7.block.3.zero_point', tensor(90)),\n             ('features.5.7.block.3._packed_params.dtype', torch.qint8),\n             ('features.5.7.block.3._packed_params._packed_params',\n              (tensor([[-0.0471, -0.0891, -0.0982,  ...,  0.1035, -0.0105, -0.0341],\n                       [-0.0708,  0.0760,  0.0090,  ...,  0.1056,  0.0129,  0.0206],\n                       [-0.1601,  0.0113, -0.0138,  ...,  0.0838,  0.0525, -0.0413],\n                       ...,\n                       [-0.0781, -0.0175, -0.0849,  ..., -0.0741, -0.0175, -0.0579],\n                       [-0.0140,  0.0385, -0.0490,  ...,  0.0070, -0.0070, -0.0175],\n                       [-0.0408,  0.0714, -0.0248,  ...,  0.0539, -0.0320,  0.0437]],\n                      size=(1536, 384), dtype=torch.qint8,\n                      quantization_scheme=torch.per_channel_affine,\n                      scale=tensor([0.0013, 0.0013, 0.0013,  ..., 0.0013, 0.0035, 0.0015],\n                      dtype=torch.float64),\n                      zero_point=tensor([0, 0, 0,  ..., 0, 0, 0]), axis=0),\n               Parameter containing:\n               tensor([-0.0322, -0.0464, -0.0451,  ..., -0.0287, -0.0537, -0.0381],\n                      requires_grad=True))),\n             ('features.5.7.block.5.scale', tensor(0.1276)),\n             ('features.5.7.block.5.zero_point', tensor(14)),\n             ('features.5.7.block.5._packed_params.dtype', torch.qint8),\n             ('features.5.7.block.5._packed_params._packed_params',\n              (tensor([[-0.0107, -0.0215, -0.0567,  ..., -0.0061, -0.0337, -0.0107],\n                       [-0.0345,  0.1295, -0.1036,  ...,  0.0621,  0.1018,  0.1122],\n                       [ 0.0000,  0.0497, -0.1125,  ...,  0.0248,  0.0447,  0.0414],\n                       ...,\n                       [ 0.0307, -0.0268,  0.0038,  ...,  0.0863, -0.0383, -0.0537],\n                       [-0.0534, -0.0205, -0.1088,  ...,  0.0595, -0.0164,  0.0205],\n                       [ 0.0629, -0.0097,  0.0645,  ..., -0.0306, -0.0210,  0.0435]],\n                      size=(384, 1536), dtype=torch.qint8,\n                      quantization_scheme=torch.per_channel_affine,\n                      scale=tensor([0.0015, 0.0017, 0.0017, 0.0017, 0.0014, 0.0016, 0.0019, 0.0016, 0.0017,\n                       0.0015, 0.0015, 0.0019, 0.0027, 0.0023, 0.0015, 0.0014, 0.0018, 0.0015,\n                       0.0015, 0.0014, 0.0016, 0.0015, 0.0018, 0.0014, 0.0014, 0.0024, 0.0018,\n                       0.0028, 0.0015, 0.0022, 0.0025, 0.0019, 0.0019, 0.0017, 0.0015, 0.0016,\n                       0.0016, 0.0016, 0.0018, 0.0015, 0.0026, 0.0017, 0.0020, 0.0016, 0.0022,\n                       0.0019, 0.0020, 0.0016, 0.0017, 0.0021, 0.0017, 0.0037, 0.0017, 0.0018,\n                       0.0016, 0.0019, 0.0016, 0.0014, 0.0014, 0.0015, 0.0019, 0.0024, 0.0026,\n                       0.0015, 0.0017, 0.0027, 0.0016, 0.0015, 0.0017, 0.0023, 0.0020, 0.0016,\n                       0.0017, 0.0041, 0.0015, 0.0017, 0.0018, 0.0012, 0.0020, 0.0015, 0.0019,\n                       0.0020, 0.0018, 0.0029, 0.0020, 0.0023, 0.0018, 0.0018, 0.0018, 0.0015,\n                       0.0015, 0.0017, 0.0016, 0.0014, 0.0018, 0.0017, 0.0015, 0.0020, 0.0015,\n                       0.0019, 0.0015, 0.0018, 0.0028, 0.0015, 0.0017, 0.0014, 0.0014, 0.0016,\n                       0.0013, 0.0032, 0.0020, 0.0018, 0.0021, 0.0017, 0.0023, 0.0017, 0.0016,\n                       0.0021, 0.0019, 0.0018, 0.0013, 0.0017, 0.0014, 0.0019, 0.0022, 0.0033,\n                       0.0017, 0.0018, 0.0015, 0.0015, 0.0013, 0.0017, 0.0025, 0.0017, 0.0016,\n                       0.0022, 0.0016, 0.0019, 0.0012, 0.0015, 0.0023, 0.0015, 0.0015, 0.0015,\n                       0.0015, 0.0019, 0.0024, 0.0015, 0.0016, 0.0017, 0.0022, 0.0018, 0.0017,\n                       0.0020, 0.0015, 0.0016, 0.0096, 0.0021, 0.0017, 0.0018, 0.0016, 0.0017,\n                       0.0017, 0.0024, 0.0016, 0.0015, 0.0016, 0.0016, 0.0014, 0.0015, 0.0015,\n                       0.0017, 0.0016, 0.0018, 0.0019, 0.0037, 0.0014, 0.0017, 0.0014, 0.0019,\n                       0.0019, 0.0020, 0.0019, 0.0023, 0.0016, 0.0016, 0.0040, 0.0015, 0.0025,\n                       0.0030, 0.0029, 0.0018, 0.0015, 0.0015, 0.0016, 0.0015, 0.0013, 0.0021,\n                       0.0017, 0.0017, 0.0016, 0.0016, 0.0013, 0.0021, 0.0013, 0.0018, 0.0016,\n                       0.0016, 0.0016, 0.0026, 0.0014, 0.0035, 0.0014, 0.0017, 0.0018, 0.0024,\n                       0.0022, 0.0014, 0.0015, 0.0026, 0.0015, 0.0017, 0.0016, 0.0016, 0.0015,\n                       0.0016, 0.0026, 0.0016, 0.0016, 0.0014, 0.0018, 0.0016, 0.0017, 0.0015,\n                       0.0018, 0.0015, 0.0014, 0.0024, 0.0026, 0.0016, 0.0016, 0.0016, 0.0017,\n                       0.0019, 0.0018, 0.0022, 0.0015, 0.0019, 0.0023, 0.0014, 0.0032, 0.0016,\n                       0.0019, 0.0018, 0.0018, 0.0018, 0.0015, 0.0018, 0.0018, 0.0017, 0.0016,\n                       0.0015, 0.0014, 0.0016, 0.0027, 0.0018, 0.0019, 0.0019, 0.0015, 0.0017,\n                       0.0018, 0.0020, 0.0021, 0.0018, 0.0025, 0.0016, 0.0014, 0.0015, 0.0017,\n                       0.0016, 0.0015, 0.0017, 0.0017, 0.0016, 0.0013, 0.0016, 0.0017, 0.0017,\n                       0.0018, 0.0013, 0.0020, 0.0019, 0.0014, 0.0017, 0.0016, 0.0014, 0.0019,\n                       0.0021, 0.0014, 0.0016, 0.0016, 0.0019, 0.0021, 0.0020, 0.0014, 0.0016,\n                       0.0016, 0.0015, 0.0015, 0.0019, 0.0021, 0.0017, 0.0014, 0.0018, 0.0028,\n                       0.0015, 0.0014, 0.0014, 0.0019, 0.0015, 0.0018, 0.0017, 0.0017, 0.0024,\n                       0.0014, 0.0023, 0.0013, 0.0017, 0.0016, 0.0019, 0.0015, 0.0020, 0.0019,\n                       0.0015, 0.0016, 0.0016, 0.0016, 0.0016, 0.0028, 0.0018, 0.0016, 0.0020,\n                       0.0015, 0.0018, 0.0025, 0.0018, 0.0019, 0.0023, 0.0022, 0.0017, 0.0016,\n                       0.0022, 0.0014, 0.0029, 0.0015, 0.0031, 0.0019, 0.0014, 0.0023, 0.0014,\n                       0.0023, 0.0017, 0.0019, 0.0014, 0.0023, 0.0018, 0.0018, 0.0014, 0.0020,\n                       0.0016, 0.0015, 0.0016, 0.0018, 0.0017, 0.0014, 0.0020, 0.0020, 0.0018,\n                       0.0018, 0.0024, 0.0017, 0.0019, 0.0021, 0.0016], dtype=torch.float64),\n                      zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n                      axis=0),\n               Parameter containing:\n               tensor([ 2.1617e-02, -2.2233e-03,  7.2957e-03, -3.0430e-04, -1.5312e-02,\n                       -5.0953e-03, -3.4006e-03, -7.0617e-03,  1.7572e-02, -9.5635e-04,\n                       -2.4509e-02, -6.5877e-03,  1.4732e-01, -6.2453e-03, -1.1504e-02,\n                       -2.1159e-03, -4.7621e-03,  1.7813e-02,  1.4688e-03,  7.0730e-03,\n                       -1.0348e-02, -7.5667e-03,  2.5736e-02,  1.0196e-02, -3.0397e-03,\n                       -1.3058e-02, -8.3567e-03,  6.1010e-03,  1.0621e-02,  8.6552e-02,\n                       -5.0238e-03, -3.2819e-05,  2.0775e-03, -1.9051e-02, -6.4005e-04,\n                       -1.3344e-02,  2.1098e-02,  1.5175e-02,  3.7271e-03,  1.6045e-02,\n                        2.4685e-02, -1.9309e-02, -3.1742e-02,  3.2525e-03, -3.2262e-03,\n                        2.9488e-02, -1.4553e-01,  2.7896e-03,  3.9580e-03,  4.3866e-03,\n                       -2.4786e-03, -7.3464e-02, -1.0261e-02, -3.6000e-02, -5.4828e-03,\n                        3.2309e-03, -2.4536e-02,  1.9852e-03,  1.6826e-03,  3.6356e-03,\n                       -1.5917e-02,  4.3410e-02,  4.4912e-03,  4.6411e-03,  1.2262e-02,\n                        4.7620e-02, -3.0413e-03, -1.7742e-02,  6.8633e-03, -4.6453e-02,\n                       -8.0112e-03, -5.4943e-03, -2.0625e-02,  2.7596e-03,  7.3757e-03,\n                        1.3374e-02,  1.3294e-02,  5.6965e-02,  5.5889e-03, -1.5269e-02,\n                        1.6201e-02,  1.6344e-02,  2.6488e-03,  1.1431e-02,  7.7963e-03,\n                        5.7433e-03,  1.2222e-02,  7.3567e-03, -5.7482e-03, -1.3940e-03,\n                       -2.1909e-02,  1.0199e-02,  3.4137e-02, -4.3890e-03, -2.4954e-03,\n                        9.9845e-03,  3.8613e-03, -1.0739e-02, -6.2506e-04, -1.1239e-02,\n                        9.6636e-03, -2.0896e-03, -3.0287e-03, -2.1284e-02, -1.1161e-02,\n                       -7.8855e-03,  7.5302e-03, -1.6188e-02, -1.7063e-02, -3.5308e-02,\n                       -8.0516e-03,  1.0861e-02,  1.7709e-02, -8.9679e-03, -1.8623e-02,\n                       -3.1063e-03,  5.2159e-03, -3.5927e-02, -8.0378e-03, -5.2255e-03,\n                       -1.1557e-02, -4.2160e-03,  4.0205e-03, -9.2188e-03, -1.5510e-01,\n                        1.6893e-01,  3.2972e-03,  5.4871e-03,  5.2334e-03,  1.9427e-02,\n                        2.8850e-02,  2.3227e-03, -4.4601e-03,  2.2813e-03,  9.4665e-03,\n                        4.6432e-02, -9.3085e-03,  2.8027e-02,  9.9317e-03, -1.9802e-02,\n                       -8.6411e-03,  5.6607e-03,  1.4321e-02,  8.8570e-03,  5.9090e-04,\n                       -4.4186e-03,  1.8651e-02, -1.0535e-02, -3.5969e-04,  1.9488e-02,\n                        1.2511e-02,  8.8959e-03, -4.6675e-03, -1.0640e-02, -1.9995e-03,\n                        7.6210e-04,  1.1115e+00,  1.5398e-02, -1.0768e-02,  8.2338e-03,\n                        1.1546e-02,  1.0004e-02, -3.6023e-03, -6.5096e-03, -2.1744e-03,\n                       -5.8755e-03,  1.0953e-02,  1.8347e-02, -1.0936e-02,  3.2773e-03,\n                        1.1611e-03,  4.6251e-03, -1.4524e-02, -2.0857e-02, -9.8355e-03,\n                        2.7821e-02,  9.3260e-03, -3.9602e-03, -2.7365e-04, -2.7911e-03,\n                        3.0198e-02, -2.8257e-02,  3.9485e-03, -3.4292e-02, -1.0388e-02,\n                        1.2365e-02,  1.8993e-02,  1.4557e-02, -3.3673e-03, -4.5708e-02,\n                       -8.1133e-04,  5.1163e-03,  5.7596e-04, -2.3091e-04,  8.9165e-03,\n                        1.8935e-02,  1.3844e-02,  3.2360e-02, -2.2670e-02,  1.6823e-02,\n                        4.2656e-03,  6.7062e-03, -4.9307e-03, -9.6282e-03,  9.5792e-03,\n                       -1.4688e-04, -1.1232e-03,  4.8165e-04,  7.1716e-03,  9.4783e-03,\n                        1.4665e-02, -1.3618e-01,  1.0564e-02, -1.6427e-04,  1.5098e-02,\n                        6.5904e-02,  9.1274e-03, -2.1950e-02,  9.4719e-03, -7.0419e-03,\n                        6.5796e-03,  1.8736e-02,  5.7225e-03,  2.1136e-02,  5.7564e-03,\n                       -3.3182e-02,  4.5152e-03, -1.7382e-02,  1.5071e-02,  4.3637e-03,\n                        4.1646e-02,  2.3831e-03, -1.7002e-02,  4.5229e-03,  5.6414e-03,\n                       -1.3254e-02,  3.2223e-03,  1.0965e-02,  5.3029e-02, -6.2593e-03,\n                       -2.1428e-02, -1.1405e-02,  3.0732e-03,  2.2420e-02,  2.5124e-02,\n                        3.9875e-03,  2.6673e-03,  2.5128e-02,  3.8719e-02, -2.5486e-03,\n                        3.1084e-03,  9.7023e-03, -3.3306e-03, -2.7475e-03, -2.1591e-02,\n                        3.6673e-03,  1.1925e-02, -2.0632e-02,  1.3449e-02, -3.7959e-03,\n                        1.4256e-02, -4.7969e-03, -5.7913e-03, -3.2842e-03, -5.5301e-02,\n                        3.8890e-03, -3.4266e-02,  8.3917e-03, -7.4501e-03,  1.2963e-02,\n                       -5.5311e-03,  5.4011e-03,  5.8102e-03,  5.8577e-03, -7.8015e-02,\n                       -6.0035e-03,  1.3474e-02, -1.1660e-02, -1.4134e-02,  1.7009e-02,\n                        2.9535e-02, -1.0826e-02, -2.3773e-02, -3.7424e-03,  4.6939e-03,\n                        1.4105e-02,  5.3156e-03,  1.5822e-02,  3.3830e-03, -7.0734e-03,\n                       -9.2516e-03,  3.8241e-03, -1.5885e-03, -4.9745e-02, -6.0945e-03,\n                        7.6129e-03, -2.7214e-03,  5.2698e-03, -1.3357e-02, -7.3251e-03,\n                       -2.4847e-02,  6.1182e-03,  2.0950e-02,  3.1996e-02, -1.9201e-03,\n                       -6.4052e-03, -3.3261e-03, -1.8903e-02, -2.0278e-03, -5.0989e-03,\n                       -6.1014e-03, -1.8819e-02, -4.2454e-03,  1.4161e-02, -3.0974e-02,\n                        6.1130e-03, -3.9458e-03, -2.9691e-02,  1.8911e-02, -3.6244e-02,\n                       -3.1262e-03, -8.3429e-03, -9.4400e-04,  3.0999e-02,  6.2414e-03,\n                       -3.8347e-03, -1.1271e-02, -1.6222e-02,  2.6020e-03, -1.4300e-02,\n                        3.9752e-02, -1.6605e-05, -3.8219e-03, -8.4264e-03,  2.1397e-03,\n                        2.1996e-02, -1.8177e-03,  1.1428e-02, -1.8779e-02,  2.7368e-02,\n                       -6.9429e-03,  3.4654e-04,  3.8754e-05, -3.2042e-03, -8.8096e-02,\n                        1.0956e-02, -1.2783e-02, -1.0192e-02,  1.9873e-01,  1.3478e-03,\n                        1.2148e-02, -1.8019e-04, -8.1238e-03, -3.6376e-02,  1.1948e-02,\n                        4.8322e-02,  1.9346e-02, -4.7977e-03, -7.8396e-03, -1.5633e-03,\n                        8.4473e-03, -9.6218e-03,  2.0231e-03,  1.8707e-03, -4.2347e-02,\n                       -1.1023e-02, -1.4690e-02,  8.9418e-03, -4.1435e-03,  5.3356e-02,\n                        1.0373e-02, -4.0707e-03, -1.0349e-02, -9.5814e-03, -4.5243e-03,\n                       -4.2640e-02, -5.6936e-02, -4.3733e-03, -1.3526e-02, -2.5469e-02,\n                       -1.2698e-02,  1.3834e-02, -2.2373e-03, -1.0571e-02],\n                      requires_grad=True))),\n             ('features.5.8.layer_scale',\n              tensor([[[-1.5414e-01]],\n              \n                      [[-9.3758e-02]],\n              \n                      [[-2.0573e-01]],\n              \n                      [[-3.1835e-01]],\n              \n                      [[-2.9805e-01]],\n              \n                      [[-2.5758e-01]],\n              \n                      [[ 9.5650e-02]],\n              \n                      [[ 2.9700e-01]],\n              \n                      [[-2.8841e-01]],\n              \n                      [[ 1.6129e-01]],\n              \n                      [[ 1.2499e-01]],\n              \n                      [[-3.1921e-01]],\n              \n                      [[-4.3822e-01]],\n              \n                      [[-3.5591e-01]],\n              \n                      [[-2.6189e-01]],\n              \n                      [[-2.0443e-01]],\n              \n                      [[ 3.5089e-01]],\n              \n                      [[ 2.3938e-01]],\n              \n                      [[ 2.7182e-01]],\n              \n                      [[ 2.2374e-01]],\n              \n                      [[ 3.8388e-01]],\n              \n                      [[-2.1157e-01]],\n              \n                      [[ 4.6552e-01]],\n              \n                      [[-2.5884e-01]],\n              \n                      [[ 1.2783e-01]],\n              \n                      [[ 7.6833e-02]],\n              \n                      [[ 1.3482e-01]],\n              \n                      [[-1.1329e-01]],\n              \n                      [[-3.0592e-01]],\n              \n                      [[-5.8322e-01]],\n              \n                      [[-3.7513e-01]],\n              \n                      [[ 1.4170e-01]],\n              \n                      [[ 2.5902e-01]],\n              \n                      [[ 2.8406e-01]],\n              \n                      [[-2.5349e-01]],\n              \n                      [[-2.8094e-01]],\n              \n                      [[ 2.4225e-01]],\n              \n                      [[ 3.4580e-01]],\n              \n                      [[-2.9141e-01]],\n              \n                      [[-2.4805e-01]],\n              \n                      [[ 3.4513e-01]],\n              \n                      [[-2.3583e-01]],\n              \n                      [[-4.2492e-01]],\n              \n                      [[ 3.0447e-01]],\n              \n                      [[ 3.1656e-01]],\n              \n                      [[ 3.9719e-01]],\n              \n                      [[ 1.3765e-01]],\n              \n                      [[-3.0458e-01]],\n              \n                      [[ 1.5941e-01]],\n              \n                      [[-2.6858e-01]],\n              \n                      [[ 3.4668e-01]],\n              \n                      [[ 3.9130e-01]],\n              \n                      [[ 1.7700e-01]],\n              \n                      [[ 3.3995e-01]],\n              \n                      [[ 2.0219e-01]],\n              \n                      [[-8.7448e-05]],\n              \n                      [[ 3.2307e-01]],\n              \n                      [[ 1.7803e-01]],\n              \n                      [[ 1.4743e-01]],\n              \n                      [[ 2.3045e-01]],\n              \n                      [[-1.2422e-01]],\n              \n                      [[-3.7351e-01]],\n              \n                      [[ 9.6416e-02]],\n              \n                      [[ 2.4661e-01]],\n              \n                      [[ 2.9484e-01]],\n              \n                      [[-3.1042e-01]],\n              \n                      [[ 1.0424e-01]],\n              \n                      [[-4.3054e-01]],\n              \n                      [[-3.2542e-01]],\n              \n                      [[-8.3040e-02]],\n              \n                      [[-2.7665e-01]],\n              \n                      [[-1.7025e-01]],\n              \n                      [[ 4.6446e-01]],\n              \n                      [[-7.2798e-02]],\n              \n                      [[ 1.9019e-01]],\n              \n                      [[-2.5279e-01]],\n              \n                      [[ 2.9371e-01]],\n              \n                      [[ 1.8090e-02]],\n              \n                      [[ 2.9260e-01]],\n              \n                      [[-2.2239e-01]],\n              \n                      [[ 2.6015e-01]],\n              \n                      [[-2.9790e-01]],\n              \n                      [[-2.7067e-01]],\n              \n                      [[ 4.5777e-01]],\n              \n                      [[-2.5530e-01]],\n              \n                      [[-3.6817e-01]],\n              \n                      [[ 4.3986e-01]],\n              \n                      [[-2.2354e-01]],\n              \n                      [[ 1.2461e-01]],\n              \n                      [[-2.4211e-01]],\n              \n                      [[ 2.8800e-01]],\n              \n                      [[-2.9151e-01]],\n              \n                      [[-2.1625e-01]],\n              \n                      [[-2.4866e-01]],\n              \n                      [[ 2.2024e-01]],\n              \n                      [[-2.6730e-01]],\n              \n                      [[ 2.4687e-01]],\n              \n                      [[-3.7986e-01]],\n              \n                      [[-2.6816e-01]],\n              \n                      [[ 3.8764e-01]],\n              \n                      [[ 2.0467e-01]],\n              \n                      [[-2.7478e-01]],\n              \n                      [[-4.0831e-01]],\n              \n                      [[ 2.3708e-01]],\n              \n                      [[ 1.6553e-01]],\n              \n                      [[-1.7049e-01]],\n              \n                      [[-1.6112e-01]],\n              \n                      [[ 4.2366e-01]],\n              \n                      [[-2.0523e-01]],\n              \n                      [[ 3.8497e-01]],\n              \n                      [[ 3.7197e-01]],\n              \n                      [[-2.8959e-01]],\n              \n                      [[ 4.1898e-01]],\n              \n                      [[ 1.4365e-01]],\n              \n                      [[-4.0177e-01]],\n              \n                      [[ 3.0841e-01]],\n              \n                      [[-2.4488e-01]],\n              \n                      [[ 3.8758e-01]],\n              \n                      [[ 1.0936e-01]],\n              \n                      [[-2.7060e-01]],\n              \n                      [[-2.0463e-01]],\n              \n                      [[ 1.8074e-01]],\n              \n                      [[ 2.1079e-01]],\n              \n                      [[ 4.0635e-01]],\n              \n                      [[-4.1219e-01]],\n              \n                      [[-4.3296e-01]],\n              \n                      [[ 8.8147e-02]],\n              \n                      [[ 2.7037e-01]],\n              \n                      [[-2.1900e-01]],\n              \n                      [[ 1.0184e-01]],\n              \n                      [[ 7.6956e-04]],\n              \n                      [[ 2.8248e-01]],\n              \n                      [[-4.9336e-01]],\n              \n                      [[ 3.5972e-01]],\n              \n                      [[ 1.9811e-01]],\n              \n                      [[ 4.0700e-01]],\n              \n                      [[-1.8138e-01]],\n              \n                      [[ 9.9609e-02]],\n              \n                      [[ 3.6492e-02]],\n              \n                      [[-1.7847e-01]],\n              \n                      [[ 3.6732e-01]],\n              \n                      [[ 2.6089e-01]],\n              \n                      [[ 2.3882e-01]],\n              \n                      [[-1.5472e-01]],\n              \n                      [[ 1.1851e-01]],\n              \n                      [[-2.9103e-01]],\n              \n                      [[-3.8147e-01]],\n              \n                      [[-2.0235e-01]],\n              \n                      [[-2.2676e-01]],\n              \n                      [[-3.0651e-01]],\n              \n                      [[-3.2131e-01]],\n              \n                      [[ 1.9048e-01]],\n              \n                      [[-2.7565e-01]],\n              \n                      [[-2.9745e-01]],\n              \n                      [[-2.8814e-01]],\n              \n                      [[ 4.1895e-01]],\n              \n                      [[-1.9758e+00]],\n              \n                      [[-3.5869e-01]],\n              \n                      [[-2.3942e-01]],\n              \n                      [[-1.7829e-01]],\n              \n                      [[-3.4064e-01]],\n              \n                      [[ 1.9596e-01]],\n              \n                      [[ 2.4731e-01]],\n              \n                      [[ 4.5118e-01]],\n              \n                      [[-3.5485e-01]],\n              \n                      [[-2.3222e-01]],\n              \n                      [[ 2.6512e-01]],\n              \n                      [[ 2.3754e-01]],\n              \n                      [[-2.5088e-01]],\n              \n                      [[-2.5743e-01]],\n              \n                      [[ 2.2322e-01]],\n              \n                      [[ 2.5604e-01]],\n              \n                      [[ 2.3516e-01]],\n              \n                      [[-2.4923e-01]],\n              \n                      [[-2.7523e-01]],\n              \n                      [[-6.7660e-02]],\n              \n                      [[ 2.6788e-01]],\n              \n                      [[ 1.1941e-01]],\n              \n                      [[-2.6206e-01]],\n              \n                      [[-3.2538e-01]],\n              \n                      [[-9.4162e-02]],\n              \n                      [[-3.6784e-01]],\n              \n                      [[ 3.3859e-01]],\n              \n                      [[-3.0978e-01]],\n              \n                      [[-2.5372e-01]],\n              \n                      [[ 4.0825e-01]],\n              \n                      [[ 9.4857e-02]],\n              \n                      [[-2.4979e-01]],\n              \n                      [[-4.6064e-01]],\n              \n                      [[-4.8789e-01]],\n              \n                      [[-4.5093e-01]],\n              \n                      [[-3.7771e-02]],\n              \n                      [[-1.7402e-01]],\n              \n                      [[ 1.0005e-01]],\n              \n                      [[ 2.5725e-01]],\n              \n                      [[-9.2938e-02]],\n              \n                      [[ 1.5633e-01]],\n              \n                      [[ 8.5616e-02]],\n              \n                      [[-3.3145e-01]],\n              \n                      [[ 3.5296e-01]],\n              \n                      [[ 2.9323e-01]],\n              \n                      [[ 2.3374e-01]],\n              \n                      [[ 1.8457e-01]],\n              \n                      [[-3.5260e-01]],\n              \n                      [[-2.0941e-01]],\n              \n                      [[-3.0632e-01]],\n              \n                      [[-1.9472e-01]],\n              \n                      [[ 1.6711e-01]],\n              \n                      [[-2.2689e-01]],\n              \n                      [[ 4.2361e-01]],\n              \n                      [[-1.5941e-01]],\n              \n                      [[-3.8894e-01]],\n              \n                      [[-2.6299e-01]],\n              \n                      [[-2.3227e-01]],\n              \n                      [[-1.5903e-01]],\n              \n                      [[-4.1229e-01]],\n              \n                      [[ 1.8837e-01]],\n              \n                      [[-8.8813e-02]],\n              \n                      [[ 2.6193e-01]],\n              \n                      [[ 4.6129e-01]],\n              \n                      [[ 2.0025e-01]],\n              \n                      [[-9.8978e-02]],\n              \n                      [[ 3.5420e-01]],\n              \n                      [[ 1.2502e-01]],\n              \n                      [[ 3.1513e-01]],\n              \n                      [[-4.2492e-01]],\n              \n                      [[ 2.9805e-01]],\n              \n                      [[ 1.8264e-01]],\n              \n                      [[-2.7012e-01]],\n              \n                      [[-2.5528e-01]],\n              \n                      [[ 3.9111e-01]],\n              \n                      [[-1.9821e-01]],\n              \n                      [[ 4.0703e-01]],\n              \n                      [[-2.5907e-01]],\n              \n                      [[ 2.8902e-01]],\n              \n                      [[ 1.6291e-01]],\n              \n                      [[-1.5814e-01]],\n              \n                      [[ 4.2346e-01]],\n              \n                      [[ 4.4276e-01]],\n              \n                      [[ 2.1308e-01]],\n              \n                      [[ 1.3089e-01]],\n              \n                      [[-2.8681e-01]],\n              \n                      [[-1.7446e-01]],\n              \n                      [[-3.2321e-01]],\n              \n                      [[-3.1396e-01]],\n              \n                      [[ 3.5159e-01]],\n              \n                      [[ 2.6757e-01]],\n              \n                      [[ 3.9717e-01]],\n              \n                      [[-3.4870e-01]],\n              \n                      [[-1.7247e-01]],\n              \n                      [[-1.0206e-01]],\n              \n                      [[-2.8843e-01]],\n              \n                      [[ 1.0723e-01]],\n              \n                      [[-1.9461e-01]],\n              \n                      [[ 3.1429e-01]],\n              \n                      [[-2.7646e-01]],\n              \n                      [[-1.9894e-01]],\n              \n                      [[ 3.2434e-01]],\n              \n                      [[-3.1219e-01]],\n              \n                      [[-2.2410e-01]],\n              \n                      [[ 2.5546e-01]],\n              \n                      [[-3.3312e-01]],\n              \n                      [[-2.3830e-01]],\n              \n                      [[-2.3738e-01]],\n              \n                      [[-1.9011e-01]],\n              \n                      [[-3.9477e-01]],\n              \n                      [[ 3.8859e-01]],\n              \n                      [[-1.8883e-01]],\n              \n                      [[-3.3954e-01]],\n              \n                      [[-2.9735e-01]],\n              \n                      [[ 1.9866e-01]],\n              \n                      [[-1.2554e-01]],\n              \n                      [[-3.1059e-01]],\n              \n                      [[-3.4549e-01]],\n              \n                      [[-1.2836e-01]],\n              \n                      [[-3.1324e-01]],\n              \n                      [[ 2.2497e-01]],\n              \n                      [[-2.1618e-01]],\n              \n                      [[-3.2417e-01]],\n              \n                      [[-2.3823e-01]],\n              \n                      [[ 8.9597e-02]],\n              \n                      [[ 3.6510e-01]],\n              \n                      [[-2.8029e-01]],\n              \n                      [[ 2.9209e-01]],\n              \n                      [[-1.4427e-01]],\n              \n                      [[ 3.2665e-01]],\n              \n                      [[ 1.1051e-01]],\n              \n                      [[ 2.1383e-01]],\n              \n                      [[-2.5115e-01]],\n              \n                      [[ 1.8068e-01]],\n              \n                      [[ 3.2866e-01]],\n              \n                      [[-4.6344e-01]],\n              \n                      [[ 2.5137e-01]],\n              \n                      [[ 7.0328e-02]],\n              \n                      [[-2.5509e-01]],\n              \n                      [[ 1.7115e-01]],\n              \n                      [[ 2.6710e-01]],\n              \n                      [[-8.3971e-02]],\n              \n                      [[-1.2492e-01]],\n              \n                      [[ 1.3599e-01]],\n              \n                      [[ 2.1748e-01]],\n              \n                      [[ 2.2927e-01]],\n              \n                      [[ 4.5315e-01]],\n              \n                      [[-3.5871e-01]],\n              \n                      [[-9.8350e-02]],\n              \n                      [[-1.5624e-01]],\n              \n                      [[ 2.0428e-01]],\n              \n                      [[-1.8498e-01]],\n              \n                      [[-1.5073e-01]],\n              \n                      [[ 3.1007e-01]],\n              \n                      [[ 2.6095e-01]],\n              \n                      [[ 1.9535e-01]],\n              \n                      [[ 2.5906e-01]],\n              \n                      [[-2.7217e-01]],\n              \n                      [[ 1.5679e-03]],\n              \n                      [[ 1.7365e-01]],\n              \n                      [[-2.6111e-01]],\n              \n                      [[-3.6218e-01]],\n              \n                      [[ 1.8735e-01]],\n              \n                      [[-1.2565e-01]],\n              \n                      [[-3.3757e-01]],\n              \n                      [[ 2.1153e-01]],\n              \n                      [[ 2.3929e-01]],\n              \n                      [[ 4.2080e-01]],\n              \n                      [[ 2.8876e-01]],\n              \n                      [[ 4.0556e-01]],\n              \n                      [[ 5.8833e-01]],\n              \n                      [[-3.1723e-01]],\n              \n                      [[ 2.5058e-01]],\n              \n                      [[-3.9513e-01]],\n              \n                      [[ 1.9949e-01]],\n              \n                      [[-2.4625e-01]],\n              \n                      [[-2.1596e-01]],\n              \n                      [[-2.6383e-01]],\n              \n                      [[ 2.3193e-01]],\n              \n                      [[ 2.7093e-01]],\n              \n                      [[ 2.9492e-01]],\n              \n                      [[-1.0734e-01]],\n              \n                      [[-1.8312e-03]],\n              \n                      [[ 5.1099e-01]],\n              \n                      [[ 9.7188e-02]],\n              \n                      [[ 3.1493e-01]],\n              \n                      [[ 1.2935e-01]],\n              \n                      [[-3.5074e-01]],\n              \n                      [[-4.4571e-01]],\n              \n                      [[-2.3793e-01]],\n              \n                      [[ 4.1102e-01]],\n              \n                      [[ 8.7953e-02]],\n              \n                      [[ 3.8541e-01]],\n              \n                      [[-2.0361e-01]],\n              \n                      [[ 1.4424e-01]],\n              \n                      [[-2.7021e-01]],\n              \n                      [[ 2.5445e-01]],\n              \n                      [[-1.4416e-01]],\n              \n                      [[-1.8637e-01]],\n              \n                      [[ 5.9173e-05]],\n              \n                      [[-3.3372e-01]],\n              \n                      [[ 2.2010e-01]],\n              \n                      [[ 3.8087e-01]],\n              \n                      [[-2.4311e-01]],\n              \n                      [[ 6.7759e-02]],\n              \n                      [[ 1.7212e-01]],\n              \n                      [[-3.2355e-01]],\n              \n                      [[ 2.2513e-01]],\n              \n                      [[-3.5406e-01]],\n              \n                      [[-2.5531e-01]],\n              \n                      [[-2.6837e-01]],\n              \n                      [[ 2.0486e-01]],\n              \n                      [[ 3.7927e-01]],\n              \n                      [[ 3.8247e-01]],\n              \n                      [[-1.4253e-01]],\n              \n                      [[ 2.6160e-01]],\n              \n                      [[-2.8219e-01]],\n              \n                      [[ 3.6224e-01]],\n              \n                      [[-2.2875e-01]],\n              \n                      [[ 2.9091e-01]],\n              \n                      [[ 3.1379e-01]],\n              \n                      [[ 3.4298e-01]],\n              \n                      [[-2.7800e-01]],\n              \n                      [[ 4.1966e-01]],\n              \n                      [[-2.5452e-01]],\n              \n                      [[ 1.4082e-01]],\n              \n                      [[ 3.0092e-01]],\n              \n                      [[ 2.3311e-01]]])),\n             ('features.5.8.block.0.weight',\n              tensor([[[[-0.0079, -0.0079, -0.0099,  ...,  0.0020, -0.0099, -0.0119],\n                        [-0.0059, -0.0020,  0.0000,  ...,  0.0020,  0.0040, -0.0040],\n                        [-0.0079,  0.0000, -0.0099,  ..., -0.0297, -0.0059, -0.0079],\n                        ...,\n                        [ 0.0079,  0.0079,  0.0040,  ...,  0.2101, -0.0337,  0.0059],\n                        [-0.0178,  0.0000, -0.0159,  ..., -0.0020, -0.0079, -0.0099],\n                        [ 0.0020, -0.0079,  0.0000,  ...,  0.0000,  0.0000, -0.0099]]],\n              \n              \n                      [[[ 0.0036, -0.0053, -0.0036,  ...,  0.0143,  0.0000,  0.0053],\n                        [ 0.0000,  0.0071,  0.0053,  ..., -0.0018,  0.0018,  0.0036],\n                        [ 0.0000,  0.0071,  0.0071,  ...,  0.0588,  0.0018,  0.0018],\n                        ...,\n                        [ 0.0071,  0.0018,  0.0463,  ..., -0.1817,  0.0819,  0.0071],\n                        [ 0.0053,  0.0000,  0.0160,  ...,  0.0249,  0.0107, -0.0018],\n                        [ 0.0071,  0.0036,  0.0053,  ...,  0.0053,  0.0107,  0.0071]]],\n              \n              \n                      [[[ 0.0115,  0.0000,  0.0164,  ..., -0.0016,  0.0115,  0.0033],\n                        [-0.0033, -0.0016,  0.0016,  ..., -0.0033,  0.0132, -0.0033],\n                        [ 0.0066, -0.0016,  0.0099,  ...,  0.0132,  0.0115,  0.0016],\n                        ...,\n                        [-0.0049,  0.0000, -0.0033,  ..., -0.1760,  0.0362, -0.0099],\n                        [ 0.0132,  0.0049,  0.0197,  ...,  0.0181,  0.0164,  0.0132],\n                        [ 0.0033,  0.0016,  0.0033,  ..., -0.0132,  0.0164,  0.0000]]],\n              \n              \n                      ...,\n              \n              \n                      [[[ 0.0119,  0.0119,  0.0080,  ...,  0.0099,  0.0119,  0.0139],\n                        [ 0.0099,  0.0040,  0.0040,  ..., -0.0060,  0.0020,  0.0099],\n                        [ 0.0080,  0.0060,  0.0179,  ...,  0.0239,  0.0159,  0.0119],\n                        ...,\n                        [-0.0040, -0.0060, -0.0060,  ..., -0.2248,  0.0418, -0.0080],\n                        [ 0.0080,  0.0040,  0.0179,  ..., -0.0020,  0.0159,  0.0099],\n                        [ 0.0099,  0.0040,  0.0119,  ...,  0.0040,  0.0060,  0.0159]]],\n              \n              \n                      [[[ 0.0102,  0.0058,  0.0029,  ...,  0.0117,  0.0088, -0.0015],\n                        [-0.0044,  0.0044,  0.0044,  ...,  0.0000,  0.0029,  0.0058],\n                        [ 0.0058,  0.0102,  0.0117,  ...,  0.0379,  0.0029,  0.0058],\n                        ...,\n                        [-0.0131,  0.0015,  0.0015,  ..., -0.1518,  0.0379, -0.0044],\n                        [ 0.0088,  0.0088,  0.0161,  ...,  0.0058,  0.0175,  0.0015],\n                        [ 0.0029,  0.0029,  0.0044,  ...,  0.0044,  0.0058,  0.0044]]],\n              \n              \n                      [[[ 0.0017, -0.0017,  0.0067,  ..., -0.0083,  0.0083,  0.0000],\n                        [ 0.0017,  0.0000,  0.0067,  ..., -0.0067,  0.0083,  0.0117],\n                        [ 0.0200, -0.0033,  0.0133,  ...,  0.0017,  0.0133, -0.0017],\n                        ...,\n                        [-0.0117, -0.0117, -0.0300,  ..., -0.1915,  0.0050, -0.0117],\n                        [ 0.0067,  0.0033,  0.0150,  ..., -0.0250,  0.0183,  0.0033],\n                        [ 0.0033,  0.0033,  0.0033,  ..., -0.0033,  0.0067, -0.0050]]]],\n                     size=(384, 1, 7, 7), dtype=torch.qint8,\n                     quantization_scheme=torch.per_channel_affine,\n                     scale=tensor([1.9820e-03, 1.7814e-03, 1.6447e-03, 1.4742e-03, 1.4238e-03, 1.4649e-03,\n                      2.0204e-03, 1.4859e-03, 1.4797e-03, 1.9008e-03, 1.9656e-03, 1.8188e-03,\n                      1.9685e-03, 4.2615e-04, 1.5325e-03, 1.6618e-03, 1.2735e-03, 1.6085e-03,\n                      8.3825e-04, 1.7141e-03, 1.2387e-03, 1.6946e-03, 1.2853e-03, 1.3629e-03,\n                      1.9052e-03, 1.9794e-03, 2.0220e-03, 1.9344e-03, 1.2593e-03, 2.1097e-03,\n                      9.7210e-04, 1.9799e-03, 1.4382e-03, 1.7053e-03, 1.2651e-03, 1.7493e-03,\n                      1.7427e-03, 1.4320e-03, 1.3267e-03, 1.7381e-03, 1.3874e-03, 1.8001e-03,\n                      1.2533e-03, 1.5148e-03, 1.2564e-03, 9.6828e-04, 1.1063e-03, 1.5980e-03,\n                      1.8975e-03, 1.5951e-03, 1.1514e-03, 8.6131e-04, 1.8189e-03, 1.3537e-03,\n                      1.8196e-03, 1.5258e-03, 8.1703e-04, 1.7413e-03, 2.0129e-03, 1.5463e-03,\n                      1.8924e-03, 1.5225e-03, 1.9750e-03, 1.5933e-03, 1.3681e-03, 8.9547e-04,\n                      2.0507e-03, 1.0535e-03, 8.3623e-04, 1.9658e-03, 9.4009e-04, 1.8502e-03,\n                      9.8807e-04, 2.0213e-03, 1.6460e-03, 1.4444e-03, 9.7013e-04, 1.1751e-03,\n                      1.4170e-03, 1.5390e-03, 1.5163e-03, 1.5968e-03, 1.6777e-03, 2.0629e-03,\n                      1.2637e-03, 1.3961e-03, 7.5484e-04, 1.6488e-03, 1.9950e-03, 1.4660e-03,\n                      1.8018e-03, 1.5796e-03, 1.4524e-03, 1.6117e-03, 1.5985e-03, 1.4935e-03,\n                      1.6189e-03, 9.3511e-04, 1.4057e-03, 8.3268e-04, 1.6909e-03, 1.6117e-03,\n                      4.3354e-04, 1.6415e-03, 1.6312e-03, 1.7996e-03, 1.9117e-03, 1.1336e-03,\n                      1.6108e-03, 4.5766e-04, 9.7929e-04, 1.4420e-03, 6.0650e-04, 1.9543e-03,\n                      1.1340e-03, 1.1987e-03, 1.5029e-03, 2.0077e-03, 2.0446e-03, 1.3074e-03,\n                      1.6532e-03, 1.9422e-03, 1.6588e-03, 1.0468e-03, 2.3544e-03, 1.3030e-03,\n                      1.9691e-03, 1.3619e-03, 1.6966e-03, 8.3856e-04, 6.4406e-04, 1.4707e-03,\n                      9.9809e-04, 9.7014e-04, 1.7379e-03, 1.3156e-03, 1.7302e-03, 1.9480e-03,\n                      1.0672e-03, 1.9226e-03, 1.3532e-03, 1.6039e-03, 1.4735e-03, 1.7409e-03,\n                      1.9897e-03, 1.0833e-03, 1.6407e-03, 1.6397e-03, 1.6449e-03, 9.5274e-04,\n                      1.3942e-03, 1.7701e-03, 1.2803e-03, 6.4162e-04, 1.4100e-03, 1.1012e-03,\n                      1.0597e-05, 1.4491e-03, 1.6971e-03, 1.8396e-03, 1.2374e-03, 1.6451e-03,\n                      1.6356e-03, 1.0363e-03, 1.2565e-03, 1.5701e-03, 1.2239e-03, 1.3501e-03,\n                      1.4097e-03, 1.5728e-03, 1.5994e-03, 1.8191e-03, 1.5675e-03, 1.1810e-03,\n                      1.5623e-03, 2.1450e-03, 1.5544e-03, 2.0718e-03, 1.4253e-03, 1.3485e-03,\n                      1.9763e-03, 1.8546e-03, 1.3491e-03, 1.3237e-03, 1.5293e-03, 1.3613e-03,\n                      2.0788e-03, 1.3019e-03, 7.6660e-04, 7.7902e-04, 1.4508e-03, 1.0188e-03,\n                      1.7714e-03, 1.8627e-03, 1.5918e-03, 9.2254e-04, 1.9619e-03, 1.9004e-03,\n                      1.2619e-03, 1.2073e-03, 1.1254e-03, 1.5086e-03, 1.7774e-03, 1.2695e-03,\n                      1.6158e-03, 1.1637e-03, 1.7230e-03, 1.8673e-03, 1.6411e-03, 1.5717e-03,\n                      1.7992e-03, 1.4811e-03, 1.5963e-03, 1.5520e-03, 1.8270e-03, 9.5870e-04,\n                      1.6814e-03, 5.7602e-04, 1.3677e-03, 1.6043e-03, 1.8835e-03, 2.0755e-03,\n                      1.0619e-03, 1.9800e-03, 1.1879e-03, 7.6756e-04, 1.4642e-03, 1.8515e-03,\n                      1.4591e-03, 1.4673e-03, 1.0243e-03, 1.7175e-03, 1.4146e-03, 1.6778e-03,\n                      1.4178e-03, 1.8904e-03, 1.9743e-03, 1.2902e-03, 1.3471e-03, 1.6448e-03,\n                      8.1965e-04, 1.4143e-03, 1.8315e-03, 1.7920e-03, 1.1562e-03, 1.3110e-03,\n                      1.3748e-03, 1.1695e-03, 1.5830e-03, 1.7731e-03, 2.0043e-03, 1.1235e-03,\n                      1.9721e-03, 1.7168e-03, 1.2843e-03, 1.4630e-03, 1.7738e-03, 1.2946e-03,\n                      1.3403e-03, 1.4602e-03, 1.4415e-03, 1.0183e-03, 1.2635e-03, 8.1220e-04,\n                      8.3420e-04, 1.7045e-03, 7.6707e-04, 1.7620e-03, 1.2145e-03, 1.5607e-03,\n                      1.7020e-03, 2.0313e-03, 1.6417e-03, 1.4264e-03, 2.2656e-03, 1.1434e-03,\n                      1.6791e-03, 1.6178e-03, 1.2587e-03, 1.4119e-03, 2.1910e-03, 1.4047e-03,\n                      1.0495e-03, 1.6558e-03, 1.9370e-03, 1.2365e-03, 2.0247e-03, 1.4785e-03,\n                      1.4611e-03, 1.6838e-03, 1.6878e-03, 8.4968e-04, 1.5925e-03, 9.0269e-04,\n                      1.3618e-03, 1.7603e-03, 1.6341e-03, 2.0350e-03, 2.0115e-03, 2.0025e-03,\n                      1.7744e-03, 1.5354e-03, 6.2075e-04, 1.7427e-03, 2.0704e-03, 1.9654e-03,\n                      1.7401e-03, 1.7328e-03, 1.9460e-03, 1.6013e-03, 1.3979e-03, 1.1864e-03,\n                      1.4844e-03, 1.3995e-03, 1.8418e-03, 1.8007e-03, 1.5646e-03, 1.5429e-03,\n                      7.3894e-04, 1.7897e-03, 1.2262e-03, 1.6721e-03, 1.4120e-03, 7.9322e-04,\n                      1.3705e-03, 1.9453e-03, 1.3318e-03, 1.5803e-03, 1.3766e-03, 1.2262e-03,\n                      1.6895e-03, 1.5766e-03, 1.5517e-03, 1.3693e-03, 1.6372e-03, 1.6582e-03,\n                      1.3533e-03, 2.0729e-03, 2.0857e-03, 1.0256e-03, 2.0791e-03, 1.3390e-03,\n                      1.9921e-03, 1.2517e-03, 6.6629e-04, 1.5218e-03, 1.3373e-03, 2.0435e-03,\n                      1.2004e-03, 1.6512e-03, 1.8963e-03, 1.1791e-03, 1.2247e-03, 1.3524e-03,\n                      1.7923e-03, 2.2185e-03, 1.2781e-03, 1.5148e-03, 1.7077e-03, 1.4405e-03,\n                      2.0542e-03, 1.8460e-03, 1.2926e-03, 1.5739e-03, 8.6668e-04, 1.5275e-03,\n                      1.3874e-03, 1.6911e-03, 1.2883e-03, 1.5247e-03, 1.7780e-03, 1.3965e-03,\n                      1.3511e-03, 1.5504e-03, 1.5912e-03, 1.3176e-03, 1.2945e-03, 1.5336e-03,\n                      1.4791e-03, 1.1100e-03, 1.5740e-03, 1.9892e-03, 1.4591e-03, 1.6654e-03],\n                     dtype=torch.float64),\n                     zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n                     axis=0)),\n             ('features.5.8.block.0.bias',\n              Parameter containing:\n              tensor([-3.2374e-03,  1.9233e-02, -2.6212e-02, -2.0902e-05, -2.6575e-02,\n                      -9.5937e-03,  1.1996e-02,  3.4130e-02, -2.1513e-02, -2.9632e-02,\n                      -2.9651e-03,  5.6290e-02,  8.6561e-02, -1.2464e-02,  1.9423e-02,\n                       3.3874e-02, -3.1645e-02,  1.1990e-02, -1.9455e-02, -4.5998e-03,\n                      -1.8874e-03, -4.8124e-03,  3.5182e-03,  1.2267e-03,  1.3110e-02,\n                       2.8046e-02, -1.9979e-03, -2.7101e-03,  1.0745e-02,  1.7691e-01,\n                       2.1452e-02,  1.6224e-03, -1.6145e-02, -6.4621e-02, -1.5197e-02,\n                      -1.7211e-02, -1.1602e-02, -8.8764e-03, -5.0641e-02, -1.7286e-02,\n                      -2.7478e-02, -5.5277e-03, -8.4090e-03,  1.1751e-02,  2.1565e-03,\n                      -1.9819e-02, -4.0641e-02,  7.0814e-03,  1.4307e-02,  2.7803e-03,\n                      -4.5996e-03,  4.1055e-02,  1.6807e-02, -1.9973e-02,  1.3976e-02,\n                      -3.1237e-03,  5.6014e-03, -5.5493e-03, -1.2536e-02,  1.2804e-02,\n                       3.2193e-03,  7.5568e-02,  2.0206e-03,  4.2876e-03,  2.1983e-02,\n                      -9.0387e-03, -3.8704e-03, -3.1034e-02, -3.3341e-03, -4.6322e-02,\n                      -5.3490e-04, -9.4485e-03,  2.2698e-02,  2.1047e-03, -3.6154e-04,\n                      -2.2096e-02, -1.8977e-02,  1.3416e-02,  2.6846e-03, -9.2356e-03,\n                       1.5334e-02, -2.1883e-03, -1.1782e-02, -2.1833e-02,  4.3969e-02,\n                      -1.5066e-03,  3.1876e-02,  1.4718e-02, -1.1153e-02,  5.0954e-03,\n                       8.7228e-03, -3.6927e-03,  4.7578e-02,  1.0810e-02, -2.0870e-02,\n                       9.6036e-03,  3.7677e-03,  1.5416e-02,  2.0849e-02,  1.0394e-02,\n                      -2.2685e-04,  6.6878e-03,  4.6892e-02, -1.0692e-02,  3.7891e-03,\n                      -3.0370e-02, -1.4812e-02,  1.0035e-02,  1.0365e-02,  2.7934e-02,\n                       2.0435e-02,  2.7576e-02, -5.3236e-02, -2.0612e-03, -2.1392e-02,\n                       2.1780e-03, -1.2259e-02,  1.0965e-01, -2.4753e-03, -2.4928e-03,\n                      -7.8727e-03,  5.7447e-03,  7.7938e-03,  9.6235e-02,  6.5535e-02,\n                      -2.7787e-02,  3.3048e-03, -9.2448e-03,  3.8018e-03,  1.7980e-02,\n                      -7.5793e-03,  2.6094e-02, -4.8968e-02,  2.5180e-02,  1.6940e-02,\n                      -5.0350e-02, -7.4147e-04,  1.4516e-02,  1.6871e-02,  3.5754e-03,\n                      -9.3435e-03, -1.4648e-02, -1.0835e-02,  2.4423e-02,  1.1137e-02,\n                      -3.7895e-03,  7.6971e-02,  2.1848e-02,  3.0388e-02, -7.8936e-04,\n                      -1.2551e-01,  1.3913e-02, -1.6332e-02,  3.4554e-02,  9.7852e-03,\n                       2.1671e-02, -4.5165e-02,  1.5254e-03,  3.1130e-03,  1.4793e-02,\n                       3.0696e-03, -7.9775e-03,  7.5806e-03, -4.2532e-02,  1.9292e-02,\n                       4.5934e-02,  1.5447e-02,  2.9726e-02, -5.6744e-03,  2.3907e-02,\n                       7.8394e-04,  9.5240e-03, -4.6876e-03,  1.4776e-02, -2.2833e-02,\n                      -2.8428e-02, -3.2257e-03,  6.2704e-03, -1.8998e-02, -2.7179e-03,\n                       3.1367e-02, -6.6544e-02,  2.1899e-02,  9.9738e-03,  3.5085e-03,\n                       1.5990e-02, -9.2581e-04,  1.0080e-02, -2.5619e-02,  3.0247e-03,\n                       2.8319e-02,  1.0806e-02, -3.7103e-03, -5.3351e-03, -1.5069e-02,\n                       2.8812e-02,  7.7660e-03,  3.4229e-02,  1.2852e-02, -1.6099e-02,\n                      -1.2534e-02,  8.0205e-04,  8.3235e-03, -3.2097e-02,  1.7145e-03,\n                      -2.2530e-02, -2.7622e-03, -1.1224e-03, -8.5831e-03, -5.0136e-03,\n                       5.5651e-04, -4.0541e-02, -8.6266e-03,  5.5708e-03, -2.0355e-02,\n                       4.0662e-02,  1.4617e-02,  2.1177e-02, -2.8890e-03,  9.5464e-03,\n                      -1.2214e-02,  1.5724e-03, -7.6375e-04,  1.0226e-02,  4.3985e-02,\n                       2.6463e-02, -5.0620e-03,  7.6812e-03, -2.1780e-02,  2.7294e-02,\n                      -1.6557e-02,  1.7796e-02, -1.3479e-02, -9.0239e-03,  2.5813e-02,\n                      -7.2801e-03,  5.1391e-03,  5.3549e-02, -1.2950e-02,  1.8311e-02,\n                       1.8729e-02, -1.0237e-02, -1.2632e-03,  1.0416e-01, -6.9408e-02,\n                      -2.8993e-03,  6.4893e-03, -5.2299e-03, -4.8067e-02,  2.8346e-03,\n                      -1.2872e-02, -7.8620e-03, -3.2171e-04, -9.5350e-03, -1.5209e-02,\n                       2.0720e-03, -1.3267e-02, -1.2550e-02,  2.2119e-02, -8.4249e-03,\n                       2.2239e-02, -1.2222e-02, -5.7513e-03, -1.6049e-02, -5.2394e-02,\n                       2.9300e-03, -1.7799e-02, -3.5513e-04,  7.4786e-03,  3.2841e-03,\n                      -7.9826e-03,  5.6592e-03, -2.8140e-02,  2.0218e-02, -3.2118e-02,\n                       3.5900e-03, -1.2202e-02,  2.4246e-02,  3.6556e-03, -8.1011e-03,\n                      -2.1850e-02,  3.8251e-03, -1.8069e-02,  3.5679e-03,  4.3855e-03,\n                      -1.7952e-02, -5.2247e-04,  1.8814e-02,  1.8948e-02, -5.2517e-03,\n                      -3.1410e-03, -3.3928e-02,  5.1454e-03, -2.8861e-02, -2.0430e-02,\n                      -3.4556e-03, -1.5619e-02,  1.0877e-03, -1.1693e-03,  1.5452e-03,\n                      -1.8932e-02, -2.6234e-02,  3.7589e-02,  1.7977e-03, -1.4464e-02,\n                       8.2030e-03, -7.2174e-03, -2.1764e-02,  7.9276e-03, -5.1861e-03,\n                       1.4052e-02,  3.6145e-02,  6.6198e-03,  2.9374e-03, -4.6328e-02,\n                      -2.6004e-02,  1.1123e-02,  8.9379e-03, -4.3797e-02,  2.2151e-03,\n                       1.1596e-02,  8.2728e-03, -7.3477e-03, -3.9453e-02, -1.6820e-02,\n                       4.4984e-03,  4.6962e-03,  3.2575e-02,  7.5809e-04,  1.4762e-03,\n                      -7.2577e-05, -1.3072e-02, -6.8026e-04,  2.6655e-03, -1.4720e-02,\n                      -1.3813e-02, -1.2630e-02,  5.1652e-03, -1.4829e-02,  1.2189e-02,\n                       3.4477e-03,  7.8336e-03,  2.9137e-03, -1.7673e-03,  3.0803e-02,\n                       1.7534e-02,  1.4281e-02, -5.8661e-03, -1.7310e-02,  1.3706e-04,\n                      -1.4014e-02, -4.2509e-02,  6.6104e-03,  2.5411e-02,  8.7687e-03,\n                      -3.7363e-02, -3.9034e-02,  5.9183e-04,  3.6284e-02, -1.2303e-02,\n                       5.6018e-03,  1.0115e-02, -4.9859e-02,  1.5975e-02, -5.1454e-04,\n                       2.2835e-02,  1.4533e-02,  5.8076e-03,  3.0894e-03, -1.4176e-02,\n                      -2.5286e-02,  4.0485e-02,  3.0530e-02, -7.1647e-03,  3.5918e-03,\n                      -5.9081e-03, -8.4907e-02,  7.6158e-03, -4.2533e-02,  7.2797e-03,\n                       1.0445e-02, -9.9022e-03, -9.1705e-03,  3.9210e-02],\n                     requires_grad=True)),\n             ('features.5.8.block.0.scale', tensor(0.0140)),\n             ('features.5.8.block.0.zero_point', tensor(58)),\n             ('features.5.8.block.2.weight',\n              tensor([2.0200, 3.5352, 1.4311, 1.9732, 1.2642, 0.8781, 2.5876, 0.8011, 1.0234,\n                      2.0541, 2.4306, 0.6135, 2.2357, 0.9970, 1.3040, 1.4469, 1.5116, 1.3271,\n                      0.7416, 1.5702, 1.1016, 1.5446, 1.3981, 1.3633, 2.1755, 3.8194, 2.3608,\n                      2.7458, 0.8064, 0.4694, 1.1740, 2.1712, 1.3651, 0.7981, 0.8998, 0.6056,\n                      2.0291, 0.8295, 0.6125, 0.8769, 1.4002, 1.9307, 1.7980, 1.1125, 0.5122,\n                      0.8687, 0.8243, 1.1523, 1.7510, 1.4785, 1.1998, 0.7431, 1.6172, 0.5349,\n                      1.5712, 3.2975, 0.6099, 1.4707, 2.2311, 1.3497, 2.7666, 0.5422, 2.8669,\n                      1.2715, 1.1233, 1.0804, 2.4807, 1.1513, 0.8519, 2.6640, 0.7079, 1.8170,\n                      1.0089, 2.9806, 1.5409, 0.9297, 1.1816, 0.9649, 1.3031, 1.1979, 1.2159,\n                      1.5449, 1.9928, 0.2519, 0.6461, 1.7221, 1.2567, 1.3618, 2.0069, 1.2839,\n                      1.3094, 0.9691, 1.1526, 1.7461, 1.3891, 1.5354, 1.5826, 0.8179, 1.1630,\n                      1.0009, 1.1235, 0.6188, 0.9997, 1.6277, 2.3606, 1.9464, 1.5142, 0.5744,\n                      1.5924, 1.0194, 0.6317, 0.5243, 0.7944, 2.1117, 1.7239, 1.1548, 1.2166,\n                      0.4816, 2.3852, 1.2215, 1.5722, 1.7204, 1.4955, 0.5422, 0.5485, 1.6716,\n                      3.1725, 0.9743, 1.2459, 1.0105, 0.5705, 1.2921, 0.4236, 0.7750, 1.9905,\n                      1.5988, 1.6396, 2.5382, 0.9994, 2.0402, 1.2244, 0.8635, 1.3405, 2.2425,\n                      2.3849, 1.2399, 0.5791, 1.6172, 1.0014, 1.2789, 0.5020, 1.6013, 1.1987,\n                      0.8248, 0.9060, 1.0936, 0.0728, 1.6683, 1.8393, 1.8087, 1.3177, 1.5587,\n                      0.8952, 0.4017, 1.2366, 1.3773, 1.3573, 1.1839, 0.8478, 1.4011, 1.3950,\n                      0.9157, 1.3110, 0.9723, 0.5956, 2.6563, 1.3672, 2.3586, 0.7034, 0.6330,\n                      3.8906, 0.4960, 0.6187, 1.0759, 1.3333, 0.8995, 2.7371, 0.6980, 1.0222,\n                      0.9355, 0.3651, 0.9106, 1.6659, 3.0351, 1.1917, 0.9147, 2.0771, 3.6978,\n                      0.9070, 0.9395, 1.1737, 1.4099, 1.6507, 1.3418, 1.5543, 1.0201, 1.6887,\n                      1.8305, 1.5670, 0.9532, 2.6083, 0.9130, 1.2943, 1.3248, 1.9090, 0.7341,\n                      2.0166, 0.8458, 0.8528, 0.3229, 2.5884, 3.1623, 1.1705, 2.0333, 0.6382,\n                      1.0964, 1.6566, 1.8579, 0.7995, 1.2903, 0.6759, 1.5294, 1.3322, 1.6917,\n                      0.6940, 2.2491, 2.3621, 0.4590, 0.5003, 1.4294, 0.9917, 1.3551, 1.6651,\n                      0.6279, 0.5220, 1.6375, 1.2630, 1.5237, 0.8838, 2.2044, 2.3656, 1.3269,\n                      2.5328, 1.6340, 0.4823, 0.8775, 1.4940, 1.1317, 1.1328, 1.3813, 1.1410,\n                      1.0312, 1.3290, 1.1322, 1.1617, 1.3064, 0.9024, 1.7732, 1.2044, 1.5282,\n                      1.6209, 2.2951, 0.3894, 1.2179, 2.3919, 0.9788, 1.4254, 1.3595, 1.5334,\n                      1.1277, 2.3282, 1.1489, 1.1083, 2.1080, 1.8690, 1.2686, 2.5919, 1.5070,\n                      1.1792, 1.4676, 2.0574, 0.4658, 1.1976, 0.9639, 1.0777, 1.5734, 1.1460,\n                      3.1162, 2.1509, 2.0399, 1.1337, 1.5061, 0.5942, 0.3931, 2.9920, 1.9119,\n                      1.5692, 1.7083, 2.0832, 1.9334, 1.1834, 1.4190, 1.1556, 1.1504, 4.0747,\n                      1.3545, 1.3085, 1.9137, 0.8281, 2.9743, 0.9127, 1.5013, 1.4847, 0.5048,\n                      1.1983, 0.4899, 1.4934, 1.4102, 1.1425, 1.3297, 1.9757, 0.7974, 1.0973,\n                      0.9642, 1.3109, 1.5585, 0.8570, 2.7931, 3.9408, 1.2724, 2.7217, 0.9727,\n                      2.2886, 1.1346, 1.1173, 1.0190, 1.3756, 2.8092, 1.0260, 1.6329, 2.3728,\n                      0.7680, 1.1464, 3.1593, 1.6794, 2.9594, 0.8762, 1.4048, 0.5403, 1.1821,\n                      3.6015, 1.6462, 0.7023, 1.3638, 0.8445, 1.0605, 1.0576, 1.6931, 1.2787,\n                      1.9386, 1.9922, 0.6842, 1.3329, 0.8614, 1.4122, 2.1075, 1.6740, 1.6697,\n                      1.3087, 1.4223, 1.3342, 2.0628, 1.3996, 1.1265])),\n             ('features.5.8.block.2.bias',\n              tensor([-6.3931e-01, -2.2888e-01,  2.5753e-01, -3.0628e-02,  1.3848e-01,\n                      -9.4181e-02, -3.7801e-02, -3.8372e-01,  2.1355e-01,  5.4341e-01,\n                       5.3299e-01, -7.0173e-01,  1.8592e-02,  1.5544e-01, -3.0882e-01,\n                      -4.6287e-01,  5.8102e-01,  3.0871e-02,  6.7200e-02,  1.1885e-01,\n                      -1.5062e-02, -4.3556e-03, -4.7305e-01, -4.1454e-03, -1.1013e-01,\n                       5.0307e-01,  1.4311e-01,  1.9637e-01, -1.8228e-01, -1.3795e+00,\n                      -5.8629e-01, -2.3482e-01,  1.8430e-01,  1.0022e+00,  1.6191e-01,\n                      -2.0150e-01,  4.1874e-01, -3.1785e-02,  5.4399e-01,  4.6025e-01,\n                       1.7714e-01, -1.1911e-01, -1.0852e-03, -7.5182e-02, -2.3361e-01,\n                       2.6538e-01,  5.3840e-01, -2.1324e-01, -2.4674e-01, -5.5406e-03,\n                       4.0552e-02, -6.0641e-01, -3.5436e-01,  4.3589e-02, -3.2262e-01,\n                       1.1042e-01,  5.7321e-02,  1.0553e-01,  1.2032e-01, -1.3120e-01,\n                      -7.8563e-02, -6.9839e-01, -1.5553e-01, -2.1432e-01, -4.7798e-02,\n                      -1.3656e-01, -6.6920e-02,  4.9119e-01,  1.6185e-03, -1.4955e+00,\n                      -5.7182e-02,  8.7154e-02, -1.5909e-01,  2.4212e-01,  8.1946e-02,\n                      -7.3069e-02,  2.3904e-01, -2.0898e-01, -2.9958e-03,  5.5072e-02,\n                      -2.7581e-01, -4.7088e-02, -9.0903e-02, -2.2634e-01, -4.8277e-01,\n                       7.2460e-03, -2.7217e-01, -2.6166e-01,  1.5977e-01, -1.2828e-01,\n                      -3.2300e-02,  5.0548e-01, -5.2041e-01, -3.5753e-02,  2.5369e-01,\n                      -1.6819e-01,  1.0697e-01, -2.2292e-01, -3.0367e-01, -1.9633e-01,\n                      -3.5502e-02, -1.4656e-01, -8.3512e-01,  1.1424e-01,  1.3388e-01,\n                       4.3631e-01,  2.1055e-01, -3.0961e-02, -1.5221e-01, -3.5823e-01,\n                      -4.3430e-01, -5.3915e-01,  4.5049e-01, -6.7855e-02,  3.5187e-01,\n                      -2.6487e-02, -8.6546e-02, -1.7771e+00, -4.5138e-01,  7.9486e-02,\n                       5.6251e-02, -1.3856e-01, -2.2029e-01, -5.0536e-01, -3.3564e+00,\n                      -4.0715e+00, -3.2723e-01,  1.0103e-01,  5.9130e-02, -2.6557e-01,\n                       1.1067e-01, -4.2144e-01,  2.4838e-01, -2.8370e-01, -5.9341e-01,\n                       8.5347e-01, -1.8971e-01,  4.3320e-01, -3.3431e-01, -2.9608e-01,\n                       1.1523e-01,  1.0824e-03,  2.5932e-01, -3.8261e-01, -1.6372e-01,\n                      -5.7151e-02, -1.8925e+00, -6.9922e-01, -1.8079e-01, -2.6335e-02,\n                       1.1613e+00, -3.9708e-01,  1.6740e-01, -3.7785e-01,  3.7891e-03,\n                      -2.8287e-01, -1.1640e-01,  1.5092e-01,  4.9550e-03, -2.6086e-01,\n                      -7.7636e-02,  7.6450e-02, -2.0211e-01, -3.6503e-03, -1.3719e-01,\n                      -5.1391e-01, -2.4449e-01, -4.2235e-01, -1.9547e-01, -2.6565e-01,\n                      -1.3754e-01, -3.6954e-01,  2.6094e-01, -4.2109e-02,  9.4401e-02,\n                      -3.9052e-01,  1.3691e-02, -1.5370e-01,  1.7539e-02,  9.1458e-02,\n                       4.3865e-01,  1.3409e+00, -3.9006e-01, -6.8658e-03, -1.9187e-01,\n                      -2.2019e-01,  1.0098e-01, -4.2547e-01,  3.7476e-02, -8.5968e-02,\n                      -2.6100e-01,  4.4051e-01, -2.9110e-02,  1.0374e-01,  1.4823e-01,\n                      -2.6133e-01, -5.6423e-01,  8.4073e-01, -1.8215e-01,  6.3592e-02,\n                       1.0874e-01,  1.9699e-01, -6.4766e-02,  3.3438e-01,  6.8128e-03,\n                       1.0450e-02, -7.4433e-02, -1.1871e-01, -8.0353e-02,  3.3039e-01,\n                       3.4145e-01, -2.2310e-02,  1.5597e-01, -1.4673e-01,  1.8817e-01,\n                      -3.2945e-01, -1.8107e-01, -2.4046e-01, -1.4487e-01, -3.9663e-01,\n                      -1.5963e-01, -3.2558e-01, -9.1753e-02, -2.6179e-01, -3.0560e-01,\n                      -2.0104e-01,  2.1340e-02, -8.6671e-02,  2.0602e-01, -3.2329e-01,\n                      -8.4734e-02, -2.8714e-01,  1.8134e-01,  9.1367e-02, -1.8726e-01,\n                      -8.1056e-03, -3.1796e-01, -3.9392e-01, -1.0580e+00, -1.5166e-01,\n                      -2.1962e-01,  3.2156e-02,  2.5935e-02, -1.4220e+00,  3.4772e-01,\n                      -5.3491e-02,  3.0080e-02,  3.6176e-02,  6.6151e-01, -2.7086e-01,\n                       3.7214e-01,  7.7878e-02,  3.7059e-02,  1.2011e-01,  3.3738e-01,\n                      -1.5332e-01,  1.8551e-01,  1.1354e-01, -3.7521e-01,  1.0517e-01,\n                      -2.3360e-01, -9.7512e-02, -3.0160e-02,  1.1626e-01,  9.6342e-01,\n                      -2.8689e-01, -5.9499e-02, -3.8567e-01,  1.8824e-02, -8.1854e-03,\n                       9.4869e-02, -1.3247e-01, -1.4369e-01, -3.2952e-01, -2.4123e+00,\n                      -8.6991e-02,  7.0110e-02, -4.9845e-01, -1.1442e-01, -6.8386e-03,\n                      -4.4567e-01, -1.4035e-01,  1.9943e-01, -1.2816e-01, -1.5838e-02,\n                       1.3255e-02,  9.2906e-03, -1.9779e-01, -2.3913e-01,  3.9773e-02,\n                       7.1601e-01,  1.5300e-01,  5.4645e-02,  2.8347e-01,  1.1107e-01,\n                      -1.2864e-02,  1.0154e-01, -3.5482e-01,  7.0814e-02, -1.0764e-02,\n                      -4.9145e-03,  2.0248e-01, -9.5785e-02, -7.1212e-01,  4.1688e-01,\n                      -2.4846e-01,  6.7404e-02,  5.1851e-01, -2.3687e-01,  1.6398e-01,\n                      -1.9738e-01, -5.2958e-01,  4.3526e-03, -1.1533e-01, -6.0050e-01,\n                       1.9698e-01, -1.7496e-01, -2.2753e-01,  4.0877e-01, -4.4607e-01,\n                      -1.6862e-01, -1.2436e-01,  1.0329e-01, -1.0639e-01,  1.5608e-02,\n                      -9.9929e-01, -1.1345e-01, -6.3385e-01, -9.0417e-03, -7.5188e-02,\n                       4.4055e-01,  1.8015e-01, -2.2613e-01, -1.1912e-01, -5.5345e-02,\n                      -5.3654e-02,  4.7146e-02, -3.1484e-01, -1.8974e-01, -1.5171e-01,\n                      -1.9033e-01, -1.0970e-01,  1.4546e-01,  2.8559e-02,  2.8897e-01,\n                      -1.3765e-01, -1.8882e-02,  4.4036e-02,  2.9942e+00, -1.6945e-01,\n                       4.5897e-01,  2.7162e-01,  2.9995e-02, -7.0255e-01, -5.4650e-02,\n                      -6.4748e-01,  1.8847e-01,  1.0813e-02, -1.1754e+00,  1.1270e-01,\n                       6.5064e-02, -2.4420e-01,  4.4497e-01, -3.0934e-01, -8.7354e-01,\n                      -1.5994e-01, -9.5835e-02,  3.7366e-02, -5.0383e-02,  6.9043e-01,\n                       4.8226e-01, -6.6634e-01, -4.5397e-01,  1.7966e-02, -1.7271e-01,\n                       2.0727e-01,  1.0339e+00, -3.8007e-02,  3.6252e-01, -3.1039e-01,\n                      -4.2815e-02,  9.3016e-02,  6.2349e-02, -5.1415e-01])),\n             ('features.5.8.block.2.scale', tensor(0.1959)),\n             ('features.5.8.block.2.zero_point', tensor(72)),\n             ('features.5.8.block.3.scale', tensor(0.0918)),\n             ('features.5.8.block.3.zero_point', tensor(83)),\n             ('features.5.8.block.3._packed_params.dtype', torch.qint8),\n             ('features.5.8.block.3._packed_params._packed_params',\n              (tensor([[ 0.0609, -0.1795, -0.0837,  ...,  0.0304, -0.0335,  0.0137],\n                       [ 0.0058,  0.0174,  0.0087,  ...,  0.0218, -0.0276, -0.0131],\n                       [ 0.0556,  0.0232,  0.0355,  ..., -0.0618, -0.0556, -0.0463],\n                       ...,\n                       [ 0.0399, -0.0293, -0.0120,  ...,  0.0186,  0.0519,  0.0505],\n                       [ 0.0447, -0.0366,  0.0264,  ...,  0.0102,  0.0305,  0.0264],\n                       [ 0.0064, -0.0579,  0.0016,  ...,  0.0772,  0.0064, -0.0531]],\n                      size=(1536, 384), dtype=torch.qint8,\n                      quantization_scheme=torch.per_channel_affine,\n                      scale=tensor([0.0015, 0.0015, 0.0015,  ..., 0.0013, 0.0020, 0.0016],\n                      dtype=torch.float64),\n                      zero_point=tensor([0, 0, 0,  ..., 0, 0, 0]), axis=0),\n               Parameter containing:\n               tensor([-0.0481, -0.0688, -0.0476,  ..., -0.0347, -0.0137, -0.0569],\n                      requires_grad=True))),\n             ('features.5.8.block.5.scale', tensor(0.1137)),\n             ('features.5.8.block.5.zero_point', tensor(94)),\n             ('features.5.8.block.5._packed_params.dtype', torch.qint8),\n             ('features.5.8.block.5._packed_params._packed_params',\n              (tensor([[ 0.0286, -0.0135, -0.0236,  ..., -0.0219,  0.0067,  0.0168],\n                       [-0.0762,  0.0029,  0.0367,  ..., -0.0161, -0.0894,  0.0059],\n                       [-0.0315,  0.0702,  0.0530,  ...,  0.0559, -0.0315, -0.1132],\n                       ...,\n                       [-0.0167, -0.0258,  0.0562,  ..., -0.0106, -0.0030, -0.0380],\n                       [-0.0364,  0.0255, -0.0419,  ..., -0.1657,  0.0346,  0.1566],\n                       [ 0.0462,  0.0781,  0.0107,  ..., -0.0195,  0.0391, -0.1119]],\n                      size=(384, 1536), dtype=torch.qint8,\n                      quantization_scheme=torch.per_channel_affine,\n                      scale=tensor([0.0017, 0.0015, 0.0014, 0.0016, 0.0018, 0.0015, 0.0014, 0.0017, 0.0018,\n                       0.0015, 0.0019, 0.0015, 0.0026, 0.0030, 0.0017, 0.0017, 0.0021, 0.0015,\n                       0.0016, 0.0020, 0.0015, 0.0016, 0.0018, 0.0018, 0.0015, 0.0025, 0.0014,\n                       0.0014, 0.0017, 0.0015, 0.0021, 0.0015, 0.0017, 0.0016, 0.0015, 0.0017,\n                       0.0017, 0.0016, 0.0015, 0.0016, 0.0017, 0.0017, 0.0019, 0.0016, 0.0016,\n                       0.0019, 0.0025, 0.0014, 0.0016, 0.0015, 0.0017, 0.0023, 0.0015, 0.0018,\n                       0.0016, 0.0009, 0.0016, 0.0019, 0.0017, 0.0015, 0.0015, 0.0018, 0.0017,\n                       0.0017, 0.0016, 0.0018, 0.0017, 0.0017, 0.0016, 0.0015, 0.0017, 0.0015,\n                       0.0019, 0.0025, 0.0014, 0.0016, 0.0016, 0.0013, 0.0020, 0.0015, 0.0017,\n                       0.0017, 0.0014, 0.0019, 0.0016, 0.0016, 0.0021, 0.0016, 0.0014, 0.0017,\n                       0.0017, 0.0019, 0.0018, 0.0014, 0.0017, 0.0018, 0.0016, 0.0021, 0.0015,\n                       0.0018, 0.0015, 0.0014, 0.0026, 0.0016, 0.0016, 0.0015, 0.0017, 0.0022,\n                       0.0017, 0.0037, 0.0016, 0.0019, 0.0025, 0.0016, 0.0018, 0.0015, 0.0015,\n                       0.0028, 0.0015, 0.0016, 0.0018, 0.0018, 0.0015, 0.0018, 0.0023, 0.0018,\n                       0.0018, 0.0016, 0.0015, 0.0020, 0.0010, 0.0019, 0.0017, 0.0018, 0.0013,\n                       0.0016, 0.0015, 0.0016, 0.0016, 0.0015, 0.0020, 0.0017, 0.0016, 0.0018,\n                       0.0016, 0.0016, 0.0026, 0.0019, 0.0017, 0.0018, 0.0029, 0.0015, 0.0018,\n                       0.0018, 0.0016, 0.0018, 0.0099, 0.0022, 0.0018, 0.0016, 0.0019, 0.0014,\n                       0.0016, 0.0018, 0.0015, 0.0016, 0.0020, 0.0016, 0.0015, 0.0014, 0.0016,\n                       0.0016, 0.0016, 0.0013, 0.0015, 0.0019, 0.0018, 0.0016, 0.0015, 0.0016,\n                       0.0014, 0.0035, 0.0016, 0.0015, 0.0014, 0.0018, 0.0027, 0.0015, 0.0017,\n                       0.0035, 0.0016, 0.0016, 0.0016, 0.0014, 0.0018, 0.0015, 0.0019, 0.0018,\n                       0.0016, 0.0017, 0.0020, 0.0017, 0.0014, 0.0016, 0.0019, 0.0020, 0.0018,\n                       0.0015, 0.0021, 0.0022, 0.0017, 0.0026, 0.0013, 0.0019, 0.0015, 0.0022,\n                       0.0015, 0.0012, 0.0014, 0.0016, 0.0016, 0.0017, 0.0019, 0.0018, 0.0015,\n                       0.0018, 0.0017, 0.0016, 0.0017, 0.0017, 0.0020, 0.0015, 0.0016, 0.0016,\n                       0.0016, 0.0014, 0.0016, 0.0019, 0.0020, 0.0018, 0.0022, 0.0015, 0.0016,\n                       0.0021, 0.0016, 0.0019, 0.0017, 0.0016, 0.0021, 0.0016, 0.0025, 0.0023,\n                       0.0015, 0.0015, 0.0017, 0.0015, 0.0018, 0.0018, 0.0016, 0.0015, 0.0017,\n                       0.0016, 0.0017, 0.0017, 0.0033, 0.0023, 0.0018, 0.0016, 0.0016, 0.0016,\n                       0.0015, 0.0019, 0.0021, 0.0016, 0.0026, 0.0016, 0.0014, 0.0013, 0.0016,\n                       0.0014, 0.0015, 0.0018, 0.0016, 0.0015, 0.0016, 0.0015, 0.0016, 0.0013,\n                       0.0020, 0.0017, 0.0016, 0.0018, 0.0015, 0.0018, 0.0016, 0.0017, 0.0017,\n                       0.0019, 0.0014, 0.0013, 0.0015, 0.0016, 0.0019, 0.0017, 0.0015, 0.0015,\n                       0.0014, 0.0016, 0.0017, 0.0019, 0.0017, 0.0017, 0.0015, 0.0015, 0.0011,\n                       0.0016, 0.0018, 0.0016, 0.0024, 0.0017, 0.0018, 0.0016, 0.0019, 0.0019,\n                       0.0016, 0.0020, 0.0019, 0.0017, 0.0014, 0.0017, 0.0019, 0.0018, 0.0014,\n                       0.0020, 0.0017, 0.0015, 0.0015, 0.0020, 0.0010, 0.0020, 0.0018, 0.0018,\n                       0.0014, 0.0019, 0.0032, 0.0016, 0.0015, 0.0015, 0.0026, 0.0016, 0.0018,\n                       0.0015, 0.0016, 0.0026, 0.0016, 0.0010, 0.0017, 0.0015, 0.0021, 0.0014,\n                       0.0017, 0.0017, 0.0016, 0.0015, 0.0021, 0.0015, 0.0018, 0.0015, 0.0018,\n                       0.0019, 0.0015, 0.0016, 0.0017, 0.0015, 0.0016, 0.0018, 0.0014, 0.0017,\n                       0.0015, 0.0015, 0.0016, 0.0015, 0.0018, 0.0018], dtype=torch.float64),\n                      zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n                      axis=0),\n               Parameter containing:\n               tensor([-8.2659e-03,  4.3520e-02, -1.1601e-02, -3.2611e-03, -8.0404e-03,\n                       -3.3936e-03, -2.7132e-02, -1.0068e-02,  1.2436e-02, -3.2579e-02,\n                        1.8720e-04, -1.0123e-02, -6.7243e-02, -4.3424e-02,  6.8550e-03,\n                        1.3100e-03,  2.5069e-02,  3.6357e-03, -1.5980e-02, -1.4847e-02,\n                        9.3781e-03, -4.1373e-03, -7.1743e-03,  1.2037e-02,  4.2456e-02,\n                        3.2770e-04,  9.6083e-04,  2.6731e-02, -8.6121e-03, -4.2210e-02,\n                       -1.5114e-02,  8.9104e-03, -1.7038e-03, -8.9305e-03,  9.4938e-03,\n                        5.3971e-03, -5.8417e-03, -1.3573e-02,  1.3099e-02,  1.2619e-03,\n                       -1.0700e-03,  2.6111e-02,  3.0571e-02,  1.9557e-02,  3.9926e-03,\n                       -1.7745e-03, -1.3582e-01,  2.5098e-02, -9.9317e-03, -3.5916e-03,\n                        1.5454e-02, -9.5033e-04,  1.1945e-02,  8.6678e-04, -2.9077e-03,\n                       -3.6411e-03, -4.2919e-02, -4.4138e-03, -5.1414e-03,  3.6640e-03,\n                        7.8067e-03, -1.5103e-02, -1.3596e-02, -1.3917e-02,  1.8940e-02,\n                       -2.1602e-02, -3.5944e-03,  9.4279e-03,  8.3847e-03,  6.1567e-03,\n                       -5.2058e-03,  2.8645e-03, -1.2743e-02, -8.6723e-03, -1.8481e-02,\n                        1.7769e-03,  2.8796e-02, -4.6238e-02, -1.8418e-02, -1.3726e-02,\n                        1.3182e-02,  2.0166e-02, -2.2359e-02, -1.4792e-03, -9.1099e-03,\n                       -4.2592e-02, -9.2188e-03,  8.7245e-03, -3.1456e-03, -8.5155e-03,\n                       -8.8323e-03, -1.8784e-02,  1.8352e-02,  5.2896e-03, -3.8563e-03,\n                        2.0156e-02, -1.5424e-03,  2.1488e-03,  1.0361e-02, -3.0311e-02,\n                        3.3598e-03, -5.7166e-03,  1.5769e-03,  3.1164e-02, -1.1266e-04,\n                        2.4935e-02, -9.9330e-03,  2.1297e-02, -1.3483e-02,  6.6712e-02,\n                       -1.5742e-03,  2.3095e-02,  1.2746e-02,  2.5360e-03, -3.1044e-02,\n                        1.1418e-02, -5.9051e-03,  6.4111e-03, -6.4856e-03, -2.3476e-02,\n                        2.1362e-02, -1.3662e-02, -1.2380e-02,  1.6197e-02, -1.0395e-01,\n                        1.4471e-02,  1.7573e-02,  2.0087e-02,  7.1710e-03, -1.3975e-01,\n                       -6.1553e-03,  2.7949e-02, -1.6228e-02,  1.1429e-02, -8.7984e-03,\n                       -7.0509e-02,  1.3819e-02,  4.3094e-02,  1.5179e-02, -7.8717e-04,\n                       -3.8183e-03,  2.9027e-02, -1.3239e-02, -1.1419e-03,  5.7736e-03,\n                       -5.1635e-03, -9.0177e-03, -4.9010e-03, -1.4489e-02, -3.4591e-02,\n                        1.7162e-02, -1.1464e-02, -1.3954e-03, -1.1555e-02,  2.7581e-04,\n                        1.1509e-02, -1.5269e+00, -3.2225e-02, -2.8501e-02, -2.9773e-04,\n                        1.1614e-03,  1.0657e-02,  8.1716e-03, -3.7266e-03, -1.4055e-02,\n                       -3.6249e-03,  3.7181e-03, -5.0342e-05, -4.0707e-03, -5.3150e-03,\n                       -8.7790e-03,  1.1598e-03, -1.3497e-02, -1.8322e-02, -1.1454e-03,\n                        1.2407e-02,  3.0276e-03,  5.9593e-03,  1.1459e-02,  1.7615e-02,\n                       -2.1074e-02,  2.4351e-02, -6.9049e-03,  3.5332e-02, -9.1099e-03,\n                       -5.3693e-06,  2.0670e-02,  1.7200e-02, -6.6048e-03, -5.3186e-02,\n                       -9.4126e-03, -4.6738e-02,  9.2648e-03,  1.7398e-02, -6.9026e-03,\n                        1.2650e-01,  1.5805e-02, -1.2577e-02,  7.9689e-03,  7.4612e-03,\n                       -2.0290e-02, -2.8637e-03,  7.4824e-03,  5.3928e-03,  2.2445e-03,\n                        3.0191e-02,  2.8081e-03,  1.1792e-02,  2.3814e-02, -1.6333e-02,\n                       -1.5311e-02, -7.5716e-02,  1.0292e-02,  7.9457e-03,  3.0732e-02,\n                       -6.1261e-02,  1.2282e-02,  9.3853e-02,  2.6469e-02,  1.2900e-02,\n                        6.5169e-04, -9.2607e-03, -2.7528e-03,  1.7690e-02, -4.4696e-03,\n                        1.4871e-02,  1.2269e-03,  6.8460e-04, -6.8781e-03, -1.2926e-02,\n                       -8.0204e-03, -9.2091e-03, -7.8706e-03,  2.4010e-02, -3.5377e-03,\n                        2.2559e-02,  1.2370e-04, -1.9146e-02, -1.8542e-02,  2.5715e-03,\n                       -1.1799e-01, -7.9724e-03, -5.7523e-03, -8.8488e-03,  3.7022e-02,\n                        1.0439e-02, -2.0239e-02,  2.5158e-02, -2.9691e-02, -1.1509e-02,\n                        4.5443e-04, -7.7150e-04, -1.8390e-03, -5.4345e-03, -1.5533e-02,\n                        7.0570e-03,  2.3240e-02,  1.8290e-03,  2.0004e-03,  8.3563e-03,\n                        2.0810e-02, -1.5189e-02, -5.2579e-03,  1.6006e-02, -3.3089e-03,\n                        4.9382e-03,  1.9321e-03, -6.7504e-03, -5.7886e-03, -2.2481e-02,\n                        1.2992e-05,  1.7373e-02,  5.9151e-03, -1.3128e-02,  3.8654e-02,\n                       -3.1107e-02,  2.2600e-02, -2.0809e-02,  3.7873e-02,  7.9565e-03,\n                       -3.9156e-02, -2.1442e-02,  1.6093e-03,  1.5699e-02, -3.2518e-03,\n                        2.0112e-02, -2.7563e-03,  1.3797e-02,  2.8195e-02, -4.5942e-03,\n                        3.1648e-02, -3.8109e-03, -1.4374e-02, -1.2072e-02, -3.3961e-03,\n                       -5.4598e-03, -1.2936e-02,  2.1595e-02,  1.4851e-03, -9.5024e-03,\n                        1.5235e-02, -1.1908e-02,  8.1111e-03, -3.3462e-02, -1.0241e-02,\n                        9.8348e-03,  1.2503e-02, -7.4523e-03,  1.4614e-02, -3.6935e-04,\n                       -1.3507e-03, -2.0455e-02, -5.2988e-03,  4.8239e-03, -7.8771e-04,\n                       -7.3491e-03,  5.4123e-03,  1.2191e-02, -1.4187e-01,  2.7388e-02,\n                       -1.3102e-02, -2.8807e-02, -3.2282e-03, -4.9863e-03,  1.0869e-02,\n                        6.7832e-03, -2.9800e-02,  1.9263e-02, -3.2158e-03, -1.0351e-02,\n                        2.3904e-02,  1.0352e-03, -1.6531e-02,  6.2764e-03,  3.3395e-03,\n                       -7.8183e-03, -1.4074e-02, -1.2232e-02,  9.9230e-03, -1.1000e-02,\n                       -1.0082e-03, -1.2158e-02,  6.0811e-03,  3.6500e-03,  5.9023e-02,\n                        4.2657e-03, -6.3801e-02, -1.0633e-02, -4.5354e-02,  1.6948e-02,\n                       -3.1997e-03, -2.0742e-02, -6.6031e-03, -7.8034e-02, -9.0962e-04,\n                       -6.4910e-03,  3.5421e-03, -7.9906e-03, -5.0023e-03, -1.8956e-03,\n                        4.9650e-03, -2.1700e-02,  4.5236e-03,  1.6583e-02,  4.6233e-03,\n                       -1.5809e-03, -1.8766e-02,  8.6984e-03, -4.4226e-02, -1.5033e-02,\n                       -1.4813e-02,  1.2481e-02, -2.2201e-02, -2.5327e-02,  3.6030e-02,\n                       -5.9554e-02,  3.3508e-02, -3.7228e-02, -1.3109e-02,  1.3775e-02,\n                        4.2979e-03, -4.7754e-03, -2.3503e-03,  1.4247e-02],\n                      requires_grad=True))),\n             ('features.6.0.weight',\n              tensor([0.3561, 0.3783, 0.3303, 0.1444, 0.2561, 0.2667, 0.5425, 0.2471, 0.2532,\n                      0.3795, 0.4475, 0.2181, 0.0742, 0.1612, 0.2513, 0.3487, 0.1789, 0.2637,\n                      0.2406, 0.3192, 0.1560, 0.3295, 0.1304, 0.2675, 0.4537, 0.5548, 0.3716,\n                      0.4197, 0.2340, 0.0804, 0.1340, 0.4485, 0.2640, 0.2292, 0.2420, 0.2602,\n                      0.2567, 0.2104, 0.2193, 0.2696, 0.1960, 0.2843, 0.1362, 0.2276, 0.2300,\n                      0.1370, 0.0184, 0.2247, 0.4202, 0.2387, 0.2021, 0.1229, 0.3729, 0.1997,\n                      0.3480, 0.2249, 0.1934, 0.3583, 0.4648, 0.3099, 0.4352, 0.1673, 0.4848,\n                      0.2995, 0.2214, 0.2149, 0.5593, 0.1536, 0.2207, 0.4322, 0.2554, 0.3940,\n                      0.1105, 0.6521, 0.3224, 0.2519, 0.2359, 0.0775, 0.2422, 0.2806, 0.2694,\n                      0.2369, 0.2503, 0.1563, 0.2437, 0.1753, 0.1389, 0.3051, 0.4776, 0.2869,\n                      0.2515, 0.2103, 0.2615, 0.2510, 0.2817, 0.2421, 0.2586, 0.1825, 0.2542,\n                      0.1336, 0.3042, 0.2561, 0.1564, 0.2837, 0.1891, 0.3711, 0.4023, 0.1525,\n                      0.2792, 0.1330, 0.1907, 0.2399, 0.1743, 0.4511, 0.1472, 0.2276, 0.2821,\n                      0.1778, 0.5093, 0.2476, 0.3159, 0.3841, 0.3421, 0.1710, 0.0896, 0.1179,\n                      0.5181, 0.2627, 0.3269, 0.0185, 0.0493, 0.2484, 0.1327, 0.1732, 0.3206,\n                      0.1099, 0.3797, 0.4220, 0.1024, 0.3153, 0.1961, 0.2603, 0.2857, 0.3265,\n                      0.4793, 0.2524, 0.1995, 0.3167, 0.2819, 0.2266, 0.1894, 0.3273, 0.2443,\n                      0.2331, 0.2309, 0.1664, 0.0149, 0.1624, 0.2924, 0.3882, 0.2016, 0.3376,\n                      0.2881, 0.1572, 0.1830, 0.2771, 0.2472, 0.2794, 0.2631, 0.2518, 0.3046,\n                      0.2807, 0.2794, 0.2418, 0.2589, 0.4736, 0.2691, 0.5139, 0.2474, 0.2034,\n                      0.3824, 0.1881, 0.2094, 0.2129, 0.2713, 0.1657, 0.6111, 0.2295, 0.1239,\n                      0.1181, 0.1599, 0.1122, 0.3599, 0.4823, 0.2812, 0.0418, 0.3775, 0.4801,\n                      0.2061, 0.2101, 0.2188, 0.2901, 0.3744, 0.2042, 0.3315, 0.2174, 0.3553,\n                      0.3931, 0.2862, 0.1364, 0.3562, 0.1400, 0.2071, 0.2990, 0.3471, 0.1299,\n                      0.2974, 0.0327, 0.2578, 0.1574, 0.3123, 0.5287, 0.1887, 0.4766, 0.2371,\n                      0.1375, 0.2246, 0.3655, 0.2558, 0.2906, 0.1687, 0.3384, 0.1443, 0.2463,\n                      0.2529, 0.3964, 0.4010, 0.1529, 0.1549, 0.3240, 0.0216, 0.2441, 0.3705,\n                      0.2049, 0.2257, 0.1639, 0.2612, 0.1886, 0.1930, 0.3073, 0.5659, 0.2222,\n                      0.4980, 0.3532, 0.2234, 0.2494, 0.3426, 0.2160, 0.2317, 0.2768, 0.2667,\n                      0.1852, 0.2588, 0.2707, 0.1720, 0.1724, 0.1645, 0.3280, 0.2054, 0.1957,\n                      0.3409, 0.4983, 0.1587, 0.1862, 0.3089, 0.2331, 0.3226, 0.3176, 0.1773,\n                      0.2836, 0.4300, 0.2034, 0.2591, 0.2108, 0.4522, 0.2061, 0.5291, 0.2876,\n                      0.2853, 0.3453, 0.1815, 0.1351, 0.2742, 0.0313, 0.2813, 0.3969, 0.2693,\n                      0.5913, 0.4951, 0.4606, 0.3011, 0.2830, 0.1560, 0.1295, 0.5651, 0.4032,\n                      0.3333, 0.3703, 0.4374, 0.2043, 0.2566, 0.2209, 0.2671, 0.2332, 0.5059,\n                      0.3700, 0.2701, 0.1344, 0.0249, 0.3672, 0.2271, 0.3283, 0.2776, 0.1611,\n                      0.2334, 0.1816, 0.0629, 0.2031, 0.2649, 0.1025, 0.2391, 0.2745, 0.2753,\n                      0.2521, 0.2766, 0.2340, 0.2406, 0.5466, 0.5117, 0.1123, 0.5675, 0.2289,\n                      0.4816, 0.2073, 0.1315, 0.2813, 0.1496, 0.5684, 0.1401, 0.3284, 0.4103,\n                      0.2412, 0.2696, 0.3053, 0.3447, 0.2658, 0.2164, 0.2956, 0.1835, 0.2684,\n                      0.5502, 0.3943, 0.2242, 0.3020, 0.1871, 0.2503, 0.2309, 0.3213, 0.1869,\n                      0.1602, 0.4315, 0.2514, 0.2070, 0.1949, 0.2835, 0.0929, 0.2059, 0.1820,\n                      0.2455, 0.1348, 0.2713, 0.4736, 0.2236, 0.3141])),\n             ('features.6.0.bias',\n              tensor([ 1.5942e-02,  2.0949e-02,  1.5701e-02,  5.9306e-03,  1.1443e-02,\n                       1.2517e-02,  2.8928e-02,  7.7244e-03,  1.2851e-02,  1.8060e-02,\n                       1.8513e-02,  7.7491e-03, -1.9548e-03,  1.1389e-02,  1.3934e-02,\n                       1.5552e-02,  1.1940e-02,  9.4460e-03,  1.3483e-02,  1.5198e-02,\n                       7.3709e-03,  1.7894e-02,  8.5457e-03,  1.5465e-02,  2.0891e-02,\n                       1.6426e-02,  1.8339e-02,  2.4094e-02,  9.2600e-03, -2.0439e-03,\n                       1.2662e-02,  2.0866e-02,  1.1342e-02,  2.0028e-02,  1.3705e-02,\n                       1.3987e-02,  1.2962e-02,  1.2483e-02,  1.4516e-02,  1.7316e-02,\n                       9.8355e-03,  1.5456e-02,  9.5415e-03,  1.1243e-02,  1.3909e-02,\n                      -1.4788e-04,  1.2361e-03,  1.3531e-02,  2.1422e-02,  9.8304e-03,\n                       4.5213e-03,  4.1785e-04,  1.6307e-02,  1.2867e-02,  1.8937e-02,\n                       1.1786e-02,  1.3889e-02,  1.7706e-02,  2.4440e-02,  9.8833e-03,\n                       2.3104e-02,  1.1834e-02,  2.2523e-02,  1.7290e-02,  5.1768e-03,\n                       1.7027e-02,  2.8615e-02,  6.1370e-03,  1.2362e-02,  1.3867e-02,\n                       7.0508e-03,  1.8757e-02,  6.9562e-03,  3.1496e-02,  1.5475e-02,\n                       1.8268e-02,  6.4181e-03,  4.0742e-03,  1.3374e-02,  1.2363e-02,\n                       1.4211e-02,  9.4643e-03,  9.0135e-03,  8.4200e-03,  1.8046e-02,\n                       6.1251e-03,  1.3928e-02,  1.7048e-02,  2.1313e-02,  1.2829e-02,\n                       1.7162e-02,  9.3466e-03,  1.4632e-02,  9.7744e-03,  1.4266e-02,\n                       1.2107e-02,  1.0827e-02,  1.1062e-02,  1.2477e-02,  8.7824e-03,\n                       1.6707e-02,  1.2989e-02,  1.6397e-02,  8.8789e-03,  1.0922e-02,\n                       1.8993e-02,  1.8333e-02,  1.0277e-02,  1.3917e-02, -9.1702e-03,\n                       1.0894e-03,  1.4856e-02,  5.4444e-03,  2.1067e-02,  1.1524e-02,\n                       8.7259e-03,  1.8436e-02,  7.0054e-03,  2.3839e-02,  1.1359e-02,\n                       1.7762e-02,  2.1884e-02,  1.9109e-02,  9.0674e-03, -2.3357e-03,\n                       4.1786e-02,  2.3227e-02,  1.2090e-02,  1.7873e-02,  1.0260e-03,\n                       2.0530e-03,  1.0938e-02,  6.7269e-03,  1.2694e-02,  1.3501e-02,\n                      -1.0570e-03,  2.0001e-02,  6.9169e-03,  5.0297e-03,  1.7334e-02,\n                       1.1461e-02,  9.0941e-03,  1.4611e-02,  1.5881e-02,  2.1819e-02,\n                       1.3274e-02,  7.3924e-03,  1.2475e-02,  8.1453e-03,  7.9953e-03,\n                       7.4510e-03,  1.6789e-02,  1.2725e-02,  8.5966e-03,  1.2737e-02,\n                       6.6282e-03, -2.8221e-01,  9.4278e-03,  9.4336e-03,  1.8091e-02,\n                       9.4893e-03,  1.6933e-02,  1.7888e-02,  9.9510e-03,  1.0761e-02,\n                       9.8488e-03,  8.5478e-03,  1.6943e-02,  1.5243e-02,  1.3477e-02,\n                       1.6202e-02,  8.8802e-03,  1.2253e-02,  7.3050e-03,  1.2865e-02,\n                       2.1947e-02,  1.1525e-02,  2.4721e-02,  1.2190e-02,  9.0870e-03,\n                       1.8119e-02,  1.1054e-02,  1.5576e-02,  1.5949e-02,  1.3307e-02,\n                       8.3359e-03,  2.8135e-02,  1.1357e-02,  9.9898e-03,  5.9391e-03,\n                       7.0425e-03,  6.7728e-03,  1.9004e-02,  2.1718e-02,  1.5596e-02,\n                       2.1920e-03,  2.0326e-02,  2.0249e-02,  1.0230e-02,  9.3241e-03,\n                       9.8665e-03,  8.0092e-03,  1.7958e-02,  1.0759e-02,  1.6708e-02,\n                       9.6070e-03,  1.8851e-02,  1.5529e-02,  1.9049e-02,  1.4959e-03,\n                       1.3127e-02,  1.3416e-02,  1.6944e-02,  1.5686e-02,  2.0597e-02,\n                       3.8394e-03,  1.4188e-02,  1.6554e-03,  1.5371e-02,  1.1045e-02,\n                       1.4496e-02,  2.6226e-02,  1.2523e-02,  2.3310e-02,  1.2588e-02,\n                       9.1980e-03,  8.4273e-03,  1.6769e-02,  1.1295e-02,  1.1883e-02,\n                       1.2739e-02,  1.5792e-02,  8.2563e-03,  1.0892e-02,  1.1084e-02,\n                       1.6445e-02,  1.6655e-02,  9.4987e-03,  1.5352e-02,  1.6756e-02,\n                       1.1074e-03,  1.3422e-02,  1.7247e-02,  5.8533e-03,  1.0443e-02,\n                       7.1644e-03,  1.4573e-02,  8.5400e-03,  6.7626e-03,  1.2612e-02,\n                       2.7685e-02,  9.5717e-03,  2.1126e-02,  1.5801e-02,  1.8154e-02,\n                       1.1496e-02,  1.6939e-02,  3.5602e-03,  1.1437e-02,  1.5342e-02,\n                       5.6063e-03,  1.2735e-02,  1.0812e-02,  1.2722e-02,  2.3866e-02,\n                       1.0378e-02,  1.9872e-03,  1.6098e-02,  7.3809e-03,  5.8474e-03,\n                       1.6922e-02,  2.5641e-02,  9.9350e-03,  7.9013e-03,  4.0138e-02,\n                       9.5978e-03,  1.8984e-02,  1.4314e-02,  9.3218e-03,  1.2294e-02,\n                       3.6936e-02,  1.6293e-02,  9.4791e-03,  1.0846e-02,  2.1097e-02,\n                       1.5102e-02,  2.5431e-02,  1.3428e-02,  1.6673e-02,  1.6088e-02,\n                       1.1817e-02,  8.2794e-03,  1.6994e-02,  1.4715e-03,  1.4792e-02,\n                       1.9827e-02,  1.5638e-02,  3.0449e-02,  2.3154e-02,  2.2922e-02,\n                       1.1687e-02,  1.5029e-02,  6.2245e-03, -7.6609e-04,  2.6202e-02,\n                       2.2014e-02,  1.2934e-02,  1.6921e-02,  2.1282e-02,  9.0156e-03,\n                       1.4316e-02, -3.1674e-03,  1.4897e-02,  1.1335e-02,  3.0134e-02,\n                       2.2693e-02,  1.1881e-02,  7.4075e-03,  1.8138e-03,  1.9903e-02,\n                       9.5936e-03,  1.7452e-02,  1.2198e-02,  4.1221e-03,  1.1662e-02,\n                       1.0689e-02,  3.8376e-03,  1.4797e-02,  1.3005e-02,  2.3372e-03,\n                       1.2643e-02,  1.6484e-02,  1.2360e-02,  1.4173e-02,  1.5545e-02,\n                       1.2319e-02,  9.9996e-03,  2.7384e-02,  2.4830e-02,  3.9596e-03,\n                       2.8010e-02,  1.0600e-02,  2.3867e-02,  8.5678e-03,  3.1187e-02,\n                       1.2730e-02,  9.8723e-03,  2.8879e-02, -4.5161e-02,  1.9505e-02,\n                       1.9664e-02,  9.9726e-03,  1.2741e-02, -1.4745e-03,  1.4787e-02,\n                       1.0517e-03,  1.1179e-02,  1.4128e-02,  1.2367e-02,  1.4166e-02,\n                       2.5957e-02,  1.8827e-02,  1.3473e-02,  1.1780e-02,  2.4671e-02,\n                       1.5176e-02,  1.0834e-02,  1.6150e-02,  1.2319e-02,  3.1502e-03,\n                       1.8979e-02,  1.1346e-02,  1.4373e-02,  1.1170e-02,  1.6798e-02,\n                       2.2667e-03,  8.0388e-03,  8.8661e-03,  1.3468e-02,  6.6967e-03,\n                       1.6599e-02,  2.3005e-02,  7.7366e-03,  1.4866e-02])),\n             ('features.6.1.weight',\n              tensor([[[[-0.0275, -0.0119],\n                        [-0.0191, -0.0131]],\n              \n                       [[-0.0072, -0.0119],\n                        [-0.0203, -0.0131]],\n              \n                       [[-0.0442, -0.0418],\n                        [-0.0478, -0.0597]],\n              \n                       ...,\n              \n                       [[-0.0096, -0.0191],\n                        [-0.0239, -0.0239]],\n              \n                       [[ 0.0215,  0.0406],\n                        [ 0.0299,  0.0406]],\n              \n                       [[-0.0442, -0.0358],\n                        [-0.0633, -0.0514]]],\n              \n              \n                      [[[ 0.0244,  0.0325],\n                        [ 0.0122,  0.0190]],\n              \n                       [[-0.0542, -0.0569],\n                        [-0.0583, -0.0583]],\n              \n                       [[-0.0596, -0.0542],\n                        [-0.0610, -0.0474]],\n              \n                       ...,\n              \n                       [[ 0.0190,  0.0000],\n                        [ 0.0081,  0.0081]],\n              \n                       [[ 0.0393,  0.0501],\n                        [ 0.0583,  0.0488]],\n              \n                       [[-0.0623, -0.0488],\n                        [-0.0691, -0.0325]]],\n              \n              \n                      [[[-0.0421, -0.0432],\n                        [-0.0532, -0.0321]],\n              \n                       [[-0.0067,  0.0011],\n                        [ 0.0111, -0.0033]],\n              \n                       [[-0.0355, -0.0488],\n                        [-0.0255, -0.0277]],\n              \n                       ...,\n              \n                       [[ 0.0488,  0.0576],\n                        [ 0.0599,  0.0632]],\n              \n                       [[-0.0133,  0.0155],\n                        [ 0.0166,  0.0233]],\n              \n                       [[ 0.0477,  0.0333],\n                        [ 0.0144, -0.0133]]],\n              \n              \n                      ...,\n              \n              \n                      [[[ 0.0681,  0.0695],\n                        [ 0.0775,  0.0735]],\n              \n                       [[-0.0080, -0.0067],\n                        [-0.0160, -0.0094]],\n              \n                       [[ 0.0735,  0.0895],\n                        [ 0.1029,  0.1002]],\n              \n                       ...,\n              \n                       [[ 0.0200,  0.0441],\n                        [ 0.0294,  0.0387]],\n              \n                       [[-0.0481, -0.0334],\n                        [-0.0721, -0.0828]],\n              \n                       [[ 0.0361,  0.0214],\n                        [ 0.0187, -0.0053]]],\n              \n              \n                      [[[-0.0599, -0.0393],\n                        [-0.0655, -0.0449]],\n              \n                       [[ 0.0000,  0.0075],\n                        [ 0.0094,  0.0112]],\n              \n                       [[-0.0580, -0.0449],\n                        [-0.0356, -0.0468]],\n              \n                       ...,\n              \n                       [[-0.0693, -0.0655],\n                        [-0.0599, -0.0562]],\n              \n                       [[-0.0281, -0.0187],\n                        [-0.0431, -0.0468]],\n              \n                       [[-0.0562, -0.0562],\n                        [-0.0711, -0.0599]]],\n              \n              \n                      [[[-0.0025,  0.0000],\n                        [ 0.0088,  0.0075]],\n              \n                       [[-0.0138, -0.0075],\n                        [-0.0214, -0.0138]],\n              \n                       [[-0.0189, -0.0428],\n                        [-0.0453, -0.0327]],\n              \n                       ...,\n              \n                       [[-0.0440, -0.0440],\n                        [-0.0440, -0.0579]],\n              \n                       [[ 0.0428,  0.0415],\n                        [ 0.0717,  0.0654]],\n              \n                       [[ 0.0792,  0.0642],\n                        [ 0.1019,  0.0830]]]], size=(768, 384, 2, 2), dtype=torch.qint8,\n                     quantization_scheme=torch.per_channel_affine,\n                     scale=tensor([0.0012, 0.0014, 0.0011, 0.0013, 0.0011, 0.0011, 0.0011, 0.0013, 0.0015,\n                      0.0020, 0.0022, 0.0014, 0.0019, 0.0015, 0.0037, 0.0011, 0.0011, 0.0015,\n                      0.0014, 0.0012, 0.0015, 0.0012, 0.0014, 0.0011, 0.0016, 0.0013, 0.0011,\n                      0.0011, 0.0012, 0.0011, 0.0012, 0.0013, 0.0015, 0.0011, 0.0014, 0.0012,\n                      0.0013, 0.0010, 0.0011, 0.0011, 0.0012, 0.0014, 0.0013, 0.0013, 0.0012,\n                      0.0014, 0.0011, 0.0024, 0.0012, 0.0010, 0.0015, 0.0015, 0.0025, 0.0012,\n                      0.0014, 0.0014, 0.0013, 0.0011, 0.0014, 0.0014, 0.0014, 0.0016, 0.0012,\n                      0.0012, 0.0013, 0.0012, 0.0010, 0.0015, 0.0013, 0.0010, 0.0013, 0.0014,\n                      0.0015, 0.0012, 0.0011, 0.0013, 0.0012, 0.0012, 0.0015, 0.0013, 0.0015,\n                      0.0009, 0.0015, 0.0011, 0.0012, 0.0013, 0.0013, 0.0017, 0.0012, 0.0010,\n                      0.0013, 0.0011, 0.0013, 0.0013, 0.0013, 0.0015, 0.0016, 0.0015, 0.0013,\n                      0.0014, 0.0013, 0.0014, 0.0010, 0.0017, 0.0015, 0.0011, 0.0014, 0.0014,\n                      0.0012, 0.0010, 0.0010, 0.0014, 0.0015, 0.0013, 0.0013, 0.0022, 0.0012,\n                      0.0015, 0.0019, 0.0025, 0.0012, 0.0009, 0.0013, 0.0022, 0.0012, 0.0015,\n                      0.0017, 0.0014, 0.0012, 0.0014, 0.0022, 0.0017, 0.0011, 0.0016, 0.0014,\n                      0.0011, 0.0014, 0.0016, 0.0012, 0.0012, 0.0010, 0.0014, 0.0015, 0.0014,\n                      0.0014, 0.0019, 0.0014, 0.0012, 0.0013, 0.0013, 0.0013, 0.0012, 0.0012,\n                      0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0013, 0.0016, 0.0012, 0.0012,\n                      0.0016, 0.0014, 0.0014, 0.0018, 0.0017, 0.0012, 0.0011, 0.0011, 0.0025,\n                      0.0016, 0.0014, 0.0012, 0.0014, 0.0015, 0.0015, 0.0015, 0.0012, 0.0017,\n                      0.0018, 0.0012, 0.0011, 0.0013, 0.0012, 0.0014, 0.0015, 0.0015, 0.0011,\n                      0.0012, 0.0012, 0.0018, 0.0011, 0.0014, 0.0011, 0.0016, 0.0010, 0.0012,\n                      0.0013, 0.0013, 0.0024, 0.0014, 0.0014, 0.0013, 0.0027, 0.0011, 0.0025,\n                      0.0015, 0.0012, 0.0016, 0.0010, 0.0011, 0.0013, 0.0029, 0.0014, 0.0027,\n                      0.0012, 0.0017, 0.0014, 0.0014, 0.0011, 0.0013, 0.0016, 0.0013, 0.0012,\n                      0.0018, 0.0014, 0.0015, 0.0012, 0.0013, 0.0027, 0.0015, 0.0013, 0.0012,\n                      0.0011, 0.0012, 0.0032, 0.0013, 0.0009, 0.0015, 0.0014, 0.0017, 0.0012,\n                      0.0014, 0.0015, 0.0013, 0.0012, 0.0013, 0.0013, 0.0028, 0.0016, 0.0011,\n                      0.0013, 0.0012, 0.0011, 0.0014, 0.0010, 0.0012, 0.0011, 0.0012, 0.0012,\n                      0.0013, 0.0014, 0.0011, 0.0070, 0.0013, 0.0033, 0.0014, 0.0014, 0.0013,\n                      0.0013, 0.0014, 0.0012, 0.0025, 0.0017, 0.0010, 0.0013, 0.0014, 0.0015,\n                      0.0014, 0.0013, 0.0012, 0.0018, 0.0014, 0.0013, 0.0020, 0.0012, 0.0014,\n                      0.0036, 0.0013, 0.0011, 0.0019, 0.0013, 0.0015, 0.0013, 0.0016, 0.0011,\n                      0.0014, 0.0017, 0.0017, 0.0014, 0.0014, 0.0012, 0.0012, 0.0013, 0.0012,\n                      0.0012, 0.0014, 0.0018, 0.0012, 0.0014, 0.0016, 0.0012, 0.0013, 0.0014,\n                      0.0012, 0.0010, 0.0013, 0.0017, 0.0012, 0.0010, 0.0014, 0.0012, 0.0023,\n                      0.0012, 0.0011, 0.0012, 0.0015, 0.0010, 0.0014, 0.0019, 0.0012, 0.0009,\n                      0.0014, 0.0011, 0.0014, 0.0009, 0.0012, 0.0014, 0.0021, 0.0014, 0.0017,\n                      0.0012, 0.0018, 0.0012, 0.0011, 0.0014, 0.0017, 0.0016, 0.0012, 0.0014,\n                      0.0014, 0.0010, 0.0013, 0.0015, 0.0016, 0.0014, 0.0012, 0.0008, 0.0016,\n                      0.0015, 0.0013, 0.0012, 0.0011, 0.0015, 0.0013, 0.0015, 0.0013, 0.0014,\n                      0.0011, 0.0016, 0.0012, 0.0013, 0.0010, 0.0013, 0.0014, 0.0013, 0.0013,\n                      0.0012, 0.0013, 0.0009, 0.0013, 0.0012, 0.0012, 0.0012, 0.0011, 0.0012,\n                      0.0016, 0.0014, 0.0010, 0.0013, 0.0015, 0.0013, 0.0013, 0.0016, 0.0028,\n                      0.0011, 0.0012, 0.0012, 0.0011, 0.0012, 0.0017, 0.0015, 0.0021, 0.0012,\n                      0.0014, 0.0015, 0.0011, 0.0011, 0.0010, 0.0013, 0.0010, 0.0012, 0.0014,\n                      0.0020, 0.0012, 0.0014, 0.0020, 0.0013, 0.0012, 0.0021, 0.0013, 0.0012,\n                      0.0012, 0.0010, 0.0013, 0.0010, 0.0014, 0.0013, 0.0012, 0.0010, 0.0012,\n                      0.0014, 0.0012, 0.0009, 0.0014, 0.0011, 0.0014, 0.0014, 0.0013, 0.0013,\n                      0.0013, 0.0016, 0.0016, 0.0012, 0.0015, 0.0023, 0.0012, 0.0015, 0.0028,\n                      0.0012, 0.0017, 0.0013, 0.0024, 0.0013, 0.0010, 0.0029, 0.0015, 0.0012,\n                      0.0018, 0.0012, 0.0016, 0.0017, 0.0012, 0.0013, 0.0010, 0.0010, 0.0012,\n                      0.0012, 0.0014, 0.0014, 0.0012, 0.0016, 0.0015, 0.0012, 0.0016, 0.0013,\n                      0.0020, 0.0011, 0.0012, 0.0012, 0.0010, 0.0013, 0.0012, 0.0009, 0.0011,\n                      0.0015, 0.0012, 0.0011, 0.0014, 0.0010, 0.0014, 0.0018, 0.0014, 0.0014,\n                      0.0017, 0.0016, 0.0020, 0.0012, 0.0012, 0.0024, 0.0013, 0.0011, 0.0013,\n                      0.0011, 0.0017, 0.0011, 0.0011, 0.0016, 0.0012, 0.0014, 0.0016, 0.0011,\n                      0.0011, 0.0022, 0.0015, 0.0011, 0.0014, 0.0014, 0.0014, 0.0012, 0.0020,\n                      0.0018, 0.0011, 0.0012, 0.0013, 0.0013, 0.0014, 0.0012, 0.0012, 0.0012,\n                      0.0022, 0.0015, 0.0014, 0.0012, 0.0025, 0.0017, 0.0014, 0.0012, 0.0017,\n                      0.0011, 0.0013, 0.0031, 0.0017, 0.0015, 0.0012, 0.0017, 0.0011, 0.0011,\n                      0.0012, 0.0012, 0.0019, 0.0012, 0.0010, 0.0012, 0.0011, 0.0018, 0.0013,\n                      0.0013, 0.0013, 0.0011, 0.0011, 0.0015, 0.0011, 0.0012, 0.0011, 0.0012,\n                      0.0013, 0.0009, 0.0091, 0.0012, 0.0011, 0.0012, 0.0014, 0.0012, 0.0014,\n                      0.0014, 0.0012, 0.0011, 0.0013, 0.0027, 0.0009, 0.0011, 0.0031, 0.0019,\n                      0.0015, 0.0015, 0.0013, 0.0019, 0.0016, 0.0016, 0.0013, 0.0013, 0.0012,\n                      0.0010, 0.0009, 0.0012, 0.0010, 0.0011, 0.0015, 0.0011, 0.0012, 0.0013,\n                      0.0011, 0.0009, 0.0015, 0.0010, 0.0024, 0.0015, 0.0016, 0.0011, 0.0016,\n                      0.0013, 0.0012, 0.0028, 0.0016, 0.0017, 0.0013, 0.0011, 0.0012, 0.0009,\n                      0.0010, 0.0012, 0.0013, 0.0010, 0.0014, 0.0011, 0.0013, 0.0014, 0.0014,\n                      0.0027, 0.0012, 0.0011, 0.0014, 0.0015, 0.0009, 0.0013, 0.0012, 0.0009,\n                      0.0015, 0.0011, 0.0023, 0.0011, 0.0012, 0.0016, 0.0010, 0.0013, 0.0014,\n                      0.0014, 0.0013, 0.0012, 0.0012, 0.0012, 0.0022, 0.0013, 0.0012, 0.0021,\n                      0.0012, 0.0014, 0.0016, 0.0012, 0.0013, 0.0018, 0.0016, 0.0011, 0.0012,\n                      0.0015, 0.0014, 0.0015, 0.0013, 0.0016, 0.0015, 0.0013, 0.0010, 0.0020,\n                      0.0013, 0.0018, 0.0017, 0.0014, 0.0011, 0.0014, 0.0014, 0.0012, 0.0013,\n                      0.0020, 0.0019, 0.0016, 0.0014, 0.0011, 0.0012, 0.0010, 0.0014, 0.0012,\n                      0.0014, 0.0011, 0.0012, 0.0011, 0.0019, 0.0013, 0.0011, 0.0075, 0.0011,\n                      0.0014, 0.0011, 0.0011, 0.0012, 0.0010, 0.0075, 0.0010, 0.0015, 0.0011,\n                      0.0012, 0.0010, 0.0014, 0.0024, 0.0011, 0.0011, 0.0014, 0.0014, 0.0010,\n                      0.0013, 0.0016, 0.0015, 0.0013, 0.0010, 0.0012, 0.0016, 0.0011, 0.0014,\n                      0.0013, 0.0015, 0.0013, 0.0015, 0.0015, 0.0015, 0.0012, 0.0013, 0.0010,\n                      0.0014, 0.0015, 0.0012, 0.0015, 0.0035, 0.0016, 0.0034, 0.0012, 0.0013,\n                      0.0013, 0.0010, 0.0037, 0.0010, 0.0014, 0.0013, 0.0013, 0.0013, 0.0011,\n                      0.0011, 0.0013, 0.0013, 0.0011, 0.0010, 0.0011, 0.0012, 0.0012, 0.0014,\n                      0.0013, 0.0019, 0.0013], dtype=torch.float64),\n                     zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n                     axis=0)),\n             ('features.6.1.bias',\n              Parameter containing:\n              tensor([-2.0468e-03,  6.2502e-03, -4.7188e-03,  3.9325e-03, -5.1043e-03,\n                       3.7468e-03, -6.0837e-03,  7.2178e-03, -2.9287e-03,  7.9780e-02,\n                      -2.7999e-03, -5.1216e-03,  5.6062e-04,  2.1629e-04,  1.9199e-01,\n                      -5.2489e-03, -2.1501e-02,  2.5011e-03,  6.4653e-04, -2.3062e-03,\n                       5.0893e-04, -6.2149e-03, -5.9490e-03, -1.1980e-03, -6.2912e-03,\n                       2.7853e-03,  1.8860e-04, -3.0791e-03, -7.1107e-04, -1.1426e-02,\n                       1.2845e-02, -9.3317e-04, -6.5708e-03, -6.8420e-03,  3.4404e-03,\n                       6.0894e-03,  1.0590e-02,  9.0870e-04,  5.9064e-03,  1.6024e-03,\n                      -3.6567e-03,  6.8382e-03, -3.0371e-04,  1.3863e-03,  5.0972e-03,\n                       6.5476e-03, -5.1404e-04,  4.1989e-03,  5.6037e-03,  4.4888e-03,\n                      -7.2471e-03, -1.0153e-02,  1.0280e-02, -3.9553e-03, -1.2082e-02,\n                       6.3674e-03,  2.3493e-03,  6.5920e-03, -2.7188e-03, -2.0618e-03,\n                       4.5146e-03, -7.6238e-04, -3.0255e-03, -1.4723e-03,  8.4512e-04,\n                      -4.2147e-03, -1.1661e-03, -6.0185e-03,  1.2689e-02,  5.2787e-03,\n                      -1.1361e-02, -2.3549e-03, -1.9485e-04, -1.1182e-03, -1.7923e-03,\n                      -1.0550e-03, -8.3008e-04, -4.1072e-04, -9.8765e-04,  2.4530e-03,\n                      -8.6475e-03, -4.6417e-05,  1.1404e-03, -7.1450e-04,  9.3715e-03,\n                      -2.2639e-03,  6.3933e-04,  3.8500e-02, -8.8906e-04,  2.4594e-03,\n                      -3.8061e-03,  2.1430e-03, -6.1562e-03, -3.7672e-03,  5.4898e-03,\n                       5.3804e-03,  6.1599e-03,  3.7847e-03,  3.7987e-03,  6.1476e-03,\n                       4.6988e-03, -3.9752e-04,  5.3520e-03,  3.7218e-03,  2.8353e-03,\n                       1.8647e-03,  4.8369e-03, -5.6831e-04,  3.4073e-04,  4.4844e-05,\n                      -1.0246e-03, -5.9446e-04,  4.5349e-03, -3.7569e-03, -1.8669e-03,\n                      -9.3221e-03, -2.2425e-04, -5.3893e-03, -9.1019e-03,  9.4137e-03,\n                       4.2300e-03, -2.1842e-03,  5.5337e-03,  9.8998e-03,  2.6013e-03,\n                      -7.2009e-03,  3.4510e-02,  8.0434e-03, -2.1205e-03, -5.9011e-03,\n                       3.5085e-03,  2.3017e-03, -7.8397e-04,  7.3321e-03, -9.1611e-03,\n                      -3.2617e-03, -2.0906e-04,  8.6443e-03,  5.6744e-03, -1.6467e-02,\n                       9.2064e-03, -1.3152e-02, -2.4149e-03, -6.4956e-02, -4.0154e-02,\n                       1.4097e-02, -5.1007e-04,  4.7602e-03,  6.8648e-03, -6.9951e-04,\n                       7.3925e-03,  6.9797e-03, -2.1828e-02,  2.7304e-03,  4.2425e-03,\n                       7.9452e-03, -8.0928e-03,  6.2875e-03, -9.5462e-05,  1.4909e-03,\n                       4.2243e-03,  1.0833e-03,  1.2307e-02,  4.0943e-03,  7.3740e-04,\n                      -1.2214e-02, -1.4962e-02, -6.2493e-03, -7.0498e-04,  1.2937e-03,\n                       2.0507e-02, -1.8518e-03,  8.4439e-04,  3.0308e-03, -7.8100e-03,\n                      -3.2902e-03,  5.6140e-04, -2.0910e-02,  5.3154e-03,  2.5154e-03,\n                      -7.1555e-03, -2.3868e-03, -6.0170e-07,  1.2498e-02,  3.4932e-03,\n                      -7.0192e-04, -5.2278e-03, -4.1181e-03,  1.2790e-02, -3.1201e-03,\n                       2.1825e-03,  3.4631e-03,  5.1167e-03, -4.5539e-03, -3.0048e-03,\n                       2.4899e-03,  1.4480e-03,  3.1415e-03, -1.8811e-03,  9.0911e-03,\n                      -3.9869e-03,  4.9906e-03, -8.8767e-03, -5.8103e-04,  6.6328e-03,\n                      -5.3137e-03,  5.0929e-03,  3.4974e-03, -1.3139e-03, -1.2740e-02,\n                       7.0022e-03,  1.0175e-02, -3.1066e-03,  2.1541e-02, -5.6723e-02,\n                       5.5342e-03, -9.5207e-03,  3.2098e-02, -2.9097e-02,  4.2956e-03,\n                      -1.4534e-03,  3.9354e-03, -3.1631e-03, -1.6870e-03,  3.1688e-03,\n                      -2.5875e-03, -2.4487e-03, -3.1356e-03,  6.8405e-03, -3.2227e-03,\n                      -1.5899e-02, -2.2135e-04,  5.5347e-03,  4.7813e-03,  8.2188e-03,\n                       4.1884e-03, -4.7727e-03,  4.6745e-03,  1.1795e-02, -2.8819e-03,\n                       3.2451e-04, -2.9261e-03, -5.2518e-03, -8.4495e-03,  6.0289e-04,\n                       5.5903e-03, -1.1522e-02,  2.4670e-02,  5.2494e-03,  3.9697e-02,\n                       1.7697e-03,  5.5987e-03,  4.0720e-03, -9.2827e-04, -1.8651e-03,\n                       7.3634e-04,  2.2110e-03,  2.6597e-03, -3.5960e-03, -3.1187e-04,\n                       1.3961e-03, -1.3473e-02, -1.0272e-02, -1.7934e-03,  2.4476e-03,\n                       3.0900e-03, -3.4181e-01, -1.2039e-02, -1.9694e-02,  3.6669e-03,\n                       3.8826e-04,  2.3852e-04,  1.3353e-02,  3.5617e-03, -1.4959e-03,\n                      -7.1725e-03, -1.7874e-03,  1.0188e-02,  8.9110e-03, -2.4900e-03,\n                      -7.7721e-04, -1.1885e-03,  6.0444e-03,  5.0856e-04,  6.3033e-03,\n                       1.7442e-02, -1.0475e-03,  1.8825e-03,  4.1477e-02,  1.7167e-03,\n                       2.3284e-03,  9.9544e-03, -1.9975e-02, -4.3348e-03,  3.8356e-04,\n                      -2.8312e-03, -1.5274e-03,  1.7515e-03, -5.9665e-03, -7.4014e-03,\n                       2.9605e-03,  4.5773e-03, -4.4171e-03, -5.5722e-03, -1.2264e-02,\n                       2.6984e-04, -1.5477e-03,  6.5937e-03, -1.8365e-02, -1.2597e-02,\n                       1.1383e-02,  8.1125e-04,  3.5443e-03, -2.9499e-03,  3.1776e-03,\n                       3.9466e-03, -3.3134e-03,  1.4025e-01,  2.0187e-04, -1.6846e-03,\n                       1.9876e-03,  7.1066e-03, -2.0393e-03, -9.2237e-03,  4.2240e-03,\n                      -1.4577e-02,  6.2411e-03,  6.2378e-03, -1.5182e-03,  1.6655e-03,\n                       1.7267e-02, -2.5553e-03, -5.2065e-03, -1.5428e-02, -7.9066e-03,\n                       1.3605e-03,  1.1226e-02, -7.3544e-03, -1.3770e-02, -2.3395e-02,\n                      -2.6654e-02,  5.5864e-03, -4.6828e-03, -7.4419e-03, -1.1701e-03,\n                      -6.8274e-03, -7.1045e-03,  3.7945e-03,  6.4968e-03,  1.3885e-03,\n                      -3.7278e-03, -3.1487e-03,  3.8955e-03,  1.6536e-03,  1.9532e-03,\n                      -9.2654e-03, -1.0534e-02, -5.2543e-03, -9.1519e-03,  6.8326e-03,\n                       2.7684e-02,  9.8483e-04, -1.7973e-03,  5.4872e-03,  9.9645e-03,\n                      -7.6985e-03,  1.1530e-02,  4.4574e-03, -9.0223e-03, -8.9040e-03,\n                      -2.5039e-03, -9.7006e-04, -3.4194e-03,  6.2291e-03, -2.8881e-03,\n                       9.6360e-04,  7.3157e-03,  1.4649e-02,  6.3541e-03,  5.1507e-03,\n                      -1.3396e-03, -2.4727e-03, -1.2780e-03, -7.5880e-03, -8.0119e-03,\n                       2.4059e-03, -1.1273e-03,  1.0083e-02,  7.0203e-03,  5.6117e-04,\n                      -2.1698e-03, -1.3478e-02,  2.9626e-03,  6.4125e-03,  3.8974e-03,\n                      -1.3874e-02, -4.6535e-03, -1.8592e-02, -9.1917e-03, -3.6060e-03,\n                      -1.2584e-02, -1.0476e-02, -6.3201e-03, -9.1224e-03, -2.1038e-02,\n                      -9.7157e-04,  1.6233e-03, -2.9655e-03, -6.4013e-03, -5.1605e-04,\n                      -1.1045e-02,  6.3462e-03, -1.6563e-03,  4.8007e-04, -3.1387e-03,\n                      -2.6769e-03,  3.5327e-04, -5.2196e-03,  8.6271e-03,  3.6233e-03,\n                      -2.0158e-02, -3.2068e-03,  4.3968e-05,  3.8683e-03, -2.2338e-03,\n                      -4.9533e-03, -4.8649e-03, -3.7417e-03,  5.3906e-03, -3.1899e-03,\n                       6.6796e-03, -7.2255e-04, -4.2836e-03, -1.9352e-03,  1.5421e-04,\n                      -1.1799e-03,  5.8641e-03,  2.2497e-03,  9.5725e-04, -2.1198e-03,\n                      -4.6014e-03,  3.0701e-03, -6.4632e-03, -2.6932e-02, -3.2765e-03,\n                      -4.6965e-03, -8.7626e-02,  9.0523e-03, -1.4328e-03,  1.7952e-02,\n                       2.5259e-03, -2.2811e-02, -7.2363e-04, -2.9682e-02,  1.4686e-03,\n                       1.5278e-03, -3.7544e-02,  6.1124e-03, -5.8858e-03, -1.6392e-03,\n                      -5.6646e-03, -2.4296e-03,  1.9503e-02,  4.3868e-03, -1.4210e-03,\n                       9.5838e-03, -4.4604e-03, -2.2991e-03,  6.7131e-03, -4.4070e-03,\n                      -5.6988e-03,  1.1256e-02,  8.8469e-04,  9.8672e-06,  9.8777e-04,\n                      -1.0244e-02,  1.4876e-03, -1.8023e-03,  4.7845e-03, -2.1224e-03,\n                       4.3492e-03, -4.0714e-05,  6.4465e-03,  8.7621e-03, -3.1284e-03,\n                      -1.1903e-02,  3.1983e-03, -6.7157e-04,  1.1697e-02, -2.8934e-03,\n                      -6.5632e-03, -2.3740e-03,  7.1456e-04, -9.3641e-04,  7.9300e-03,\n                       5.2115e-03,  6.3345e-04,  2.3879e-02, -4.1290e-06,  3.4489e-04,\n                      -1.3571e-02,  3.1740e-03, -1.0155e-02, -9.6328e-03,  2.8099e-03,\n                       2.0415e-03, -4.0393e-03,  8.6851e-03,  8.0769e-03,  2.7401e-03,\n                       7.4769e-03,  7.7856e-04, -3.0068e-03, -4.5033e-03, -4.3325e-03,\n                      -5.3150e-03,  8.9764e-03, -2.8366e-03, -4.8403e-03,  2.2331e-03,\n                       2.3280e-03, -4.8943e-03,  1.1303e-02, -2.0956e-03,  2.4246e-03,\n                      -5.9966e-03,  4.5715e-03,  5.9039e-03,  1.2334e-03,  6.7699e-03,\n                       3.9369e-03, -8.9528e-03,  2.0276e-03,  4.3488e-03,  2.5165e-03,\n                      -4.1377e-04,  2.1023e-02, -2.8632e-03, -4.4622e-03, -4.6798e-03,\n                      -7.6638e-03,  3.4390e-03,  3.5974e-02,  3.1651e-03,  8.2795e-03,\n                       2.0127e-03,  3.1485e-03,  5.1612e-03,  1.6802e-04,  4.0310e-03,\n                       2.7750e-02,  3.1255e-03, -5.7368e-03,  2.0863e-03, -7.6542e-03,\n                       4.7848e-03,  1.6941e-03, -2.8118e-03,  3.9989e-03, -8.0811e-04,\n                      -8.6825e-04, -1.9947e-03, -3.1922e-03, -4.6280e-03,  1.1341e-02,\n                       5.3254e-03, -9.0513e-03,  6.8220e-03, -1.4577e-03,  1.5369e-03,\n                       7.2013e-03, -7.8225e-03, -1.2071e-03,  1.0036e-02, -3.3798e-03,\n                       8.8654e-03, -2.8262e-03,  2.7664e-03,  8.1879e-03, -7.8596e-03,\n                       2.7999e-02, -2.4374e-03,  4.3179e-03, -6.3178e-02, -6.3899e-02,\n                      -7.7908e-03, -9.5693e-04,  7.0053e-04,  6.0537e-03, -6.4753e-03,\n                       1.5932e-03, -7.2488e-03,  9.4237e-03, -3.0351e-03, -1.0120e-03,\n                       1.3983e-02,  5.2491e-03,  1.6582e-03,  1.7713e-03, -4.9149e-03,\n                       1.8402e-04,  4.6959e-03,  1.1040e-02, -6.6772e-03,  6.3493e-03,\n                       1.2423e-02,  3.2948e-03,  1.9102e-03,  3.6547e-03,  3.7251e-04,\n                      -1.8915e-03,  1.0945e-02,  3.9686e-03,  6.0640e-04, -1.5357e-02,\n                      -1.1776e-03,  4.0153e-03, -2.3567e-03,  1.9648e-03,  1.2613e-03,\n                      -2.8952e-04,  7.2982e-03,  1.2525e-03, -1.0878e-03, -7.3976e-03,\n                       3.2547e-03, -1.1022e-02,  2.8155e-03, -3.5645e-03,  4.3095e-04,\n                      -2.0524e-02,  3.1256e-03,  2.1296e-02, -2.6537e-03, -1.2126e-03,\n                      -1.5380e-03,  1.7869e-03, -1.4878e-03, -5.1555e-03,  7.5121e-03,\n                       1.7061e-03, -5.4561e-03, -6.5741e-03, -1.4918e-02, -6.0095e-03,\n                      -4.8635e-05,  5.0861e-03, -2.1183e-03, -6.4145e-04, -1.4702e-02,\n                      -4.7618e-03, -4.6789e-04, -5.2708e-03,  6.5614e-02,  6.2817e-03,\n                       2.8195e-03, -2.6924e-03,  2.9262e-03,  3.5380e-03, -6.0614e-04,\n                      -6.0503e-03,  2.9438e-03, -1.1172e-02, -4.4269e-04, -2.6247e-02,\n                      -6.4702e-03, -1.2467e-02, -6.6512e-03,  8.3214e-03,  1.1090e-03,\n                       1.6505e-02, -4.3571e-03, -5.6433e-03, -5.5825e-03, -3.0949e-03,\n                       6.7037e-03,  7.5177e-03,  4.9050e-03, -1.9085e-03, -6.0112e-03,\n                       8.9885e-03, -3.3993e-03, -4.7994e-03, -5.2071e-03,  3.2288e-03,\n                       4.0669e-03, -6.6540e-03, -4.3576e-04, -1.5048e-02, -7.0496e-04,\n                       2.9694e-03,  1.8816e-04, -2.3319e-04,  6.2348e-03,  8.4606e-03,\n                       3.0058e-03, -4.4600e-03, -5.4693e-04,  1.0708e-02, -2.0461e-03,\n                      -2.0398e-03,  2.9581e-03,  4.1787e-03,  1.2000e-02, -4.0371e-04,\n                      -4.4529e-03, -1.3182e-03,  3.3584e-01,  1.9214e-03,  8.9296e-03,\n                      -6.1436e-03, -3.5383e-03,  1.2573e-03,  5.3852e-03, -5.0476e-03,\n                       9.9558e-04,  7.5382e-05,  1.6195e-02,  5.1248e-03, -4.3723e-03,\n                      -5.5965e-03,  2.0450e-03,  4.6014e-04, -3.9177e-04, -2.7316e-04,\n                       6.6750e-03,  9.2046e-03,  2.1909e-03,  1.0639e-03, -2.7657e-04,\n                       7.4519e-03, -9.5546e-04, -1.3990e-02,  1.1390e-02, -4.4656e-04,\n                       2.8181e-03, -6.2196e-03,  3.0475e-03, -5.8003e-03, -1.5077e-03,\n                       1.1900e-02, -1.8633e-03, -2.6846e-03, -2.6302e-03,  2.6995e-04,\n                       6.5536e-03, -2.5732e-03,  1.7732e-03, -1.1109e-02,  4.8379e-02,\n                      -3.5619e-03,  3.3136e-04, -3.8701e-03,  8.0474e-03, -1.5576e-02,\n                       8.0666e-03, -1.0302e-03, -2.3666e-03, -4.5969e-03,  6.5015e-03,\n                      -6.7078e-03,  1.0218e-02, -7.9785e-03,  3.1495e-03, -7.8971e-03,\n                       5.4841e-02, -3.5943e-03, -5.5418e-03], requires_grad=True)),\n             ('features.6.1.scale', tensor(0.0256)),\n             ('features.6.1.zero_point', tensor(70)),\n             ('features.7.0.layer_scale',\n              tensor([[[-0.0562]],\n              \n                      [[-0.1472]],\n              \n                      [[-0.0906]],\n              \n                      [[ 0.1143]],\n              \n                      [[-0.2279]],\n              \n                      [[ 0.1067]],\n              \n                      [[ 0.1267]],\n              \n                      [[ 0.1589]],\n              \n                      [[ 0.0784]],\n              \n                      [[-0.0696]],\n              \n                      [[ 0.0539]],\n              \n                      [[ 0.1645]],\n              \n                      [[ 0.0955]],\n              \n                      [[-0.1422]],\n              \n                      [[ 0.1216]],\n              \n                      [[-0.7699]],\n              \n                      [[ 0.7534]],\n              \n                      [[-0.0563]],\n              \n                      [[-0.0617]],\n              \n                      [[-0.2249]],\n              \n                      [[ 0.1053]],\n              \n                      [[ 0.0831]],\n              \n                      [[-0.0546]],\n              \n                      [[-0.1987]],\n              \n                      [[ 0.5722]],\n              \n                      [[ 0.1843]],\n              \n                      [[ 0.0850]],\n              \n                      [[-0.1963]],\n              \n                      [[-0.2019]],\n              \n                      [[-0.1747]],\n              \n                      [[ 0.1359]],\n              \n                      [[-0.2259]],\n              \n                      [[ 0.0908]],\n              \n                      [[ 0.1119]],\n              \n                      [[-0.1121]],\n              \n                      [[-0.3116]],\n              \n                      [[ 0.1112]],\n              \n                      [[-0.1560]],\n              \n                      [[-0.1556]],\n              \n                      [[ 0.2032]],\n              \n                      [[ 0.0567]],\n              \n                      [[ 0.0601]],\n              \n                      [[-0.0714]],\n              \n                      [[-0.3307]],\n              \n                      [[-0.3892]],\n              \n                      [[ 0.1804]],\n              \n                      [[-0.2443]],\n              \n                      [[ 0.0898]],\n              \n                      [[ 0.1257]],\n              \n                      [[-0.1071]],\n              \n                      [[ 0.1696]],\n              \n                      [[-0.1081]],\n              \n                      [[-0.1765]],\n              \n                      [[-0.0687]],\n              \n                      [[ 0.1586]],\n              \n                      [[ 0.0643]],\n              \n                      [[-0.2033]],\n              \n                      [[-0.0920]],\n              \n                      [[ 0.2032]],\n              \n                      [[ 0.2080]],\n              \n                      [[ 0.0542]],\n              \n                      [[-0.1317]],\n              \n                      [[ 0.1647]],\n              \n                      [[ 0.0790]],\n              \n                      [[-0.2339]],\n              \n                      [[-0.0670]],\n              \n                      [[-0.1867]],\n              \n                      [[ 0.1632]],\n              \n                      [[-0.7146]],\n              \n                      [[ 0.1690]],\n              \n                      [[ 0.1484]],\n              \n                      [[ 0.1000]],\n              \n                      [[ 0.0702]],\n              \n                      [[-0.3852]],\n              \n                      [[-0.1340]],\n              \n                      [[-0.0523]],\n              \n                      [[-0.1367]],\n              \n                      [[-0.2060]],\n              \n                      [[ 0.0557]],\n              \n                      [[-0.2032]],\n              \n                      [[-0.1166]],\n              \n                      [[ 0.1827]],\n              \n                      [[ 0.1145]],\n              \n                      [[ 0.2079]],\n              \n                      [[-0.1555]],\n              \n                      [[ 0.1132]],\n              \n                      [[ 0.2440]],\n              \n                      [[ 0.0999]],\n              \n                      [[ 0.1776]],\n              \n                      [[ 0.1255]],\n              \n                      [[-0.1157]],\n              \n                      [[ 0.1507]],\n              \n                      [[ 0.1307]],\n              \n                      [[-0.1240]],\n              \n                      [[-0.0836]],\n              \n                      [[-0.0843]],\n              \n                      [[-0.1996]],\n              \n                      [[-0.0610]],\n              \n                      [[-0.0751]],\n              \n                      [[-0.1534]],\n              \n                      [[-0.1081]],\n              \n                      [[-0.1602]],\n              \n                      [[-0.1811]],\n              \n                      [[-0.0145]],\n              \n                      [[-0.1102]],\n              \n                      [[ 0.1984]],\n              \n                      [[-0.1808]],\n              \n                      [[ 0.0696]],\n              \n                      [[ 0.1181]],\n              \n                      [[-0.1925]],\n              \n                      [[-0.1301]],\n              \n                      [[ 0.1318]],\n              \n                      [[ 0.0550]],\n              \n                      [[ 0.1041]],\n              \n                      [[-0.1087]],\n              \n                      [[ 0.1608]],\n              \n                      [[ 0.0675]],\n              \n                      [[-0.1683]],\n              \n                      [[ 0.1352]],\n              \n                      [[ 0.0772]],\n              \n                      [[ 0.1205]],\n              \n                      [[-0.1894]],\n              \n                      [[ 0.1570]],\n              \n                      [[ 0.1378]],\n              \n                      [[ 0.1733]],\n              \n                      [[-0.0581]],\n              \n                      [[-0.1256]],\n              \n                      [[-0.0862]],\n              \n                      [[ 0.1148]],\n              \n                      [[-0.1335]],\n              \n                      [[-0.1526]],\n              \n                      [[-0.0636]],\n              \n                      [[-0.1956]],\n              \n                      [[-0.1260]],\n              \n                      [[ 0.1813]],\n              \n                      [[-0.1261]],\n              \n                      [[-0.1688]],\n              \n                      [[ 0.0754]],\n              \n                      [[-0.1851]],\n              \n                      [[ 0.2902]],\n              \n                      [[-0.2604]],\n              \n                      [[-0.3230]],\n              \n                      [[-0.0513]],\n              \n                      [[ 0.1382]],\n              \n                      [[-0.2066]],\n              \n                      [[-0.1213]],\n              \n                      [[-0.0702]],\n              \n                      [[ 0.1547]],\n              \n                      [[ 0.0609]],\n              \n                      [[ 0.0603]],\n              \n                      [[-0.0966]],\n              \n                      [[ 0.1154]],\n              \n                      [[-0.1671]],\n              \n                      [[ 0.1593]],\n              \n                      [[-0.1667]],\n              \n                      [[ 0.1643]],\n              \n                      [[-0.0686]],\n              \n                      [[-0.1974]],\n              \n                      [[-0.1337]],\n              \n                      [[ 0.1313]],\n              \n                      [[ 0.0538]],\n              \n                      [[ 0.1813]],\n              \n                      [[ 0.0932]],\n              \n                      [[-0.0658]],\n              \n                      [[-0.1093]],\n              \n                      [[ 0.0776]],\n              \n                      [[ 0.0922]],\n              \n                      [[ 0.1714]],\n              \n                      [[ 0.1088]],\n              \n                      [[ 0.0761]],\n              \n                      [[ 0.6579]],\n              \n                      [[ 0.1381]],\n              \n                      [[ 0.0601]],\n              \n                      [[ 0.0941]],\n              \n                      [[ 0.1982]],\n              \n                      [[ 0.2385]],\n              \n                      [[ 0.0607]],\n              \n                      [[ 0.0405]],\n              \n                      [[-0.1465]],\n              \n                      [[-0.0731]],\n              \n                      [[ 0.1091]],\n              \n                      [[-0.1709]],\n              \n                      [[-0.1885]],\n              \n                      [[ 0.1011]],\n              \n                      [[ 0.0719]],\n              \n                      [[-0.0710]],\n              \n                      [[ 0.7157]],\n              \n                      [[-0.1920]],\n              \n                      [[ 0.1535]],\n              \n                      [[-0.1795]],\n              \n                      [[ 0.0734]],\n              \n                      [[-0.1048]],\n              \n                      [[-0.1991]],\n              \n                      [[-0.1797]],\n              \n                      [[-0.1532]],\n              \n                      [[-0.1887]],\n              \n                      [[ 0.2014]],\n              \n                      [[-0.1835]],\n              \n                      [[-0.1402]],\n              \n                      [[ 0.0553]],\n              \n                      [[-0.0989]],\n              \n                      [[ 0.2569]],\n              \n                      [[ 0.1827]],\n              \n                      [[-0.1322]],\n              \n                      [[-0.1239]],\n              \n                      [[ 0.3291]],\n              \n                      [[-0.0947]],\n              \n                      [[-0.2682]],\n              \n                      [[ 0.1917]],\n              \n                      [[ 0.0495]],\n              \n                      [[-0.1495]],\n              \n                      [[-0.1849]],\n              \n                      [[-0.1407]],\n              \n                      [[-0.1591]],\n              \n                      [[ 0.0928]],\n              \n                      [[ 0.0816]],\n              \n                      [[-0.1585]],\n              \n                      [[ 0.1214]],\n              \n                      [[ 0.0641]],\n              \n                      [[ 0.1369]],\n              \n                      [[ 0.1987]],\n              \n                      [[ 0.0533]],\n              \n                      [[ 0.0600]],\n              \n                      [[ 0.0715]],\n              \n                      [[ 0.1289]],\n              \n                      [[-0.0877]],\n              \n                      [[ 0.0960]],\n              \n                      [[ 0.1101]],\n              \n                      [[-0.1851]],\n              \n                      [[-0.1515]],\n              \n                      [[-0.0767]],\n              \n                      [[-0.0683]],\n              \n                      [[-0.1112]],\n              \n                      [[-0.0621]],\n              \n                      [[ 0.1657]],\n              \n                      [[-0.1307]],\n              \n                      [[-0.1700]],\n              \n                      [[ 0.0844]],\n              \n                      [[-0.1418]],\n              \n                      [[ 0.1398]],\n              \n                      [[-0.1090]],\n              \n                      [[-0.0577]],\n              \n                      [[ 0.1471]],\n              \n                      [[ 0.1311]],\n              \n                      [[ 0.1133]],\n              \n                      [[-0.0589]],\n              \n                      [[ 0.1233]],\n              \n                      [[ 0.1025]],\n              \n                      [[-0.0960]],\n              \n                      [[ 0.1112]],\n              \n                      [[ 0.1204]],\n              \n                      [[ 0.1471]],\n              \n                      [[-0.0702]],\n              \n                      [[-0.1871]],\n              \n                      [[-0.2217]],\n              \n                      [[-0.0607]],\n              \n                      [[-0.3449]],\n              \n                      [[-0.1191]],\n              \n                      [[-0.1187]],\n              \n                      [[-0.1364]],\n              \n                      [[-0.1768]],\n              \n                      [[-0.1433]],\n              \n                      [[-0.0539]],\n              \n                      [[-0.1998]],\n              \n                      [[ 0.0009]],\n              \n                      [[ 0.1143]],\n              \n                      [[ 0.3212]],\n              \n                      [[ 0.1125]],\n              \n                      [[-0.0538]],\n              \n                      [[-0.1929]],\n              \n                      [[-0.1472]],\n              \n                      [[-0.0780]],\n              \n                      [[-0.1592]],\n              \n                      [[ 0.1819]],\n              \n                      [[ 0.1095]],\n              \n                      [[ 0.1242]],\n              \n                      [[ 0.0813]],\n              \n                      [[ 0.0518]],\n              \n                      [[ 0.0914]],\n              \n                      [[ 0.1170]],\n              \n                      [[ 0.1010]],\n              \n                      [[-0.1764]],\n              \n                      [[-0.3147]],\n              \n                      [[-0.1162]],\n              \n                      [[ 0.1638]],\n              \n                      [[-0.0456]],\n              \n                      [[ 0.0653]],\n              \n                      [[ 0.0955]],\n              \n                      [[ 0.0698]],\n              \n                      [[ 0.0567]],\n              \n                      [[ 0.1713]],\n              \n                      [[-0.7693]],\n              \n                      [[-0.1425]],\n              \n                      [[ 0.2178]],\n              \n                      [[ 0.0455]],\n              \n                      [[ 0.3674]],\n              \n                      [[ 0.1850]],\n              \n                      [[-0.1940]],\n              \n                      [[-0.0940]],\n              \n                      [[-0.0356]],\n              \n                      [[-0.0645]],\n              \n                      [[ 0.2126]],\n              \n                      [[ 0.1174]],\n              \n                      [[-0.0900]],\n              \n                      [[-0.0523]],\n              \n                      [[ 0.1057]],\n              \n                      [[ 0.0625]],\n              \n                      [[ 0.1675]],\n              \n                      [[ 0.0861]],\n              \n                      [[-0.1345]],\n              \n                      [[-0.1650]],\n              \n                      [[-0.1132]],\n              \n                      [[-0.1392]],\n              \n                      [[ 0.1172]],\n              \n                      [[-0.0699]],\n              \n                      [[ 0.0534]],\n              \n                      [[-0.2197]],\n              \n                      [[-0.0594]],\n              \n                      [[-0.1087]],\n              \n                      [[-0.1248]],\n              \n                      [[ 0.2427]],\n              \n                      [[ 0.0499]],\n              \n                      [[-0.1153]],\n              \n                      [[ 0.0798]],\n              \n                      [[-0.1331]],\n              \n                      [[ 0.6603]],\n              \n                      [[ 0.1118]],\n              \n                      [[ 0.1099]],\n              \n                      [[-0.1941]],\n              \n                      [[ 0.0690]],\n              \n                      [[-0.2466]],\n              \n                      [[-0.0480]],\n              \n                      [[ 0.1464]],\n              \n                      [[ 0.2048]],\n              \n                      [[ 0.1204]],\n              \n                      [[-0.0503]],\n              \n                      [[ 0.1388]],\n              \n                      [[ 0.0497]],\n              \n                      [[ 0.1705]],\n              \n                      [[-0.1889]],\n              \n                      [[-0.1825]],\n              \n                      [[ 0.0556]],\n              \n                      [[-0.1326]],\n              \n                      [[-0.1158]],\n              \n                      [[ 0.0558]],\n              \n                      [[ 0.1580]],\n              \n                      [[-0.0435]],\n              \n                      [[ 0.0530]],\n              \n                      [[-0.0566]],\n              \n                      [[ 0.0467]],\n              \n                      [[ 0.1613]],\n              \n                      [[-0.1623]],\n              \n                      [[-0.1613]],\n              \n                      [[ 0.0595]],\n              \n                      [[ 0.1289]],\n              \n                      [[ 0.2011]],\n              \n                      [[ 0.0917]],\n              \n                      [[-0.1685]],\n              \n                      [[ 0.2355]],\n              \n                      [[ 0.0555]],\n              \n                      [[ 0.7362]],\n              \n                      [[-0.1027]],\n              \n                      [[-0.1736]],\n              \n                      [[-0.1464]],\n              \n                      [[-0.1491]],\n              \n                      [[-0.1116]],\n              \n                      [[-0.3587]],\n              \n                      [[ 0.1190]],\n              \n                      [[ 0.0834]],\n              \n                      [[ 0.0560]],\n              \n                      [[-0.1078]],\n              \n                      [[ 0.2089]],\n              \n                      [[ 0.1811]],\n              \n                      [[ 0.1621]],\n              \n                      [[ 0.0514]],\n              \n                      [[-0.0463]],\n              \n                      [[-0.1928]],\n              \n                      [[-0.1256]],\n              \n                      [[-0.0506]],\n              \n                      [[ 0.1021]],\n              \n                      [[ 0.1627]],\n              \n                      [[ 0.4473]],\n              \n                      [[-0.8145]],\n              \n                      [[-0.0689]],\n              \n                      [[-0.2384]],\n              \n                      [[-0.1333]],\n              \n                      [[-0.2022]],\n              \n                      [[ 0.1369]],\n              \n                      [[-0.0775]],\n              \n                      [[ 0.2032]],\n              \n                      [[ 0.0766]],\n              \n                      [[ 0.1894]],\n              \n                      [[ 0.0589]],\n              \n                      [[-0.1868]],\n              \n                      [[-0.0960]],\n              \n                      [[-0.1242]],\n              \n                      [[-0.1499]],\n              \n                      [[ 0.1218]],\n              \n                      [[ 0.2102]],\n              \n                      [[-0.1569]],\n              \n                      [[-0.2409]],\n              \n                      [[ 0.1443]],\n              \n                      [[ 0.1568]],\n              \n                      [[-0.1507]],\n              \n                      [[-0.5807]],\n              \n                      [[ 0.0688]],\n              \n                      [[-0.0685]],\n              \n                      [[-0.1528]],\n              \n                      [[ 0.1583]],\n              \n                      [[ 0.1266]],\n              \n                      [[-0.1610]],\n              \n                      [[ 0.1687]],\n              \n                      [[ 0.0485]],\n              \n                      [[-0.1542]],\n              \n                      [[-0.0869]],\n              \n                      [[-0.0761]],\n              \n                      [[ 0.1346]],\n              \n                      [[ 0.1515]],\n              \n                      [[-0.0826]],\n              \n                      [[ 0.1062]],\n              \n                      [[ 0.0493]],\n              \n                      [[-0.1109]],\n              \n                      [[-0.1728]],\n              \n                      [[ 0.1787]],\n              \n                      [[ 0.1804]],\n              \n                      [[-0.0513]],\n              \n                      [[-0.1679]],\n              \n                      [[-0.1117]],\n              \n                      [[ 0.1658]],\n              \n                      [[-0.1830]],\n              \n                      [[-0.1878]],\n              \n                      [[ 0.0674]],\n              \n                      [[-0.1699]],\n              \n                      [[ 0.1329]],\n              \n                      [[-0.2214]],\n              \n                      [[ 0.0732]],\n              \n                      [[ 0.6892]],\n              \n                      [[-0.1213]],\n              \n                      [[ 0.1804]],\n              \n                      [[ 0.0856]],\n              \n                      [[-0.1831]],\n              \n                      [[-0.1098]],\n              \n                      [[-0.2449]],\n              \n                      [[-0.0496]],\n              \n                      [[ 0.1587]],\n              \n                      [[ 0.1046]],\n              \n                      [[ 0.3286]],\n              \n                      [[ 0.1054]],\n              \n                      [[ 0.0635]],\n              \n                      [[-0.2069]],\n              \n                      [[ 0.1889]],\n              \n                      [[ 0.0955]],\n              \n                      [[ 0.0659]],\n              \n                      [[-0.1993]],\n              \n                      [[-0.1157]],\n              \n                      [[ 0.1683]],\n              \n                      [[-0.0836]],\n              \n                      [[-0.1399]],\n              \n                      [[ 0.1259]],\n              \n                      [[-0.1242]],\n              \n                      [[-0.1504]],\n              \n                      [[ 0.0773]],\n              \n                      [[ 0.0861]],\n              \n                      [[ 0.1109]],\n              \n                      [[-0.0745]],\n              \n                      [[ 0.1504]],\n              \n                      [[ 0.1679]],\n              \n                      [[ 0.1548]],\n              \n                      [[-0.1857]],\n              \n                      [[ 0.1371]],\n              \n                      [[ 0.0978]],\n              \n                      [[ 0.0925]],\n              \n                      [[ 0.0716]],\n              \n                      [[-0.1285]],\n              \n                      [[ 0.8405]],\n              \n                      [[-0.1433]],\n              \n                      [[ 0.0919]],\n              \n                      [[ 0.0559]],\n              \n                      [[-0.1180]],\n              \n                      [[-0.0993]],\n              \n                      [[-0.1831]],\n              \n                      [[-0.1976]],\n              \n                      [[-0.0736]],\n              \n                      [[ 0.1885]],\n              \n                      [[-0.1858]],\n              \n                      [[ 0.1128]],\n              \n                      [[ 0.0911]],\n              \n                      [[-0.1299]],\n              \n                      [[-0.1210]],\n              \n                      [[ 0.1181]],\n              \n                      [[-0.2200]],\n              \n                      [[ 0.1634]],\n              \n                      [[ 0.7963]],\n              \n                      [[-0.0620]],\n              \n                      [[ 0.0811]],\n              \n                      [[ 0.1775]],\n              \n                      [[-0.1021]],\n              \n                      [[-0.1335]],\n              \n                      [[-0.1725]],\n              \n                      [[-0.1294]],\n              \n                      [[ 0.0552]],\n              \n                      [[-0.0735]],\n              \n                      [[-0.1642]],\n              \n                      [[ 0.0778]],\n              \n                      [[ 0.1177]],\n              \n                      [[ 0.0854]],\n              \n                      [[-0.1247]],\n              \n                      [[-0.0936]],\n              \n                      [[-0.1130]],\n              \n                      [[-0.2128]],\n              \n                      [[ 0.1156]],\n              \n                      [[ 0.1750]],\n              \n                      [[ 0.0987]],\n              \n                      [[-0.1961]],\n              \n                      [[-0.1673]],\n              \n                      [[ 0.0572]],\n              \n                      [[ 0.1866]],\n              \n                      [[-0.0643]],\n              \n                      [[ 0.0622]],\n              \n                      [[-0.0600]],\n              \n                      [[-0.1026]],\n              \n                      [[-0.1674]],\n              \n                      [[-0.1573]],\n              \n                      [[-0.1231]],\n              \n                      [[ 0.0562]],\n              \n                      [[-0.1144]],\n              \n                      [[ 0.1755]],\n              \n                      [[ 0.1903]],\n              \n                      [[ 0.2018]],\n              \n                      [[-0.0822]],\n              \n                      [[-0.0790]],\n              \n                      [[-0.0834]],\n              \n                      [[ 0.1569]],\n              \n                      [[-0.2193]],\n              \n                      [[-0.0610]],\n              \n                      [[-0.0822]],\n              \n                      [[-0.2230]],\n              \n                      [[-0.1228]],\n              \n                      [[-0.0873]],\n              \n                      [[ 0.1053]],\n              \n                      [[-0.0637]],\n              \n                      [[-0.0549]],\n              \n                      [[ 0.7561]],\n              \n                      [[-0.1014]],\n              \n                      [[ 0.1783]],\n              \n                      [[-0.1061]],\n              \n                      [[-0.1348]],\n              \n                      [[ 0.0857]],\n              \n                      [[-0.2232]],\n              \n                      [[ 0.2273]],\n              \n                      [[ 0.1463]],\n              \n                      [[ 0.0583]],\n              \n                      [[ 0.1453]],\n              \n                      [[-0.1116]],\n              \n                      [[ 0.0745]],\n              \n                      [[ 0.3546]],\n              \n                      [[ 0.1204]],\n              \n                      [[ 0.0817]],\n              \n                      [[ 0.1564]],\n              \n                      [[ 0.1007]],\n              \n                      [[-0.1638]],\n              \n                      [[ 0.1594]],\n              \n                      [[-0.0556]],\n              \n                      [[-0.1627]],\n              \n                      [[-0.0607]],\n              \n                      [[ 0.1851]],\n              \n                      [[-0.2115]],\n              \n                      [[-0.1995]],\n              \n                      [[-0.2102]],\n              \n                      [[-0.8951]],\n              \n                      [[ 0.1493]],\n              \n                      [[-0.1627]],\n              \n                      [[-0.0626]],\n              \n                      [[-0.0937]],\n              \n                      [[-0.1696]],\n              \n                      [[-0.1585]],\n              \n                      [[ 0.2163]],\n              \n                      [[ 0.2095]],\n              \n                      [[-0.0575]],\n              \n                      [[ 0.0768]],\n              \n                      [[-0.0864]],\n              \n                      [[-0.1868]],\n              \n                      [[ 0.1623]],\n              \n                      [[-0.0966]],\n              \n                      [[-0.1214]],\n              \n                      [[ 0.1149]],\n              \n                      [[-0.0785]],\n              \n                      [[-0.0784]],\n              \n                      [[ 0.0438]],\n              \n                      [[ 0.1126]],\n              \n                      [[-0.1596]],\n              \n                      [[ 0.1557]],\n              \n                      [[-0.0556]],\n              \n                      [[ 0.1179]],\n              \n                      [[-0.1984]],\n              \n                      [[-0.1827]],\n              \n                      [[ 0.0854]],\n              \n                      [[-0.1002]],\n              \n                      [[ 0.1982]],\n              \n                      [[-0.0577]],\n              \n                      [[-0.1512]],\n              \n                      [[-0.1488]],\n              \n                      [[-0.1290]],\n              \n                      [[-0.1763]],\n              \n                      [[ 0.1897]],\n              \n                      [[-0.1582]],\n              \n                      [[ 0.1803]],\n              \n                      [[-0.0490]],\n              \n                      [[-0.1390]],\n              \n                      [[ 0.1739]],\n              \n                      [[ 0.0984]],\n              \n                      [[ 0.1237]],\n              \n                      [[-0.6205]],\n              \n                      [[-0.2223]],\n              \n                      [[-0.1352]],\n              \n                      [[ 0.0779]],\n              \n                      [[ 0.0570]],\n              \n                      [[ 0.0638]],\n              \n                      [[ 0.1484]],\n              \n                      [[ 0.1908]],\n              \n                      [[ 0.1608]],\n              \n                      [[ 0.7452]],\n              \n                      [[-0.1366]],\n              \n                      [[-0.1722]],\n              \n                      [[ 0.2053]],\n              \n                      [[-0.0665]],\n              \n                      [[-0.6430]],\n              \n                      [[ 0.2176]],\n              \n                      [[-0.0657]],\n              \n                      [[ 0.0532]],\n              \n                      [[ 0.1491]],\n              \n                      [[-0.0566]],\n              \n                      [[-0.1023]],\n              \n                      [[ 0.0470]],\n              \n                      [[ 0.0652]],\n              \n                      [[ 0.2113]],\n              \n                      [[ 0.1272]],\n              \n                      [[-0.8003]],\n              \n                      [[ 0.2390]],\n              \n                      [[-0.1444]],\n              \n                      [[-0.1852]],\n              \n                      [[-0.0575]],\n              \n                      [[ 0.1888]],\n              \n                      [[-0.0649]],\n              \n                      [[ 0.0871]],\n              \n                      [[-0.1629]],\n              \n                      [[-0.1195]],\n              \n                      [[ 0.1058]],\n              \n                      [[ 0.0653]],\n              \n                      [[-0.0567]],\n              \n                      [[-0.1219]],\n              \n                      [[ 0.1648]],\n              \n                      [[ 0.1953]],\n              \n                      [[ 0.0672]],\n              \n                      [[-0.1997]],\n              \n                      [[-0.1384]],\n              \n                      [[ 0.1356]],\n              \n                      [[-0.0888]],\n              \n                      [[ 0.0142]],\n              \n                      [[-0.0592]],\n              \n                      [[ 0.6713]],\n              \n                      [[-0.0561]],\n              \n                      [[ 0.1219]],\n              \n                      [[-0.0611]],\n              \n                      [[ 0.1076]],\n              \n                      [[-0.1439]],\n              \n                      [[-0.0574]],\n              \n                      [[-0.0649]],\n              \n                      [[-0.1025]],\n              \n                      [[ 0.1751]],\n              \n                      [[-0.0506]],\n              \n                      [[-0.1700]],\n              \n                      [[ 0.0941]],\n              \n                      [[ 0.1488]],\n              \n                      [[ 0.0475]],\n              \n                      [[ 0.1025]],\n              \n                      [[-0.1710]],\n              \n                      [[-0.1420]],\n              \n                      [[ 0.1398]],\n              \n                      [[ 0.1035]],\n              \n                      [[-0.1236]],\n              \n                      [[-0.1841]],\n              \n                      [[-0.1835]],\n              \n                      [[ 0.0630]],\n              \n                      [[ 0.1234]],\n              \n                      [[-0.1260]],\n              \n                      [[-0.1579]],\n              \n                      [[ 0.1045]],\n              \n                      [[ 0.1694]],\n              \n                      [[ 0.0516]],\n              \n                      [[-0.1766]],\n              \n                      [[-0.5089]],\n              \n                      [[-0.1249]],\n              \n                      [[-0.1527]],\n              \n                      [[ 0.2208]],\n              \n                      [[-0.1773]],\n              \n                      [[ 0.1941]],\n              \n                      [[-0.0956]],\n              \n                      [[-0.1140]],\n              \n                      [[ 0.2169]],\n              \n                      [[-0.3098]],\n              \n                      [[-0.1943]],\n              \n                      [[-0.1211]],\n              \n                      [[-0.1937]],\n              \n                      [[ 0.1956]],\n              \n                      [[-0.1267]],\n              \n                      [[-0.1842]],\n              \n                      [[-0.5714]],\n              \n                      [[-0.1634]],\n              \n                      [[-0.0916]],\n              \n                      [[ 0.1886]],\n              \n                      [[ 0.1067]],\n              \n                      [[ 0.1831]],\n              \n                      [[-0.1790]],\n              \n                      [[-0.1201]],\n              \n                      [[ 0.0527]],\n              \n                      [[ 0.1902]],\n              \n                      [[ 0.1832]],\n              \n                      [[ 0.1823]],\n              \n                      [[ 0.1591]],\n              \n                      [[ 0.0948]],\n              \n                      [[ 0.0479]],\n              \n                      [[-0.1210]],\n              \n                      [[ 0.0648]],\n              \n                      [[ 0.2146]],\n              \n                      [[-0.1010]],\n              \n                      [[-0.0441]],\n              \n                      [[ 0.0883]],\n              \n                      [[-0.0667]],\n              \n                      [[-0.1968]],\n              \n                      [[-0.0630]],\n              \n                      [[ 0.1910]],\n              \n                      [[-0.5451]],\n              \n                      [[ 0.1535]],\n              \n                      [[-0.0343]],\n              \n                      [[-0.1739]],\n              \n                      [[-0.0993]],\n              \n                      [[ 0.1708]],\n              \n                      [[ 0.0463]],\n              \n                      [[-0.1080]],\n              \n                      [[-0.1419]],\n              \n                      [[-0.0809]],\n              \n                      [[-0.1146]],\n              \n                      [[-0.1119]],\n              \n                      [[ 0.1049]],\n              \n                      [[-0.4419]],\n              \n                      [[-0.1312]],\n              \n                      [[ 0.0547]],\n              \n                      [[ 0.2102]],\n              \n                      [[-0.0895]],\n              \n                      [[-0.1921]],\n              \n                      [[ 0.1776]],\n              \n                      [[ 0.0904]],\n              \n                      [[ 0.1303]],\n              \n                      [[-0.0776]],\n              \n                      [[ 0.2092]],\n              \n                      [[ 0.1975]],\n              \n                      [[ 0.0984]],\n              \n                      [[-0.1350]],\n              \n                      [[ 0.1663]],\n              \n                      [[ 0.2697]],\n              \n                      [[-0.1503]],\n              \n                      [[-0.1572]],\n              \n                      [[ 0.1362]],\n              \n                      [[ 0.0847]],\n              \n                      [[-0.1417]],\n              \n                      [[-0.0492]],\n              \n                      [[-0.1390]]])),\n             ('features.7.0.block.0.weight',\n              tensor([[[[ 0.0303, -0.0038, -0.0019,  ...,  0.0095, -0.0057,  0.0455],\n                        [ 0.0114, -0.0076, -0.0114,  ..., -0.0057,  0.0057,  0.0019],\n                        [-0.0114, -0.0133,  0.0284,  ...,  0.0303, -0.0133,  0.0019],\n                        ...,\n                        [-0.0095, -0.0133,  0.0265,  ...,  0.0284, -0.0208,  0.0038],\n                        [ 0.0000,  0.0076, -0.0057,  ..., -0.0076,  0.0038,  0.0038],\n                        [ 0.0474,  0.0019, -0.0038,  ..., -0.0095,  0.0019,  0.0588]]],\n              \n              \n                      [[[-0.0778, -0.0194,  0.0043,  ...,  0.0065, -0.0086, -0.0713],\n                        [-0.0259,  0.0130,  0.0043,  ...,  0.0022,  0.0022, -0.0238],\n                        [ 0.0043,  0.0194, -0.0778,  ..., -0.0562,  0.0065,  0.0173],\n                        ...,\n                        [-0.0108, -0.0022, -0.0648,  ..., -0.0518,  0.0043, -0.0043],\n                        [-0.0324,  0.0194, -0.0043,  ..., -0.0022,  0.0022, -0.0108],\n                        [-0.0778, -0.0086,  0.0086,  ...,  0.0043, -0.0216, -0.0929]]],\n              \n              \n                      [[[-0.0066,  0.0033,  0.0215,  ...,  0.0099,  0.0141,  0.0058],\n                        [ 0.0008,  0.0033,  0.0074,  ...,  0.0157, -0.0025, -0.0033],\n                        [-0.0116,  0.0033, -0.0033,  ...,  0.0116, -0.0132, -0.0141],\n                        ...,\n                        [-0.0190, -0.0215, -0.0223,  ..., -0.0116, -0.0116, -0.0323],\n                        [-0.0298, -0.0041,  0.0240,  ...,  0.0182, -0.0058, -0.0166],\n                        [ 0.0017,  0.0050,  0.0157,  ...,  0.0207,  0.0058, -0.0008]]],\n              \n              \n                      ...,\n              \n              \n                      [[[-0.0506, -0.0033, -0.0228,  ..., -0.0228,  0.0082, -0.0457],\n                        [-0.0253,  0.0212, -0.0277,  ..., -0.0106,  0.0098, -0.0179],\n                        [-0.0082, -0.0253, -0.0326,  ..., -0.0408, -0.0171, -0.0139],\n                        ...,\n                        [ 0.0065, -0.0196, -0.0473,  ..., -0.0383, -0.0049, -0.0114],\n                        [-0.0269,  0.0065, -0.0073,  ..., -0.0294,  0.0122, -0.0196],\n                        [-0.0522, -0.0237, -0.0286,  ..., -0.0196, -0.0016, -0.0865]]],\n              \n              \n                      [[[-0.0074,  0.0148, -0.0018,  ...,  0.0166,  0.0037, -0.0055],\n                        [ 0.0018,  0.0055,  0.0074,  ...,  0.0018, -0.0055,  0.0148],\n                        [ 0.0055,  0.0055, -0.0074,  ..., -0.0018,  0.0074,  0.0055],\n                        ...,\n                        [ 0.0092,  0.0018,  0.0018,  ...,  0.0037,  0.0129,  0.0111],\n                        [ 0.0055,  0.0055,  0.0000,  ...,  0.0000,  0.0000, -0.0111],\n                        [-0.0037,  0.0092,  0.0018,  ...,  0.0092,  0.0111,  0.0018]]],\n              \n              \n                      [[[-0.0585, -0.0101, -0.0060,  ...,  0.0121, -0.0020, -0.0564],\n                        [-0.0141,  0.0060,  0.0000,  ...,  0.0000,  0.0040, -0.0161],\n                        [-0.0040,  0.0000, -0.0524,  ..., -0.0464, -0.0101,  0.0000],\n                        ...,\n                        [ 0.0060,  0.0121, -0.0524,  ..., -0.0524,  0.0161, -0.0121],\n                        [-0.0222, -0.0161,  0.0000,  ..., -0.0020,  0.0000, -0.0081],\n                        [-0.0464, -0.0081,  0.0020,  ..., -0.0060, -0.0121, -0.0464]]]],\n                     size=(768, 1, 7, 7), dtype=torch.qint8,\n                     quantization_scheme=torch.per_channel_affine,\n                     scale=tensor([0.0019, 0.0022, 0.0008, 0.0018, 0.0016, 0.0013, 0.0016, 0.0021, 0.0011,\n                      0.0009, 0.0019, 0.0018, 0.0019, 0.0013, 0.0010, 0.0007, 0.0006, 0.0018,\n                      0.0022, 0.0012, 0.0018, 0.0012, 0.0017, 0.0011, 0.0009, 0.0015, 0.0016,\n                      0.0013, 0.0015, 0.0014, 0.0019, 0.0013, 0.0010, 0.0020, 0.0013, 0.0013,\n                      0.0014, 0.0013, 0.0017, 0.0014, 0.0016, 0.0018, 0.0018, 0.0013, 0.0017,\n                      0.0013, 0.0014, 0.0014, 0.0016, 0.0021, 0.0011, 0.0018, 0.0009, 0.0016,\n                      0.0008, 0.0018, 0.0015, 0.0014, 0.0016, 0.0015, 0.0019, 0.0016, 0.0017,\n                      0.0019, 0.0015, 0.0018, 0.0008, 0.0016, 0.0015, 0.0012, 0.0018, 0.0011,\n                      0.0018, 0.0010, 0.0018, 0.0020, 0.0014, 0.0012, 0.0018, 0.0012, 0.0013,\n                      0.0014, 0.0013, 0.0017, 0.0013, 0.0014, 0.0014, 0.0016, 0.0012, 0.0014,\n                      0.0014, 0.0013, 0.0014, 0.0018, 0.0017, 0.0020, 0.0016, 0.0018, 0.0015,\n                      0.0021, 0.0014, 0.0013, 0.0012, 0.0017, 0.0017, 0.0008, 0.0017, 0.0017,\n                      0.0017, 0.0014, 0.0012, 0.0017, 0.0018, 0.0018, 0.0018, 0.0005, 0.0017,\n                      0.0019, 0.0018, 0.0016, 0.0014, 0.0014, 0.0016, 0.0024, 0.0017, 0.0017,\n                      0.0010, 0.0019, 0.0014, 0.0011, 0.0027, 0.0018, 0.0014, 0.0013, 0.0018,\n                      0.0018, 0.0012, 0.0015, 0.0012, 0.0008, 0.0014, 0.0021, 0.0020, 0.0018,\n                      0.0010, 0.0005, 0.0019, 0.0016, 0.0020, 0.0019, 0.0012, 0.0015, 0.0015,\n                      0.0017, 0.0014, 0.0010, 0.0018, 0.0011, 0.0018, 0.0015, 0.0019, 0.0017,\n                      0.0019, 0.0018, 0.0018, 0.0016, 0.0018, 0.0012, 0.0014, 0.0017, 0.0006,\n                      0.0010, 0.0019, 0.0021, 0.0018, 0.0008, 0.0019, 0.0018, 0.0010, 0.0016,\n                      0.0013, 0.0017, 0.0016, 0.0015, 0.0016, 0.0018, 0.0014, 0.0014, 0.0014,\n                      0.0015, 0.0018, 0.0019, 0.0017, 0.0015, 0.0012, 0.0008, 0.0014, 0.0014,\n                      0.0016, 0.0019, 0.0013, 0.0008, 0.0018, 0.0015, 0.0028, 0.0016, 0.0013,\n                      0.0007, 0.0009, 0.0009, 0.0012, 0.0015, 0.0010, 0.0011, 0.0014, 0.0018,\n                      0.0016, 0.0014, 0.0015, 0.0018, 0.0018, 0.0018, 0.0019, 0.0015, 0.0014,\n                      0.0017, 0.0012, 0.0008, 0.0016, 0.0016, 0.0014, 0.0019, 0.0010, 0.0013,\n                      0.0016, 0.0017, 0.0004, 0.0018, 0.0013, 0.0016, 0.0012, 0.0019, 0.0017,\n                      0.0019, 0.0013, 0.0018, 0.0019, 0.0018, 0.0015, 0.0026, 0.0017, 0.0007,\n                      0.0018, 0.0016, 0.0015, 0.0017, 0.0016, 0.0020, 0.0017, 0.0017, 0.0015,\n                      0.0017, 0.0017, 0.0013, 0.0014, 0.0009, 0.0037, 0.0013, 0.0015, 0.0010,\n                      0.0015, 0.0018, 0.0017, 0.0026, 0.0018, 0.0016, 0.0017, 0.0019, 0.0019,\n                      0.0017, 0.0019, 0.0013, 0.0013, 0.0018, 0.0015, 0.0018, 0.0012, 0.0018,\n                      0.0011, 0.0018, 0.0014, 0.0008, 0.0016, 0.0013, 0.0019, 0.0012, 0.0014,\n                      0.0017, 0.0025, 0.0021, 0.0018, 0.0014, 0.0010, 0.0019, 0.0019, 0.0012,\n                      0.0019, 0.0010, 0.0017, 0.0005, 0.0013, 0.0008, 0.0019, 0.0020, 0.0009,\n                      0.0017, 0.0013, 0.0011, 0.0013, 0.0019, 0.0007, 0.0012, 0.0016, 0.0024,\n                      0.0017, 0.0016, 0.0015, 0.0017, 0.0014, 0.0018, 0.0026, 0.0018, 0.0015,\n                      0.0016, 0.0017, 0.0011, 0.0011, 0.0011, 0.0020, 0.0037, 0.0009, 0.0017,\n                      0.0018, 0.0013, 0.0017, 0.0014, 0.0021, 0.0018, 0.0018, 0.0018, 0.0017,\n                      0.0013, 0.0016, 0.0015, 0.0018, 0.0011, 0.0017, 0.0015, 0.0008, 0.0018,\n                      0.0024, 0.0017, 0.0017, 0.0013, 0.0015, 0.0014, 0.0007, 0.0016, 0.0022,\n                      0.0015, 0.0017, 0.0013, 0.0012, 0.0013, 0.0018, 0.0020, 0.0012, 0.0015,\n                      0.0018, 0.0018, 0.0008, 0.0007, 0.0012, 0.0018, 0.0013, 0.0016, 0.0013,\n                      0.0007, 0.0015, 0.0012, 0.0019, 0.0018, 0.0018, 0.0010, 0.0014, 0.0019,\n                      0.0016, 0.0013, 0.0013, 0.0011, 0.0012, 0.0005, 0.0018, 0.0007, 0.0018,\n                      0.0017, 0.0018, 0.0013, 0.0013, 0.0016, 0.0012, 0.0011, 0.0019, 0.0017,\n                      0.0016, 0.0018, 0.0016, 0.0016, 0.0022, 0.0019, 0.0012, 0.0018, 0.0015,\n                      0.0011, 0.0011, 0.0018, 0.0013, 0.0017, 0.0008, 0.0011, 0.0016, 0.0018,\n                      0.0008, 0.0016, 0.0015, 0.0018, 0.0013, 0.0016, 0.0013, 0.0017, 0.0013,\n                      0.0016, 0.0005, 0.0019, 0.0016, 0.0019, 0.0005, 0.0010, 0.0014, 0.0003,\n                      0.0018, 0.0020, 0.0018, 0.0025, 0.0017, 0.0013, 0.0009, 0.0013, 0.0016,\n                      0.0012, 0.0017, 0.0018, 0.0021, 0.0015, 0.0018, 0.0012, 0.0016, 0.0014,\n                      0.0013, 0.0014, 0.0013, 0.0017, 0.0018, 0.0017, 0.0013, 0.0013, 0.0017,\n                      0.0019, 0.0012, 0.0011, 0.0013, 0.0013, 0.0018, 0.0018, 0.0014, 0.0010,\n                      0.0011, 0.0011, 0.0013, 0.0012, 0.0009, 0.0017, 0.0015, 0.0018, 0.0016,\n                      0.0007, 0.0020, 0.0012, 0.0013, 0.0014, 0.0019, 0.0019, 0.0016, 0.0021,\n                      0.0014, 0.0014, 0.0016, 0.0012, 0.0013, 0.0013, 0.0013, 0.0019, 0.0008,\n                      0.0015, 0.0024, 0.0016, 0.0013, 0.0018, 0.0018, 0.0019, 0.0017, 0.0004,\n                      0.0019, 0.0012, 0.0018, 0.0017, 0.0021, 0.0014, 0.0012, 0.0017, 0.0020,\n                      0.0014, 0.0023, 0.0017, 0.0018, 0.0017, 0.0008, 0.0022, 0.0018, 0.0017,\n                      0.0020, 0.0016, 0.0007, 0.0023, 0.0014, 0.0017, 0.0017, 0.0019, 0.0013,\n                      0.0013, 0.0016, 0.0020, 0.0017, 0.0014, 0.0015, 0.0008, 0.0016, 0.0011,\n                      0.0013, 0.0017, 0.0011, 0.0015, 0.0017, 0.0014, 0.0014, 0.0015, 0.0008,\n                      0.0018, 0.0013, 0.0012, 0.0013, 0.0016, 0.0019, 0.0012, 0.0015, 0.0017,\n                      0.0015, 0.0014, 0.0019, 0.0017, 0.0012, 0.0015, 0.0015, 0.0010, 0.0007,\n                      0.0015, 0.0019, 0.0019, 0.0021, 0.0022, 0.0018, 0.0017, 0.0015, 0.0009,\n                      0.0015, 0.0007, 0.0017, 0.0015, 0.0015, 0.0019, 0.0019, 0.0015, 0.0017,\n                      0.0014, 0.0015, 0.0007, 0.0015, 0.0018, 0.0017, 0.0018, 0.0017, 0.0009,\n                      0.0014, 0.0013, 0.0010, 0.0020, 0.0018, 0.0020, 0.0015, 0.0018, 0.0011,\n                      0.0013, 0.0011, 0.0016, 0.0017, 0.0019, 0.0015, 0.0014, 0.0018, 0.0018,\n                      0.0006, 0.0018, 0.0018, 0.0014, 0.0018, 0.0012, 0.0016, 0.0015, 0.0012,\n                      0.0017, 0.0014, 0.0018, 0.0014, 0.0012, 0.0024, 0.0014, 0.0013, 0.0018,\n                      0.0012, 0.0018, 0.0015, 0.0005, 0.0017, 0.0006, 0.0013, 0.0017, 0.0012,\n                      0.0013, 0.0015, 0.0011, 0.0012, 0.0019, 0.0007, 0.0018, 0.0011, 0.0015,\n                      0.0018, 0.0019, 0.0014, 0.0011, 0.0020, 0.0011, 0.0020, 0.0011, 0.0018,\n                      0.0018, 0.0021, 0.0014, 0.0012, 0.0016, 0.0016, 0.0022, 0.0012, 0.0016,\n                      0.0014, 0.0016, 0.0013, 0.0017, 0.0018, 0.0019, 0.0013, 0.0013, 0.0018,\n                      0.0020, 0.0014, 0.0018, 0.0011, 0.0021, 0.0015, 0.0013, 0.0015, 0.0013,\n                      0.0012, 0.0013, 0.0013, 0.0016, 0.0016, 0.0010, 0.0016, 0.0016, 0.0014,\n                      0.0012, 0.0013, 0.0012, 0.0023, 0.0020, 0.0015, 0.0013, 0.0019, 0.0015,\n                      0.0015, 0.0018, 0.0020, 0.0019, 0.0011, 0.0016, 0.0018, 0.0018, 0.0018,\n                      0.0014, 0.0016, 0.0015, 0.0017, 0.0004, 0.0016, 0.0014, 0.0013, 0.0011,\n                      0.0017, 0.0019, 0.0014, 0.0018, 0.0004, 0.0018, 0.0012, 0.0013, 0.0017,\n                      0.0018, 0.0015, 0.0006, 0.0012, 0.0007, 0.0018, 0.0016, 0.0018, 0.0012,\n                      0.0016, 0.0017, 0.0017, 0.0014, 0.0014, 0.0010, 0.0017, 0.0014, 0.0017,\n                      0.0008, 0.0018, 0.0020], dtype=torch.float64),\n                     zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n                     axis=0)),\n             ('features.7.0.block.0.bias',\n              Parameter containing:\n              tensor([ 1.2737e-02, -1.7102e-02,  7.7046e-03,  1.0245e-02, -7.4413e-03,\n                      -1.7452e-03,  1.5048e-03, -9.5317e-03, -3.4619e-03, -1.2828e-02,\n                       9.9090e-03,  2.4463e-02,  1.2996e-02, -1.0984e-03, -1.1110e-01,\n                      -4.4324e-02,  8.5656e-03,  5.4947e-03,  7.2646e-03, -1.3320e-03,\n                       2.1064e-03, -9.9843e-03, -1.2464e-02,  3.0422e-03, -1.9675e-02,\n                      -1.1170e-02, -7.0211e-03,  1.2911e-02,  3.0653e-05,  5.1300e-03,\n                      -3.6069e-02, -1.4732e-02,  8.8932e-03, -8.9173e-03,  8.2237e-03,\n                       2.8844e-03, -1.4126e-02, -7.8765e-03, -1.3687e-02, -2.4024e-03,\n                      -3.3222e-03,  1.8663e-03, -6.1075e-03, -1.2596e-03,  7.6567e-03,\n                      -5.9953e-03, -5.3393e-03, -1.3278e-03, -2.3163e-03,  3.0587e-02,\n                      -1.8819e-02,  1.9753e-02, -4.0407e-03,  1.4183e-02,  1.1675e-02,\n                       1.7527e-03, -1.5013e-02,  5.7898e-03, -3.7805e-02,  8.7917e-03,\n                      -8.8501e-04,  1.6076e-02,  6.7545e-03,  9.5802e-03,  4.0249e-04,\n                       2.5070e-03,  5.7989e-03,  5.6759e-03,  2.1651e-02,  3.8919e-03,\n                       1.1529e-02, -1.1243e-02, -2.3155e-05, -1.0167e-02, -8.9284e-03,\n                       7.3262e-03,  7.8947e-03, -7.0751e-03, -3.5092e-03, -7.5515e-03,\n                       1.0406e-03,  1.4479e-02, -1.5400e-02, -1.3831e-03, -1.2769e-03,\n                       2.3832e-02,  4.1028e-03, -1.9596e-02, -3.8111e-03, -3.6377e-03,\n                      -1.4435e-04,  4.5130e-03, -5.2295e-03, -6.1230e-03,  9.6119e-03,\n                      -3.3619e-03,  1.7632e-02, -5.4556e-03,  1.6140e-02,  1.6612e-02,\n                      -1.1466e-02, -9.8211e-03, -3.6291e-03,  6.3856e-03,  6.7687e-03,\n                      -7.9207e-03, -1.7964e-02, -4.7238e-03, -1.2752e-03, -7.7936e-03,\n                       1.3231e-02,  2.4565e-03,  1.3902e-02,  8.9443e-04, -3.7741e-03,\n                      -1.4303e-02, -3.8112e-03, -2.3366e-02,  1.9572e-02, -5.4163e-03,\n                      -1.5008e-02, -8.2757e-03,  9.9147e-03, -2.0894e-02,  1.3052e-02,\n                      -9.2240e-03,  1.9474e-02, -1.6869e-02, -7.7424e-03, -9.0784e-03,\n                      -7.6642e-02,  3.6896e-03,  4.5774e-03, -1.4543e-02,  7.6684e-03,\n                      -7.9847e-03, -1.0648e-02, -1.0785e-02, -2.9463e-02,  9.0530e-03,\n                      -4.1122e-03, -1.7598e-02, -8.0454e-03,  1.4978e-02,  1.6975e-02,\n                      -1.4098e-02,  1.1191e-02,  7.6890e-03,  1.1917e-02,  1.2354e-02,\n                       6.1884e-04, -7.7360e-03, -1.7990e-02,  8.3765e-03, -1.0177e-02,\n                      -3.5578e-03,  5.1309e-03,  2.0695e-04, -5.1525e-03,  7.6485e-03,\n                       1.0441e-02, -4.6262e-03,  2.4027e-02, -5.2820e-03,  6.4045e-03,\n                      -8.4352e-03,  1.7356e-02,  6.8424e-03,  2.6431e-03,  7.8337e-04,\n                       1.8960e-02, -6.1795e-03,  9.5801e-04,  1.1881e-02,  2.5098e-03,\n                      -2.8491e-02, -4.8368e-03, -3.2894e-02, -1.2360e-02, -9.9781e-04,\n                       1.2129e-02,  6.1623e-03, -4.8507e-03,  1.3578e-02,  4.5167e-03,\n                       4.9768e-03,  1.8533e-02,  8.8884e-03, -9.6034e-03, -2.2449e-02,\n                      -2.0546e-04,  2.0211e-02,  8.4082e-03,  1.2910e-03, -4.8162e-03,\n                       9.2088e-03,  5.1658e-03,  4.0825e-03,  2.2708e-02, -7.1576e-03,\n                       8.5397e-03, -8.6289e-04, -3.0479e-02, -4.5341e-03, -1.4268e-02,\n                       1.9609e-03,  5.0171e-04,  2.1199e-02, -6.4941e-04,  5.6395e-03,\n                       1.8919e-02,  1.0202e-02, -3.5469e-03,  1.6865e-02, -6.2455e-03,\n                      -2.0641e-02,  1.3395e-02,  5.8618e-02,  6.5824e-03,  2.2374e-02,\n                       8.4795e-03,  1.2213e-03,  6.5479e-04, -8.8352e-03,  1.4746e-02,\n                      -5.4252e-03, -7.9013e-03,  1.3441e-02,  5.1071e-04, -1.9341e-05,\n                      -9.6646e-03, -1.2680e-02,  7.6379e-03, -1.5236e-02,  7.5696e-03,\n                       3.4482e-03,  6.8051e-04, -1.7257e-03,  2.9994e-02,  9.8151e-04,\n                      -1.4486e-02,  2.7687e-03, -1.2508e-02,  1.6074e-02, -3.8689e-03,\n                       2.5356e-03, -1.8775e-02,  1.3530e-02, -7.4219e-03, -9.6909e-02,\n                      -1.4126e-02, -1.9194e-02, -1.7388e-03, -5.4401e-03,  1.7591e-02,\n                      -3.6467e-03,  1.5795e-02, -2.1402e-02, -1.7207e-02,  5.9211e-03,\n                      -1.4270e-02, -1.2651e-02, -1.4060e-02,  8.7700e-03, -7.1141e-03,\n                       9.5732e-03, -3.6395e-01, -5.1762e-03, -2.2716e-02, -1.0532e-03,\n                      -1.7270e-02, -4.3434e-03,  1.4752e-02, -2.6819e-03, -1.1832e-02,\n                      -1.8342e-02, -6.5371e-03,  1.6647e-02,  1.8957e-02,  1.1879e-02,\n                       1.5646e-02,  5.2798e-04, -2.1024e-02, -8.8237e-04,  1.7942e-02,\n                       5.7189e-03, -1.3540e-02,  5.9534e-04, -2.1686e-02,  3.2092e-03,\n                       4.3965e-03,  5.1624e-03, -6.4068e-03, -7.8455e-03, -7.8498e-03,\n                       1.3907e-02,  6.2506e-03,  2.4237e-02,  3.6714e-03, -2.0731e-02,\n                       1.1457e-02, -7.5128e-04, -1.9917e-02, -2.1144e-03, -4.8158e-03,\n                      -3.0550e-03,  1.7766e-03, -1.2339e-02, -2.9093e-02,  1.4036e-03,\n                      -2.6228e-02, -6.7427e-03, -2.6908e-02,  5.0681e-03, -9.0873e-03,\n                       3.3057e-03, -2.4486e-03,  1.6310e-02,  1.1112e-02,  1.8702e-02,\n                       7.1110e-03, -1.6553e-03,  1.9168e-03,  1.7745e-02, -3.7938e-03,\n                       3.3002e-03,  1.6302e-02,  1.4607e-03,  2.1059e-04,  5.4464e-03,\n                      -5.5584e-02,  4.9975e-03, -1.2812e-02,  1.7187e-02,  2.6883e-02,\n                      -1.1210e-02,  1.9260e-02,  1.0308e-02,  4.6844e-02,  9.0430e-02,\n                       1.3758e-03,  8.9314e-03, -3.9035e-03,  3.7447e-03,  1.5098e-03,\n                      -1.2895e-02, -1.5646e-02,  6.3890e-03,  4.3273e-03, -5.0184e-03,\n                       1.5226e-02,  1.5695e-02, -9.4564e-03,  5.7374e-03,  9.2379e-03,\n                      -3.5422e-03,  1.4746e-02, -1.7891e-02,  2.0496e-02,  3.2989e-03,\n                       6.4310e-02,  1.7592e-03,  9.3379e-03, -2.4604e-02, -9.5778e-03,\n                       2.3859e-03,  3.2153e-03, -2.6045e-02,  2.9633e-02, -3.9934e-03,\n                       1.7611e-02,  6.9510e-04,  2.1606e-02,  1.6404e-03,  6.5033e-03,\n                       1.8900e-02,  9.7782e-03,  9.3351e-03,  5.4104e-04,  4.3996e-03,\n                      -2.0723e-04,  1.2358e-02,  3.0845e-04,  2.8775e-03,  6.8096e-03,\n                       8.0285e-03, -6.0099e-04,  2.0970e-02,  2.1670e-03, -2.6208e-03,\n                       3.7657e-03,  1.2043e-02,  1.1020e-02,  4.1404e-03,  9.2626e-04,\n                       1.9811e-02,  1.0673e-02, -7.8261e-03, -2.7121e-02,  8.3410e-03,\n                      -3.0632e-03, -2.2929e-03, -1.7901e-02,  6.5421e-03, -9.1648e-03,\n                       4.1543e-03,  1.5404e-02,  4.2285e-03, -1.0407e-02,  1.2807e-02,\n                      -6.8052e-03, -2.9815e-03, -1.3407e-02,  2.0528e-02, -3.0163e-03,\n                      -1.5119e-02, -9.4052e-03,  1.1363e-02,  1.9543e-02, -8.2125e-03,\n                       9.1143e-03,  1.2633e-02,  2.0612e-02, -2.3796e-03,  8.5451e-03,\n                       9.1352e-03, -1.8336e-02,  1.0664e-02,  9.7655e-03,  1.8915e-02,\n                      -9.2903e-03,  1.7167e-02, -2.0261e-02,  2.5643e-02,  4.6756e-03,\n                      -6.4785e-03, -3.5726e-03,  4.6566e-03, -4.8748e-03,  9.8055e-03,\n                       1.2621e-02,  5.4140e-03, -1.2198e-02,  2.0711e-02, -2.0882e-02,\n                      -1.0223e-02, -1.5276e-02,  1.0100e-02, -1.0739e-02,  2.9415e-03,\n                       1.2664e-02, -9.2669e-04,  3.7147e-03, -2.5967e-02, -6.8821e-03,\n                      -1.7792e-02, -3.5224e-02,  3.6760e-03,  8.9440e-03,  1.2767e-02,\n                       8.9249e-03,  1.3857e-02,  6.5755e-03,  1.7853e-02,  1.4945e-02,\n                       1.6616e-02, -1.5994e-02, -1.6036e-03, -2.5568e-04,  4.3795e-03,\n                      -1.1078e-02, -6.6788e-03,  8.0296e-04,  4.2155e-03, -9.6018e-03,\n                      -5.2546e-05, -1.8625e-02,  6.2702e-03,  4.7015e-03,  4.0566e-03,\n                      -5.9138e-03,  4.5312e-03,  1.1413e-02, -2.8960e-02, -1.2979e-02,\n                      -2.3808e-03, -1.6084e-03, -7.1045e-03,  1.4359e-03,  1.1464e-02,\n                       6.5928e-03,  8.8674e-03, -1.1700e-02,  1.2157e-02, -2.7507e-02,\n                       2.1581e-02, -7.8313e-03, -1.9388e-02,  6.2414e-03,  5.8894e-03,\n                      -1.1257e-02, -3.4961e-03, -3.6284e-03,  2.4230e-02,  5.6813e-03,\n                       6.4486e-03,  8.3056e-03,  1.5025e-02, -3.1250e-03, -6.0882e-03,\n                       2.0015e-03,  2.7420e-02,  2.1800e-02,  6.4676e-03, -1.3954e-02,\n                      -4.8114e-03,  1.7927e-02, -4.2574e-03,  8.2129e-03, -5.0774e-03,\n                      -1.1734e-02,  7.1388e-04, -1.8993e-02, -1.0533e-02,  8.6753e-03,\n                       3.3579e-03,  1.5554e-02, -1.2937e-02, -4.5545e-03,  3.1987e-03,\n                       7.6392e-03,  4.1571e-03, -2.9213e-02, -7.8372e-03, -3.7922e-03,\n                       1.1019e-03, -8.0134e-03,  3.1627e-02,  4.3904e-03, -1.3154e-02,\n                      -1.9124e-02, -4.3093e-03, -3.7429e-02,  4.2020e-04, -5.3456e-04,\n                      -1.1365e-02, -4.4464e-04,  8.4113e-03,  4.9867e-03, -1.5558e-02,\n                       1.1832e-03,  1.1458e-02,  8.6057e-03, -3.3191e-02,  8.5716e-03,\n                      -1.4547e-02,  8.1368e-03, -3.3334e-03,  8.2081e-03, -6.6143e-03,\n                       8.3278e-03, -1.6872e-02,  9.2373e-03,  1.3815e-02,  1.6434e-02,\n                       2.0632e-02,  2.0504e-02,  4.1308e-02, -1.0435e-03,  2.5827e-03,\n                      -7.2212e-03,  1.9761e-02,  1.5737e-02, -1.7019e-02, -1.3510e-02,\n                       5.6517e-03,  1.1500e-02,  1.2037e-03, -9.4217e-03,  5.7097e-04,\n                       1.0404e-02, -7.1982e-04, -1.3748e-02, -2.7286e-02,  5.7029e-02,\n                       2.3963e-02, -3.5437e-03, -5.2798e-03,  2.0643e-03,  5.6512e-02,\n                       1.0066e-02, -7.5380e-03,  2.9434e-03, -6.8405e-03, -4.2923e-03,\n                      -2.7908e-02,  3.8244e-03,  1.8645e-03, -1.4565e-02,  1.2146e-03,\n                       1.8594e-02,  9.7630e-03, -3.8968e-02,  1.9221e-02, -3.7023e-03,\n                      -1.4690e-02, -1.2446e-02, -3.0365e-03, -1.3830e-02, -1.1954e-02,\n                       1.0308e-03,  5.1890e-02, -5.8051e-03,  6.1861e-03, -2.4100e-02,\n                      -1.8610e-02, -2.2384e-03,  1.0989e-02,  9.4129e-03,  7.8866e-03,\n                      -1.0476e-02, -1.5456e-02,  1.2829e-02, -2.9653e-02,  2.0868e-02,\n                       8.2776e-03,  8.3132e-03, -2.6635e-03,  1.4794e-03,  3.5228e-03,\n                      -2.6926e-02,  1.1066e-02, -3.5945e-03,  5.3323e-03, -4.9283e-03,\n                      -6.5309e-03, -1.8563e-02,  8.6412e-03,  2.2831e-03,  4.1398e-04,\n                      -1.3978e-03,  8.7170e-03, -1.7039e-02,  5.7851e-03, -1.6439e-02,\n                       2.2372e-03,  1.5603e-03, -2.6050e-02,  5.5739e-03,  3.4508e-03,\n                      -7.7658e-03,  9.0277e-03, -9.6553e-03,  8.5129e-03,  8.9795e-03,\n                      -2.0853e-02, -3.9402e-04, -1.7817e-02,  3.0994e-03,  3.4827e-03,\n                       4.5711e-03, -8.0283e-03, -2.2325e-03,  2.8209e-03,  2.0048e-02,\n                       3.1858e-03, -1.0601e-02, -1.2308e-02,  3.1831e-03, -5.0631e-03,\n                       3.4320e-04,  8.2073e-03, -1.9019e-02,  1.5223e-02,  1.0857e-02,\n                      -7.1682e-03, -2.7271e-02,  2.5221e-03,  1.1262e-02,  9.3124e-04,\n                      -1.1881e-02, -7.2257e-03, -4.5772e-03,  2.1559e-03, -1.4613e-02,\n                       7.0467e-03,  3.1289e-02,  2.3960e-02,  2.8767e-02, -1.1630e-02,\n                       7.0608e-03,  8.5722e-03, -1.8546e-03,  4.2318e-02,  1.7982e-03,\n                      -7.0638e-03, -2.8380e-03, -2.7325e-02, -8.3127e-03,  6.7688e-03,\n                      -6.3372e-03,  2.2298e-04,  5.7583e-03, -8.8982e-03, -1.5939e-03,\n                      -8.8576e-03,  1.0793e-02,  4.0403e-02,  1.2483e-02, -1.4751e-02,\n                      -5.0050e-03, -5.2711e-03, -4.0286e-03,  1.2170e-02,  3.0779e-02,\n                      -8.4417e-03, -4.0184e-03,  4.6053e-03, -3.1463e-02,  6.0529e-03,\n                      -1.3239e-02,  4.7729e-03,  1.9215e-02, -1.6342e-02,  2.5495e-02,\n                      -7.3281e-03, -2.2856e-03,  2.0494e-03, -1.1367e-02, -3.9257e-03,\n                       1.3844e-02,  3.1981e-03,  1.0650e-02, -2.2148e-02,  4.2598e-03,\n                       9.8271e-04,  5.2608e-03,  3.5684e-03, -1.3290e-02,  2.3464e-02,\n                       2.7204e-03,  2.0970e-02, -1.7434e-02,  7.9224e-03, -2.8269e-03,\n                      -2.0201e-03,  3.7146e-03, -2.7545e-03, -2.4622e-03, -4.5318e-03,\n                      -7.9934e-03, -3.0354e-03, -3.2738e-03, -2.0031e-02, -1.3407e-02,\n                      -3.0883e-03,  1.2650e-02, -4.7582e-03,  6.6171e-03, -1.1517e-03,\n                      -8.3140e-03, -5.2228e-03, -1.4416e-02,  1.0120e-02, -2.7826e-03,\n                       3.8026e-02, -1.3605e-02,  2.7544e-02], requires_grad=True)),\n             ('features.7.0.block.0.scale', tensor(0.0226)),\n             ('features.7.0.block.0.zero_point', tensor(86)),\n             ('features.7.0.block.2.weight',\n              tensor([2.6375, 1.6510, 2.4716, 3.0858, 3.8825, 3.1142, 3.2604, 1.6586, 2.5713,\n                      4.1030, 4.4811, 1.5422, 2.5908, 2.9314, 1.7266, 2.3304, 2.4762, 3.8598,\n                      3.0932, 2.7170, 3.8180, 2.7075, 2.6633, 2.9543, 1.6899, 3.7921, 3.6675,\n                      2.4975, 3.6735, 3.3487, 1.6249, 3.3279, 2.8576, 1.9249, 3.1792, 2.8608,\n                      3.0191, 2.8955, 2.7360, 2.9720, 3.8259, 4.1772, 3.1560, 2.7889, 1.4374,\n                      1.8155, 2.9075, 3.2935, 2.1114, 2.2458, 3.1115, 2.0006, 3.0492, 2.4946,\n                      2.9037, 2.3963, 4.1257, 2.5663, 1.5729, 1.5563, 3.6098, 1.4585, 2.1578,\n                      3.0467, 2.6081, 3.6665, 3.0386, 3.6177, 1.6274, 2.5628, 3.9673, 2.8288,\n                      2.9459, 2.8996, 2.5598, 3.4399, 3.5265, 3.0227, 3.5303, 2.9132, 2.9788,\n                      3.3008, 2.7563, 3.4192, 2.7976, 2.0686, 2.7365, 1.4465, 2.7938, 3.4164,\n                      3.2446, 2.8299, 3.3010, 2.8306, 3.3287, 2.2710, 3.4780, 3.3444, 3.8991,\n                      1.8102, 2.8592, 3.2361, 2.7419, 4.1077, 1.9490, 2.3802, 4.3665, 3.3318,\n                      2.9664, 2.8282, 2.3914, 2.8733, 3.3743, 2.4729, 3.2912, 3.3960, 3.5031,\n                      2.0133, 2.5964, 2.9138, 3.4414, 2.6178, 3.4552, 1.5804, 3.5212, 3.3185,\n                      3.0196, 2.8292, 3.1650, 2.9836, 1.2328, 3.7651, 3.4081, 2.9222, 1.8353,\n                      3.0893, 2.9744, 3.7456, 3.1169, 2.5837, 3.4527, 1.4746, 2.7926, 3.4202,\n                      2.9241, 3.0715, 2.2013, 2.0314, 3.4293, 3.7209, 2.5073, 3.8662, 1.9372,\n                      2.3872, 1.8647, 2.5934, 3.2346, 2.8776, 2.8351, 3.1781, 3.1480, 1.9826,\n                      2.8782, 3.8316, 3.2291, 3.9875, 4.1945, 3.1732, 2.8100, 2.2382, 2.6012,\n                      3.0094, 2.6720, 2.1274, 1.3618, 2.7540, 2.8813, 3.6135, 2.8595, 4.1378,\n                      2.7011, 2.3070, 3.9489, 2.5117, 2.2310, 3.0728, 2.8009, 3.2149, 3.5329,\n                      3.5009, 3.3987, 2.5290, 3.2558, 2.7038, 2.0429, 2.7944, 2.8814, 3.5139,\n                      2.2204, 3.4009, 2.9373, 2.8758, 1.6756, 3.4949, 1.9180, 3.4861, 3.1465,\n                      2.9783, 2.8308, 2.5297, 3.3504, 1.7853, 2.9550, 2.7488, 3.6418, 4.4989,\n                      2.2926, 2.4557, 4.1723, 1.6121, 3.6444, 3.5027, 3.1373, 3.6455, 3.3663,\n                      3.7513, 2.9540, 2.7261, 1.0669, 1.8086, 3.3345, 2.8698, 2.6279, 2.4753,\n                      3.7316, 2.4005, 2.5003, 2.8116, 2.6027, 3.4859, 3.0193, 2.9256, 2.7482,\n                      2.2725, 2.9969, 2.4696, 1.6563, 1.9004, 3.2685, 1.4444, 1.8153, 2.6971,\n                      2.9934, 3.9632, 2.7728, 3.9967, 3.6921, 2.6791, 2.6318, 3.6807, 3.4185,\n                      1.9464, 2.5073, 2.8545, 4.6364, 2.9048, 0.4036, 3.2094, 3.2361, 2.9092,\n                      3.5809, 2.8924, 1.7292, 1.4990, 2.9135, 2.8450, 2.9368, 3.3272, 3.6052,\n                      2.7943, 2.9071, 2.8678, 2.6083, 3.0493, 3.2145, 4.0485, 2.8212, 2.7221,\n                      3.3737, 3.2250, 3.4847, 2.7858, 3.8402, 1.6520, 3.4928, 2.6370, 3.0325,\n                      3.7213, 1.9830, 3.2591, 2.6601, 2.8685, 1.5241, 2.7234, 2.9699, 2.9055,\n                      2.7646, 2.4246, 3.9420, 2.5202, 1.7857, 2.3592, 1.7522, 2.0179, 2.6130,\n                      2.3714, 2.7110, 2.2533, 2.3984, 2.0173, 3.1340, 2.5584, 3.6358, 2.7947,\n                      3.2830, 4.0202, 3.1429, 3.2774, 2.6633, 3.1597, 1.4847, 3.6734, 3.8185,\n                      4.4610, 2.2007, 2.0715, 2.8875, 2.6868, 1.7833, 0.9648, 2.3381, 4.2237,\n                      3.0816, 3.0523, 3.5816, 3.6234, 3.3904, 3.8810, 3.1028, 4.0080, 2.6948,\n                      3.3794, 3.8902, 2.8252, 2.9406, 1.6422, 3.5057, 3.2168, 3.0609, 3.5939,\n                      1.2936, 4.1056, 1.8668, 3.4245, 3.4644, 2.3568, 3.1129, 1.7409, 2.3912,\n                      3.0761, 3.8039, 2.9308, 2.9858, 3.3690, 2.4926, 3.0660, 3.1386, 3.6237,\n                      2.6062, 2.7475, 2.4380, 2.3466, 2.7391, 3.8751, 2.7564, 4.1849, 2.8135,\n                      2.7125, 3.6577, 2.6373, 2.2085, 3.7340, 3.3158, 1.6393, 2.7439, 4.4416,\n                      3.7627, 2.6885, 2.9849, 2.6298, 2.5484, 2.8741, 1.6776, 2.7928, 1.5098,\n                      3.1130, 2.6108, 2.4981, 2.6988, 1.7654, 2.6601, 2.6718, 3.8714, 1.7088,\n                      4.0820, 4.0178, 3.7062, 4.0205, 2.3307, 2.2794, 3.6117, 2.5483, 1.6777,\n                      1.5805, 2.5756, 2.5294, 2.8567, 2.5221, 3.1605, 2.6925, 3.6306, 2.6231,\n                      2.5108, 3.9325, 3.8241, 3.0681, 2.8556, 3.6972, 2.6558, 3.2905, 1.2978,\n                      2.4987, 2.3792, 3.1879, 3.7988, 3.0031, 2.6196, 2.6353, 3.1568, 2.3842,\n                      1.9772, 3.2709, 2.9369, 1.3149, 1.9734, 3.5538, 2.7592, 3.0262, 2.0273,\n                      3.1025, 3.2804, 3.3801, 3.1246, 3.6316, 2.3369, 2.5868, 3.5630, 2.9693,\n                      2.8382, 3.9461, 3.0397, 1.9941, 3.9404, 1.8864, 2.8192, 3.1388, 2.4817,\n                      3.0916, 2.6627, 2.2028, 1.7758, 2.9433, 2.9341, 1.9060, 2.7207, 2.1023,\n                      1.9591, 3.0128, 2.1395, 3.0289, 2.7473, 3.3811, 2.1873, 3.0471, 2.3757,\n                      2.7061, 2.1657, 3.3505, 2.4410, 3.6427, 4.3012, 3.4986, 3.7156, 2.7898,\n                      2.2042, 3.0433, 3.6205, 2.4743, 2.9784, 2.8435, 3.0931, 2.3236, 2.5485,\n                      3.2859, 1.5973, 4.1454, 3.3722, 2.9794, 3.5673, 2.7204, 2.3446, 2.5575,\n                      1.7891, 1.7312, 3.4227, 3.6141, 1.0795, 2.8102, 2.6674, 3.9389, 2.1479,\n                      3.5423, 1.2828, 2.1691, 3.9333, 4.6711, 3.1560, 1.7282, 2.9622, 3.9570,\n                      3.7208, 3.8904, 3.3346, 1.8378, 3.3749, 2.7903, 2.1288, 3.2101, 2.9522,\n                      1.6114, 3.5672, 3.0962, 2.3139, 1.7433, 3.4908, 2.4717, 4.4517, 2.4739,\n                      3.2679, 3.1938, 2.9071, 1.6742, 2.7592, 2.9921, 3.4137, 3.4712, 2.4846,\n                      1.8301, 2.7705, 3.9798, 3.7641, 3.5655, 3.0668, 2.8499, 3.7172, 1.4585,\n                      2.9815, 3.0543, 3.2658, 3.4858, 2.8603, 3.7098, 3.7054, 2.6433, 3.8722,\n                      1.8950, 3.3077, 3.2813, 2.9621, 1.7287, 2.8836, 1.7395, 4.1378, 2.8841,\n                      3.4179, 2.2433, 3.2563, 3.7269, 1.7722, 3.0843, 1.7083, 3.4681, 1.9458,\n                      3.3870, 3.7962, 2.9763, 2.8079, 4.4609, 1.7135, 2.6076, 3.1071, 2.2890,\n                      3.4657, 2.9533, 2.9333, 2.9921, 2.7752, 2.8484, 2.0730, 2.8428, 3.0615,\n                      2.7850, 3.1344, 2.0863, 1.6494, 3.3929, 2.6702, 2.9944, 3.4113, 2.7138,\n                      3.9336, 3.3205, 3.3116, 3.7074, 3.4895, 3.4312, 3.8349, 3.4666, 2.6512,\n                      2.4828, 2.6077, 4.6550, 3.0936, 2.9238, 1.8373, 3.4950, 2.6759, 2.1894,\n                      2.7546, 2.5389, 3.6450, 2.8344, 2.2728, 2.9110, 3.0944, 1.8953, 4.1198,\n                      2.5827, 3.7125, 2.3386, 2.3726, 3.4452, 3.7181, 2.6364, 3.1030, 3.4042,\n                      4.0015, 2.8362, 2.9578, 1.7769, 3.2472, 3.2821, 2.3306, 2.5786, 4.3447,\n                      2.0746, 1.5693, 3.1876, 3.0370, 3.9000, 2.0654, 1.9500, 2.7006, 3.6029,\n                      2.7984, 3.7668, 2.9776, 2.1701, 1.5453, 3.3268, 2.7277, 2.9538, 3.1046,\n                      1.8186, 2.7997, 3.0152, 2.8423, 1.9217, 1.9053, 2.9258, 4.5846, 2.7180,\n                      3.1034, 3.2163, 2.8941, 2.7437, 3.2233, 1.4212, 3.8147, 2.7319, 3.7662,\n                      2.5622, 2.8770, 1.5082, 2.1333, 3.3317, 3.7790, 2.0118, 1.6337, 3.5479,\n                      1.8136, 3.7131, 2.4360, 3.0024, 3.2535, 2.3847, 3.0742, 3.2420, 3.8787,\n                      3.4566, 3.8925, 2.6999, 2.3890, 2.9942, 4.1720, 2.9214, 2.9307, 2.3604,\n                      4.0578, 3.0139, 3.1273, 2.8911, 2.2685, 3.1101, 3.3771, 2.8568, 3.4960,\n                      3.6460, 3.6646, 3.4249, 2.7048, 2.9677, 3.1842, 2.1619, 4.1062, 3.1625,\n                      3.7662, 3.2421, 1.8560, 3.6906, 2.7760, 2.6197, 3.0613, 1.8774, 2.6464,\n                      1.9871, 3.7565, 1.8469])),\n             ('features.7.0.block.2.bias',\n              tensor([-1.6034e+00,  2.3393e+00, -3.0668e-01, -9.0465e-01,  4.7690e-01,\n                      -1.7972e-02, -1.0085e-01,  2.5085e+00,  2.0619e-01, -6.2841e-01,\n                      -1.4655e+00, -1.5637e+00, -2.3535e+00, -6.2271e-02, -1.4881e+00,\n                       1.6384e+00, -5.4586e-01, -4.3269e-01, -3.6853e+00,  4.6666e-02,\n                      -1.4754e-01,  3.5928e-01,  2.0820e+00, -4.1096e-02, -4.5115e-01,\n                       5.2871e-01,  5.3521e-01, -6.0855e-01, -8.4197e-02, -1.4573e-01,\n                       2.8788e+00,  6.8273e-01, -4.8016e-01,  1.9018e+00, -5.1514e-01,\n                      -2.8991e-01,  1.0341e+00,  3.1555e-01,  5.9910e-01,  5.0718e-02,\n                       3.9598e-01, -3.9683e-01,  7.4525e-02, -4.1654e-02, -5.9993e-02,\n                       7.2469e-01,  2.9871e-01,  1.4382e-01,  3.1515e-01, -2.1186e+00,\n                       7.7733e-01, -1.3071e+00,  2.0702e-01, -1.4759e+00, -7.2045e-01,\n                      -6.3572e-01,  8.0870e-01, -2.7549e-01,  1.6430e+00,  1.1483e-01,\n                      -2.3157e-01,  1.1627e+00, -3.0338e-01, -8.2171e-01, -1.5051e-02,\n                      -1.5170e-01, -7.5553e-02, -4.4309e-01, -1.3472e+00, -2.0728e-01,\n                      -7.5079e-01,  6.0446e-01,  8.3478e-02,  4.2186e-01,  3.2861e-01,\n                       2.9759e+00, -7.0053e-01,  2.3019e-01,  5.0612e-01,  4.4521e-01,\n                      -1.8815e-01, -9.0677e-01,  9.2528e-01, -1.6619e-01, -7.5612e-03,\n                      -1.2816e+00, -2.0234e-01,  2.0993e+00,  1.9543e-01,  3.0499e-01,\n                      -9.6392e-02, -3.0332e-01,  1.7339e-01,  4.4033e-01, -8.3228e-01,\n                       2.1346e+00, -1.1034e+00,  1.0590e+00, -1.0965e+00, -2.4295e+00,\n                       6.9633e-01,  6.2081e-01,  2.5089e-01, -8.2882e-01, -3.0604e-01,\n                       3.0757e-01,  1.3274e+00,  1.6306e-01,  1.2452e-01,  3.5083e-01,\n                      -6.2715e-01, -1.5495e-01, -2.3013e+00,  7.1988e-02,  1.4944e-01,\n                       2.6794e-01,  1.6809e-01,  1.7274e+00, -1.1551e+00,  1.4402e+00,\n                       7.0308e-01,  3.9209e-01, -6.5842e-01, -2.9551e+00, -9.1045e-01,\n                       1.4497e+00, -7.8153e-01,  9.3487e-01,  2.5840e-01,  5.1870e-01,\n                      -2.1934e+00, -5.7673e-01, -3.0499e-01,  7.6751e-01, -8.6859e-01,\n                       6.6228e-01,  4.0438e-01,  5.9961e-01,  1.4621e+00, -5.6849e-01,\n                       2.5554e-01,  2.1000e+00, -3.4130e+00, -1.4353e+00, -1.1429e+00,\n                       4.5295e-01, -1.1843e+00, -8.9125e-01, -2.5845e+00, -1.3122e+00,\n                      -1.2302e-02,  5.4200e-01,  1.7242e+00, -6.0263e-01,  1.6894e-02,\n                       6.0450e-02, -4.4166e-01, -7.3941e-02,  2.8312e-01, -5.4011e-01,\n                      -1.7514e+00,  2.8211e-01, -2.0484e+00,  7.0055e-01, -5.0304e-01,\n                       8.1497e-01, -1.1261e+00, -4.6664e-01,  2.3527e-02,  4.0855e-02,\n                      -1.0829e+00,  1.6568e-01, -1.0793e+00, -3.0783e+00, -7.2157e-01,\n                       7.5316e-01,  4.7919e-01, -7.7051e-01,  6.7567e-01, -1.8108e-01,\n                      -1.0290e+00, -4.0914e-01,  3.0205e-01, -4.2185e-01, -1.5032e-01,\n                      -6.0502e-01, -1.0451e+00, -6.7814e-01,  2.4165e-01,  1.3286e+00,\n                      -1.8391e-01, -1.5323e+00, -5.7637e-01, -2.6172e-01,  6.6110e-01,\n                      -6.1859e-01, -2.0772e-01, -4.9382e-01, -1.3097e+00,  7.2787e-01,\n                      -6.2125e-01,  2.1046e-01,  2.4177e+00,  1.2176e-01,  4.9748e+00,\n                      -2.6510e-01,  1.7324e-01, -1.1024e+00,  4.6185e-01, -4.0174e-01,\n                      -1.1823e+00, -1.1828e-01,  4.3009e-02,  8.5698e-01, -1.4331e-01,\n                       1.8528e+00, -1.3825e+00, -2.3789e+00, -1.1512e+00, -1.6505e+00,\n                      -5.2581e-01, -4.3070e-01,  4.7166e-01,  7.1094e-01, -6.4931e-01,\n                       2.3645e-01,  5.2609e-01, -4.4771e-01, -4.3877e-01, -1.4624e-01,\n                       4.6422e-01,  9.1999e-01, -4.8751e-01,  9.8487e-01, -5.1766e-01,\n                      -1.3768e-01, -3.1512e-02,  1.6412e-01, -1.3158e+00, -1.9104e-01,\n                       6.7853e-01, -1.8074e+00,  5.9365e-01, -1.4210e+00, -1.3281e-01,\n                      -1.6759e-01,  2.5367e+00,  7.7945e-01,  2.4939e-01, -1.4369e+00,\n                       2.2001e-01,  8.1035e-01,  1.4360e-01,  2.5870e-01, -8.9712e-01,\n                       5.0410e-01, -9.2443e-01,  1.5622e+00,  1.5066e+00, -2.1282e-01,\n                       8.9658e-01,  2.0699e+00,  2.8022e-01, -6.3763e-01,  3.9282e+00,\n                      -4.8769e-01,  3.0082e+00,  4.5674e-01,  1.4148e+00, -5.2903e-02,\n                       9.0751e-01,  4.8742e-01, -1.9755e+00, -3.4479e+00,  6.8912e-01,\n                       7.3896e-01,  5.5085e-01, -2.2116e+00, -1.9685e+00, -7.7524e-01,\n                      -4.4908e+00, -5.4938e-02,  8.9254e-01,  6.0240e-02, -1.0515e+00,\n                      -1.0490e+00,  7.7476e-01, -3.6073e-01,  6.3214e-01, -6.1538e-01,\n                      -3.3886e-01, -6.3967e-01,  5.2979e-01,  9.6599e-01,  2.4031e+00,\n                      -8.4914e-01, -3.0277e-01, -1.5314e+00,  3.4512e+00, -4.8118e+00,\n                      -1.1896e+00,  1.6193e-03,  6.0833e-01,  4.5580e+00,  3.8321e-01,\n                      -1.8807e-01, -2.8953e-01, -1.1306e+00,  1.7296e+00,  3.9857e-02,\n                       1.7966e+00,  2.1943e-01,  2.7863e+00, -4.4931e-01,  3.1564e-01,\n                       2.5705e-01,  9.4366e-02, -9.4455e-01, -4.1286e-01, -1.4698e+00,\n                      -3.7249e-01,  8.1875e-02, -3.0879e-01,  5.8610e+00,  3.5900e-02,\n                      -1.3221e-01, -1.1205e+00, -9.6215e-02, -1.8235e-01, -7.5313e-01,\n                       3.6413e+00, -1.2610e+00,  8.5867e-01, -1.9888e+00, -1.9124e+00,\n                       3.8278e-01, -1.1719e+00, -5.1389e-01, -3.1989e+00, -3.6919e+00,\n                      -8.6005e-02, -8.1201e-01,  3.2593e-01, -2.6343e-01,  1.1270e-01,\n                       9.4286e-01,  3.7721e+00, -4.2370e-01, -9.2683e-01,  5.6574e-01,\n                      -9.8798e-01, -9.2792e-01,  6.1397e-01, -4.1672e-01, -5.7454e-01,\n                      -4.6420e-01, -7.2543e-01,  7.8132e-01, -9.7197e-01, -8.0549e-01,\n                      -3.4563e+00, -3.2356e-01, -3.2179e-01,  1.5262e+00,  4.2080e-01,\n                      -3.1344e-01, -1.9372e-01,  9.2364e-01, -4.1163e+00,  1.5397e-01,\n                      -1.3259e+00, -1.1725e-01, -1.0972e+00, -1.8352e-02,  5.6442e-01,\n                      -3.3990e+00, -4.8500e-01, -6.6957e-01, -1.7688e+00, -1.2268e+00,\n                      -1.7472e-02, -3.7935e-01, -1.4161e-01, -5.8127e-01, -4.4428e-01,\n                      -5.4048e-01, -9.7872e-02, -1.0666e+00, -2.5805e-01,  1.0494e-01,\n                       2.3828e-01, -1.2169e+00, -9.1142e-01,  8.2134e-01, -3.0413e-01,\n                      -2.1887e+00, -6.0831e-01,  3.8295e-01,  1.2660e+00, -4.0689e-01,\n                       2.0285e-01, -1.2528e-01,  9.3126e-01, -3.7280e-01, -9.8440e-01,\n                      -2.9406e-01, -8.3501e-01, -1.4139e-01,  5.8373e-01, -1.1923e+00,\n                       3.8102e-01,  2.0996e-01,  2.1131e+00, -1.9304e+00, -1.1426e-01,\n                       1.5567e+00,  3.9653e-01, -7.8440e-01, -2.8630e+00,  1.1329e+00,\n                      -7.4143e-01, -1.0467e+00, -1.3350e+00, -2.0519e-01, -1.1158e-01,\n                      -1.6675e+00,  1.0087e+00, -5.8433e-01, -3.7679e-01, -8.7522e-01,\n                       3.9460e-01, -1.9108e+00,  8.3730e-01, -1.6418e+00, -3.4965e-01,\n                       4.0907e-01,  2.3585e-01, -3.2938e-01,  1.0534e-01, -6.8960e-01,\n                      -4.9172e-01, -4.3341e-01,  6.0832e-01,  2.6335e+00,  1.2124e+00,\n                       1.1373e+00,  1.4020e+00, -5.0835e-01,  2.7380e-01,  2.6408e-01,\n                      -1.2770e+00, -1.6869e+00,  3.4191e-02,  4.0906e+00,  1.1230e+00,\n                       9.8688e-01,  1.9694e+00, -3.6543e-01, -1.1443e+00, -8.9659e-01,\n                      -5.3734e-01, -1.2273e+00, -3.5576e+00, -1.2381e+00, -9.4728e-01,\n                      -7.8865e-01,  9.3381e-01,  1.0954e-01,  6.0184e-02, -5.6516e-01,\n                       1.0508e+00,  1.1120e+00,  2.3004e-01, -4.6059e-01,  4.3843e-01,\n                      -1.4886e-01,  1.5520e+00, -2.1143e+00, -2.4055e-01, -2.3793e-01,\n                       4.0578e-01, -3.7879e-01, -1.7040e+00,  2.0331e+00,  6.1786e-01,\n                       1.4881e+00,  7.4749e-01,  4.3875e-01, -1.6698e-02, -8.4839e-01,\n                      -5.0969e-01, -6.0901e-01,  1.5168e+00, -8.9672e-01,  2.4381e+00,\n                      -1.1142e+00,  7.2480e-01,  7.5223e-01, -4.1415e-01, -2.4777e-01,\n                       1.6764e+00, -8.3563e-04,  4.0308e-02, -2.5508e+00, -4.3953e-01,\n                      -3.1042e-01, -5.9520e-01, -5.5346e-01,  8.2119e-02,  3.6350e-01,\n                      -9.6388e-02, -1.3163e+00, -8.8462e-01, -2.7878e-01, -3.6028e+00,\n                       5.1582e-01, -9.8768e-01,  3.6685e-01, -7.2647e-01,  5.6222e-02,\n                       1.1180e+00,  1.6773e-02,  2.1711e+00,  4.9812e-01, -1.3713e+00,\n                      -1.8516e-01, -3.8046e-01,  7.2832e-01,  2.3731e-01, -4.4207e-01,\n                      -2.4842e+00, -2.1342e+00, -7.1649e-01,  6.5264e-01,  2.2691e-01,\n                      -3.4622e-01,  5.0135e-01, -2.4473e+00, -3.2638e-01,  7.7632e-01,\n                       1.6451e+00,  1.3748e-01,  1.5903e+00,  2.9446e+00, -8.2300e-02,\n                       2.1119e+00,  3.8552e-01, -9.2947e-01, -3.1262e-01,  4.6787e-01,\n                       4.0099e-01, -1.5402e+00, -6.8832e-01,  1.1109e+00, -4.0810e-01,\n                       5.8166e-01, -6.3803e-01,  5.4914e-02, -6.8222e-01,  3.0892e-01,\n                      -5.4112e-01,  6.8965e-01, -1.0260e+00, -8.1118e-01, -8.6764e-01,\n                      -1.1924e+00, -1.3521e+00, -2.3831e+00, -2.6982e-02, -6.2921e-01,\n                       5.7841e-01, -1.0859e+00, -1.6476e+00,  9.8637e-01,  6.5837e-01,\n                      -2.9066e-01, -7.5729e-01, -1.7393e-01,  1.3642e+00, -5.1995e-01,\n                      -1.8346e+00, -7.2330e-02,  6.7245e-01,  1.1280e+00, -2.5135e+00,\n                      -9.2023e-01,  4.5349e-01, -2.9824e-01, -3.3653e+00,  2.0925e+00,\n                      -6.7752e-01,  2.3641e+00, -2.0935e-01,  3.1638e-01,  2.9147e-01,\n                       9.7402e-01, -2.1562e-01, -1.6055e-01,  7.1942e-01, -4.7768e-01,\n                      -2.4084e+00, -5.0846e-01,  2.8271e+00, -1.1543e+00,  2.2030e-01,\n                       1.0595e+00,  6.0347e-01,  5.8925e-01,  7.8048e-01,  7.2662e-01,\n                       6.9222e-02, -2.8886e+00,  8.8255e-02, -3.9779e-01, -1.1983e+00,\n                       1.9871e+00,  3.7315e-01, -3.8486e+00, -8.8578e-01, -4.7559e-01,\n                       4.0545e-01,  7.8147e-01, -8.7645e-01,  1.5368e+00, -1.2895e+00,\n                      -1.4646e+00, -3.7349e-01,  1.8836e-01, -2.8732e-01, -1.0973e+00,\n                       1.3685e+00, -9.5856e-01, -1.5653e-01, -4.6547e-01,  3.0740e-01,\n                       3.5158e-01,  9.7628e-01, -4.3840e-01, -2.2298e-01,  1.1979e-01,\n                       9.3182e-02, -1.3253e+00,  8.8748e-01, -2.2675e-01, -3.5025e+00,\n                      -1.5864e-01, -1.4739e-01,  1.9340e+00, -4.1192e-01, -2.2192e+00,\n                       3.3917e-01, -1.9782e-01,  2.5989e-01,  4.4436e-01, -6.0900e-01,\n                       9.6627e-01,  1.6722e-01,  7.7996e-01, -2.6914e-01,  8.6351e-03,\n                      -2.8391e-01,  1.1367e+00, -1.4980e-01,  8.3568e-01, -8.2562e-01,\n                      -2.9675e-01,  7.5583e-01,  5.1611e-01,  8.9179e-02,  7.7823e-01,\n                       3.5490e+00, -5.9325e-01,  2.6768e+00, -4.7913e-01, -1.3290e+00,\n                       2.1199e+00,  3.0196e+00, -3.5404e-01, -5.9317e-01, -4.2243e-01,\n                       9.2266e-01,  4.1080e+00,  4.2293e-01, -5.0044e-01,  1.0335e+00,\n                      -5.6301e-01, -1.8201e+00, -1.2643e+00, -2.5755e+00,  9.8314e-01,\n                      -3.7528e-01, -5.3409e-01,  4.0166e-02, -2.6880e+00, -2.0731e-01,\n                       3.4403e-02,  5.7822e-02,  3.8838e+00,  9.2003e-01, -4.2198e-01,\n                       1.5815e-01, -7.0006e-02, -3.6900e-01,  2.1297e-01,  6.3777e-02,\n                       3.5967e-01, -6.4257e-01, -2.0388e+00, -7.0259e-01,  5.5903e-01,\n                       2.7121e-01,  3.4489e-01,  1.4252e-01, -9.0551e-01,  3.8926e+00,\n                       2.3521e+00, -6.3800e-03, -1.4144e-01,  2.0929e+00, -3.5112e-01,\n                       3.0986e-01, -6.8764e-01, -1.8913e+00,  1.3745e+00, -1.4431e+00,\n                       9.7018e-01,  1.7090e+00, -3.8421e-02,  1.0323e+00,  1.2438e-01,\n                      -1.0414e+00, -2.0145e-01, -8.1001e-01,  1.1934e+00, -4.9689e-01,\n                      -3.9337e-02, -3.6776e-01, -1.7980e-01,  1.5327e+00, -1.7100e+00,\n                      -1.9879e-01, -1.5664e+00,  6.1556e-01, -5.5038e-01, -1.5422e-01,\n                       7.9406e-02, -1.0935e-01,  3.2892e-01,  9.8832e-02,  8.7966e-02,\n                       2.6674e-01,  2.4696e-01,  4.2523e-01,  1.4058e+00,  1.2889e+00,\n                       5.7811e-02, -8.7317e-01,  1.9902e-01, -7.0709e-01,  5.5573e-02,\n                       2.9668e-01,  2.0355e-01,  8.6251e-01, -7.5301e-01,  1.0571e+00,\n                       6.2497e-02,  9.5705e-01, -1.8413e+00])),\n             ('features.7.0.block.2.scale', tensor(0.2836)),\n             ('features.7.0.block.2.zero_point', tensor(77)),\n             ('features.7.0.block.3.scale', tensor(0.2473)),\n             ('features.7.0.block.3.zero_point', tensor(94)),\n             ('features.7.0.block.3._packed_params.dtype', torch.qint8),\n             ('features.7.0.block.3._packed_params._packed_params',\n              (tensor([[ 0.0770, -0.0342,  0.0411,  ...,  0.0342, -0.0137,  0.0308],\n                       [-0.0524, -0.0012, -0.0037,  ...,  0.1110,  0.0329,  0.0805],\n                       [-0.0423, -0.0355,  0.0343,  ...,  0.0149, -0.0343,  0.0206],\n                       ...,\n                       [ 0.0474, -0.0799,  0.0393,  ...,  0.0420, -0.1463, -0.0609],\n                       [ 0.1080, -0.0856, -0.1038,  ..., -0.1066,  0.0224, -0.0337],\n                       [-0.0097, -0.0097,  0.0048,  ..., -0.0582,  0.0097, -0.0226]],\n                      size=(3072, 768), dtype=torch.qint8,\n                      quantization_scheme=torch.per_channel_affine,\n                      scale=tensor([0.0017, 0.0012, 0.0011,  ..., 0.0014, 0.0014, 0.0016],\n                      dtype=torch.float64),\n                      zero_point=tensor([0, 0, 0,  ..., 0, 0, 0]), axis=0),\n               Parameter containing:\n               tensor([-0.0288, -0.0454, -0.0308,  ..., -0.0391, -0.0344, -0.0318],\n                      requires_grad=True))),\n             ('features.7.0.block.5.scale', tensor(0.0885)),\n             ('features.7.0.block.5.zero_point', tensor(74)),\n             ('features.7.0.block.5._packed_params.dtype', torch.qint8),\n             ('features.7.0.block.5._packed_params._packed_params',\n              (tensor([[ 0.0055,  0.0125,  0.0028,  ..., -0.0264, -0.0375, -0.0735],\n                       [ 0.0018,  0.0319, -0.0426,  ...,  0.0567,  0.0461, -0.0035],\n                       [-0.0458, -0.0251,  0.0503,  ...,  0.0015, -0.0991, -0.0651],\n                       ...,\n                       [ 0.0637,  0.0539, -0.0360,  ..., -0.0441, -0.0376,  0.0687],\n                       [ 0.0911, -0.0371, -0.0067,  ..., -0.0624, -0.0067, -0.0321],\n                       [-0.0934,  0.0060,  0.0648,  ...,  0.0075, -0.0618, -0.0332]],\n                      size=(768, 3072), dtype=torch.qint8,\n                      quantization_scheme=torch.per_channel_affine,\n                      scale=tensor([0.0014, 0.0018, 0.0015, 0.0016, 0.0017, 0.0018, 0.0015, 0.0014, 0.0015,\n                       0.0031, 0.0017, 0.0016, 0.0021, 0.0018, 0.0035, 0.0018, 0.0018, 0.0017,\n                       0.0016, 0.0018, 0.0016, 0.0016, 0.0021, 0.0017, 0.0017, 0.0017, 0.0017,\n                       0.0018, 0.0017, 0.0015, 0.0016, 0.0015, 0.0019, 0.0014, 0.0015, 0.0021,\n                       0.0014, 0.0015, 0.0016, 0.0016, 0.0016, 0.0016, 0.0014, 0.0019, 0.0019,\n                       0.0016, 0.0017, 0.0014, 0.0015, 0.0016, 0.0017, 0.0014, 0.0017, 0.0016,\n                       0.0015, 0.0015, 0.0016, 0.0015, 0.0017, 0.0016, 0.0018, 0.0016, 0.0018,\n                       0.0016, 0.0017, 0.0016, 0.0015, 0.0020, 0.0023, 0.0017, 0.0015, 0.0017,\n                       0.0015, 0.0020, 0.0014, 0.0015, 0.0016, 0.0018, 0.0015, 0.0014, 0.0018,\n                       0.0016, 0.0015, 0.0015, 0.0016, 0.0015, 0.0018, 0.0016, 0.0017, 0.0016,\n                       0.0016, 0.0015, 0.0016, 0.0015, 0.0015, 0.0016, 0.0016, 0.0014, 0.0014,\n                       0.0017, 0.0016, 0.0016, 0.0014, 0.0013, 0.0015, 0.0014, 0.0016, 0.0017,\n                       0.0016, 0.0015, 0.0016, 0.0018, 0.0017, 0.0014, 0.0018, 0.0027, 0.0015,\n                       0.0017, 0.0014, 0.0015, 0.0016, 0.0017, 0.0015, 0.0013, 0.0017, 0.0016,\n                       0.0021, 0.0017, 0.0016, 0.0016, 0.0014, 0.0015, 0.0016, 0.0017, 0.0015,\n                       0.0016, 0.0014, 0.0015, 0.0016, 0.0019, 0.0017, 0.0018, 0.0018, 0.0018,\n                       0.0028, 0.0019, 0.0014, 0.0015, 0.0015, 0.0019, 0.0015, 0.0015, 0.0015,\n                       0.0016, 0.0015, 0.0014, 0.0016, 0.0018, 0.0016, 0.0017, 0.0015, 0.0014,\n                       0.0017, 0.0017, 0.0017, 0.0015, 0.0016, 0.0018, 0.0016, 0.0013, 0.0017,\n                       0.0017, 0.0018, 0.0015, 0.0015, 0.0018, 0.0014, 0.0029, 0.0015, 0.0021,\n                       0.0013, 0.0013, 0.0015, 0.0017, 0.0014, 0.0014, 0.0019, 0.0014, 0.0016,\n                       0.0017, 0.0016, 0.0016, 0.0015, 0.0017, 0.0016, 0.0031, 0.0015, 0.0016,\n                       0.0017, 0.0014, 0.0017, 0.0017, 0.0014, 0.0017, 0.0016, 0.0019, 0.0015,\n                       0.0019, 0.0017, 0.0016, 0.0015, 0.0014, 0.0015, 0.0041, 0.0016, 0.0017,\n                       0.0015, 0.0015, 0.0013, 0.0016, 0.0016, 0.0015, 0.0018, 0.0016, 0.0015,\n                       0.0015, 0.0015, 0.0015, 0.0014, 0.0015, 0.0030, 0.0015, 0.0015, 0.0016,\n                       0.0016, 0.0016, 0.0014, 0.0015, 0.0029, 0.0014, 0.0014, 0.0016, 0.0015,\n                       0.0014, 0.0015, 0.0016, 0.0019, 0.0014, 0.0017, 0.0016, 0.0016, 0.0014,\n                       0.0014, 0.0015, 0.0015, 0.0022, 0.0018, 0.0017, 0.0016, 0.0014, 0.0016,\n                       0.0014, 0.0015, 0.0016, 0.0011, 0.0016, 0.0028, 0.0015, 0.0018, 0.0016,\n                       0.0016, 0.0016, 0.0015, 0.0015, 0.0015, 0.0015, 0.0016, 0.0014, 0.0014,\n                       0.0015, 0.0017, 0.0015, 0.0015, 0.0016, 0.0017, 0.0014, 0.0015, 0.0015,\n                       0.0039, 0.0015, 0.0016, 0.0019, 0.0015, 0.0017, 0.0014, 0.0017, 0.0017,\n                       0.0018, 0.0019, 0.0022, 0.0015, 0.0017, 0.0015, 0.0016, 0.0014, 0.0015,\n                       0.0017, 0.0016, 0.0021, 0.0018, 0.0016, 0.0023, 0.0014, 0.0016, 0.0018,\n                       0.0021, 0.0018, 0.0041, 0.0016, 0.0015, 0.0018, 0.0016, 0.0015, 0.0020,\n                       0.0016, 0.0021, 0.0016, 0.0017, 0.0015, 0.0017, 0.0015, 0.0016, 0.0019,\n                       0.0015, 0.0015, 0.0017, 0.0016, 0.0015, 0.0015, 0.0014, 0.0033, 0.0016,\n                       0.0016, 0.0015, 0.0014, 0.0021, 0.0014, 0.0012, 0.0014, 0.0017, 0.0015,\n                       0.0016, 0.0014, 0.0015, 0.0016, 0.0020, 0.0016, 0.0015, 0.0018, 0.0018,\n                       0.0020, 0.0017, 0.0014, 0.0018, 0.0015, 0.0014, 0.0017, 0.0014, 0.0016,\n                       0.0014, 0.0016, 0.0016, 0.0018, 0.0018, 0.0016, 0.0015, 0.0016, 0.0013,\n                       0.0018, 0.0016, 0.0015, 0.0019, 0.0019, 0.0015, 0.0016, 0.0016, 0.0014,\n                       0.0016, 0.0014, 0.0015, 0.0016, 0.0036, 0.0014, 0.0014, 0.0017, 0.0036,\n                       0.0016, 0.0017, 0.0015, 0.0016, 0.0016, 0.0020, 0.0017, 0.0019, 0.0019,\n                       0.0018, 0.0015, 0.0018, 0.0016, 0.0017, 0.0016, 0.0014, 0.0016, 0.0018,\n                       0.0019, 0.0017, 0.0016, 0.0023, 0.0016, 0.0015, 0.0028, 0.0017, 0.0015,\n                       0.0019, 0.0017, 0.0014, 0.0015, 0.0014, 0.0017, 0.0015, 0.0016, 0.0015,\n                       0.0015, 0.0017, 0.0015, 0.0017, 0.0018, 0.0014, 0.0016, 0.0016, 0.0015,\n                       0.0014, 0.0016, 0.0023, 0.0015, 0.0018, 0.0019, 0.0018, 0.0016, 0.0019,\n                       0.0016, 0.0029, 0.0016, 0.0015, 0.0015, 0.0016, 0.0031, 0.0016, 0.0019,\n                       0.0014, 0.0015, 0.0018, 0.0022, 0.0014, 0.0017, 0.0017, 0.0014, 0.0014,\n                       0.0017, 0.0036, 0.0017, 0.0017, 0.0016, 0.0014, 0.0019, 0.0015, 0.0018,\n                       0.0016, 0.0014, 0.0014, 0.0017, 0.0018, 0.0017, 0.0015, 0.0014, 0.0014,\n                       0.0017, 0.0017, 0.0018, 0.0016, 0.0016, 0.0016, 0.0020, 0.0018, 0.0016,\n                       0.0016, 0.0015, 0.0017, 0.0014, 0.0015, 0.0016, 0.0016, 0.0016, 0.0016,\n                       0.0016, 0.0015, 0.0014, 0.0017, 0.0016, 0.0017, 0.0015, 0.0015, 0.0015,\n                       0.0015, 0.0015, 0.0014, 0.0016, 0.0016, 0.0017, 0.0016, 0.0014, 0.0015,\n                       0.0018, 0.0015, 0.0015, 0.0016, 0.0018, 0.0020, 0.0019, 0.0015, 0.0015,\n                       0.0014, 0.0014, 0.0017, 0.0017, 0.0016, 0.0019, 0.0016, 0.0016, 0.0016,\n                       0.0015, 0.0016, 0.0017, 0.0015, 0.0016, 0.0014, 0.0016, 0.0015, 0.0014,\n                       0.0016, 0.0016, 0.0019, 0.0017, 0.0016, 0.0016, 0.0020, 0.0015, 0.0014,\n                       0.0015, 0.0014, 0.0017, 0.0016, 0.0016, 0.0014, 0.0015, 0.0017, 0.0016,\n                       0.0015, 0.0017, 0.0021, 0.0017, 0.0014, 0.0018, 0.0017, 0.0016, 0.0016,\n                       0.0020, 0.0017, 0.0015, 0.0016, 0.0016, 0.0015, 0.0015, 0.0034, 0.0036,\n                       0.0015, 0.0015, 0.0015, 0.0017, 0.0018, 0.0015, 0.0015, 0.0013, 0.0015,\n                       0.0017, 0.0050, 0.0016, 0.0015, 0.0017, 0.0016, 0.0015, 0.0015, 0.0020,\n                       0.0014, 0.0016, 0.0038, 0.0016, 0.0014, 0.0020, 0.0016, 0.0016, 0.0037,\n                       0.0020, 0.0017, 0.0028, 0.0016, 0.0015, 0.0016, 0.0016, 0.0017, 0.0015,\n                       0.0019, 0.0016, 0.0017, 0.0017, 0.0020, 0.0019, 0.0016, 0.0014, 0.0014,\n                       0.0050, 0.0016, 0.0016, 0.0016, 0.0018, 0.0015, 0.0019, 0.0023, 0.0018,\n                       0.0015, 0.0015, 0.0015, 0.0016, 0.0015, 0.0018, 0.0016, 0.0015, 0.0016,\n                       0.0016, 0.0017, 0.0021, 0.0017, 0.0017, 0.0027, 0.0016, 0.0016, 0.0050,\n                       0.0015, 0.0013, 0.0016, 0.0018, 0.0016, 0.0021, 0.0015, 0.0017, 0.0018,\n                       0.0016, 0.0015, 0.0015, 0.0016, 0.0017, 0.0014, 0.0015, 0.0017, 0.0013,\n                       0.0015, 0.0013, 0.0015, 0.0018, 0.0016, 0.0016, 0.0016, 0.0015, 0.0015,\n                       0.0014, 0.0019, 0.0015, 0.0018, 0.0014, 0.0015, 0.0016, 0.0017, 0.0016,\n                       0.0015, 0.0018, 0.0014, 0.0016, 0.0016, 0.0015, 0.0017, 0.0017, 0.0017,\n                       0.0016, 0.0016, 0.0014, 0.0014, 0.0016, 0.0034, 0.0014, 0.0015, 0.0019,\n                       0.0014, 0.0016, 0.0016, 0.0020, 0.0016, 0.0016, 0.0016, 0.0016, 0.0014,\n                       0.0014, 0.0014, 0.0017, 0.0015, 0.0016, 0.0019, 0.0016, 0.0015, 0.0014,\n                       0.0015, 0.0016, 0.0018, 0.0021, 0.0016, 0.0015, 0.0016, 0.0015, 0.0019,\n                       0.0014, 0.0016, 0.0018, 0.0019, 0.0014, 0.0020, 0.0016, 0.0019, 0.0017,\n                       0.0019, 0.0019, 0.0027, 0.0018, 0.0016, 0.0014, 0.0016, 0.0017, 0.0017,\n                       0.0016, 0.0016, 0.0018, 0.0015, 0.0016, 0.0016, 0.0016, 0.0014, 0.0015,\n                       0.0016, 0.0017, 0.0015], dtype=torch.float64),\n                      zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n                      axis=0),\n               Parameter containing:\n               tensor([-2.0481e-02, -2.2960e-02, -1.6543e-02, -1.2091e-02, -1.5098e-02,\n                       -1.5197e-02,  9.5530e-03,  5.4415e-02,  4.5267e-03, -1.9040e-01,\n                        2.7234e-02, -3.1680e-02, -1.2721e-01,  2.3011e-03,  3.5734e-01,\n                       -6.7057e-02,  6.6016e-02, -1.9023e-02, -1.9942e-02, -1.5360e-02,\n                       -2.0908e-02,  5.1583e-02,  2.4546e-02,  1.5544e-02,  2.5002e-02,\n                        7.9261e-03, -5.7135e-02,  7.2294e-03,  1.0591e-02, -4.1806e-02,\n                       -4.9163e-03, -2.4637e-02,  8.0833e-02,  2.2219e-02,  9.2620e-03,\n                       -1.1269e-02,  3.1130e-03, -6.3528e-02, -5.1450e-03, -1.4113e-02,\n                       -2.6395e-03, -1.6675e-02, -5.8103e-03, -3.3532e-03,  1.4187e-02,\n                        1.3950e-02, -6.3380e-03,  4.9112e-04, -9.4515e-04,  7.5558e-02,\n                        3.5763e-02, -4.2840e-02, -7.8880e-02,  4.0233e-03,  2.3434e-02,\n                       -3.1542e-02,  8.5078e-03,  6.7463e-02,  6.7186e-03, -4.5404e-04,\n                        5.4003e-02, -3.6278e-02, -1.0918e-02,  2.5679e-02, -7.4199e-03,\n                       -1.6063e-02,  2.2215e-02,  1.9931e-02, -5.2658e-02, -8.5915e-03,\n                       -6.5199e-03,  8.5060e-03,  1.8871e-02,  1.3722e-02,  1.4809e-02,\n                       -1.4613e-02, -6.4514e-03, -3.6552e-04, -4.8307e-02, -2.1075e-02,\n                        1.0030e-01, -1.4516e-03, -1.1475e-02, -1.8116e-02, -7.0775e-03,\n                       -1.8798e-02,  9.4500e-03,  9.7447e-02,  9.2104e-03,  5.9287e-03,\n                        1.4423e-02,  1.5938e-02, -6.8838e-03,  7.6553e-03, -7.2742e-02,\n                        2.9828e-02,  1.6950e-02, -1.5590e-02, -2.8164e-02, -3.4201e-03,\n                       -1.2219e-02, -6.5318e-02,  1.6292e-02,  2.3642e-02,  4.8046e-02,\n                       -2.7323e-02, -4.7468e-03, -5.7725e-02,  2.8867e-03,  9.3501e-03,\n                       -4.0708e-02,  1.3355e-02, -4.8471e-02,  5.7645e-03, -2.4996e-02,\n                       -5.4580e-02,  3.1885e-02, -1.4546e-02, -2.1776e-02, -5.0665e-02,\n                        9.2534e-03, -5.6852e-02, -3.7977e-02, -1.0912e-02, -1.0694e-02,\n                       -1.3470e-02, -5.2349e-02,  1.6057e-02,  2.6190e-02, -8.3065e-02,\n                       -2.5127e-02,  1.6021e-02,  3.7269e-02, -1.6116e-03,  1.4539e-02,\n                       -6.2039e-02, -1.8325e-02,  5.3564e-02, -8.1064e-03, -7.2087e-04,\n                       -3.2633e-04, -2.8628e-02, -7.7084e-02,  1.0755e-01, -4.5742e-01,\n                        1.7804e-02,  1.8299e-02, -1.9300e-02, -1.6690e-02, -8.4731e-03,\n                        1.7773e-02, -4.5172e-02, -3.3327e-02, -3.4142e-02, -5.8422e-02,\n                        2.3812e-02, -4.6703e-02, -8.6845e-03, -2.1603e-02, -8.9025e-03,\n                        1.8171e-02,  6.6483e-03, -9.8662e-03,  5.4692e-02,  9.8064e-03,\n                        3.9491e-02,  9.8231e-03,  8.5904e-04,  4.0213e-03,  1.9320e-02,\n                       -1.1070e-01,  1.5151e-02,  1.1364e-02,  2.2262e-02,  1.1178e-02,\n                       -1.3048e-02,  3.7768e-02,  2.1061e-02,  9.5125e-03, -2.0316e-02,\n                       -4.4974e-03,  1.8753e-02, -1.5770e-02,  5.5034e-02, -4.1363e-02,\n                       -1.8690e-03,  2.3648e-02, -2.4071e-02,  1.4737e-02, -9.2623e-03,\n                       -7.5443e-02,  1.7574e-02,  7.1198e-03, -1.8170e-02, -1.6418e-02,\n                       -1.9936e-02, -1.0476e-02,  1.9880e-02, -7.8421e-02, -6.5543e-03,\n                        2.6591e-02,  4.9264e-03,  2.1193e-02, -3.5875e-02,  7.6247e-02,\n                        2.7562e-02, -3.0662e-02, -3.5095e-03,  2.1143e-02,  3.7011e-02,\n                       -3.6817e-03,  1.4703e-02, -2.1432e-02,  2.0933e-02,  2.8647e-01,\n                        1.0622e-02, -4.8793e-02, -4.0431e-01,  8.9204e-02, -2.8197e-02,\n                        2.4046e-02, -6.1890e-02,  2.0234e-02,  9.3490e-03,  4.0303e-02,\n                        5.6880e-03, -3.4375e-02, -3.2089e-02,  4.8193e-02, -4.1646e-02,\n                       -1.6572e-01,  3.1021e-02, -1.2818e-03,  9.2747e-03, -3.4029e-02,\n                       -5.2628e-03,  2.4852e-02,  8.6040e-03, -9.7695e-02,  2.3057e-02,\n                       -1.2066e-02, -2.1377e-02, -3.2330e-02,  1.4192e-02, -1.4864e-02,\n                        4.4337e-02,  5.7113e-03, -8.7703e-02,  1.3473e-03,  1.6749e-02,\n                       -4.7783e-03, -6.5938e-02, -3.7642e-02,  4.1244e-02,  1.2142e-02,\n                        1.1619e-02,  5.8393e-02, -5.5683e-03, -1.9991e-02,  4.2682e-03,\n                       -1.3750e-02, -2.4874e-02, -1.9344e-02, -1.2279e-02,  6.9136e-03,\n                        5.3183e-02, -4.7118e-01, -1.8506e-02, -3.7686e-02, -3.5269e-02,\n                       -2.1436e-02,  8.0475e-03,  1.5070e-02,  8.3564e-02, -7.8609e-04,\n                        1.4741e-01, -4.3600e-02,  1.6874e-02,  3.7877e-02,  1.6532e-02,\n                       -4.5114e-03,  1.9927e-02,  1.1287e-01,  2.6767e-02, -3.8612e-02,\n                        8.5091e-02, -3.2641e-02, -1.1637e-02,  4.2472e-01,  1.0715e-02,\n                       -4.6102e-02, -1.3006e-02, -6.6534e-02, -4.3134e-02, -4.5096e-03,\n                        2.3434e-02,  1.2163e-03, -1.6207e-02, -9.6044e-02,  4.3908e-02,\n                       -8.2814e-02,  2.9439e-03, -5.4065e-03, -7.3877e-02, -4.8257e-03,\n                       -3.4837e-02, -3.5143e-02, -1.5032e-01,  2.5309e-02, -2.5009e-02,\n                        2.9189e-02, -8.1330e-02,  2.0842e-02,  1.1428e-02, -1.6794e-02,\n                       -1.9056e-02, -1.3349e-02, -5.1153e-02, -1.6859e-02, -2.4046e-02,\n                        2.0703e-02,  8.4518e-03, -2.3036e-02,  2.6437e-02, -1.4225e-02,\n                        2.4746e-02,  4.2216e-03, -3.1935e-02, -1.3128e-02, -7.9280e-03,\n                       -8.6181e-02,  2.6249e-02,  3.5239e-02,  7.1304e-03, -2.2147e-03,\n                        1.2641e-02,  1.4892e-02, -2.2024e-02,  3.3204e-02, -6.2392e-02,\n                       -1.1305e-01, -1.2333e-03, -8.7691e-03, -1.6522e-02, -1.5801e-02,\n                        2.8416e-02, -2.6015e-02, -4.6937e-03,  4.5617e-03,  1.1171e-03,\n                       -1.2389e-02, -2.8074e-02,  1.8213e-02,  1.8740e-02, -1.6096e-02,\n                       -3.8882e-02,  1.3428e-01, -9.7727e-03,  5.7434e-02, -2.9671e-02,\n                        4.1204e-02, -1.7888e-03,  2.0619e-02, -9.0114e-03, -2.3457e-02,\n                       -3.0108e-02,  5.9899e-02, -2.3182e-02,  2.3913e-02,  3.3743e-02,\n                        1.7058e-02,  4.7263e-02, -1.9842e-02, -3.4671e-02, -3.4858e-02,\n                        9.7223e-03,  2.5631e-03, -3.5905e-02,  3.8621e-03, -2.8846e-03,\n                       -9.5301e-03,  7.0109e-03,  1.6342e-02, -8.9438e-02, -3.6786e-02,\n                       -1.7722e-02, -3.6964e-03,  4.1926e-02,  1.4954e-01, -3.9769e-03,\n                        3.7698e-02,  6.8741e-02, -2.5926e-02,  4.6406e-02, -2.6527e-02,\n                        9.3584e-02, -4.8429e-02,  1.3486e-01, -2.1481e-02, -5.6297e-03,\n                       -7.4320e-02, -1.8166e-02, -3.8660e-03,  1.2488e-02, -3.9498e-02,\n                        1.8724e-02,  3.1933e-02,  1.7975e-02,  8.7666e-03,  3.0347e-02,\n                       -1.6209e-02,  1.7738e-02,  1.7254e-02, -1.7330e-02,  1.7788e-01,\n                       -1.3179e-02, -4.3311e-02,  1.7182e-02,  1.3887e-02, -6.5597e-02,\n                       -5.0096e-03, -7.1834e-02, -2.3628e-02,  6.1154e-03, -3.1017e-02,\n                        4.1600e-02,  3.6015e-03,  1.0562e-03,  4.0078e-04, -1.4011e-02,\n                        7.2092e-02,  2.5110e-02, -5.4284e-03,  4.9027e-03, -4.1293e-02,\n                        1.6819e-02, -6.2888e-02,  1.9379e-02, -6.8763e-03, -1.7892e-02,\n                       -1.8347e-02,  9.4124e-03,  2.8887e-02,  4.6938e-02, -1.5938e-02,\n                        1.1792e-02,  2.2100e-01, -1.4770e-02, -2.7060e-02, -2.1558e-01,\n                        1.8153e-02,  8.8562e-02,  5.2361e-03, -4.8089e-02,  3.9881e-02,\n                       -1.9329e-02, -3.7446e-01,  2.5454e-02,  2.7657e-03, -1.5090e-02,\n                       -2.0420e-02,  1.8341e-02, -1.0046e-01, -1.8106e-03,  2.0656e-02,\n                        1.7498e-03, -1.5358e-02,  1.0709e-02, -1.0529e-02, -1.7075e-04,\n                       -1.1075e-02, -2.0584e-02,  1.4877e-02,  1.3566e-02, -1.6239e-02,\n                       -1.2497e-02,  8.7437e-03,  3.1285e-04, -5.1529e-02,  1.8953e-02,\n                       -7.0425e-03, -1.6440e-02, -5.5065e-03, -1.5674e-03, -7.0422e-03,\n                        5.5496e-02,  5.7930e-03, -2.4315e-02, -3.2109e-02,  1.3017e-02,\n                       -2.3407e-02, -8.9634e-03, -3.4680e-02, -7.1446e-03,  3.5760e-02,\n                       -5.0796e-03, -8.9073e-02, -1.5417e-02, -6.3621e-03, -1.4625e-02,\n                        6.7980e-02, -2.9819e-02,  4.3083e-03,  1.9859e-02, -3.8767e-03,\n                       -1.0676e-02,  1.7673e-03,  3.0975e-02, -2.5930e-03, -4.6126e-02,\n                       -8.8282e-03, -7.2621e-03,  8.5729e-03, -1.0995e-02, -5.2097e-02,\n                       -1.6999e-02, -2.5194e-02,  2.4301e-02,  4.6931e-02,  1.1552e-02,\n                        1.1319e-02, -4.8555e-02,  5.4190e-02,  3.3416e-03,  2.9766e-03,\n                        1.8719e-02,  3.6028e-03, -1.0192e-02, -2.3232e-02,  4.2575e-02,\n                        3.3414e-02, -8.4617e-04, -5.0353e-02,  7.5217e-03, -2.3406e-02,\n                        4.6540e-04,  5.9586e-02, -2.7221e-02, -3.8513e-02, -1.1680e-02,\n                       -2.5787e-03, -2.7694e-02, -2.2705e-02,  6.8715e-03, -3.6735e-03,\n                        3.0761e-02, -7.4960e-03, -3.3624e-02,  3.2469e-03, -3.0429e-02,\n                       -6.7720e-02,  7.5024e-03, -1.4507e-02,  4.3229e-02,  4.2758e-02,\n                        1.2397e-03,  7.4325e-03,  1.2614e-02, -2.2725e-02, -1.5843e-02,\n                       -1.6174e-02, -1.6830e-02, -9.3771e-03,  6.2642e-04,  3.4045e-02,\n                        3.4522e-02, -2.2434e-02, -1.2711e-02, -1.8595e-02, -5.6025e-02,\n                       -2.6327e-03, -4.0056e-02,  1.9917e-03, -8.9266e-03,  4.8542e-02,\n                        4.7994e-02,  3.4855e-03, -2.5085e-02, -2.3937e-03,  1.7265e-02,\n                       -4.4812e-02, -1.0958e-02,  1.5833e-02, -1.0911e-01, -2.9653e-01,\n                       -2.6426e-02,  1.3397e-02, -1.0972e-02, -6.9730e-02,  1.8677e-02,\n                       -1.9601e-02,  1.7377e-02, -3.2711e-02, -1.5308e-02,  8.2811e-03,\n                        6.9805e-02, -7.2682e-03, -1.8062e-02, -3.6786e-02,  5.3371e-03,\n                       -1.2533e-02, -5.3814e-03, -1.0458e-01, -1.8621e-02,  2.0354e-03,\n                        1.0279e-01,  2.4256e-03, -5.8266e-02,  4.1843e-02,  1.4726e-02,\n                       -2.3930e-02, -2.0988e-01, -3.6549e-02, -1.9776e-03, -2.0475e-01,\n                        2.0280e-02,  1.4623e-02, -4.0138e-02, -2.2483e-02,  2.1469e-02,\n                        2.0009e-02,  1.0279e-01,  3.0867e-02, -3.6132e-02, -1.4380e-02,\n                       -1.9644e-02,  2.1032e-02, -4.1269e-02, -1.3404e-02, -2.7529e-02,\n                       -2.5627e-01,  9.7723e-03,  2.7652e-03,  3.3157e-02, -8.7077e-03,\n                        1.6181e-02, -9.5310e-03, -5.8359e-02,  2.4496e-02,  1.7473e-02,\n                       -3.9250e-02,  1.7127e-02, -4.3041e-03, -7.6316e-02, -2.5508e-02,\n                       -2.6687e-03,  2.0979e-02,  6.3276e-03,  1.5774e-03, -3.9988e-02,\n                        3.2771e-04, -9.6861e-03, -3.2756e-03, -8.1331e-02, -6.7286e-03,\n                       -1.6069e-02, -2.6514e-02, -1.0635e-02,  4.6533e-05,  7.9161e-03,\n                       -3.5120e-02,  7.6409e-03,  1.4103e-01, -7.8073e-03,  2.1484e-01,\n                        3.7355e-02, -4.6765e-02, -1.6070e-03, -1.8354e-05,  4.1574e-02,\n                        1.8617e-02, -9.2672e-03,  8.5127e-02,  6.4528e-02, -1.2720e-02,\n                       -4.4885e-02,  1.9751e-02, -1.0757e-02,  3.9272e-03,  1.1307e-02,\n                        4.0026e-03, -2.2414e-02, -5.8379e-02,  3.8146e-02, -9.8280e-03,\n                        7.5764e-02, -2.5240e-02, -6.1708e-03,  4.8513e-02, -5.7010e-03,\n                        5.7675e-03, -8.1704e-02, -1.9267e-02,  2.3003e-02, -6.7643e-03,\n                        7.1648e-03,  8.2717e-03, -3.6352e-02, -2.7535e-02,  2.0415e-03,\n                        1.3138e-02, -5.4459e-03, -1.4258e-03,  3.0577e-03, -9.0793e-03,\n                       -3.8348e-02, -1.6669e-02, -4.2651e-01,  7.0499e-02,  2.9921e-04,\n                        9.9548e-03, -2.1951e-02,  1.0317e-02,  1.6270e-02,  1.0402e-02,\n                       -2.0917e-02, -2.6854e-02,  4.7817e-03, -3.9596e-03,  2.1295e-02,\n                       -2.9988e-02, -2.9644e-02, -7.5381e-02,  9.3603e-03, -2.2028e-02,\n                        1.1378e-03,  3.1254e-02, -1.0409e-02, -9.6227e-03,  3.4945e-02,\n                        2.4036e-02,  3.2888e-03, -5.6291e-02, -1.9501e-02, -2.7772e-02,\n                       -4.4149e-03, -3.2838e-02, -5.9592e-03,  2.6200e-03, -1.1845e-02,\n                       -2.5623e-02,  1.0203e-02, -5.5908e-03, -5.2742e-02,  1.3351e-02,\n                        3.7064e-02, -8.1738e-03, -1.7362e-02,  3.2008e-02, -6.7945e-02,\n                       -1.5340e-02,  1.5387e-03, -4.4990e-03, -1.6368e-02, -2.1275e-02,\n                       -2.4477e-02,  1.6315e-02,  6.7104e-03, -6.2355e-03, -1.6905e-03,\n                        6.8467e-03, -2.9006e-03, -7.2933e-03, -6.8989e-03,  1.5222e-02,\n                        1.6389e-01,  5.0017e-03, -5.9994e-03], requires_grad=True))),\n             ('features.7.1.layer_scale',\n              tensor([[[ 0.0773]],\n              \n                      [[-0.1773]],\n              \n                      [[-0.1134]],\n              \n                      [[-0.1604]],\n              \n                      [[ 0.2485]],\n              \n                      [[ 0.1874]],\n              \n                      [[ 0.1644]],\n              \n                      [[ 0.1791]],\n              \n                      [[-0.1053]],\n              \n                      [[ 0.3951]],\n              \n                      [[-0.0684]],\n              \n                      [[ 0.2391]],\n              \n                      [[-0.0931]],\n              \n                      [[ 0.2435]],\n              \n                      [[-0.0877]],\n              \n                      [[-0.5839]],\n              \n                      [[-0.5691]],\n              \n                      [[-0.0877]],\n              \n                      [[ 0.0816]],\n              \n                      [[-0.2467]],\n              \n                      [[ 0.1880]],\n              \n                      [[ 0.1097]],\n              \n                      [[-0.0794]],\n              \n                      [[ 0.2162]],\n              \n                      [[-0.4245]],\n              \n                      [[-0.2139]],\n              \n                      [[ 0.1346]],\n              \n                      [[-0.2713]],\n              \n                      [[-0.2619]],\n              \n                      [[ 0.2690]],\n              \n                      [[-0.2119]],\n              \n                      [[-0.2543]],\n              \n                      [[-0.1043]],\n              \n                      [[ 0.1361]],\n              \n                      [[ 0.1649]],\n              \n                      [[-0.2640]],\n              \n                      [[ 0.1753]],\n              \n                      [[ 0.2390]],\n              \n                      [[ 0.2049]],\n              \n                      [[ 0.2694]],\n              \n                      [[ 0.0871]],\n              \n                      [[-0.0872]],\n              \n                      [[-0.1062]],\n              \n                      [[ 0.3097]],\n              \n                      [[-0.3290]],\n              \n                      [[ 0.2695]],\n              \n                      [[ 0.2508]],\n              \n                      [[ 0.1621]],\n              \n                      [[-0.1633]],\n              \n                      [[-0.0874]],\n              \n                      [[-0.2079]],\n              \n                      [[-0.1401]],\n              \n                      [[-0.1896]],\n              \n                      [[-0.1065]],\n              \n                      [[-0.2265]],\n              \n                      [[ 0.0908]],\n              \n                      [[-0.2868]],\n              \n                      [[ 0.1101]],\n              \n                      [[ 0.2454]],\n              \n                      [[ 0.2555]],\n              \n                      [[ 0.0733]],\n              \n                      [[ 0.1922]],\n              \n                      [[ 0.2250]],\n              \n                      [[-0.1120]],\n              \n                      [[-0.2815]],\n              \n                      [[ 0.1034]],\n              \n                      [[ 0.2885]],\n              \n                      [[ 0.2129]],\n              \n                      [[-0.4993]],\n              \n                      [[-0.1973]],\n              \n                      [[-0.1900]],\n              \n                      [[ 0.1204]],\n              \n                      [[ 0.0986]],\n              \n                      [[ 0.3333]],\n              \n                      [[-0.1704]],\n              \n                      [[ 0.0868]],\n              \n                      [[ 0.1402]],\n              \n                      [[-0.2493]],\n              \n                      [[-0.0842]],\n              \n                      [[-0.2684]],\n              \n                      [[-0.1180]],\n              \n                      [[ 0.2795]],\n              \n                      [[-0.1972]],\n              \n                      [[-0.2410]],\n              \n                      [[-0.2109]],\n              \n                      [[-0.1995]],\n              \n                      [[ 0.2523]],\n              \n                      [[ 0.8594]],\n              \n                      [[-0.2272]],\n              \n                      [[-0.2035]],\n              \n                      [[-0.2152]],\n              \n                      [[ 0.1980]],\n              \n                      [[-0.1733]],\n              \n                      [[-0.1716]],\n              \n                      [[ 0.1143]],\n              \n                      [[-0.1149]],\n              \n                      [[ 0.2561]],\n              \n                      [[-0.0983]],\n              \n                      [[ 0.0980]],\n              \n                      [[-0.1764]],\n              \n                      [[-0.1841]],\n              \n                      [[ 0.2120]],\n              \n                      [[ 0.2237]],\n              \n                      [[ 0.0717]],\n              \n                      [[ 0.1373]],\n              \n                      [[ 0.2707]],\n              \n                      [[ 0.2507]],\n              \n                      [[-0.1111]],\n              \n                      [[-0.1735]],\n              \n                      [[ 0.2602]],\n              \n                      [[-0.1320]],\n              \n                      [[ 0.1661]],\n              \n                      [[ 0.0723]],\n              \n                      [[-0.1478]],\n              \n                      [[-0.1574]],\n              \n                      [[ 0.1842]],\n              \n                      [[-0.0995]],\n              \n                      [[-0.2015]],\n              \n                      [[-0.1833]],\n              \n                      [[-0.1268]],\n              \n                      [[-0.2249]],\n              \n                      [[ 0.2871]],\n              \n                      [[ 0.2261]],\n              \n                      [[-0.1667]],\n              \n                      [[-0.2389]],\n              \n                      [[-0.0887]],\n              \n                      [[-0.2013]],\n              \n                      [[-0.1282]],\n              \n                      [[ 0.1427]],\n              \n                      [[-0.1626]],\n              \n                      [[-0.1570]],\n              \n                      [[ 0.0943]],\n              \n                      [[-0.2478]],\n              \n                      [[-0.1848]],\n              \n                      [[ 0.2148]],\n              \n                      [[-0.1823]],\n              \n                      [[ 0.2104]],\n              \n                      [[ 0.0996]],\n              \n                      [[-0.2303]],\n              \n                      [[-0.2838]],\n              \n                      [[ 0.2850]],\n              \n                      [[-0.2851]],\n              \n                      [[-0.0752]],\n              \n                      [[-0.1632]],\n              \n                      [[-0.1702]],\n              \n                      [[ 0.2125]],\n              \n                      [[-0.0971]],\n              \n                      [[-0.1990]],\n              \n                      [[ 0.0789]],\n              \n                      [[ 0.0867]],\n              \n                      [[-0.1240]],\n              \n                      [[ 0.1631]],\n              \n                      [[ 0.2415]],\n              \n                      [[-0.2059]],\n              \n                      [[-0.2321]],\n              \n                      [[-0.2125]],\n              \n                      [[-0.1000]],\n              \n                      [[ 0.2299]],\n              \n                      [[-0.1899]],\n              \n                      [[-0.2168]],\n              \n                      [[ 0.0889]],\n              \n                      [[-0.2309]],\n              \n                      [[-0.1200]],\n              \n                      [[ 0.1014]],\n              \n                      [[ 0.1585]],\n              \n                      [[ 0.1324]],\n              \n                      [[ 0.0765]],\n              \n                      [[ 0.2323]],\n              \n                      [[-0.1874]],\n              \n                      [[-0.1059]],\n              \n                      [[ 0.5374]],\n              \n                      [[-0.1896]],\n              \n                      [[ 0.0862]],\n              \n                      [[ 0.1205]],\n              \n                      [[-0.2138]],\n              \n                      [[ 0.2778]],\n              \n                      [[-0.0801]],\n              \n                      [[-0.0107]],\n              \n                      [[ 0.2002]],\n              \n                      [[ 0.0826]],\n              \n                      [[-0.1772]],\n              \n                      [[ 0.2118]],\n              \n                      [[-0.2402]],\n              \n                      [[ 0.1638]],\n              \n                      [[-0.1018]],\n              \n                      [[ 0.1065]],\n              \n                      [[ 0.5421]],\n              \n                      [[-0.2422]],\n              \n                      [[ 0.2133]],\n              \n                      [[ 0.2567]],\n              \n                      [[ 0.0921]],\n              \n                      [[-0.1345]],\n              \n                      [[-0.2590]],\n              \n                      [[ 0.2542]],\n              \n                      [[ 0.2894]],\n              \n                      [[-0.1990]],\n              \n                      [[ 0.2616]],\n              \n                      [[ 0.2452]],\n              \n                      [[ 0.1836]],\n              \n                      [[ 0.0829]],\n              \n                      [[ 0.1898]],\n              \n                      [[-0.3000]],\n              \n                      [[-0.2331]],\n              \n                      [[ 0.2436]],\n              \n                      [[-0.2052]],\n              \n                      [[ 0.2879]],\n              \n                      [[-0.1693]],\n              \n                      [[ 0.2660]],\n              \n                      [[-0.2399]],\n              \n                      [[ 0.0766]],\n              \n                      [[-0.1637]],\n              \n                      [[-0.2204]],\n              \n                      [[ 0.2208]],\n              \n                      [[-0.1478]],\n              \n                      [[ 0.1097]],\n              \n                      [[-0.0940]],\n              \n                      [[-0.2295]],\n              \n                      [[ 0.1328]],\n              \n                      [[-0.0832]],\n              \n                      [[ 0.1740]],\n              \n                      [[ 0.2722]],\n              \n                      [[ 0.0810]],\n              \n                      [[-0.0848]],\n              \n                      [[ 0.0993]],\n              \n                      [[ 0.2131]],\n              \n                      [[-0.1224]],\n              \n                      [[-0.1392]],\n              \n                      [[-0.1866]],\n              \n                      [[-0.2314]],\n              \n                      [[ 0.1875]],\n              \n                      [[-0.0851]],\n              \n                      [[ 0.1050]],\n              \n                      [[ 0.1469]],\n              \n                      [[ 0.0856]],\n              \n                      [[ 0.2239]],\n              \n                      [[-0.1882]],\n              \n                      [[ 0.1833]],\n              \n                      [[-0.1137]],\n              \n                      [[-0.8293]],\n              \n                      [[ 0.2059]],\n              \n                      [[-0.1861]],\n              \n                      [[ 0.0950]],\n              \n                      [[ 0.1870]],\n              \n                      [[ 0.1755]],\n              \n                      [[-0.1848]],\n              \n                      [[-0.0880]],\n              \n                      [[-0.1919]],\n              \n                      [[ 0.1210]],\n              \n                      [[-0.1464]],\n              \n                      [[-0.1122]],\n              \n                      [[ 0.1587]],\n              \n                      [[-0.2070]],\n              \n                      [[-0.1187]],\n              \n                      [[-0.2335]],\n              \n                      [[-0.2326]],\n              \n                      [[ 0.0942]],\n              \n                      [[ 0.2800]],\n              \n                      [[ 0.1704]],\n              \n                      [[-0.1582]],\n              \n                      [[ 0.1776]],\n              \n                      [[ 0.1769]],\n              \n                      [[ 0.2181]],\n              \n                      [[ 0.0766]],\n              \n                      [[-0.2847]],\n              \n                      [[ 0.0014]],\n              \n                      [[ 0.1754]],\n              \n                      [[-0.5644]],\n              \n                      [[ 0.1874]],\n              \n                      [[-0.0867]],\n              \n                      [[-0.2603]],\n              \n                      [[-0.2037]],\n              \n                      [[ 0.1195]],\n              \n                      [[ 0.2698]],\n              \n                      [[-0.1752]],\n              \n                      [[-0.1554]],\n              \n                      [[-0.9255]],\n              \n                      [[ 0.1124]],\n              \n                      [[-0.0799]],\n              \n                      [[-0.1253]],\n              \n                      [[-0.1509]],\n              \n                      [[-0.1751]],\n              \n                      [[ 0.2548]],\n              \n                      [[ 0.2592]],\n              \n                      [[-0.1457]],\n              \n                      [[ 0.2267]],\n              \n                      [[-0.0644]],\n              \n                      [[-0.0985]],\n              \n                      [[ 0.1384]],\n              \n                      [[ 0.1122]],\n              \n                      [[-0.0876]],\n              \n                      [[-0.2539]],\n              \n                      [[ 0.5446]],\n              \n                      [[-0.2545]],\n              \n                      [[ 0.2476]],\n              \n                      [[-0.0755]],\n              \n                      [[ 0.3062]],\n              \n                      [[-0.2379]],\n              \n                      [[-0.2556]],\n              \n                      [[-0.0795]],\n              \n                      [[ 0.0615]],\n              \n                      [[-0.0943]],\n              \n                      [[ 0.2649]],\n              \n                      [[ 0.2319]],\n              \n                      [[-0.1126]],\n              \n                      [[ 0.0852]],\n              \n                      [[ 0.1500]],\n              \n                      [[-0.0912]],\n              \n                      [[-0.1739]],\n              \n                      [[-0.1439]],\n              \n                      [[ 0.1982]],\n              \n                      [[-0.2734]],\n              \n                      [[ 0.1808]],\n              \n                      [[-0.1984]],\n              \n                      [[-0.1617]],\n              \n                      [[ 0.1039]],\n              \n                      [[-0.0882]],\n              \n                      [[-0.2462]],\n              \n                      [[-0.4472]],\n              \n                      [[ 0.1633]],\n              \n                      [[ 0.1643]],\n              \n                      [[-0.2827]],\n              \n                      [[-0.0771]],\n              \n                      [[-0.1520]],\n              \n                      [[-0.1336]],\n              \n                      [[ 0.1935]],\n              \n                      [[-0.4593]],\n              \n                      [[ 0.2020]],\n              \n                      [[ 0.1565]],\n              \n                      [[ 0.3230]],\n              \n                      [[-0.1030]],\n              \n                      [[-0.2418]],\n              \n                      [[-0.0696]],\n              \n                      [[-0.2422]],\n              \n                      [[ 0.2705]],\n              \n                      [[-0.2247]],\n              \n                      [[ 0.0756]],\n              \n                      [[-0.2251]],\n              \n                      [[-0.0785]],\n              \n                      [[ 0.2182]],\n              \n                      [[ 0.2596]],\n              \n                      [[-0.1216]],\n              \n                      [[ 0.0933]],\n              \n                      [[-0.1844]],\n              \n                      [[ 0.1978]],\n              \n                      [[-0.0905]],\n              \n                      [[-0.2207]],\n              \n                      [[ 0.0668]],\n              \n                      [[-0.0798]],\n              \n                      [[-0.0876]],\n              \n                      [[ 0.0801]],\n              \n                      [[ 0.2092]],\n              \n                      [[ 0.2198]],\n              \n                      [[ 0.2014]],\n              \n                      [[-0.0844]],\n              \n                      [[ 0.1975]],\n              \n                      [[ 0.2398]],\n              \n                      [[ 0.1139]],\n              \n                      [[ 0.2179]],\n              \n                      [[ 0.2686]],\n              \n                      [[-0.0851]],\n              \n                      [[ 0.5224]],\n              \n                      [[-0.1431]],\n              \n                      [[-0.1989]],\n              \n                      [[-0.2365]],\n              \n                      [[ 0.2293]],\n              \n                      [[-0.1997]],\n              \n                      [[-0.3185]],\n              \n                      [[ 0.1776]],\n              \n                      [[ 0.1096]],\n              \n                      [[ 0.0776]],\n              \n                      [[-0.1430]],\n              \n                      [[ 0.2638]],\n              \n                      [[-0.2252]],\n              \n                      [[-0.2611]],\n              \n                      [[ 0.0828]],\n              \n                      [[ 0.0853]],\n              \n                      [[ 0.2331]],\n              \n                      [[-0.1656]],\n              \n                      [[-0.0905]],\n              \n                      [[ 0.1360]],\n              \n                      [[-0.2333]],\n              \n                      [[-0.3316]],\n              \n                      [[-0.6736]],\n              \n                      [[ 0.0842]],\n              \n                      [[-0.2547]],\n              \n                      [[-0.1805]],\n              \n                      [[-0.2699]],\n              \n                      [[-0.2035]],\n              \n                      [[-0.0701]],\n              \n                      [[-0.2376]],\n              \n                      [[ 0.1120]],\n              \n                      [[-0.1281]],\n              \n                      [[-0.0915]],\n              \n                      [[ 0.2430]],\n              \n                      [[ 0.2099]],\n              \n                      [[-0.0897]],\n              \n                      [[-0.2110]],\n              \n                      [[ 0.2042]],\n              \n                      [[ 0.2609]],\n              \n                      [[ 0.2052]],\n              \n                      [[ 0.2481]],\n              \n                      [[-0.2628]],\n              \n                      [[ 0.1937]],\n              \n                      [[ 0.1640]],\n              \n                      [[ 0.4650]],\n              \n                      [[-0.1009]],\n              \n                      [[ 0.0954]],\n              \n                      [[ 0.2373]],\n              \n                      [[-0.2285]],\n              \n                      [[-0.2298]],\n              \n                      [[ 0.2790]],\n              \n                      [[-0.2159]],\n              \n                      [[ 0.0771]],\n              \n                      [[ 0.2122]],\n              \n                      [[-0.3411]],\n              \n                      [[-0.1108]],\n              \n                      [[ 0.1826]],\n              \n                      [[-0.1512]],\n              \n                      [[-0.1204]],\n              \n                      [[ 0.1419]],\n              \n                      [[ 0.0715]],\n              \n                      [[ 0.1589]],\n              \n                      [[-0.2492]],\n              \n                      [[ 0.2339]],\n              \n                      [[-0.2465]],\n              \n                      [[-0.0722]],\n              \n                      [[ 0.2800]],\n              \n                      [[ 0.1570]],\n              \n                      [[-0.2263]],\n              \n                      [[ 0.2377]],\n              \n                      [[-0.2225]],\n              \n                      [[ 0.1012]],\n              \n                      [[-0.2588]],\n              \n                      [[ 0.1596]],\n              \n                      [[ 0.2884]],\n              \n                      [[ 0.1063]],\n              \n                      [[-0.5190]],\n              \n                      [[-0.2081]],\n              \n                      [[ 0.2592]],\n              \n                      [[ 0.1287]],\n              \n                      [[ 0.2435]],\n              \n                      [[ 0.1980]],\n              \n                      [[ 0.2504]],\n              \n                      [[ 0.0855]],\n              \n                      [[ 0.2144]],\n              \n                      [[ 0.1428]],\n              \n                      [[-0.2443]],\n              \n                      [[ 0.1477]],\n              \n                      [[-0.0789]],\n              \n                      [[ 0.1888]],\n              \n                      [[ 0.2150]],\n              \n                      [[-0.0829]],\n              \n                      [[ 0.0910]],\n              \n                      [[-0.1786]],\n              \n                      [[ 0.1455]],\n              \n                      [[ 0.2294]],\n              \n                      [[ 0.2364]],\n              \n                      [[-0.2101]],\n              \n                      [[ 0.1574]],\n              \n                      [[ 0.1911]],\n              \n                      [[-0.2082]],\n              \n                      [[ 0.1089]],\n              \n                      [[-0.1233]],\n              \n                      [[ 0.1587]],\n              \n                      [[ 0.1079]],\n              \n                      [[-0.2396]],\n              \n                      [[ 0.2433]],\n              \n                      [[-0.2196]],\n              \n                      [[-0.2662]],\n              \n                      [[-0.0924]],\n              \n                      [[ 0.2015]],\n              \n                      [[-0.1233]],\n              \n                      [[-0.1025]],\n              \n                      [[-0.1898]],\n              \n                      [[-0.6531]],\n              \n                      [[ 0.2267]],\n              \n                      [[ 0.1301]],\n              \n                      [[-0.0819]],\n              \n                      [[-0.1891]],\n              \n                      [[ 0.1123]],\n              \n                      [[ 0.2424]],\n              \n                      [[-0.2898]],\n              \n                      [[ 0.1040]],\n              \n                      [[ 0.2314]],\n              \n                      [[ 0.2474]],\n              \n                      [[-0.1919]],\n              \n                      [[-0.1231]],\n              \n                      [[-0.1908]],\n              \n                      [[-0.1168]],\n              \n                      [[-0.2002]],\n              \n                      [[-0.2662]],\n              \n                      [[-0.2316]],\n              \n                      [[ 0.6118]],\n              \n                      [[-0.0881]],\n              \n                      [[ 0.1091]],\n              \n                      [[ 0.2318]],\n              \n                      [[-0.1409]],\n              \n                      [[-0.1778]],\n              \n                      [[ 0.1961]],\n              \n                      [[ 0.2465]],\n              \n                      [[-0.0683]],\n              \n                      [[-0.1013]],\n              \n                      [[-0.2283]],\n              \n                      [[-0.1081]],\n              \n                      [[-0.1645]],\n              \n                      [[-0.1678]],\n              \n                      [[-0.1979]],\n              \n                      [[-0.1351]],\n              \n                      [[-0.1766]],\n              \n                      [[-0.2626]],\n              \n                      [[-0.1982]],\n              \n                      [[-0.2074]],\n              \n                      [[ 0.1270]],\n              \n                      [[-0.2322]],\n              \n                      [[ 0.1818]],\n              \n                      [[-0.0862]],\n              \n                      [[-0.2278]],\n              \n                      [[-0.0723]],\n              \n                      [[-0.0969]],\n              \n                      [[-0.0910]],\n              \n                      [[ 0.1419]],\n              \n                      [[-0.1819]],\n              \n                      [[ 0.1970]],\n              \n                      [[-0.2090]],\n              \n                      [[-0.0888]],\n              \n                      [[-0.1596]],\n              \n                      [[ 0.2168]],\n              \n                      [[ 0.2393]],\n              \n                      [[-0.2202]],\n              \n                      [[ 0.1154]],\n              \n                      [[ 0.1085]],\n              \n                      [[-0.1276]],\n              \n                      [[ 0.1806]],\n              \n                      [[ 0.2539]],\n              \n                      [[-0.0859]],\n              \n                      [[ 0.1094]],\n              \n                      [[ 0.2866]],\n              \n                      [[-0.1712]],\n              \n                      [[-0.1307]],\n              \n                      [[ 0.1617]],\n              \n                      [[-0.0926]],\n              \n                      [[-0.0833]],\n              \n                      [[-0.5504]],\n              \n                      [[ 0.1314]],\n              \n                      [[ 0.2168]],\n              \n                      [[ 0.2086]],\n              \n                      [[-0.1749]],\n              \n                      [[ 0.1158]],\n              \n                      [[ 0.2248]],\n              \n                      [[ 0.2518]],\n              \n                      [[-0.2339]],\n              \n                      [[ 0.0871]],\n              \n                      [[ 0.2336]],\n              \n                      [[-0.8543]],\n              \n                      [[ 0.1079]],\n              \n                      [[-0.3072]],\n              \n                      [[-0.1548]],\n              \n                      [[-0.1196]],\n              \n                      [[-0.2000]],\n              \n                      [[-0.1497]],\n              \n                      [[-0.2638]],\n              \n                      [[-0.2090]],\n              \n                      [[ 0.0819]],\n              \n                      [[ 0.2243]],\n              \n                      [[ 0.0912]],\n              \n                      [[-0.2322]],\n              \n                      [[ 0.2240]],\n              \n                      [[-0.2402]],\n              \n                      [[-0.2350]],\n              \n                      [[-0.7512]],\n              \n                      [[ 0.2263]],\n              \n                      [[-0.2221]],\n              \n                      [[-0.0893]],\n              \n                      [[ 0.1279]],\n              \n                      [[-0.2370]],\n              \n                      [[-0.1882]],\n              \n                      [[-0.2777]],\n              \n                      [[-0.2623]],\n              \n                      [[ 0.0909]],\n              \n                      [[-0.1037]],\n              \n                      [[ 0.1322]],\n              \n                      [[-0.2798]],\n              \n                      [[ 0.2305]],\n              \n                      [[ 0.1718]],\n              \n                      [[ 0.1914]],\n              \n                      [[ 0.1912]],\n              \n                      [[ 0.1151]],\n              \n                      [[-0.1200]],\n              \n                      [[ 0.0750]],\n              \n                      [[-0.1298]],\n              \n                      [[ 0.2223]],\n              \n                      [[-0.2188]],\n              \n                      [[ 0.0824]],\n              \n                      [[ 0.1683]],\n              \n                      [[-0.2384]],\n              \n                      [[ 0.1200]],\n              \n                      [[ 0.1196]],\n              \n                      [[ 0.1259]],\n              \n                      [[ 0.2392]],\n              \n                      [[ 0.0942]],\n              \n                      [[-0.2294]],\n              \n                      [[-0.2407]],\n              \n                      [[-0.1876]],\n              \n                      [[-0.2143]],\n              \n                      [[-0.2155]],\n              \n                      [[ 0.1050]],\n              \n                      [[ 0.2362]],\n              \n                      [[-0.0578]],\n              \n                      [[-0.1845]],\n              \n                      [[ 0.2342]],\n              \n                      [[-0.1389]],\n              \n                      [[-0.3290]],\n              \n                      [[ 0.4192]],\n              \n                      [[ 0.2803]],\n              \n                      [[ 0.3999]],\n              \n                      [[-0.1111]],\n              \n                      [[ 0.0897]],\n              \n                      [[-0.1015]],\n              \n                      [[ 0.2120]],\n              \n                      [[ 0.2462]],\n              \n                      [[-0.2377]],\n              \n                      [[ 0.6074]],\n              \n                      [[ 0.1947]],\n              \n                      [[-0.2732]],\n              \n                      [[-0.2296]],\n              \n                      [[ 0.0943]],\n              \n                      [[ 0.4725]],\n              \n                      [[-0.2797]],\n              \n                      [[-0.0964]],\n              \n                      [[-0.0883]],\n              \n                      [[ 0.1236]],\n              \n                      [[ 0.0894]],\n              \n                      [[ 0.1855]],\n              \n                      [[ 0.0671]],\n              \n                      [[-0.0935]],\n              \n                      [[ 0.2573]],\n              \n                      [[-0.1849]],\n              \n                      [[-0.5986]],\n              \n                      [[-0.2605]],\n              \n                      [[-0.1815]],\n              \n                      [[-0.2410]],\n              \n                      [[-0.0599]],\n              \n                      [[-0.2563]],\n              \n                      [[-0.0966]],\n              \n                      [[ 0.1100]],\n              \n                      [[-0.2130]],\n              \n                      [[ 0.1463]],\n              \n                      [[-0.1421]],\n              \n                      [[ 0.0946]],\n              \n                      [[ 0.0838]],\n              \n                      [[ 0.1047]],\n              \n                      [[ 0.2399]],\n              \n                      [[ 0.2275]],\n              \n                      [[ 0.2059]],\n              \n                      [[-0.3194]],\n              \n                      [[-0.1799]],\n              \n                      [[ 0.0838]],\n              \n                      [[ 0.1240]],\n              \n                      [[ 0.0813]],\n              \n                      [[ 0.0857]],\n              \n                      [[ 0.5485]],\n              \n                      [[ 0.0852]],\n              \n                      [[ 0.9062]],\n              \n                      [[ 0.0868]],\n              \n                      [[ 0.1672]],\n              \n                      [[-0.1958]],\n              \n                      [[ 0.0876]],\n              \n                      [[-0.0911]],\n              \n                      [[ 0.1773]],\n              \n                      [[ 0.2777]],\n              \n                      [[ 0.0743]],\n              \n                      [[-0.2452]],\n              \n                      [[-0.1470]],\n              \n                      [[ 0.2353]],\n              \n                      [[ 0.0762]],\n              \n                      [[-0.1485]],\n              \n                      [[ 0.2210]],\n              \n                      [[-0.2460]],\n              \n                      [[-0.2209]],\n              \n                      [[-0.1277]],\n              \n                      [[-0.1989]],\n              \n                      [[ 0.2343]],\n              \n                      [[ 0.2402]],\n              \n                      [[-0.0954]],\n              \n                      [[-0.1764]],\n              \n                      [[-0.1107]],\n              \n                      [[-0.1987]],\n              \n                      [[ 0.1410]],\n              \n                      [[ 0.2305]],\n              \n                      [[-0.0803]],\n              \n                      [[-0.2098]],\n              \n                      [[ 0.3867]],\n              \n                      [[ 0.1698]],\n              \n                      [[-0.1971]],\n              \n                      [[ 0.2569]],\n              \n                      [[ 0.2392]],\n              \n                      [[-0.2552]],\n              \n                      [[-0.1193]],\n              \n                      [[-0.1944]],\n              \n                      [[-0.2874]],\n              \n                      [[-0.3196]],\n              \n                      [[ 0.2651]],\n              \n                      [[ 0.2150]],\n              \n                      [[-0.2222]],\n              \n                      [[ 0.2214]],\n              \n                      [[-0.1794]],\n              \n                      [[ 0.2563]],\n              \n                      [[-0.8250]],\n              \n                      [[ 0.2423]],\n              \n                      [[-0.1516]],\n              \n                      [[ 0.2192]],\n              \n                      [[ 0.1626]],\n              \n                      [[-0.2319]],\n              \n                      [[-0.2166]],\n              \n                      [[ 0.1850]],\n              \n                      [[-0.0824]],\n              \n                      [[ 0.2733]],\n              \n                      [[ 0.2410]],\n              \n                      [[ 0.2182]],\n              \n                      [[-0.2398]],\n              \n                      [[-0.1481]],\n              \n                      [[ 0.0788]],\n              \n                      [[ 0.1748]],\n              \n                      [[ 0.0939]],\n              \n                      [[-0.2406]],\n              \n                      [[ 0.1429]],\n              \n                      [[ 0.0718]],\n              \n                      [[ 0.1311]],\n              \n                      [[-0.0981]],\n              \n                      [[ 0.2282]],\n              \n                      [[ 0.0914]],\n              \n                      [[-0.2607]],\n              \n                      [[-0.3921]],\n              \n                      [[-0.2247]],\n              \n                      [[ 0.0698]],\n              \n                      [[ 0.2639]],\n              \n                      [[ 0.1757]],\n              \n                      [[-0.2608]],\n              \n                      [[ 0.0712]],\n              \n                      [[-0.1496]],\n              \n                      [[ 0.2084]],\n              \n                      [[-0.1133]],\n              \n                      [[ 0.1351]],\n              \n                      [[-0.1461]],\n              \n                      [[-0.1602]],\n              \n                      [[-0.3257]],\n              \n                      [[ 0.1855]],\n              \n                      [[ 0.0874]],\n              \n                      [[-0.2555]],\n              \n                      [[ 0.1904]],\n              \n                      [[-0.2628]],\n              \n                      [[ 0.2635]],\n              \n                      [[-0.1234]],\n              \n                      [[ 0.2334]],\n              \n                      [[ 0.1111]],\n              \n                      [[ 0.2116]],\n              \n                      [[ 0.2417]],\n              \n                      [[ 0.1386]],\n              \n                      [[ 0.2140]],\n              \n                      [[-0.2195]],\n              \n                      [[ 0.2592]],\n              \n                      [[-0.2155]],\n              \n                      [[ 0.1905]],\n              \n                      [[ 0.2538]],\n              \n                      [[ 0.1214]],\n              \n                      [[ 0.2143]],\n              \n                      [[-0.0807]],\n              \n                      [[ 0.1858]]])),\n             ('features.7.1.block.0.weight',\n              tensor([[[[-0.0096, -0.0032,  0.0016,  ..., -0.0048, -0.0032, -0.0159],\n                        [-0.0414, -0.0016, -0.0860,  ..., -0.0828, -0.0207, -0.0175],\n                        [ 0.0382,  0.0048,  0.0048,  ...,  0.0000,  0.0159,  0.0271],\n                        ...,\n                        [ 0.0048, -0.0175,  0.0303,  ...,  0.0175, -0.0127,  0.0000],\n                        [-0.0064, -0.0127,  0.0191,  ...,  0.0287, -0.0096, -0.0048],\n                        [ 0.0478,  0.0032,  0.0733,  ...,  0.0542,  0.0048,  0.0478]]],\n              \n              \n                      [[[-0.0341,  0.0122, -0.0560,  ..., -0.0463,  0.0122, -0.0511],\n                        [-0.0584, -0.0511, -0.1534,  ..., -0.1753, -0.0414, -0.0925],\n                        [ 0.0390,  0.0146, -0.0170,  ..., -0.0195,  0.0097,  0.0487],\n                        ...,\n                        [-0.0024,  0.0024,  0.0219,  ...,  0.0146, -0.0097,  0.0000],\n                        [-0.0414, -0.0195, -0.0073,  ...,  0.0073, -0.0195, -0.0341],\n                        [ 0.0414,  0.0146,  0.0584,  ...,  0.0584,  0.0024,  0.0365]]],\n              \n              \n                      [[[-0.0199,  0.0042, -0.0544,  ..., -0.0419,  0.0021, -0.0345],\n                        [-0.0084, -0.0136, -0.0105,  ...,  0.0000, -0.0084, -0.0293],\n                        [ 0.0293,  0.0042,  0.0387,  ...,  0.0481, -0.0063,  0.0419],\n                        ...,\n                        [-0.0084, -0.0147,  0.0052,  ...,  0.0105, -0.0084, -0.0178],\n                        [ 0.0136, -0.0105,  0.0167,  ...,  0.0042, -0.0115,  0.0115],\n                        [-0.0115, -0.0199, -0.0324,  ..., -0.0356, -0.0293,  0.0094]]],\n              \n              \n                      ...,\n              \n              \n                      [[[-0.0115,  0.0077, -0.0166,  ..., -0.0306,  0.0089, -0.0128],\n                        [-0.0714, -0.0013, -0.1123,  ..., -0.1148,  0.0000, -0.0689],\n                        [-0.0625,  0.0026, -0.0370,  ..., -0.0408, -0.0102, -0.0651],\n                        ...,\n                        [-0.0051, -0.0013, -0.0510,  ..., -0.0498,  0.0038, -0.0153],\n                        [-0.0217,  0.0013, -0.0434,  ..., -0.0408, -0.0026, -0.0217],\n                        [-0.0561, -0.0038, -0.0523,  ..., -0.0485, -0.0026, -0.0600]]],\n              \n              \n                      [[[ 0.0145, -0.0072,  0.0072,  ...,  0.0087,  0.0014,  0.0072],\n                        [ 0.0246,  0.0101,  0.0637,  ...,  0.0666,  0.0043,  0.0376],\n                        [-0.0347, -0.0058, -0.0145,  ..., -0.0101,  0.0014, -0.0478],\n                        ...,\n                        [-0.0116,  0.0145, -0.0174,  ..., -0.0304,  0.0130, -0.0072],\n                        [ 0.0043,  0.0130, -0.0318,  ..., -0.0391,  0.0145,  0.0087],\n                        [-0.0333, -0.0101, -0.0507,  ..., -0.0564, -0.0072, -0.0420]]],\n              \n              \n                      [[[ 0.0035,  0.0093,  0.0254,  ...,  0.0220,  0.0162,  0.0278],\n                        [ 0.0231,  0.0000,  0.0312,  ...,  0.0370, -0.0069,  0.0220],\n                        [-0.0555, -0.0127, -0.0278,  ..., -0.0393, -0.0069, -0.0648],\n                        ...,\n                        [-0.0162,  0.0058, -0.0370,  ..., -0.0324,  0.0139, -0.0081],\n                        [-0.0012,  0.0220, -0.0567,  ..., -0.0567,  0.0081,  0.0081],\n                        [-0.0775, -0.0347, -0.0810,  ..., -0.0925, -0.0208, -0.0810]]]],\n                     size=(768, 1, 7, 7), dtype=torch.qint8,\n                     quantization_scheme=torch.per_channel_affine,\n                     scale=tensor([0.0016, 0.0024, 0.0010, 0.0010, 0.0008, 0.0013, 0.0010, 0.0019, 0.0016,\n                      0.0012, 0.0020, 0.0010, 0.0020, 0.0014, 0.0029, 0.0010, 0.0011, 0.0014,\n                      0.0019, 0.0017, 0.0018, 0.0014, 0.0014, 0.0006, 0.0013, 0.0009, 0.0016,\n                      0.0009, 0.0010, 0.0014, 0.0014, 0.0020, 0.0012, 0.0019, 0.0012, 0.0018,\n                      0.0012, 0.0010, 0.0007, 0.0019, 0.0015, 0.0018, 0.0010, 0.0016, 0.0016,\n                      0.0017, 0.0017, 0.0013, 0.0015, 0.0018, 0.0020, 0.0017, 0.0027, 0.0011,\n                      0.0023, 0.0014, 0.0009, 0.0013, 0.0007, 0.0016, 0.0016, 0.0018, 0.0016,\n                      0.0012, 0.0010, 0.0015, 0.0008, 0.0017, 0.0011, 0.0014, 0.0014, 0.0010,\n                      0.0013, 0.0015, 0.0014, 0.0019, 0.0016, 0.0010, 0.0014, 0.0016, 0.0012,\n                      0.0011, 0.0015, 0.0009, 0.0018, 0.0010, 0.0012, 0.0011, 0.0008, 0.0013,\n                      0.0011, 0.0011, 0.0015, 0.0009, 0.0014, 0.0014, 0.0009, 0.0017, 0.0019,\n                      0.0013, 0.0012, 0.0018, 0.0014, 0.0017, 0.0014, 0.0015, 0.0011, 0.0016,\n                      0.0015, 0.0007, 0.0018, 0.0011, 0.0019, 0.0011, 0.0010, 0.0016, 0.0012,\n                      0.0012, 0.0009, 0.0014, 0.0017, 0.0010, 0.0013, 0.0023, 0.0012, 0.0015,\n                      0.0010, 0.0017, 0.0019, 0.0020, 0.0031, 0.0017, 0.0017, 0.0010, 0.0017,\n                      0.0011, 0.0020, 0.0017, 0.0025, 0.0019, 0.0014, 0.0018, 0.0016, 0.0009,\n                      0.0010, 0.0011, 0.0014, 0.0018, 0.0018, 0.0018, 0.0012, 0.0019, 0.0013,\n                      0.0009, 0.0009, 0.0007, 0.0014, 0.0008, 0.0015, 0.0009, 0.0015, 0.0018,\n                      0.0015, 0.0016, 0.0015, 0.0016, 0.0024, 0.0019, 0.0016, 0.0020, 0.0013,\n                      0.0014, 0.0018, 0.0014, 0.0019, 0.0009, 0.0012, 0.0020, 0.0013, 0.0019,\n                      0.0011, 0.0007, 0.0009, 0.0017, 0.0015, 0.0018, 0.0011, 0.0012, 0.0016,\n                      0.0018, 0.0016, 0.0014, 0.0014, 0.0010, 0.0013, 0.0010, 0.0008, 0.0017,\n                      0.0014, 0.0013, 0.0016, 0.0014, 0.0017, 0.0014, 0.0026, 0.0016, 0.0011,\n                      0.0017, 0.0007, 0.0019, 0.0016, 0.0014, 0.0013, 0.0006, 0.0015, 0.0019,\n                      0.0014, 0.0011, 0.0020, 0.0012, 0.0011, 0.0016, 0.0015, 0.0012, 0.0017,\n                      0.0014, 0.0018, 0.0012, 0.0023, 0.0013, 0.0017, 0.0015, 0.0021, 0.0016,\n                      0.0016, 0.0008, 0.0028, 0.0011, 0.0008, 0.0013, 0.0019, 0.0014, 0.0009,\n                      0.0011, 0.0012, 0.0015, 0.0018, 0.0012, 0.0013, 0.0025, 0.0018, 0.0017,\n                      0.0013, 0.0006, 0.0007, 0.0015, 0.0019, 0.0010, 0.0010, 0.0008, 0.0015,\n                      0.0011, 0.0013, 0.0016, 0.0010, 0.0021, 0.0028, 0.0012, 0.0016, 0.0016,\n                      0.0010, 0.0014, 0.0011, 0.0024, 0.0009, 0.0012, 0.0014, 0.0016, 0.0019,\n                      0.0011, 0.0015, 0.0014, 0.0023, 0.0009, 0.0007, 0.0016, 0.0016, 0.0011,\n                      0.0023, 0.0013, 0.0014, 0.0010, 0.0016, 0.0016, 0.0018, 0.0017, 0.0007,\n                      0.0011, 0.0021, 0.0021, 0.0016, 0.0016, 0.0015, 0.0014, 0.0013, 0.0014,\n                      0.0012, 0.0009, 0.0018, 0.0013, 0.0014, 0.0011, 0.0017, 0.0013, 0.0018,\n                      0.0013, 0.0019, 0.0018, 0.0013, 0.0015, 0.0015, 0.0017, 0.0019, 0.0022,\n                      0.0009, 0.0012, 0.0011, 0.0016, 0.0010, 0.0015, 0.0024, 0.0017, 0.0010,\n                      0.0007, 0.0018, 0.0013, 0.0014, 0.0017, 0.0012, 0.0011, 0.0008, 0.0016,\n                      0.0009, 0.0015, 0.0014, 0.0015, 0.0017, 0.0014, 0.0014, 0.0016, 0.0013,\n                      0.0011, 0.0014, 0.0018, 0.0011, 0.0018, 0.0017, 0.0015, 0.0006, 0.0016,\n                      0.0011, 0.0014, 0.0014, 0.0014, 0.0017, 0.0014, 0.0014, 0.0010, 0.0015,\n                      0.0014, 0.0020, 0.0012, 0.0023, 0.0015, 0.0016, 0.0019, 0.0019, 0.0012,\n                      0.0013, 0.0016, 0.0009, 0.0015, 0.0010, 0.0018, 0.0009, 0.0020, 0.0015,\n                      0.0027, 0.0019, 0.0017, 0.0016, 0.0008, 0.0014, 0.0014, 0.0013, 0.0017,\n                      0.0010, 0.0012, 0.0009, 0.0023, 0.0023, 0.0021, 0.0016, 0.0031, 0.0014,\n                      0.0015, 0.0014, 0.0014, 0.0017, 0.0014, 0.0011, 0.0012, 0.0015, 0.0014,\n                      0.0016, 0.0020, 0.0007, 0.0015, 0.0013, 0.0011, 0.0023, 0.0011, 0.0008,\n                      0.0016, 0.0008, 0.0016, 0.0013, 0.0011, 0.0013, 0.0015, 0.0013, 0.0011,\n                      0.0014, 0.0007, 0.0016, 0.0012, 0.0012, 0.0014, 0.0012, 0.0014, 0.0011,\n                      0.0012, 0.0024, 0.0017, 0.0006, 0.0010, 0.0021, 0.0016, 0.0020, 0.0034,\n                      0.0012, 0.0022, 0.0016, 0.0025, 0.0016, 0.0013, 0.0015, 0.0015, 0.0011,\n                      0.0017, 0.0009, 0.0016, 0.0016, 0.0015, 0.0012, 0.0012, 0.0007, 0.0021,\n                      0.0016, 0.0012, 0.0013, 0.0016, 0.0018, 0.0013, 0.0009, 0.0011, 0.0014,\n                      0.0014, 0.0014, 0.0021, 0.0016, 0.0013, 0.0012, 0.0020, 0.0010, 0.0010,\n                      0.0013, 0.0010, 0.0015, 0.0015, 0.0006, 0.0012, 0.0011, 0.0013, 0.0012,\n                      0.0013, 0.0012, 0.0010, 0.0019, 0.0017, 0.0021, 0.0012, 0.0010, 0.0017,\n                      0.0019, 0.0013, 0.0016, 0.0017, 0.0014, 0.0010, 0.0016, 0.0017, 0.0013,\n                      0.0009, 0.0026, 0.0019, 0.0018, 0.0013, 0.0015, 0.0010, 0.0008, 0.0028,\n                      0.0009, 0.0009, 0.0016, 0.0011, 0.0011, 0.0014, 0.0016, 0.0016, 0.0020,\n                      0.0015, 0.0020, 0.0012, 0.0016, 0.0021, 0.0006, 0.0018, 0.0014, 0.0015,\n                      0.0017, 0.0018, 0.0011, 0.0017, 0.0018, 0.0018, 0.0017, 0.0014, 0.0008,\n                      0.0012, 0.0009, 0.0016, 0.0012, 0.0012, 0.0017, 0.0018, 0.0019, 0.0018,\n                      0.0017, 0.0011, 0.0012, 0.0013, 0.0014, 0.0015, 0.0020, 0.0015, 0.0019,\n                      0.0010, 0.0007, 0.0009, 0.0018, 0.0016, 0.0015, 0.0009, 0.0011, 0.0017,\n                      0.0009, 0.0007, 0.0013, 0.0015, 0.0011, 0.0010, 0.0014, 0.0026, 0.0018,\n                      0.0012, 0.0015, 0.0018, 0.0018, 0.0019, 0.0007, 0.0008, 0.0022, 0.0021,\n                      0.0005, 0.0007, 0.0011, 0.0015, 0.0013, 0.0016, 0.0019, 0.0016, 0.0015,\n                      0.0025, 0.0009, 0.0008, 0.0015, 0.0019, 0.0012, 0.0010, 0.0016, 0.0007,\n                      0.0012, 0.0018, 0.0024, 0.0017, 0.0014, 0.0020, 0.0010, 0.0006, 0.0009,\n                      0.0011, 0.0018, 0.0013, 0.0006, 0.0015, 0.0012, 0.0008, 0.0016, 0.0017,\n                      0.0006, 0.0015, 0.0021, 0.0018, 0.0013, 0.0009, 0.0016, 0.0010, 0.0019,\n                      0.0010, 0.0007, 0.0022, 0.0007, 0.0019, 0.0023, 0.0016, 0.0017, 0.0013,\n                      0.0015, 0.0012, 0.0020, 0.0012, 0.0007, 0.0008, 0.0016, 0.0012, 0.0013,\n                      0.0015, 0.0017, 0.0018, 0.0013, 0.0016, 0.0007, 0.0015, 0.0014, 0.0010,\n                      0.0015, 0.0016, 0.0012, 0.0009, 0.0018, 0.0016, 0.0016, 0.0009, 0.0017,\n                      0.0015, 0.0010, 0.0009, 0.0018, 0.0018, 0.0012, 0.0022, 0.0013, 0.0019,\n                      0.0009, 0.0015, 0.0021, 0.0014, 0.0021, 0.0015, 0.0015, 0.0015, 0.0012,\n                      0.0010, 0.0011, 0.0012, 0.0019, 0.0027, 0.0013, 0.0016, 0.0006, 0.0017,\n                      0.0017, 0.0023, 0.0013, 0.0012, 0.0006, 0.0008, 0.0012, 0.0013, 0.0019,\n                      0.0017, 0.0012, 0.0010, 0.0012, 0.0018, 0.0015, 0.0010, 0.0019, 0.0009,\n                      0.0017, 0.0016, 0.0012, 0.0014, 0.0005, 0.0012, 0.0016, 0.0014, 0.0016,\n                      0.0021, 0.0019, 0.0009, 0.0013, 0.0009, 0.0018, 0.0008, 0.0016, 0.0015,\n                      0.0015, 0.0009, 0.0008, 0.0012, 0.0030, 0.0012, 0.0011, 0.0014, 0.0009,\n                      0.0015, 0.0010, 0.0011, 0.0016, 0.0008, 0.0013, 0.0014, 0.0016, 0.0023,\n                      0.0007, 0.0017, 0.0014, 0.0015, 0.0013, 0.0020, 0.0016, 0.0015, 0.0017,\n                      0.0013, 0.0014, 0.0012], dtype=torch.float64),\n                     zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n                     axis=0)),\n             ('features.7.1.block.0.bias',\n              Parameter containing:\n              tensor([-5.8714e-03,  8.5354e-03,  1.7863e-02, -2.0237e-03, -1.5676e-03,\n                       1.7446e-02, -9.8625e-03, -1.3142e-02,  1.3020e-02, -6.6069e-02,\n                      -2.8443e-02,  1.4309e-03,  1.1439e-02,  2.1063e-02, -3.6496e-01,\n                      -1.1098e-01, -1.5188e-01,  3.1769e-03,  1.7815e-03, -4.5189e-02,\n                      -1.0386e-03,  6.5940e-03,  2.2728e-03, -6.6627e-03, -4.7903e-02,\n                      -2.2567e-02, -1.1406e-03, -1.2228e-03, -1.4210e-02,  1.2479e-02,\n                       2.9742e-03,  1.8866e-02,  1.1204e-02, -1.9475e-03, -2.1398e-03,\n                      -7.1950e-02, -7.9386e-03,  1.7202e-02, -1.5410e-03,  1.7707e-02,\n                      -1.8410e-03,  2.5927e-03,  6.8097e-03, -2.6325e-02, -2.2878e-02,\n                       1.8560e-02, -4.5070e-03,  5.1935e-03, -1.3774e-02, -1.9178e-02,\n                       7.3160e-03, -4.7850e-02, -3.4505e-02, -2.3134e-02, -2.8186e-02,\n                      -2.0066e-02,  2.1491e-02,  1.1139e-02, -2.4096e-03, -2.7748e-02,\n                       9.4151e-03,  1.5883e-02, -2.2902e-02, -1.8714e-02, -7.0933e-04,\n                       3.3933e-03,  2.6508e-02, -6.5158e-03, -9.8727e-02,  1.8975e-02,\n                       3.3286e-02,  8.2034e-03, -2.2681e-02, -2.0664e-02, -2.2071e-02,\n                       1.0475e-02,  2.3828e-03,  2.0076e-02, -4.3203e-05,  7.8006e-03,\n                       1.4766e-03,  2.3662e-02,  7.1082e-03, -7.7559e-03,  3.0926e-02,\n                      -6.2513e-04,  6.5324e-03, -6.2820e-02, -3.7581e-02, -3.3016e-04,\n                       3.0955e-02,  1.8232e-02, -2.1299e-02,  1.5348e-02,  1.7976e-02,\n                      -1.8498e-02,  1.7905e-02,  5.7133e-03,  1.7338e-02,  8.4549e-03,\n                      -1.1779e-02, -1.3207e-02, -2.4343e-02, -1.9943e-03,  3.3710e-03,\n                      -1.3929e-02,  8.1050e-03,  1.9607e-03,  1.1296e-02, -2.1466e-03,\n                      -1.0699e-03,  5.9581e-03,  6.1233e-03,  1.2010e-02,  6.1579e-03,\n                      -2.6984e-03,  6.5201e-03, -8.5472e-03, -2.2120e-03, -3.7885e-03,\n                       3.7634e-02,  4.0578e-02,  1.8292e-02, -3.1840e-02,  2.5443e-02,\n                      -2.5829e-03,  7.0298e-03,  1.1173e-02,  3.8911e-03,  5.8360e-02,\n                       4.8222e-02,  9.5864e-03, -1.5161e-02,  4.3168e-03, -1.4537e-02,\n                       1.3730e-03,  1.3344e-02,  1.7225e-02,  1.3564e-02,  1.5149e-02,\n                      -1.1814e-02, -6.8595e-02,  6.1485e-03, -3.9184e-02, -6.1966e-02,\n                      -8.8064e-03,  2.0089e-03,  9.4594e-03,  1.4387e-03, -1.4009e-02,\n                       1.6172e-02, -2.4467e-02,  3.8787e-02,  1.3104e-02,  2.3415e-02,\n                      -2.4760e-03, -8.0087e-03,  6.7392e-03, -2.4168e-02, -5.5392e-03,\n                      -1.3212e-02, -1.4296e-02, -3.5788e-02, -1.9386e-03,  2.1317e-02,\n                      -3.7599e-02, -1.5246e-02, -3.1727e-02,  2.6461e-02,  1.7724e-03,\n                      -6.5835e-02,  2.5447e-03,  1.1374e-02,  1.6052e-02, -1.1844e-02,\n                       2.0640e-02,  1.3237e-02,  3.1440e-02, -1.5005e-02, -2.3053e-03,\n                       2.1892e-02,  1.7143e-02, -2.1722e-04,  7.7625e-03, -1.0091e-02,\n                       1.3711e-03, -6.1305e-02,  2.1744e-02, -1.7138e-02, -2.4108e-02,\n                       5.7381e-03,  5.6101e-03, -4.6297e-02, -2.0549e-02,  9.3796e-03,\n                      -1.3045e-02, -1.3823e-02, -1.0334e-02,  9.5302e-03,  1.2724e-02,\n                       2.0662e-02,  1.7562e-02, -1.3270e-02, -1.7065e-02,  6.5435e-04,\n                       1.7950e-02,  2.2390e-03, -1.6035e-02, -4.5689e-03, -7.2734e-03,\n                      -5.8131e-03,  2.6073e-02, -8.1238e-03,  2.5201e-02, -1.1097e-02,\n                      -1.5730e-02,  9.8595e-03,  9.2561e-03, -4.0039e-02, -5.9945e-03,\n                       1.0857e-02, -6.3511e-03,  8.4925e-04, -2.3405e-02, -3.8554e-02,\n                      -2.0177e-02, -1.6887e-02,  2.4590e-03,  4.0876e-03,  1.2660e-02,\n                      -2.8040e-02,  1.3041e-02,  1.0397e-02, -2.9112e-02,  2.4001e-02,\n                      -1.9553e-02, -2.1332e-02,  1.1096e-02, -3.7820e-02,  7.6995e-03,\n                      -6.8571e-03, -3.6227e-03, -2.5042e-02, -7.7130e-03,  6.4989e-03,\n                      -5.5280e-03, -3.3246e-02,  1.6365e-02,  2.6020e-02,  1.0642e-01,\n                      -7.6236e-03, -4.3843e-03, -1.6805e-02, -8.9853e-03,  9.1312e-03,\n                       1.5773e-03, -3.5388e-02,  4.7368e-03,  2.1828e-02, -4.7183e-03,\n                       8.7501e-03, -7.3321e-03, -7.0339e-03, -2.8366e-02, -5.7675e-02,\n                       1.8388e-02,  2.1194e-01, -1.9970e-02,  2.1428e-04,  2.6936e-03,\n                       1.4154e-02,  5.0904e-03, -2.9348e-03,  2.7212e-02, -7.2922e-03,\n                       1.1085e-02,  1.2692e-02,  1.4793e-02,  2.4722e-02,  4.7567e-03,\n                       8.7199e-03, -3.7851e-02,  4.8997e-03,  3.6038e-03,  1.1657e-02,\n                       1.1694e-02, -1.5800e-02, -8.4329e-03,  5.0003e-02, -1.9181e-02,\n                      -3.0699e-03, -1.2786e-02, -2.5347e-02,  1.8416e-02,  9.7696e-03,\n                      -4.5775e-02,  2.0304e-02,  1.2309e-02,  2.0201e-02,  1.7376e-02,\n                       2.5774e-02,  1.4544e-02,  2.4568e-02,  1.0572e-02,  1.5404e-03,\n                      -2.2212e-02,  1.5754e-03,  4.2388e-02,  1.3217e-02, -9.0602e-03,\n                      -9.2841e-03,  4.3176e-02,  2.1069e-02,  1.5711e-03,  1.0919e-02,\n                       1.0764e-02,  3.8975e-03,  4.7392e-03,  6.8323e-03, -3.7512e-02,\n                       7.0954e-03,  3.0993e-03, -1.0425e-02,  1.7558e-02,  1.0902e-02,\n                      -8.0726e-02,  9.8981e-03,  3.1768e-02, -1.8348e-02,  6.3109e-03,\n                      -4.1971e-02, -5.1534e-03, -4.2615e-02,  2.3951e-02,  3.8678e-03,\n                       7.5089e-03, -4.4037e-02, -1.6267e-02,  2.5682e-02,  4.3339e-03,\n                      -1.8060e-02,  1.3243e-02, -1.0097e-03, -2.2700e-02,  3.9166e-03,\n                       2.8788e-02, -7.2477e-04, -5.8529e-03, -1.8315e-03, -1.0109e-03,\n                      -2.0224e-02, -7.3783e-03, -1.2936e-02, -9.3312e-04,  7.6612e-06,\n                      -1.0543e-02, -4.0482e-02,  3.3131e-02,  1.6930e-03,  7.5969e-03,\n                      -9.3931e-02, -2.6226e-02,  9.3686e-03,  2.8016e-02, -1.8339e-02,\n                       2.7372e-02,  2.4174e-02, -5.1421e-03,  1.5133e-02,  1.8387e-02,\n                       6.6606e-03, -3.3536e-02, -1.2846e-02, -4.1758e-02,  6.2983e-03,\n                       2.2122e-03, -2.3273e-02, -1.1892e-02, -3.7892e-03, -1.4773e-02,\n                      -1.6071e-02, -2.6760e-02, -1.0306e-01,  9.5034e-03,  7.9914e-03,\n                       6.4188e-04,  2.2125e-03,  1.3468e-02,  4.5023e-03, -3.9972e-03,\n                       2.6627e-02, -2.8558e-03,  3.3651e-03,  2.0637e-04,  8.3876e-03,\n                      -7.4449e-03, -2.0345e-03, -4.0713e-02, -4.5013e-03,  2.1668e-02,\n                       1.2471e-02, -2.0463e-02,  2.7836e-02, -4.7120e-02, -1.5329e-01,\n                       6.5577e-03, -5.9166e-03,  4.7296e-03, -2.0195e-02, -6.3345e-03,\n                       4.1565e-02,  1.0947e-02, -1.3282e-03, -1.7625e-03,  1.0506e-01,\n                       2.0738e-02,  1.7542e-02,  3.6480e-03, -1.1984e-03, -8.2838e-03,\n                       4.2581e-03,  1.4662e-02,  3.2918e-04,  7.7870e-03,  6.8880e-03,\n                       3.5549e-03,  1.4062e-02, -1.1427e-02, -3.4794e-02, -3.0189e-03,\n                      -1.6586e-02,  1.1223e-02,  1.5969e-02, -1.7715e-03, -1.9882e-02,\n                       1.4621e-02, -8.9756e-02, -5.7016e-03,  2.6499e-02,  1.7344e-02,\n                       1.2544e-02, -1.3851e-02, -3.2933e-02, -6.3042e-03, -5.3456e-03,\n                      -6.2458e-03, -2.1201e-01, -1.6960e-02, -8.8311e-03, -1.2733e-01,\n                       2.0195e-03,  7.0012e-03,  2.6076e-02, -9.0483e-02,  2.0005e-02,\n                      -2.1116e-03,  4.4190e-02,  8.6720e-03, -1.0051e-02, -1.3536e-02,\n                      -1.5170e-02,  1.4951e-02, -2.0269e-03, -3.8207e-02,  3.8443e-03,\n                      -3.2575e-02, -1.6411e-02,  3.8169e-02,  5.9220e-03,  2.2040e-03,\n                      -7.2217e-03, -7.2238e-03, -1.3513e-02,  1.5455e-02, -7.6574e-02,\n                       1.9563e-02,  1.9508e-02,  1.5758e-03,  5.1926e-02, -4.1450e-03,\n                       8.1591e-03, -2.8763e-03,  1.0317e-02, -4.1751e-03, -9.9555e-03,\n                      -5.2246e-03,  2.2684e-03,  1.0223e-02, -1.8975e-02, -1.6063e-02,\n                      -1.8757e-02, -9.4587e-03, -1.1085e-01, -2.5962e-03, -1.5520e-02,\n                       6.5281e-02,  1.7206e-02,  9.8597e-03,  3.3895e-02, -3.4903e-02,\n                       3.2434e-03, -1.8762e-02,  1.8704e-02, -2.0052e-02,  1.3173e-02,\n                       8.3069e-03,  1.4074e-03,  2.7343e-03,  4.7307e-03,  3.3936e-02,\n                       2.4393e-02,  4.2887e-03,  2.7809e-02,  2.1632e-02,  1.6888e-03,\n                      -8.1657e-03,  1.7597e-02,  8.6199e-03, -8.7671e-03, -4.6328e-03,\n                      -2.2008e-02, -5.3136e-02, -2.3629e-02, -5.7798e-02,  7.3352e-03,\n                       1.0505e-02,  1.7648e-02,  1.5144e-02, -1.5378e-02, -1.2609e-02,\n                      -3.2357e-03,  2.3470e-02, -2.8673e-02,  2.1584e-02,  2.9756e-03,\n                      -3.2933e-03,  2.8223e-02,  3.1655e-02,  1.4771e-03,  5.7820e-03,\n                      -2.2759e-03, -3.8261e-02, -1.7069e-01,  2.7105e-03,  6.3743e-03,\n                      -1.0781e-04, -4.1506e-03, -8.7554e-03, -9.4422e-03, -3.0246e-03,\n                       4.6511e-02,  6.1976e-03, -2.4792e-02,  8.3754e-03,  2.1175e-02,\n                       1.7431e-02,  2.4663e-03,  3.0823e-03, -1.9752e-02,  6.8259e-03,\n                       3.3933e-02, -5.6023e-03, -1.2522e-02, -7.9658e-03, -1.1290e-02,\n                       8.1292e-03, -3.6819e-02, -2.0692e-02,  1.8248e-03, -1.1597e-01,\n                      -8.0631e-03,  2.5125e-02,  5.5172e-03, -2.7561e-02, -1.0678e-02,\n                      -1.9555e-02,  6.9997e-03,  5.7263e-03, -4.0074e-03, -1.4413e-02,\n                       2.5197e-02,  2.0702e-02, -3.0125e-03, -4.6827e-02, -9.8081e-03,\n                       2.8702e-02, -1.1313e-02,  1.8505e-02,  7.8242e-03, -5.7330e-02,\n                       1.2440e-02, -3.3617e-02, -2.8661e-03, -4.8953e-03,  1.7624e-02,\n                       4.2202e-03,  5.0181e-03, -4.8027e-03,  4.5544e-05,  1.5992e-03,\n                       5.5043e-03,  7.0857e-03,  4.2516e-02,  3.4278e-02,  2.3029e-02,\n                       3.2682e-03, -1.1714e-03, -9.4022e-03, -9.7901e-03, -9.6427e-03,\n                      -7.6893e-03, -3.5610e-02, -7.8939e-02,  8.8758e-03,  1.8834e-01,\n                      -1.6719e-03,  4.7550e-03, -3.9309e-04,  6.5555e-03,  4.6330e-04,\n                      -1.4295e-02, -1.4889e-01,  2.0377e-04, -3.4460e-02, -8.0526e-03,\n                       1.5406e-02, -7.5789e-02, -1.4586e-02,  1.4931e-02, -5.6677e-03,\n                       5.1157e-02,  1.3309e-03, -2.4565e-03,  2.4306e-03,  5.3573e-03,\n                      -1.0469e-02,  1.8342e-02, -9.1901e-02,  1.2019e-02, -3.2292e-03,\n                      -5.0358e-03, -6.0852e-03,  8.6274e-04, -1.4166e-03, -2.0580e-02,\n                       3.4222e-02,  4.8902e-03, -3.7461e-03, -2.3443e-03,  1.3482e-04,\n                      -1.6345e-03, -5.0812e-03, -1.9650e-03, -8.2359e-02, -1.0788e-02,\n                       1.8467e-03, -4.0116e-03, -4.1791e-02,  3.4520e-03, -1.7385e-03,\n                      -4.7713e-02,  6.6184e-03,  7.5073e-03, -1.7949e-03,  4.4336e-02,\n                      -6.0064e-03,  1.6950e-02, -3.4875e-03, -1.1269e-02,  7.2871e-03,\n                      -5.7032e-03,  6.9816e-04,  1.6448e-02, -2.4014e-02,  1.5179e-02,\n                      -5.4550e-03,  1.0101e-02, -1.3163e-02,  4.4855e-03, -2.6865e-02,\n                      -3.7945e-02, -2.2654e-03,  4.4340e-02, -1.1494e-02, -4.4014e-02,\n                       2.2247e-02, -2.6179e-02, -5.0874e-04,  4.9175e-02,  8.6774e-03,\n                       6.6442e-03, -1.6603e-02,  1.6688e-02,  1.6669e-02, -2.6558e-03,\n                      -9.3832e-03, -3.2777e-02,  3.8203e-02, -2.8567e-02, -1.3446e-02,\n                       7.4579e-03, -2.5949e-04,  2.1567e-02,  1.6171e-03,  8.9974e-03,\n                      -3.2598e-02,  8.9532e-03,  1.4495e-01,  3.6240e-02,  2.8536e-02,\n                       1.9775e-02,  8.1661e-03,  2.2003e-02, -4.1635e-02,  8.4144e-03,\n                      -1.4661e-03, -1.5273e-02,  2.4340e-03, -7.1263e-03, -1.3639e-02,\n                       1.2794e-02, -1.6299e-02, -1.8331e-02, -1.6200e-03, -1.1003e-02,\n                       1.8269e-02, -1.8738e-03,  1.3899e-02, -1.1012e-02,  1.4299e-02,\n                       5.9766e-03,  1.2224e-02, -7.2613e-02,  1.9086e-02,  1.2767e-02,\n                       6.3947e-05,  3.3699e-02,  1.4340e-02, -1.8228e-02,  3.9078e-04,\n                       4.8284e-02, -9.0582e-03,  3.5755e-02,  8.3375e-03,  7.8384e-06,\n                      -8.2650e-02, -1.2919e-03, -1.4508e-02,  5.2067e-03,  1.4782e-02,\n                       1.1077e-02,  4.3383e-03,  5.2882e-03,  3.2028e-02,  1.5055e-02,\n                      -1.0029e-03,  1.0575e-02,  1.1864e-02, -4.1322e-02, -1.0240e-02,\n                      -1.3692e-02, -2.0075e-02,  2.0336e-02, -1.9683e-02, -2.6717e-02,\n                       2.9252e-02, -7.0700e-03, -2.6386e-03], requires_grad=True)),\n             ('features.7.1.block.0.scale', tensor(0.0387)),\n             ('features.7.1.block.0.zero_point', tensor(68)),\n             ('features.7.1.block.2.weight',\n              tensor([2.5717, 1.1561, 2.2160, 1.7615, 1.0951, 2.1828, 1.6261, 0.9814, 2.4246,\n                      4.3819, 3.5790, 1.4925, 2.5246, 1.6984, 0.3866, 0.3492, 0.3286, 2.9153,\n                      3.1996, 0.9732, 2.1288, 2.2626, 2.6893, 1.3886, 0.4016, 1.5242, 2.4468,\n                      1.2696, 1.2311, 1.3657, 1.6492, 0.8845, 1.8301, 2.4978, 1.9638, 0.5143,\n                      2.3237, 1.8273, 1.6691, 1.0812, 2.6377, 3.1079, 2.0827, 0.5633, 0.5012,\n                      1.4774, 0.9587, 2.5855, 1.7894, 2.4938, 1.1381, 2.6237, 0.6000, 2.1997,\n                      1.0110, 2.4029, 1.8361, 2.2048, 1.3079, 0.8880, 2.8473, 2.1781, 1.2819,\n                      2.1560, 1.0129, 2.7842, 1.6763, 1.3146, 0.3528, 1.7838, 2.1498, 1.9906,\n                      2.4589, 0.4622, 1.5659, 3.0486, 2.7582, 1.2932, 2.8857, 1.1780, 2.4439,\n                      1.9728, 2.0886, 1.1919, 1.8053, 2.0282, 0.9016, 1.8201, 1.4539, 1.9720,\n                      2.4963, 2.1650, 1.7231, 1.7660, 2.2862, 2.2090, 1.2418, 2.7681, 2.5525,\n                      1.3195, 2.1531, 1.3152, 1.3535, 2.9517, 1.9212, 0.9912, 1.8743, 2.6059,\n                      1.8177, 1.4570, 2.5264, 1.7368, 3.0722, 1.8799, 1.9607, 4.5411, 2.6243,\n                      1.2725, 1.5697, 2.4568, 1.9189, 1.7898, 1.5559, 0.8647, 1.7809, 2.7864,\n                      1.9549, 2.1196, 2.5312, 1.1764, 0.6898, 2.5701, 1.0276, 2.1888, 1.0107,\n                      1.8155, 1.0437, 2.8548, 1.0008, 0.6383, 0.8053, 0.5677, 2.9289, 2.0112,\n                      4.6641, 3.2347, 2.2269, 1.8175, 3.0542, 2.8534, 2.3848, 1.9330, 1.5866,\n                      1.5380, 1.5215, 1.5240, 2.5939, 1.3594, 1.6515, 2.0629, 2.8782, 1.1163,\n                      2.1006, 2.7883, 2.2198, 2.5448, 2.5079, 1.1010, 2.6134, 2.2540, 0.2564,\n                      2.0678, 2.7368, 1.8455, 0.7265, 1.2674, 2.5433, 2.7772, 2.4591, 2.9028,\n                      2.1056, 1.7918, 1.2534, 2.1857, 2.7649, 2.5732, 0.3621, 2.1412, 1.5237,\n                      1.0990, 2.3919, 1.6401, 1.3565, 1.2707, 1.5613, 2.7978, 1.3381, 1.3215,\n                      1.6739, 3.0097, 2.1930, 0.8593, 1.1105, 1.7662, 1.5456, 0.5480, 2.2738,\n                      0.7403, 1.5088, 2.5884, 1.8968, 1.6743, 2.5650, 2.1982, 2.4347, 3.3606,\n                      1.6771, 2.8690, 2.4850, 1.5239, 1.2463, 2.9294, 2.9003, 2.4866, 1.6357,\n                      2.0508, 2.4987, 2.3393, 0.9083, 1.5066, 3.1685, 2.6168, 2.2252, 2.3853,\n                      1.6865, 1.6984, 0.7854, 2.2778, 1.9475, 1.7001, 2.0655, 2.6528, 1.5450,\n                      1.7332, 2.0574, 2.7127, 1.3039, 1.8165, 2.1384, 1.0042, 1.7619, 1.7517,\n                      2.6754, 1.3845, 1.7038, 2.7977, 0.5294, 1.9761, 1.8237, 1.7658, 2.3410,\n                      1.6221, 2.5314, 1.0418, 3.3920, 1.7055, 0.5516, 2.3156, 2.7019, 1.0234,\n                      1.7053, 2.1810, 1.5888, 0.7074, 1.7888, 1.5966, 2.5699, 2.9351, 2.8207,\n                      1.6642, 2.5707, 1.3013, 0.5712, 1.9176, 1.3703, 3.0192, 2.6004, 1.9472,\n                      4.4079, 2.4630, 1.2770, 0.3675, 1.7915, 0.9834, 3.2421, 0.5029, 1.4890,\n                      1.3382, 2.9123, 2.7538, 2.5085, 1.0820, 2.1439, 2.4411, 2.8379, 1.9777,\n                      2.2811, 3.7354, 2.7264, 2.6103, 1.3069, 2.9100, 1.9632, 1.6461, 2.2293,\n                      2.5308, 0.8761, 2.3621, 2.0446, 1.9449, 0.9122, 2.6909, 2.1299, 3.0442,\n                      1.8159, 0.3544, 2.2212, 2.4297, 1.1389, 2.7099, 0.6117, 2.8650, 1.6599,\n                      1.1898, 2.0054, 2.2555, 1.4686, 2.4655, 1.1982, 2.5586, 3.0554, 2.9608,\n                      1.7112, 2.1318, 2.9101, 1.3889, 3.3988, 2.9116, 2.8748, 3.0117, 1.4742,\n                      1.4804, 2.4763, 2.6625, 1.7933, 0.9457, 3.0125, 1.4035, 1.2405, 2.8364,\n                      0.3469, 2.0510, 1.3148, 1.6091, 1.3546, 2.1499, 0.5858, 2.0222, 2.2323,\n                      2.6760, 2.3911, 1.1845, 0.8513, 1.5738, 2.5187, 3.4733, 1.0123, 2.4659,\n                      2.5503, 2.2070, 1.7145, 0.3949, 0.3397, 2.9736, 1.0434, 1.9703, 1.1264,\n                      1.0328, 2.7696, 1.1038, 2.5245, 3.0571, 2.7440, 1.3497, 2.6447, 3.3418,\n                      1.4493, 2.6877, 1.2041, 1.2244, 0.6828, 1.1059, 1.4871, 0.8533, 0.3073,\n                      2.6632, 2.4402, 1.6497, 1.5837, 1.8423, 1.8561, 2.3810, 3.1075, 1.5122,\n                      3.3996, 2.7544, 1.8116, 2.6540, 2.2193, 2.0023, 3.3244, 1.8982, 1.2966,\n                      1.1492, 1.4928, 2.7258, 1.4880, 1.7346, 1.3582, 1.2967, 1.3160, 2.5260,\n                      1.6309, 2.0024, 0.8415, 2.3815, 0.3353, 2.1514, 1.4972, 2.3518, 1.9767,\n                      2.1213, 0.5139, 2.8504, 1.5448, 1.8965, 0.4031, 1.9669, 3.2202, 0.4949,\n                      1.4224, 3.4615, 2.7572, 0.6345, 2.1947, 1.4517, 4.8647, 1.7332, 1.7718,\n                      1.7861, 1.4264, 2.6132, 2.3316, 1.9876, 2.2706, 1.5813, 1.5565, 1.2840,\n                      1.3989, 3.2748, 2.4022, 2.2438, 2.7600, 1.7420, 0.3926, 2.6279, 2.2337,\n                      2.8637, 2.4316, 2.6002, 1.2307, 1.0940, 2.4612, 1.0523, 2.5318, 1.9188,\n                      2.2155, 1.9109, 2.2852, 1.9792, 1.1924, 1.3348, 0.3559, 2.7694, 2.1957,\n                      1.4294, 1.8480, 1.6339, 1.4200, 1.7568, 3.0685, 2.3933, 1.4207, 2.7211,\n                      2.1681, 2.3449, 1.9507, 2.2587, 2.4227, 1.2178, 2.2468, 1.0936, 2.1372,\n                      1.3114, 0.7824, 2.8698, 1.1452, 2.4125, 2.8764, 2.5464, 2.1148, 0.8575,\n                      1.3598, 2.0107, 2.9424, 1.9851, 2.3984, 1.4723, 1.1699, 2.4628, 2.7957,\n                      2.3929, 0.9020, 0.9610, 2.6613, 2.6019, 1.2639, 1.3735, 2.3475, 2.2952,\n                      2.8385, 2.5250, 0.2525, 1.4232, 1.0891, 2.3288, 1.6790, 2.1892, 1.3178,\n                      1.0003, 2.0619, 2.7442, 1.3955, 1.9931, 2.3272, 0.5614, 2.5116, 2.3448,\n                      1.1873, 2.0447, 1.7468, 1.4918, 2.7083, 1.7027, 2.2864, 1.0623, 0.8663,\n                      1.0095, 1.3651, 0.3065, 1.3968, 1.4936, 2.8123, 2.2148, 1.5588, 1.0053,\n                      1.2218, 1.4388, 2.7655, 2.3322, 2.5436, 1.5252, 1.4388, 2.8623, 3.9172,\n                      1.8374, 2.3587, 2.6168, 3.0979, 1.3771, 1.6364, 1.5225, 2.6246, 1.6498,\n                      1.6130, 3.1253, 2.1646, 1.9200, 0.9309, 2.8146, 1.4275, 1.5444, 2.0156,\n                      1.1131, 2.0300, 3.3395, 1.4260, 3.2319, 1.3806, 1.4391, 2.1487, 3.2813,\n                      0.3856, 0.8936, 1.1247, 2.3275, 2.6788, 2.8500, 1.7155, 1.5331, 2.3762,\n                      0.2827, 2.0366, 1.2998, 1.4891, 2.7131, 0.3386, 1.2757, 2.6259, 2.5740,\n                      5.2208, 2.8692, 2.1339, 2.5706, 2.6803, 1.1937, 1.8964, 0.3510, 0.7576,\n                      1.6030, 1.4121, 3.0761, 1.3483, 2.6751, 1.2923, 1.5226, 2.2912, 1.9297,\n                      2.4926, 2.4702, 2.8550, 2.5542, 1.3812, 2.9730, 1.1082, 1.5111, 3.8551,\n                      2.1726, 2.7551, 2.4282, 0.3293, 2.9120, 2.8685, 2.6520, 2.6819, 1.6862,\n                      3.0037, 2.5350, 2.3940, 1.3391, 3.0645, 1.7797, 2.6039, 1.8207, 3.0943,\n                      1.7081, 1.2149, 1.8739, 1.9230, 2.9730, 1.9218, 0.9224, 1.1638, 2.5117,\n                      1.8018, 2.5295, 1.1198, 1.9673, 1.3094, 2.9812, 1.2605, 0.3481, 1.7337,\n                      1.4438, 0.9800, 2.3385, 0.9591, 1.3887, 2.0144, 0.9515, 1.3250, 1.0292,\n                      1.9143, 0.9490, 1.8141, 1.6985, 2.0088, 0.6529, 2.1080, 2.2292, 0.9563,\n                      2.4606, 2.1415, 1.5799, 3.0229, 3.1170, 1.0839, 1.1715, 1.2478, 1.5196,\n                      2.5618, 2.9071, 1.7576, 2.6065, 1.4827, 2.1354, 2.4496, 2.3251, 2.7536,\n                      0.9422, 2.7316, 1.4093, 0.4328, 1.8603, 3.1246, 1.5962, 2.4377, 1.3754,\n                      2.7830, 1.9901, 2.1446, 2.3780, 0.9269, 1.7457, 2.0185, 0.4174, 1.7380,\n                      2.7529, 1.1288, 2.9336, 1.2396, 1.7548, 2.2894, 1.8131, 2.5392, 0.7731,\n                      1.4542, 2.0744, 1.6345, 1.1935, 0.8042, 1.3450, 1.7828, 1.7538, 2.2524,\n                      1.9312, 3.1656, 1.4785])),\n             ('features.7.1.block.2.bias',\n              tensor([-3.9603e-01, -5.5187e-01, -9.2658e-01, -2.9040e-01, -1.7614e-01,\n                      -8.1222e-01,  1.0361e-01,  9.0376e-02, -5.3433e-01,  5.9157e+00,\n                       1.1701e+00, -6.0051e-01, -7.5014e-01, -6.2675e-01,  3.5501e+00,\n                       2.8599e-02, -9.0511e-02, -6.0400e-01, -1.2128e+00,  3.5734e-01,\n                      -1.0011e-01, -3.8844e-01, -4.9585e-01, -8.2505e-02, -4.3015e-01,\n                       3.0692e-01, -4.7629e-01, -3.4167e-01, -1.4677e-01, -7.3886e-01,\n                      -4.8528e-01, -5.3064e-01, -4.6254e-01, -5.7812e-01, -2.6304e-01,\n                       2.9814e-01, -3.4259e-01, -6.6733e-01, -3.3011e-01, -6.8799e-01,\n                      -5.5437e-01, -7.9961e-01, -6.2713e-01, -1.6936e-01, -3.5943e-01,\n                      -4.9621e-01, -1.7805e-01, -7.0545e-01,  1.8901e-03,  3.3037e-01,\n                      -1.0761e+00,  6.6833e-01,  4.1703e-02,  6.5796e-01, -9.2811e-01,\n                      -1.0728e-02, -6.3853e-01, -3.2203e-01, -2.9075e-01,  2.2964e-01,\n                      -5.5042e-01, -7.8184e-01, -1.6788e-02,  2.5198e-01, -2.2632e-01,\n                      -6.2691e-01, -6.9332e-01, -9.9021e-02,  2.5027e-02, -6.7005e-01,\n                      -9.7537e-01, -3.7933e-01,  5.8893e-01, -5.7868e-02,  1.0209e-01,\n                      -1.1066e+00, -3.4166e-01, -5.2946e-01, -1.8653e-01, -5.1490e-01,\n                      -4.0404e-01, -7.2796e-01, -7.9087e-01, -2.0478e-01, -9.9793e-01,\n                      -2.1616e-01, -2.2221e-01,  2.1037e+00,  4.8859e-01, -1.0516e-01,\n                      -1.0615e+00, -6.9148e-01,  4.0051e-01, -5.4821e-01, -1.2126e+00,\n                      -1.6972e-01, -4.3423e-01, -3.2890e-01, -9.9795e-01, -5.2512e-01,\n                       6.2028e-02,  2.0793e-01,  1.3363e-01, -3.7211e-01, -5.4631e-01,\n                       5.3750e-03, -3.2097e-01, -4.7739e-01, -4.9569e-01, -2.5760e-01,\n                      -3.3424e-01, -3.7876e-01, -8.6392e-01, -5.2923e-01, -4.2160e-01,\n                      -3.9077e-01, -7.0979e-01, -3.8184e-01, -2.1803e-01, -5.2162e-01,\n                      -1.8769e+00, -1.1836e+00, -8.4522e-01, -3.5451e-01, -8.0176e-01,\n                      -4.0814e-01, -5.2519e-01, -6.5606e-01, -4.3825e-01, -1.1192e+00,\n                      -1.7598e+00, -7.2186e-01, -6.3142e-02, -5.3106e-01,  1.8310e-02,\n                      -3.3491e-01, -1.0868e+00, -1.0746e+00, -5.6289e-01, -4.9947e-01,\n                      -1.1607e-01, -2.9003e-02, -9.7935e-01,  6.9973e-01,  5.6567e+00,\n                       7.3752e-02, -5.7111e-01, -3.9989e-01, -7.7572e-01,  1.3770e-01,\n                      -7.9327e-01,  3.6606e-01, -1.0017e+00, -4.9108e-01, -6.7178e-01,\n                      -3.2017e-01, -1.8950e-01, -3.5420e-01,  4.8636e-01, -2.3444e-01,\n                       1.3911e-01, -1.4149e-01,  7.5261e-01, -4.2073e-01, -8.4212e-01,\n                       7.1537e-01, -1.2527e-01,  2.8268e-01, -1.3920e+00, -2.3091e-01,\n                       3.1713e-01, -3.3457e-01, -8.6790e-01, -7.2983e-01,  1.2334e-01,\n                      -5.4063e-01, -6.8110e-01, -9.1217e-01,  1.0686e-01, -1.7401e-01,\n                      -5.5489e-01, -6.6501e-01, -3.4115e-01, -7.8569e-01, -3.3730e-02,\n                      -2.9425e-01, -1.4129e-01, -7.9432e-01,  2.0054e-01,  3.5544e-01,\n                      -2.0648e-01,  7.9521e-02,  9.2694e-01,  2.8029e-01, -6.3492e-01,\n                       1.9147e-01, -4.4959e-02, -1.5526e-01, -9.5475e-01, -9.2005e-01,\n                      -8.3377e-01, -4.5024e-01, -3.4763e-02, -6.7825e-02, -9.6462e-01,\n                      -3.3788e-01, -3.3370e-02, -5.2776e-01, -2.5922e-01, -1.0400e+00,\n                       2.5156e-02, -7.7947e-01, -2.3083e-02, -6.2034e-01,  1.5476e+00,\n                       8.1299e-01, -6.0261e-01,  3.6187e+00,  8.5247e-01,  5.2327e-02,\n                      -4.2565e-01, -1.1692e-01, -2.5375e-01, -8.8611e-02,  1.7504e+00,\n                       1.9542e-01,  2.8738e-01, -2.2157e-01, -8.6381e-01, -8.4038e-01,\n                      -3.7298e-01, -6.7507e-01, -7.1785e-01,  1.6821e-01, -7.6863e-01,\n                       1.4525e-01, -9.9682e-01, -6.6446e-01,  1.0208e+00, -5.0620e-01,\n                      -1.3338e-01, -2.7952e-01,  1.8532e-01, -2.4860e-01, -4.2143e-01,\n                      -3.2969e-01,  2.2263e+00, -7.5938e-01, -6.8756e-01,  1.2662e+00,\n                      -3.8085e-01, -5.6404e-01, -2.6483e-01, -1.6928e-01, -4.2530e-01,\n                      -7.2631e-01, -5.3234e-02, -4.6964e-01, -9.2397e-01, -1.1791e-01,\n                      -4.1590e-01, -3.4919e-01, -2.6857e-01,  2.5939e-01,  2.8214e+00,\n                      -8.9854e-01, -3.8169e+00,  1.3660e-02, -8.4893e-01, -3.6506e-01,\n                      -8.0345e-01, -6.4186e-01, -3.7850e-01, -8.1113e-01, -1.2393e-01,\n                      -3.6405e-01, -1.3083e+00, -1.2733e+00, -1.3801e+00, -2.5503e-01,\n                      -2.6547e+00,  6.1932e-01, -1.0483e+00, -4.1576e-01, -5.3578e-01,\n                      -3.0603e-01,  2.9686e-01, -1.0410e-01,  1.4923e+00,  3.3338e-01,\n                      -6.9188e-02, -4.2417e-01,  1.3344e-01, -5.2935e-01, -1.1426e+00,\n                      -2.1427e-01, -6.3522e-01, -4.6639e-01, -1.1773e+00, -1.7849e+00,\n                      -1.0479e+00, -5.4091e-01, -1.0811e+00, -1.6419e+00, -8.6672e-01,\n                       1.6891e-01, -2.7938e-01, -5.7502e+00, -8.0443e-01,  8.4637e-02,\n                      -3.0238e-02, -1.4647e+00, -8.8114e-01, -3.1952e-01, -8.1875e-01,\n                      -4.7836e-01, -3.0588e-01,  1.5954e-01, -2.6101e-01,  8.9898e-01,\n                      -3.6265e-01, -5.9503e-01, -1.9028e-02, -2.0451e+00, -5.5693e-01,\n                      -4.7893e-02, -7.4522e-01, -1.0667e+00,  1.6093e-02, -4.7800e-01,\n                      -3.7749e-02, -1.5717e-01,  1.3383e+00, -5.5480e-01, -5.0288e-01,\n                      -4.9591e-01,  1.1158e+00, -2.2786e-02, -6.3452e-01, -6.7229e-01,\n                       1.1284e+00, -9.9177e-01, -2.3884e-01,  3.5061e-01, -7.5835e-01,\n                      -1.0510e+00, -1.2139e+00, -2.2999e-01, -5.2427e-01, -3.8315e-01,\n                       2.8876e-01, -2.4001e-01,  7.9990e-02, -3.6343e-01, -3.3995e-01,\n                      -4.3288e-01,  6.6324e-01, -7.0694e-01, -1.4416e-01, -7.7397e-01,\n                      -7.3255e-03,  4.0187e-01, -4.8347e-01, -1.8980e+00,  6.2864e-01,\n                      -1.2506e+00, -4.5066e-01, -1.0031e-01, -1.1942e+00, -4.3305e-01,\n                      -5.2662e-01,  3.1683e-01, -1.1644e-01,  7.3539e-01, -1.7347e-01,\n                      -1.3555e+00, -2.7220e-01,  3.4902e-02, -5.6265e-01,  3.0656e-02,\n                       4.4108e-02, -9.1399e-02,  3.3036e-02, -7.7165e-01, -2.9001e-01,\n                      -2.6760e-01, -2.9760e-01, -7.3402e-01, -4.6097e-01, -3.5804e-01,\n                      -9.6761e-01, -4.0117e-01, -5.5026e-01, -5.1372e-01, -9.6463e-01,\n                      -1.3319e-01, -2.3896e-01,  3.7446e+00, -3.9296e-01, -7.3509e-01,\n                      -6.7832e-01, -7.8973e-01, -1.0602e+00, -1.0245e+00,  5.6276e-02,\n                      -4.0851e-01, -1.4919e-01, -5.4651e-01,  2.2278e-01, -2.2193e-01,\n                      -1.4729e+00, -5.2546e-01, -3.6457e-01, -6.9424e-01, -3.0140e+00,\n                      -1.1027e+00, -5.7004e-01, -4.4697e-01, -2.7552e-01, -2.2339e-01,\n                      -1.8851e+00, -6.3495e-01, -1.7475e-01, -6.3900e-01, -5.0912e-01,\n                      -5.5499e-01, -6.7770e-01, -6.0028e-02,  3.5457e-01, -3.3321e-01,\n                      -5.3295e-02, -8.1760e-01, -9.9370e-01, -3.8524e-02, -5.5695e-02,\n                      -7.4911e-01, -1.5085e-01, -3.8058e-02, -1.0074e+00, -8.5997e-01,\n                      -4.8516e-01,  1.7352e-01,  6.0928e-03, -1.2417e+00, -1.6430e-01,\n                      -2.6709e-01, -3.1026e-01, -2.9447e-02,  6.9567e-01,  2.5303e+00,\n                      -4.3832e-01, -1.1742e+00, -1.8022e+00,  1.1424e-01, -7.8476e-01,\n                      -1.2826e-01, -3.1345e+00, -5.0298e-01, -2.4433e-01,  2.5585e-02,\n                       2.3235e-01, -7.8104e-01,  1.2741e+00,  1.4873e+00, -3.0162e-01,\n                       5.7222e-01, -8.7745e-02, -1.5407e+00, -5.0037e-01, -9.2131e-01,\n                      -4.0543e-01, -3.7809e-01,  6.5815e-02, -6.4939e-01, -6.2702e-02,\n                      -8.4919e-01, -1.3026e+00, -7.5110e-01, -3.2119e+00, -6.5965e-02,\n                      -4.4232e-01, -8.2571e-02, -5.8104e-01, -4.3721e-01,  1.0633e-01,\n                      -4.2308e-01, -3.8278e-01, -4.5594e-01,  2.4179e-01,  2.7052e-01,\n                      -6.6829e-02, -7.4242e-02, -7.9898e-03, -2.7712e-01,  1.7486e-01,\n                      -2.0350e+00, -7.4218e-01, -2.1844e-01, -8.8523e-01,  7.9825e-01,\n                      -7.0844e-01,  3.5410e-01, -5.3571e-01,  4.8464e-01, -5.4654e-01,\n                      -4.2150e-01, -3.7894e-01,  5.4502e-02, -6.2909e-01, -6.5253e-01,\n                      -1.0105e+00, -4.8875e-01, -8.8250e-01, -5.7329e-01, -7.3936e-01,\n                      -1.6700e-02, -5.3748e-01, -5.8000e-01, -1.9426e-01, -3.3289e-01,\n                       1.5821e-01, -1.2702e+00, -8.1064e-03,  1.9731e+00, -6.7547e-01,\n                      -6.3882e-01, -6.4533e-01, -5.4642e-01,  9.2923e-02,  3.0653e-01,\n                      -9.4639e-01, -6.1064e-01, -1.3001e-01, -5.0648e-01, -4.0293e-01,\n                      -2.9352e-01, -6.8953e-01, -1.1239e+00, -3.6672e-01, -4.2110e-01,\n                      -4.6931e-01,  5.0420e-01,  1.3346e-01, -5.3580e-01, -6.7616e-01,\n                      -7.6294e-01, -2.4951e-01, -5.4006e-02, -3.8567e-03, -2.1770e-01,\n                      -1.0436e+00, -4.0287e-01,  5.2998e-01, -6.2564e-01, -9.6483e-01,\n                      -6.3002e-01, -3.6525e-01, -3.7861e-01, -1.2344e-02, -5.5117e-01,\n                      -1.4301e+00, -4.3278e-01, -2.8429e-02, -1.6974e-01,  3.5033e-01,\n                      -2.7684e-01,  4.4455e-02, -1.9444e-02, -3.3744e-01,  1.0958e-01,\n                       7.7131e-02, -8.8849e-01, -6.6727e-01,  3.6838e-01, -1.1935e-01,\n                       2.5359e-01, -3.8735e-01, -3.2012e-01, -4.7325e-01,  8.1549e-02,\n                      -1.6248e+00, -1.0394e+00, -5.7801e-01,  7.6176e-01, -3.5462e-01,\n                      -7.4387e-01, -6.2160e-02, -8.7818e-01, -1.3171e+00, -8.3797e-01,\n                      -5.3406e-01, -1.8420e-01, -4.0678e-01, -1.0569e+00, -6.3092e-01,\n                      -6.3015e-01, -3.0398e-01, -3.2114e-01, -1.4874e-01, -5.3775e-01,\n                      -8.3804e-01, -4.3231e-01, -2.6423e+00, -1.2284e+00, -8.1390e-01,\n                      -1.7334e+00, -1.8907e-01, -7.9509e-03, -1.4491e-01, -1.6822e-01,\n                      -6.3899e-02,  4.3733e-01, -1.3334e-02, -6.0556e-01, -4.1390e-01,\n                      -1.1104e-01, -4.8141e-01, -8.7831e-01, -7.2573e-01, -2.5658e-01,\n                       1.6237e-01,  1.1975e-01, -4.3974e-01,  6.0352e-01, -1.3980e-01,\n                      -8.3369e-01, -9.6919e-02, -3.7247e-02, -8.7051e-01, -3.0004e-01,\n                      -4.2056e+00, -4.6197e-01, -6.6732e-01, -7.2406e-01, -4.9227e-01,\n                      -9.0700e-02, -6.7085e-01,  3.4254e-03, -7.3395e-01, -2.6436e-01,\n                      -1.7469e-01, -3.4763e-01, -1.2483e-01, -9.9958e-01,  7.3143e-02,\n                      -8.5606e-01, -4.5993e-01, -1.8688e-01, -4.0589e-01, -2.4691e-01,\n                      -1.0456e-01, -2.5293e-01, -2.3990e-01,  3.4282e+00, -8.5621e-02,\n                      -3.4591e-01,  2.3366e-01,  1.0353e+00, -6.1334e-01, -2.6889e-01,\n                      -3.2453e-01, -6.6352e-01, -8.8301e-01, -4.7032e-01, -3.4805e+00,\n                      -2.2074e-01, -1.0659e+00, -3.1112e-01,  3.2862e-01, -3.0009e-01,\n                      -9.9877e-01, -2.2829e-01, -1.2188e+00,  1.3537e+00, -9.8566e-01,\n                       8.3394e-01, -3.5899e-01, -4.0940e-02, -4.8502e-01,  7.1095e-01,\n                       1.3912e+00, -7.4141e-01, -8.9354e-01,  2.2396e-01,  6.4818e-01,\n                      -7.7945e-01,  1.3741e-01, -3.7510e-01, -1.9239e+00, -7.3004e-01,\n                      -4.2172e-01, -1.9443e-01, -6.1042e-01, -6.4960e-01, -2.4140e-01,\n                       3.0163e-03,  6.5031e-01, -1.1572e+00,  9.0848e-01, -7.3114e-02,\n                      -4.0814e-01, -5.2307e-01, -8.6148e-01, -4.1939e-01, -4.3523e-01,\n                       1.2910e+00, -4.1201e-01,  1.0371e+00, -1.0173e+00, -1.6091e+00,\n                      -5.6741e-01, -6.2636e-01, -8.2445e-01,  6.5380e-01, -6.1887e-01,\n                      -7.6413e-01,  2.3883e-01, -3.5952e-01, -1.8732e-01,  5.1768e-02,\n                      -2.6473e-01, -1.6490e-01, -4.9761e-02, -1.1721e-01, -1.6361e-01,\n                      -6.7958e-01, -3.1930e-01, -7.7037e-01,  1.6999e-01, -6.0089e-01,\n                      -6.6897e-01, -6.0578e-01, -3.3156e-01, -7.3419e-01, -3.5708e-01,\n                      -1.9393e-01, -1.4965e+00, -5.8505e-01,  2.1841e-01, -2.4940e-01,\n                      -1.4212e+00,  1.3948e-01, -1.5702e+00, -4.7885e-01, -2.3347e-01,\n                       5.1754e-02, -2.0948e-01,  2.9193e-02, -3.7775e-01, -5.1958e-01,\n                      -6.2944e-01, -3.3865e-01, -4.4158e-01, -1.8590e+00, -9.2116e-01,\n                      -3.5369e-01, -4.3757e-01, -6.3909e-01,  1.4617e+00, -9.5241e-02,\n                      -2.9410e-01, -1.6292e-01, -7.6561e-01,  1.4956e-01,  4.2109e-01,\n                      -4.9114e-01, -2.1589e-01, -5.1331e-01])),\n             ('features.7.1.block.2.scale', tensor(0.1610)),\n             ('features.7.1.block.2.zero_point', tensor(55)),\n             ('features.7.1.block.3.scale', tensor(0.1372)),\n             ('features.7.1.block.3.zero_point', tensor(89)),\n             ('features.7.1.block.3._packed_params.dtype', torch.qint8),\n             ('features.7.1.block.3._packed_params._packed_params',\n              (tensor([[ 0.0438,  0.0805,  0.0212,  ...,  0.0311,  0.0169,  0.0649],\n                       [ 0.0388, -0.0552,  0.1057,  ...,  0.0164, -0.0294, -0.0141],\n                       [ 0.1578,  0.0532,  0.0443,  ...,  0.0514, -0.0762,  0.0106],\n                       ...,\n                       [-0.0459,  0.0125,  0.0695,  ...,  0.0542, -0.0028,  0.0403],\n                       [ 0.0622,  0.0572, -0.0286,  ...,  0.0373, -0.0559, -0.0298],\n                       [ 0.0173,  0.0288,  0.0850,  ...,  0.0346, -0.0058,  0.1094]],\n                      size=(3072, 768), dtype=torch.qint8,\n                      quantization_scheme=torch.per_channel_affine,\n                      scale=tensor([0.0014, 0.0012, 0.0018,  ..., 0.0014, 0.0012, 0.0014],\n                      dtype=torch.float64),\n                      zero_point=tensor([0, 0, 0,  ..., 0, 0, 0]), axis=0),\n               Parameter containing:\n               tensor([-0.0532, -0.0218, -0.0199,  ..., -0.0285, -0.0334, -0.0396],\n                      requires_grad=True))),\n             ('features.7.1.block.5.scale', tensor(0.0529)),\n             ('features.7.1.block.5.zero_point', tensor(64)),\n             ('features.7.1.block.5._packed_params.dtype', torch.qint8),\n             ('features.7.1.block.5._packed_params._packed_params',\n              (tensor([[-0.0058,  0.0849, -0.0374,  ...,  0.0000, -0.0331,  0.0748],\n                       [-0.0192, -0.0511,  0.1054,  ..., -0.0495, -0.0064,  0.0735],\n                       [-0.0695, -0.0913, -0.0422,  ...,  0.0095,  0.0341,  0.0613],\n                       ...,\n                       [ 0.0495,  0.0062,  0.0371,  ...,  0.0371,  0.0825,  0.0021],\n                       [-0.0561, -0.0031, -0.0312,  ..., -0.0093, -0.0234, -0.0779],\n                       [-0.0201, -0.0485, -0.0585,  ...,  0.0201,  0.0401,  0.0284]],\n                      size=(768, 3072), dtype=torch.qint8,\n                      quantization_scheme=torch.per_channel_affine,\n                      scale=tensor([0.0014, 0.0016, 0.0014, 0.0016, 0.0017, 0.0015, 0.0014, 0.0015, 0.0015,\n                       0.0026, 0.0018, 0.0016, 0.0019, 0.0018, 0.0025, 0.0023, 0.0024, 0.0015,\n                       0.0015, 0.0015, 0.0015, 0.0018, 0.0014, 0.0014, 0.0020, 0.0014, 0.0016,\n                       0.0015, 0.0016, 0.0016, 0.0015, 0.0014, 0.0016, 0.0013, 0.0016, 0.0016,\n                       0.0017, 0.0017, 0.0015, 0.0014, 0.0018, 0.0015, 0.0015, 0.0021, 0.0018,\n                       0.0017, 0.0014, 0.0015, 0.0016, 0.0017, 0.0013, 0.0014, 0.0016, 0.0016,\n                       0.0018, 0.0014, 0.0016, 0.0017, 0.0015, 0.0019, 0.0017, 0.0016, 0.0015,\n                       0.0015, 0.0014, 0.0014, 0.0014, 0.0017, 0.0019, 0.0015, 0.0014, 0.0013,\n                       0.0016, 0.0016, 0.0018, 0.0015, 0.0014, 0.0014, 0.0015, 0.0016, 0.0017,\n                       0.0016, 0.0017, 0.0017, 0.0017, 0.0015, 0.0016, 0.0026, 0.0014, 0.0016,\n                       0.0014, 0.0019, 0.0016, 0.0017, 0.0020, 0.0014, 0.0017, 0.0015, 0.0018,\n                       0.0015, 0.0016, 0.0015, 0.0015, 0.0017, 0.0016, 0.0016, 0.0018, 0.0017,\n                       0.0016, 0.0019, 0.0017, 0.0014, 0.0016, 0.0015, 0.0016, 0.0022, 0.0016,\n                       0.0016, 0.0018, 0.0016, 0.0013, 0.0017, 0.0015, 0.0014, 0.0018, 0.0013,\n                       0.0024, 0.0015, 0.0015, 0.0015, 0.0016, 0.0014, 0.0016, 0.0016, 0.0016,\n                       0.0016, 0.0014, 0.0016, 0.0014, 0.0015, 0.0014, 0.0015, 0.0015, 0.0017,\n                       0.0017, 0.0021, 0.0014, 0.0017, 0.0019, 0.0014, 0.0016, 0.0016, 0.0017,\n                       0.0015, 0.0017, 0.0014, 0.0015, 0.0017, 0.0015, 0.0016, 0.0017, 0.0015,\n                       0.0020, 0.0016, 0.0015, 0.0014, 0.0018, 0.0017, 0.0014, 0.0015, 0.0021,\n                       0.0015, 0.0018, 0.0015, 0.0017, 0.0017, 0.0013, 0.0013, 0.0016, 0.0016,\n                       0.0020, 0.0017, 0.0016, 0.0016, 0.0016, 0.0015, 0.0021, 0.0016, 0.0017,\n                       0.0017, 0.0017, 0.0016, 0.0016, 0.0016, 0.0016, 0.0017, 0.0015, 0.0016,\n                       0.0017, 0.0015, 0.0019, 0.0016, 0.0018, 0.0020, 0.0016, 0.0016, 0.0014,\n                       0.0015, 0.0016, 0.0015, 0.0015, 0.0016, 0.0018, 0.0055, 0.0018, 0.0014,\n                       0.0016, 0.0023, 0.0014, 0.0016, 0.0016, 0.0016, 0.0014, 0.0017, 0.0015,\n                       0.0019, 0.0017, 0.0019, 0.0016, 0.0015, 0.0020, 0.0015, 0.0015, 0.0015,\n                       0.0015, 0.0014, 0.0015, 0.0016, 0.0027, 0.0017, 0.0016, 0.0016, 0.0014,\n                       0.0015, 0.0015, 0.0016, 0.0014, 0.0014, 0.0017, 0.0020, 0.0019, 0.0017,\n                       0.0014, 0.0017, 0.0016, 0.0015, 0.0015, 0.0017, 0.0016, 0.0017, 0.0016,\n                       0.0017, 0.0015, 0.0016, 0.0013, 0.0019, 0.0044, 0.0016, 0.0014, 0.0015,\n                       0.0017, 0.0020, 0.0016, 0.0015, 0.0017, 0.0026, 0.0014, 0.0017, 0.0015,\n                       0.0017, 0.0015, 0.0015, 0.0016, 0.0022, 0.0019, 0.0018, 0.0015, 0.0015,\n                       0.0018, 0.0019, 0.0014, 0.0022, 0.0019, 0.0014, 0.0013, 0.0016, 0.0017,\n                       0.0017, 0.0022, 0.0018, 0.0016, 0.0017, 0.0017, 0.0015, 0.0016, 0.0016,\n                       0.0014, 0.0019, 0.0020, 0.0020, 0.0015, 0.0021, 0.0015, 0.0015, 0.0013,\n                       0.0014, 0.0016, 0.0068, 0.0016, 0.0014, 0.0017, 0.0015, 0.0014, 0.0018,\n                       0.0018, 0.0020, 0.0015, 0.0016, 0.0016, 0.0017, 0.0015, 0.0017, 0.0016,\n                       0.0018, 0.0016, 0.0016, 0.0016, 0.0019, 0.0017, 0.0015, 0.0018, 0.0014,\n                       0.0014, 0.0015, 0.0017, 0.0016, 0.0016, 0.0014, 0.0015, 0.0015, 0.0016,\n                       0.0016, 0.0014, 0.0015, 0.0015, 0.0014, 0.0015, 0.0017, 0.0015, 0.0015,\n                       0.0028, 0.0016, 0.0017, 0.0016, 0.0014, 0.0015, 0.0019, 0.0019, 0.0013,\n                       0.0016, 0.0018, 0.0014, 0.0014, 0.0016, 0.0020, 0.0017, 0.0016, 0.0017,\n                       0.0014, 0.0015, 0.0016, 0.0017, 0.0026, 0.0016, 0.0018, 0.0015, 0.0017,\n                       0.0019, 0.0013, 0.0016, 0.0016, 0.0021, 0.0016, 0.0015, 0.0016, 0.0027,\n                       0.0018, 0.0018, 0.0017, 0.0016, 0.0014, 0.0037, 0.0015, 0.0018, 0.0018,\n                       0.0016, 0.0015, 0.0019, 0.0016, 0.0023, 0.0016, 0.0013, 0.0014, 0.0015,\n                       0.0018, 0.0016, 0.0015, 0.0014, 0.0016, 0.0016, 0.0019, 0.0014, 0.0015,\n                       0.0018, 0.0014, 0.0016, 0.0016, 0.0017, 0.0016, 0.0015, 0.0018, 0.0015,\n                       0.0017, 0.0019, 0.0016, 0.0015, 0.0022, 0.0016, 0.0015, 0.0016, 0.0017,\n                       0.0015, 0.0015, 0.0015, 0.0016, 0.0015, 0.0016, 0.0019, 0.0018, 0.0019,\n                       0.0015, 0.0021, 0.0015, 0.0015, 0.0015, 0.0019, 0.0035, 0.0017, 0.0016,\n                       0.0015, 0.0017, 0.0018, 0.0017, 0.0017, 0.0017, 0.0017, 0.0018, 0.0016,\n                       0.0014, 0.0018, 0.0015, 0.0015, 0.0017, 0.0015, 0.0026, 0.0016, 0.0016,\n                       0.0016, 0.0016, 0.0015, 0.0016, 0.0020, 0.0016, 0.0016, 0.0014, 0.0018,\n                       0.0016, 0.0016, 0.0014, 0.0015, 0.0016, 0.0014, 0.0024, 0.0013, 0.0016,\n                       0.0014, 0.0016, 0.0016, 0.0014, 0.0015, 0.0017, 0.0014, 0.0017, 0.0015,\n                       0.0015, 0.0015, 0.0015, 0.0014, 0.0014, 0.0019, 0.0015, 0.0015, 0.0015,\n                       0.0015, 0.0015, 0.0015, 0.0014, 0.0018, 0.0018, 0.0020, 0.0015, 0.0025,\n                       0.0014, 0.0016, 0.0015, 0.0016, 0.0017, 0.0016, 0.0016, 0.0015, 0.0021,\n                       0.0013, 0.0017, 0.0015, 0.0015, 0.0017, 0.0016, 0.0016, 0.0017, 0.0015,\n                       0.0015, 0.0016, 0.0023, 0.0016, 0.0015, 0.0015, 0.0015, 0.0018, 0.0016,\n                       0.0015, 0.0014, 0.0017, 0.0017, 0.0025, 0.0014, 0.0014, 0.0014, 0.0015,\n                       0.0018, 0.0016, 0.0016, 0.0016, 0.0014, 0.0016, 0.0014, 0.0014, 0.0015,\n                       0.0015, 0.0016, 0.0026, 0.0015, 0.0016, 0.0014, 0.0016, 0.0016, 0.0016,\n                       0.0019, 0.0015, 0.0014, 0.0016, 0.0020, 0.0018, 0.0016, 0.0026, 0.0021,\n                       0.0017, 0.0018, 0.0015, 0.0014, 0.0019, 0.0017, 0.0018, 0.0015, 0.0015,\n                       0.0015, 0.0016, 0.0016, 0.0015, 0.0015, 0.0015, 0.0016, 0.0016, 0.0014,\n                       0.0019, 0.0019, 0.0019, 0.0019, 0.0015, 0.0018, 0.0016, 0.0015, 0.0017,\n                       0.0018, 0.0016, 0.0035, 0.0015, 0.0014, 0.0014, 0.0016, 0.0016, 0.0016,\n                       0.0025, 0.0014, 0.0016, 0.0017, 0.0014, 0.0025, 0.0020, 0.0015, 0.0014,\n                       0.0045, 0.0013, 0.0014, 0.0016, 0.0014, 0.0016, 0.0016, 0.0022, 0.0017,\n                       0.0016, 0.0015, 0.0017, 0.0019, 0.0015, 0.0016, 0.0015, 0.0015, 0.0018,\n                       0.0015, 0.0016, 0.0019, 0.0016, 0.0015, 0.0021, 0.0015, 0.0014, 0.0028,\n                       0.0016, 0.0016, 0.0018, 0.0021, 0.0015, 0.0021, 0.0015, 0.0018, 0.0016,\n                       0.0018, 0.0014, 0.0016, 0.0016, 0.0015, 0.0016, 0.0017, 0.0016, 0.0015,\n                       0.0015, 0.0015, 0.0016, 0.0015, 0.0015, 0.0014, 0.0015, 0.0014, 0.0017,\n                       0.0015, 0.0017, 0.0014, 0.0015, 0.0017, 0.0013, 0.0015, 0.0017, 0.0014,\n                       0.0014, 0.0014, 0.0017, 0.0015, 0.0014, 0.0016, 0.0017, 0.0015, 0.0017,\n                       0.0017, 0.0014, 0.0016, 0.0015, 0.0017, 0.0049, 0.0016, 0.0017, 0.0019,\n                       0.0017, 0.0014, 0.0015, 0.0020, 0.0015, 0.0015, 0.0016, 0.0014, 0.0016,\n                       0.0015, 0.0015, 0.0016, 0.0015, 0.0015, 0.0016, 0.0017, 0.0015, 0.0015,\n                       0.0016, 0.0018, 0.0018, 0.0017, 0.0016, 0.0016, 0.0015, 0.0017, 0.0019,\n                       0.0014, 0.0015, 0.0021, 0.0017, 0.0016, 0.0017, 0.0019, 0.0016, 0.0016,\n                       0.0015, 0.0018, 0.0030, 0.0014, 0.0019, 0.0016, 0.0015, 0.0016, 0.0014,\n                       0.0016, 0.0014, 0.0018, 0.0015, 0.0016, 0.0016, 0.0016, 0.0017, 0.0017,\n                       0.0021, 0.0016, 0.0017], dtype=torch.float64),\n                      zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n                      axis=0),\n               Parameter containing:\n               tensor([-3.9445e-02,  1.4090e-02, -8.8922e-02,  2.2187e-02,  1.5685e-02,\n                        1.4363e-02,  2.0869e-02,  6.8448e-03, -2.4802e-02, -4.2284e-01,\n                        3.7941e-02,  3.7126e-02, -4.2957e-03,  6.1067e-03, -3.7553e-01,\n                       -3.4173e-02, -3.5609e-02, -3.9216e-03,  4.0028e-02, -7.7484e-03,\n                       -8.3557e-03, -1.5902e-02, -8.3662e-02,  4.5221e-03, -3.9806e-02,\n                       -1.4039e-02, -1.7472e-02, -3.2805e-02,  6.7591e-04,  1.7380e-02,\n                        2.2693e-02, -1.1312e-03, -2.8833e-02,  1.2007e-02,  4.3303e-03,\n                       -9.3802e-03,  1.8691e-03, -1.5932e-02, -3.7701e-04, -1.2492e-02,\n                        5.8560e-03,  2.8203e-02, -2.3852e-02,  3.4399e-02,  3.2373e-02,\n                        1.2622e-02,  4.8664e-03,  4.5638e-03, -4.3766e-02,  7.0817e-02,\n                       -3.0503e-02,  2.4633e-03, -1.6744e-02, -4.2027e-02, -1.1806e-02,\n                       -1.4535e-02, -2.1587e-03,  4.3440e-03,  8.9063e-03,  9.8414e-03,\n                       -1.8519e-02,  8.5366e-03,  3.3980e-02, -3.7875e-02,  8.0618e-03,\n                        9.1213e-03,  1.3276e-02, -3.2591e-03, -4.4323e-02,  1.5555e-02,\n                       -2.1318e-02, -2.7443e-02,  1.0974e-02, -1.5253e-03, -2.5966e-02,\n                       -5.0887e-03, -4.5962e-04,  2.3013e-02, -3.2227e-02, -2.0146e-04,\n                        2.3215e-02, -1.9853e-02,  1.8664e-02, -1.5246e-02,  7.4119e-03,\n                        1.9258e-02,  5.8298e-03, -4.1288e-02, -7.5605e-03, -3.9658e-03,\n                       -1.7900e-02,  2.3087e-03,  4.7312e-03,  9.0162e-03, -9.8360e-03,\n                       -9.0239e-03, -6.1756e-03, -1.6538e-02,  1.0379e-02, -5.4413e-03,\n                       -1.2253e-03,  3.7604e-02,  4.1007e-03,  5.2821e-03,  2.2496e-02,\n                       -2.0099e-04, -8.8028e-03,  1.3539e-02, -3.5524e-03, -5.1043e-03,\n                        3.1249e-02, -1.3448e-02, -2.1069e-02, -1.4181e-02, -3.5852e-02,\n                       -1.4904e-03,  8.5167e-03, -1.0392e-02,  1.7174e-02, -1.1018e-02,\n                       -2.8975e-02, -8.9725e-03,  4.2967e-03,  2.5359e-02, -4.3573e-03,\n                       -2.4050e-02, -9.4237e-03, -3.7033e-03,  1.4202e-03, -1.0420e-02,\n                       -6.1402e-02,  7.4607e-03, -4.8713e-03,  1.8658e-03,  3.0078e-02,\n                       -1.1625e-02,  2.7127e-02,  1.5958e-02, -1.4143e-02, -1.7976e-02,\n                       -1.5439e-02, -1.1213e-02,  2.5562e-02, -1.1860e-01, -4.4809e-01,\n                        8.2491e-03, -3.3396e-02, -1.0164e-02, -1.7002e-02, -1.4320e-02,\n                        1.0640e-02,  7.1946e-03,  2.6087e-02,  7.0414e-03,  2.7586e-02,\n                       -1.1632e-02, -9.8480e-04, -2.2640e-02, -2.5298e-02,  5.3450e-04,\n                        4.8509e-03,  1.6571e-03,  5.5534e-03,  6.5213e-03, -1.5229e-02,\n                        4.1861e-02,  5.4771e-02, -1.4162e-02, -2.2837e-02,  2.3004e-02,\n                        3.1110e-03, -1.8324e-02, -1.1561e-03, -3.9338e-03, -4.6995e-03,\n                        1.2927e-02, -1.2200e-02, -7.3078e-04,  2.7014e-02,  1.3872e-02,\n                        1.5850e-02, -2.2682e-04, -1.9903e-02, -1.0669e-02,  9.2862e-03,\n                        2.6387e-02,  4.6232e-02, -5.2031e-03, -9.0553e-03,  2.0928e-02,\n                        9.3605e-04,  8.4427e-03, -1.6050e-02,  9.6179e-03, -1.3684e-02,\n                       -4.5060e-03,  1.4187e-02,  1.6699e-04,  1.0275e-02, -1.8563e-02,\n                        1.4881e-02,  3.1580e-02, -1.5674e-02,  2.0689e-02, -1.7747e-02,\n                        3.2889e-02,  1.2515e-02, -1.6664e-02, -1.7448e-03, -3.0185e-02,\n                       -3.5240e-03,  1.1160e-02, -4.8287e-03, -6.4854e-02,  2.8914e-01,\n                       -1.4938e-02, -3.5795e-02, -3.4071e-01, -5.2267e-02, -7.1976e-03,\n                       -1.1472e-02,  1.2227e-02, -5.7207e-03,  2.1930e-02,  3.1039e-02,\n                       -7.6201e-03,  1.8176e-02,  7.3955e-03, -1.7126e-02,  1.1757e-02,\n                        9.7047e-04, -6.3209e-04, -1.8255e-02,  1.0280e-03, -1.8548e-02,\n                        2.0571e-02, -7.3225e-03,  3.9383e-03, -5.8591e-03, -1.1136e-03,\n                       -8.9673e-03,  5.7769e-03, -1.5774e-03,  1.0238e-02,  1.5199e-02,\n                        3.7622e-05,  3.0997e-02, -5.8389e-02,  2.0984e-02, -5.3635e-02,\n                       -8.4008e-03, -2.6047e-03,  9.5908e-03,  1.7746e-02,  4.2664e-03,\n                       -2.0981e-02, -1.5687e-02, -1.4723e-02,  6.2856e-04, -4.1824e-02,\n                       -1.2473e-02, -1.9512e-02,  5.2384e-02, -2.6932e-02,  2.1592e-02,\n                       -4.7252e-04,  1.0723e-01,  1.6894e-02, -1.0127e-01,  1.5714e-02,\n                       -1.4398e-03,  1.8340e-02,  2.6998e-03, -4.0491e-02, -4.9308e-03,\n                       -3.5959e-02,  3.0234e-02, -2.8122e-02, -4.0044e-02,  5.9223e-03,\n                        2.9819e-02,  2.1381e-02, -3.2089e-02,  3.4950e-04, -3.9449e-02,\n                        7.3702e-02, -2.6099e-02, -3.1906e-02, -4.5666e-01, -1.3313e-02,\n                        6.0230e-03,  8.0456e-02, -1.3775e-02, -2.8205e-03, -1.0034e-03,\n                        2.5928e-02, -1.0083e-02,  1.5529e-03, -1.0789e-01, -3.7943e-02,\n                        2.6015e-02,  5.7346e-03,  3.6989e-04,  2.0016e-02,  4.4909e-02,\n                       -1.0743e-02,  2.6935e-02, -2.4103e-01, -5.3262e-02, -7.7730e-04,\n                        1.8130e-02, -7.1050e-03,  1.2194e-03,  4.5357e-04, -1.1101e-02,\n                        2.5614e-02, -4.0225e-03,  7.9652e-02,  2.1272e-02,  1.0733e-02,\n                       -8.8439e-03, -9.4302e-03, -1.0532e-02, -9.6934e-03, -4.9708e-03,\n                       -6.3343e-02, -1.3235e-02, -1.7807e-03,  3.2233e-02,  1.1285e-03,\n                       -2.3188e-02, -2.5504e-02, -2.8141e-02,  3.2447e-02, -9.7317e-03,\n                        6.5611e-03, -4.8400e-02,  3.3570e-02,  9.7855e-03,  3.7285e-02,\n                       -1.9982e-01, -1.6626e-03, -3.0502e-02,  1.1885e-02, -1.3203e-02,\n                        1.5077e-03, -3.2768e-02, -3.2891e-02, -2.8958e-02, -7.6468e-03,\n                       -1.8255e-02,  2.7313e-03,  8.8297e-03,  3.2168e-02,  1.1351e-02,\n                        2.9974e-02,  1.8774e-02, -3.8324e-02,  1.1230e-02, -2.9887e-02,\n                       -5.7612e-03, -1.1139e-02,  3.2211e-03, -3.4342e-02, -2.9184e-03,\n                       -1.4846e-02,  4.2461e-02, -5.3079e-03,  4.0945e-02, -1.5760e-04,\n                        9.9381e-03, -6.5164e-03,  3.2083e-02, -3.3912e-03,  1.7148e-03,\n                       -1.6737e-02, -2.4337e-02,  4.1273e-03, -1.5317e-02, -1.8375e-03,\n                       -3.0038e-02, -2.2168e-02, -5.3225e-02, -2.6585e-02, -1.5049e-02,\n                       -5.5375e-03, -4.9005e-04, -3.1463e-02, -8.6243e-03, -2.7328e-02,\n                        2.4509e-02, -6.9901e-02, -1.5698e-02, -3.5961e-03, -1.5663e-02,\n                       -3.0760e-02, -1.0652e-02,  9.8481e-02,  1.6291e-02, -1.4073e-02,\n                        3.8503e-02, -2.1546e-02, -6.7863e-03, -3.1047e-03,  3.4778e-02,\n                        1.5881e-02, -1.8103e-02, -6.4943e-03, -3.0548e-02,  4.8424e-03,\n                        5.8075e-03, -1.2702e-02, -1.3571e-02,  2.3782e-02,  3.1841e-02,\n                        2.1150e-02, -3.8628e-02,  8.3061e-03,  1.2963e-02, -1.5223e-02,\n                        3.3690e-02,  1.5345e-02, -5.4017e-04,  1.3411e-02,  1.7758e-02,\n                       -6.8650e-02,  1.2626e-02,  2.2674e-03,  1.9719e-02, -7.8495e-03,\n                        1.7277e-02, -2.7255e-02,  7.6463e-03, -1.5961e-02, -3.8080e-02,\n                        8.8987e-03,  6.1700e-02, -6.8770e-03, -2.9045e-04, -2.6197e-03,\n                        5.3635e-03, -8.6691e-03,  2.5703e-02, -6.2691e-03,  5.7082e-03,\n                        6.5392e-03, -7.1938e-02, -7.1333e-02,  2.5179e-02,  6.2324e-02,\n                       -6.7817e-04, -3.9639e-02,  2.2873e-02, -1.3278e-02,  1.0280e-02,\n                       -8.1526e-03,  2.1100e-01,  1.6757e-03,  5.3478e-03,  2.0339e-03,\n                       -5.5923e-03, -2.1905e-04,  7.8479e-03, -9.3098e-03,  3.7321e-02,\n                       -5.1254e-03,  1.6684e-02, -1.0817e-02, -6.1898e-03, -5.9919e-02,\n                        1.5392e-02, -5.1803e-04, -9.8433e-03,  3.7920e-03,  2.9961e-02,\n                       -4.4655e-03, -2.7166e-03, -1.9331e-02, -2.2856e-02, -1.6315e-02,\n                        7.5852e-03, -2.0621e-02, -1.5824e-02, -1.1170e-03,  5.4505e-03,\n                       -1.8680e-02,  5.4541e-02,  8.2385e-03, -3.1532e-02, -2.1427e-04,\n                       -1.0173e-02,  1.3514e-02, -5.2474e-02,  1.7326e-03, -1.5655e-02,\n                        7.2781e-03, -7.2008e-03,  3.5619e-02, -1.6909e-02,  2.0093e-02,\n                       -4.9452e-02, -8.9738e-03, -5.4513e-03, -5.3299e-04, -7.6153e-04,\n                       -7.3252e-03, -3.5713e-03,  2.1483e-02, -1.7657e-02,  2.5962e-02,\n                        5.3861e-03,  1.3966e-02, -1.8857e-02,  8.4191e-03,  2.9624e-02,\n                        3.0468e-02,  6.0119e-03, -3.4483e-02, -4.9991e-03,  3.2668e-03,\n                       -4.8329e-02,  6.2480e-03, -2.1929e-02, -7.5867e-02,  1.5481e-02,\n                       -6.0552e-04, -4.8600e-04, -2.1049e-02, -2.1031e-02,  5.3041e-04,\n                       -1.1088e-02, -3.6248e-02,  3.9008e-04,  5.0455e-03,  2.1579e-02,\n                        4.5946e-03,  9.2991e-03, -4.7347e-03, -3.6461e-02, -7.9008e-04,\n                        3.6578e-02, -3.3501e-02, -2.8878e-02, -1.3727e-02,  1.7452e-03,\n                       -6.0519e-03,  9.4252e-03, -1.7817e-02,  7.5246e-03, -1.0960e-02,\n                        1.1437e-02, -1.3457e-02,  5.4522e-04,  4.4532e-02, -3.5599e-02,\n                        9.7189e-03, -1.6646e-02, -2.2886e-02,  1.9504e-02, -2.5614e-02,\n                        1.0656e-02, -1.7773e-03,  9.7954e-04, -4.8216e-03, -7.5176e-03,\n                        2.9566e-02,  8.1639e-03, -1.8979e-02, -1.0058e-02, -5.6104e-03,\n                        2.7095e-02,  3.5844e-03,  1.4752e-02, -4.4921e-02, -7.3004e-03,\n                        2.1215e-02,  1.6282e-02, -1.4587e-02,  2.0902e-02, -1.8304e-02,\n                       -4.0049e-02,  2.7477e-02, -1.1236e-02,  2.2116e-01,  3.3106e-01,\n                        1.3672e-02,  2.5793e-02,  1.8719e-02,  8.2146e-03, -6.4179e-02,\n                       -2.5227e-02,  9.6503e-03,  2.5707e-02, -5.4538e-03,  9.2332e-04,\n                        6.1037e-02,  5.2213e-03,  1.9306e-02, -1.7775e-02, -2.2696e-03,\n                       -1.2422e-02,  2.8018e-02,  1.4968e-02,  6.7801e-03, -1.6525e-03,\n                       -1.8353e-01, -2.8281e-02, -4.1503e-02,  1.2161e-02, -2.9227e-02,\n                       -2.6249e-02, -5.2109e-02, -1.0173e-02, -2.6990e-03,  5.6367e-02,\n                        4.1148e-02,  1.8352e-03,  1.1708e-02, -5.7485e-03, -2.2406e-02,\n                       -9.5222e-03,  2.3249e-02, -1.9216e-02,  5.1897e-03,  2.8077e-03,\n                        3.1960e-04, -4.1685e-02,  7.5111e-03,  8.7431e-03,  2.6978e-02,\n                       -8.4157e-02,  2.4153e-02,  5.0752e-03,  4.1819e-02,  6.0805e-03,\n                       -9.2457e-03, -4.4920e-03, -1.9028e-02, -1.4011e-02, -7.8782e-03,\n                       -4.8144e-03,  8.2332e-02, -1.4738e-02, -3.2984e-02, -1.8244e-03,\n                       -1.2212e-02,  1.5563e-02, -5.1639e-03,  1.0350e-02, -1.5952e-02,\n                        3.9440e-02,  5.3814e-03,  2.8794e-03, -1.4768e-02,  2.6946e-02,\n                       -2.7503e-02,  4.0246e-02, -1.0612e-02, -3.2054e-03,  4.1849e-02,\n                        5.2572e-04, -4.0277e-02,  6.0862e-03,  3.1514e-02,  1.5461e-01,\n                       -2.6317e-02, -2.8146e-02, -1.4802e-02,  1.4906e-02,  1.3152e-02,\n                        3.0533e-02, -4.6285e-03,  6.5552e-03, -7.5964e-03,  2.7937e-02,\n                        1.2127e-02, -1.2056e-02,  7.2347e-03, -6.5057e-03, -8.2022e-03,\n                        1.4963e-02,  3.0269e-02,  1.4218e-02, -1.8850e-02, -8.1790e-03,\n                       -6.4492e-02, -6.3931e-03,  1.9078e-02,  2.4129e-02,  1.5320e-02,\n                       -1.1089e-02,  3.5464e-02,  3.3628e-02,  2.8661e-02, -2.1450e-02,\n                       -4.5664e-03,  2.8291e-03, -1.0837e-02,  2.8577e-02, -2.4161e-02,\n                       -1.8870e-02,  4.8890e-03,  6.9799e-03,  5.7072e-03,  1.3531e-03,\n                        1.4511e-02, -1.2858e-02, -2.5873e-02,  9.6485e-04,  2.4151e-02,\n                       -1.0118e-03,  2.5574e-02, -9.3615e-03, -4.6380e-03, -1.1496e-02,\n                       -5.5653e-03,  8.9105e-03, -1.1511e-02,  2.8597e-03, -4.6782e-02,\n                       -2.3556e-02,  1.0251e-02, -2.4765e-02,  2.8368e-02, -1.0617e-02,\n                        7.8369e-03, -9.6187e-03, -3.0581e-03, -2.8514e-02,  1.4108e-02,\n                        1.4990e-03,  1.8938e-02, -3.7641e-02,  2.4527e-02,  2.3320e-02,\n                        2.4717e-03,  2.4108e-02, -8.6024e-03, -2.2179e-03, -4.7169e-03,\n                       -8.3346e-03, -1.0154e-02,  1.1578e-02, -1.2853e-02,  2.2012e-03,\n                        3.7611e-02,  7.6882e-03,  2.6270e-02, -1.2263e-03,  3.0802e-02,\n                       -1.0706e-02, -7.5643e-03, -1.3084e-02,  2.3190e-02,  2.0734e-02,\n                       -3.4660e-02,  5.5479e-03,  7.0328e-03,  9.1805e-04,  9.3226e-03,\n                        9.2114e-03,  1.1124e-02,  6.5529e-04,  2.8647e-03,  3.3251e-02,\n                        2.6666e-04,  1.7647e-02,  3.1008e-02], requires_grad=True))),\n             ('features.7.2.layer_scale',\n              tensor([[[ 0.1497]],\n              \n                      [[ 0.1667]],\n              \n                      [[ 0.8370]],\n              \n                      [[ 0.1401]],\n              \n                      [[ 0.1782]],\n              \n                      [[-0.1737]],\n              \n                      [[ 0.1472]],\n              \n                      [[ 0.1555]],\n              \n                      [[ 0.1694]],\n              \n                      [[ 0.8587]],\n              \n                      [[ 0.2801]],\n              \n                      [[ 0.1672]],\n              \n                      [[-0.7702]],\n              \n                      [[ 0.1733]],\n              \n                      [[ 0.7546]],\n              \n                      [[-0.4485]],\n              \n                      [[-0.4371]],\n              \n                      [[-0.1729]],\n              \n                      [[-0.2057]],\n              \n                      [[ 0.1795]],\n              \n                      [[ 0.1568]],\n              \n                      [[ 0.8522]],\n              \n                      [[ 0.8712]],\n              \n                      [[-0.1671]],\n              \n                      [[-0.3621]],\n              \n                      [[ 0.1617]],\n              \n                      [[-0.1746]],\n              \n                      [[-0.1738]],\n              \n                      [[-0.1672]],\n              \n                      [[ 0.1640]],\n              \n                      [[-0.1562]],\n              \n                      [[ 0.1816]],\n              \n                      [[-0.2474]],\n              \n                      [[-0.1536]],\n              \n                      [[ 0.8436]],\n              \n                      [[ 0.2282]],\n              \n                      [[-0.1769]],\n              \n                      [[-0.1668]],\n              \n                      [[ 0.1504]],\n              \n                      [[-0.1714]],\n              \n                      [[ 0.1781]],\n              \n                      [[ 0.2251]],\n              \n                      [[-0.1753]],\n              \n                      [[ 0.2100]],\n              \n                      [[-0.2669]],\n              \n                      [[ 0.1679]],\n              \n                      [[-0.1818]],\n              \n                      [[-0.2311]],\n              \n                      [[ 0.1602]],\n              \n                      [[ 0.8592]],\n              \n                      [[ 0.1719]],\n              \n                      [[-0.1905]],\n              \n                      [[-0.1897]],\n              \n                      [[-0.1950]],\n              \n                      [[ 0.1633]],\n              \n                      [[ 0.1962]],\n              \n                      [[ 0.1797]],\n              \n                      [[-0.2007]],\n              \n                      [[ 0.1647]],\n              \n                      [[-0.1724]],\n              \n                      [[ 0.8739]],\n              \n                      [[-0.1663]],\n              \n                      [[ 0.1533]],\n              \n                      [[-0.1545]],\n              \n                      [[-0.1816]],\n              \n                      [[-0.1557]],\n              \n                      [[ 0.1703]],\n              \n                      [[ 0.1500]],\n              \n                      [[ 0.3851]],\n              \n                      [[-0.1669]],\n              \n                      [[ 0.1589]],\n              \n                      [[ 0.2078]],\n              \n                      [[ 0.1740]],\n              \n                      [[ 0.2427]],\n              \n                      [[ 0.1543]],\n              \n                      [[-0.1904]],\n              \n                      [[ 0.1889]],\n              \n                      [[ 0.1723]],\n              \n                      [[-0.1758]],\n              \n                      [[-0.1610]],\n              \n                      [[-0.8433]],\n              \n                      [[-0.1605]],\n              \n                      [[-0.1900]],\n              \n                      [[ 0.1711]],\n              \n                      [[ 0.1716]],\n              \n                      [[ 0.1762]],\n              \n                      [[ 0.1740]],\n              \n                      [[-0.6222]],\n              \n                      [[-0.1601]],\n              \n                      [[-0.1815]],\n              \n                      [[-0.2082]],\n              \n                      [[ 0.1553]],\n              \n                      [[ 0.1650]],\n              \n                      [[ 0.1492]],\n              \n                      [[-0.1520]],\n              \n                      [[ 0.1565]],\n              \n                      [[ 0.1588]],\n              \n                      [[ 0.1670]],\n              \n                      [[ 0.1764]],\n              \n                      [[-0.1605]],\n              \n                      [[-0.1694]],\n              \n                      [[ 0.1566]],\n              \n                      [[ 0.1561]],\n              \n                      [[ 0.1675]],\n              \n                      [[ 0.1729]],\n              \n                      [[ 0.1566]],\n              \n                      [[ 0.1604]],\n              \n                      [[-0.1629]],\n              \n                      [[ 0.1479]],\n              \n                      [[-0.1714]],\n              \n                      [[-0.2382]],\n              \n                      [[ 0.1480]],\n              \n                      [[-0.8346]],\n              \n                      [[-0.1422]],\n              \n                      [[ 0.1380]],\n              \n                      [[-0.6407]],\n              \n                      [[ 0.2031]],\n              \n                      [[-0.1599]],\n              \n                      [[-0.1443]],\n              \n                      [[ 0.8513]],\n              \n                      [[-0.1408]],\n              \n                      [[ 0.1638]],\n              \n                      [[ 0.1554]],\n              \n                      [[ 0.1783]],\n              \n                      [[-0.1562]],\n              \n                      [[-0.1641]],\n              \n                      [[ 0.1564]],\n              \n                      [[-0.1457]],\n              \n                      [[-0.1726]],\n              \n                      [[-0.1561]],\n              \n                      [[ 0.2152]],\n              \n                      [[ 0.1791]],\n              \n                      [[-0.1738]],\n              \n                      [[ 0.1739]],\n              \n                      [[-0.1578]],\n              \n                      [[ 0.1423]],\n              \n                      [[ 0.1658]],\n              \n                      [[-0.2413]],\n              \n                      [[-0.1679]],\n              \n                      [[-0.2083]],\n              \n                      [[ 0.1830]],\n              \n                      [[-0.2224]],\n              \n                      [[-0.1750]],\n              \n                      [[ 0.2071]],\n              \n                      [[ 0.8620]],\n              \n                      [[-0.1659]],\n              \n                      [[ 0.1940]],\n              \n                      [[-0.1526]],\n              \n                      [[ 0.1670]],\n              \n                      [[-0.1768]],\n              \n                      [[ 0.1765]],\n              \n                      [[-0.1599]],\n              \n                      [[-0.1582]],\n              \n                      [[-0.1440]],\n              \n                      [[ 0.1599]],\n              \n                      [[ 0.1529]],\n              \n                      [[-0.1737]],\n              \n                      [[-0.1563]],\n              \n                      [[ 0.1524]],\n              \n                      [[ 0.1666]],\n              \n                      [[ 0.1650]],\n              \n                      [[-0.1700]],\n              \n                      [[-0.1705]],\n              \n                      [[ 0.1660]],\n              \n                      [[ 0.1456]],\n              \n                      [[ 0.1963]],\n              \n                      [[ 0.8486]],\n              \n                      [[ 0.1540]],\n              \n                      [[ 0.1795]],\n              \n                      [[-0.1968]],\n              \n                      [[-0.4704]],\n              \n                      [[-0.1677]],\n              \n                      [[-0.1549]],\n              \n                      [[ 0.1592]],\n              \n                      [[ 0.1704]],\n              \n                      [[-0.1605]],\n              \n                      [[ 0.1653]],\n              \n                      [[ 0.1116]],\n              \n                      [[-0.1608]],\n              \n                      [[-0.1878]],\n              \n                      [[-0.1892]],\n              \n                      [[ 0.1643]],\n              \n                      [[ 0.1651]],\n              \n                      [[-0.2719]],\n              \n                      [[-0.1954]],\n              \n                      [[ 0.1529]],\n              \n                      [[ 0.4002]],\n              \n                      [[-0.1513]],\n              \n                      [[-0.1516]],\n              \n                      [[ 0.1677]],\n              \n                      [[ 0.8566]],\n              \n                      [[-0.1557]],\n              \n                      [[ 0.1733]],\n              \n                      [[ 0.1520]],\n              \n                      [[ 0.1723]],\n              \n                      [[-0.2187]],\n              \n                      [[ 0.1615]],\n              \n                      [[-0.1723]],\n              \n                      [[ 0.1546]],\n              \n                      [[-0.2134]],\n              \n                      [[ 0.1781]],\n              \n                      [[-0.1823]],\n              \n                      [[-0.1590]],\n              \n                      [[ 0.1708]],\n              \n                      [[-0.2311]],\n              \n                      [[ 0.2123]],\n              \n                      [[ 0.1953]],\n              \n                      [[ 0.1825]],\n              \n                      [[-0.1732]],\n              \n                      [[-0.1838]],\n              \n                      [[-0.1837]],\n              \n                      [[ 0.1814]],\n              \n                      [[ 0.1555]],\n              \n                      [[-0.8493]],\n              \n                      [[ 0.8562]],\n              \n                      [[-0.2789]],\n              \n                      [[-0.1535]],\n              \n                      [[-0.3617]],\n              \n                      [[-0.8607]],\n              \n                      [[ 0.1471]],\n              \n                      [[ 0.1694]],\n              \n                      [[ 0.1777]],\n              \n                      [[-0.1816]],\n              \n                      [[-0.1912]],\n              \n                      [[-0.1497]],\n              \n                      [[-0.1801]],\n              \n                      [[ 0.1849]],\n              \n                      [[-0.8635]],\n              \n                      [[-0.1730]],\n              \n                      [[-0.1528]],\n              \n                      [[ 0.4168]],\n              \n                      [[-0.1584]],\n              \n                      [[ 0.1588]],\n              \n                      [[ 0.1692]],\n              \n                      [[ 0.1591]],\n              \n                      [[-0.1497]],\n              \n                      [[-0.1975]],\n              \n                      [[-0.1747]],\n              \n                      [[-0.6414]],\n              \n                      [[ 0.1526]],\n              \n                      [[-0.1719]],\n              \n                      [[-0.1543]],\n              \n                      [[-0.1490]],\n              \n                      [[ 0.1552]],\n              \n                      [[ 0.2033]],\n              \n                      [[ 0.1615]],\n              \n                      [[ 0.1608]],\n              \n                      [[ 0.2221]],\n              \n                      [[-0.1339]],\n              \n                      [[-0.3394]],\n              \n                      [[ 0.1546]],\n              \n                      [[ 0.1625]],\n              \n                      [[-0.1760]],\n              \n                      [[ 0.1542]],\n              \n                      [[ 0.1735]],\n              \n                      [[-0.1650]],\n              \n                      [[ 0.2391]],\n              \n                      [[-0.1428]],\n              \n                      [[-0.1605]],\n              \n                      [[ 0.1456]],\n              \n                      [[ 0.1658]],\n              \n                      [[-0.1553]],\n              \n                      [[-0.8724]],\n              \n                      [[ 0.1799]],\n              \n                      [[-0.8631]],\n              \n                      [[-0.1460]],\n              \n                      [[-0.5495]],\n              \n                      [[ 0.1905]],\n              \n                      [[-0.8638]],\n              \n                      [[-0.1714]],\n              \n                      [[-0.1664]],\n              \n                      [[-0.1625]],\n              \n                      [[-0.1789]],\n              \n                      [[ 0.2259]],\n              \n                      [[-0.1494]],\n              \n                      [[ 0.6379]],\n              \n                      [[ 0.8528]],\n              \n                      [[ 0.1759]],\n              \n                      [[-0.1851]],\n              \n                      [[ 0.1502]],\n              \n                      [[ 0.1880]],\n              \n                      [[ 0.1593]],\n              \n                      [[-0.2956]],\n              \n                      [[ 0.1432]],\n              \n                      [[ 0.1517]],\n              \n                      [[ 0.8724]],\n              \n                      [[-0.1947]],\n              \n                      [[ 0.1426]],\n              \n                      [[-0.8710]],\n              \n                      [[ 0.1589]],\n              \n                      [[ 0.1643]],\n              \n                      [[-0.4279]],\n              \n                      [[ 0.1674]],\n              \n                      [[-0.1730]],\n              \n                      [[-0.1756]],\n              \n                      [[-0.2714]],\n              \n                      [[ 0.1578]],\n              \n                      [[-0.1619]],\n              \n                      [[-0.8504]],\n              \n                      [[-0.1529]],\n              \n                      [[-0.1965]],\n              \n                      [[-0.1668]],\n              \n                      [[ 0.2096]],\n              \n                      [[-0.1770]],\n              \n                      [[-0.2011]],\n              \n                      [[ 0.8530]],\n              \n                      [[ 0.2040]],\n              \n                      [[-0.2658]],\n              \n                      [[-0.8535]],\n              \n                      [[ 0.1574]],\n              \n                      [[-0.1666]],\n              \n                      [[ 0.8510]],\n              \n                      [[ 0.1648]],\n              \n                      [[ 0.1448]],\n              \n                      [[-0.1873]],\n              \n                      [[-0.1600]],\n              \n                      [[-0.1834]],\n              \n                      [[-0.7923]],\n              \n                      [[-0.1579]],\n              \n                      [[ 0.1546]],\n              \n                      [[ 0.1784]],\n              \n                      [[-0.2032]],\n              \n                      [[ 0.1685]],\n              \n                      [[-0.1560]],\n              \n                      [[-0.1498]],\n              \n                      [[-0.3484]],\n              \n                      [[ 0.1822]],\n              \n                      [[ 0.1525]],\n              \n                      [[-0.1744]],\n              \n                      [[ 0.1565]],\n              \n                      [[-0.2067]],\n              \n                      [[ 0.1476]],\n              \n                      [[-0.1670]],\n              \n                      [[-0.1794]],\n              \n                      [[-0.1621]],\n              \n                      [[-0.1762]],\n              \n                      [[ 0.1570]],\n              \n                      [[ 0.1621]],\n              \n                      [[ 0.1538]],\n              \n                      [[ 0.1999]],\n              \n                      [[-0.8470]],\n              \n                      [[-0.2103]],\n              \n                      [[-0.1464]],\n              \n                      [[-0.1719]],\n              \n                      [[ 0.1683]],\n              \n                      [[-0.1552]],\n              \n                      [[ 0.1578]],\n              \n                      [[ 0.1557]],\n              \n                      [[-0.1627]],\n              \n                      [[-0.1604]],\n              \n                      [[ 0.1622]],\n              \n                      [[ 0.1559]],\n              \n                      [[-0.1637]],\n              \n                      [[ 0.1783]],\n              \n                      [[ 0.1569]],\n              \n                      [[ 0.1668]],\n              \n                      [[-0.2384]],\n              \n                      [[-0.1508]],\n              \n                      [[ 0.1789]],\n              \n                      [[ 0.1563]],\n              \n                      [[-0.4035]],\n              \n                      [[-0.1590]],\n              \n                      [[ 0.1579]],\n              \n                      [[-0.1623]],\n              \n                      [[ 0.1550]],\n              \n                      [[ 0.1739]],\n              \n                      [[ 0.2291]],\n              \n                      [[ 0.1966]],\n              \n                      [[-0.2052]],\n              \n                      [[ 0.1739]],\n              \n                      [[-0.1924]],\n              \n                      [[ 0.1625]],\n              \n                      [[-0.1716]],\n              \n                      [[ 0.1714]],\n              \n                      [[-0.1361]],\n              \n                      [[ 0.1948]],\n              \n                      [[-0.1677]],\n              \n                      [[-0.1740]],\n              \n                      [[-0.1640]],\n              \n                      [[ 0.1711]],\n              \n                      [[-0.2012]],\n              \n                      [[-0.2689]],\n              \n                      [[-0.4715]],\n              \n                      [[ 0.1986]],\n              \n                      [[ 0.1769]],\n              \n                      [[ 0.1545]],\n              \n                      [[-0.1681]],\n              \n                      [[-0.1590]],\n              \n                      [[-0.8456]],\n              \n                      [[-0.1721]],\n              \n                      [[-0.2285]],\n              \n                      [[ 0.3725]],\n              \n                      [[ 0.1565]],\n              \n                      [[-0.1800]],\n              \n                      [[-0.1905]],\n              \n                      [[-0.8640]],\n              \n                      [[-0.1548]],\n              \n                      [[-0.1803]],\n              \n                      [[ 0.1647]],\n              \n                      [[-0.1614]],\n              \n                      [[-0.1969]],\n              \n                      [[-0.1855]],\n              \n                      [[ 0.1631]],\n              \n                      [[ 0.1841]],\n              \n                      [[-0.3795]],\n              \n                      [[ 0.1678]],\n              \n                      [[ 0.1582]],\n              \n                      [[ 0.1730]],\n              \n                      [[ 0.1586]],\n              \n                      [[ 0.1955]],\n              \n                      [[ 0.1571]],\n              \n                      [[-0.1576]],\n              \n                      [[ 0.1582]],\n              \n                      [[ 0.1652]],\n              \n                      [[ 0.2332]],\n              \n                      [[ 0.1857]],\n              \n                      [[-0.1440]],\n              \n                      [[-0.1914]],\n              \n                      [[-0.1705]],\n              \n                      [[ 0.1494]],\n              \n                      [[ 0.1774]],\n              \n                      [[ 0.1429]],\n              \n                      [[ 0.1606]],\n              \n                      [[-0.1733]],\n              \n                      [[-0.1539]],\n              \n                      [[-0.8271]],\n              \n                      [[ 0.1653]],\n              \n                      [[-0.1372]],\n              \n                      [[ 0.1618]],\n              \n                      [[-0.1620]],\n              \n                      [[-0.1548]],\n              \n                      [[ 0.1763]],\n              \n                      [[ 0.1737]],\n              \n                      [[ 0.8462]],\n              \n                      [[ 0.1828]],\n              \n                      [[ 0.1578]],\n              \n                      [[ 0.4053]],\n              \n                      [[ 0.1654]],\n              \n                      [[ 0.1552]],\n              \n                      [[-0.1474]],\n              \n                      [[-0.1662]],\n              \n                      [[-0.1846]],\n              \n                      [[-0.2232]],\n              \n                      [[-0.1267]],\n              \n                      [[ 0.1520]],\n              \n                      [[-0.1414]],\n              \n                      [[ 0.2805]],\n              \n                      [[ 0.2155]],\n              \n                      [[ 0.1553]],\n              \n                      [[-0.2768]],\n              \n                      [[-0.1720]],\n              \n                      [[ 0.8505]],\n              \n                      [[-0.1755]],\n              \n                      [[-0.2276]],\n              \n                      [[-0.1573]],\n              \n                      [[-0.1494]],\n              \n                      [[-0.8746]],\n              \n                      [[-0.1793]],\n              \n                      [[ 0.1516]],\n              \n                      [[ 0.1854]],\n              \n                      [[ 0.1464]],\n              \n                      [[ 0.1461]],\n              \n                      [[-0.1856]],\n              \n                      [[-0.1674]],\n              \n                      [[ 0.1973]],\n              \n                      [[ 0.1518]],\n              \n                      [[ 0.1500]],\n              \n                      [[-0.1583]],\n              \n                      [[ 0.1658]],\n              \n                      [[-0.8589]],\n              \n                      [[ 0.2069]],\n              \n                      [[-0.2026]],\n              \n                      [[ 0.1873]],\n              \n                      [[ 0.1599]],\n              \n                      [[ 0.4452]],\n              \n                      [[-0.1759]],\n              \n                      [[-0.1598]],\n              \n                      [[ 0.1945]],\n              \n                      [[-0.1497]],\n              \n                      [[ 0.1939]],\n              \n                      [[-0.1636]],\n              \n                      [[ 0.1765]],\n              \n                      [[-0.1791]],\n              \n                      [[ 0.1722]],\n              \n                      [[ 0.1768]],\n              \n                      [[-0.1975]],\n              \n                      [[-0.1864]],\n              \n                      [[ 0.2031]],\n              \n                      [[ 0.8683]],\n              \n                      [[-0.2026]],\n              \n                      [[ 0.1608]],\n              \n                      [[-0.1545]],\n              \n                      [[ 0.4555]],\n              \n                      [[-0.1964]],\n              \n                      [[ 0.2430]],\n              \n                      [[-0.1688]],\n              \n                      [[-0.1432]],\n              \n                      [[ 0.8531]],\n              \n                      [[ 0.1675]],\n              \n                      [[-0.1779]],\n              \n                      [[ 0.8650]],\n              \n                      [[-0.2126]],\n              \n                      [[-0.1525]],\n              \n                      [[-0.1650]],\n              \n                      [[-0.1594]],\n              \n                      [[ 0.2360]],\n              \n                      [[ 0.1350]],\n              \n                      [[ 0.1849]],\n              \n                      [[ 0.1710]],\n              \n                      [[ 0.1573]],\n              \n                      [[-0.1663]],\n              \n                      [[ 0.1450]],\n              \n                      [[ 0.1644]],\n              \n                      [[ 0.1652]],\n              \n                      [[ 0.1868]],\n              \n                      [[-0.1832]],\n              \n                      [[-0.1598]],\n              \n                      [[ 0.8578]],\n              \n                      [[-0.1887]],\n              \n                      [[-0.1852]],\n              \n                      [[-0.1460]],\n              \n                      [[ 0.2025]],\n              \n                      [[-0.1617]],\n              \n                      [[ 0.1514]],\n              \n                      [[ 0.1609]],\n              \n                      [[-0.1502]],\n              \n                      [[ 0.1591]],\n              \n                      [[-0.1584]],\n              \n                      [[ 0.1872]],\n              \n                      [[-0.1634]],\n              \n                      [[ 0.2072]],\n              \n                      [[-0.2712]],\n              \n                      [[-0.1690]],\n              \n                      [[ 0.1778]],\n              \n                      [[ 0.1605]],\n              \n                      [[-0.2396]],\n              \n                      [[ 0.1806]],\n              \n                      [[-0.1602]],\n              \n                      [[ 0.1530]],\n              \n                      [[ 0.1670]],\n              \n                      [[ 0.1886]],\n              \n                      [[ 0.2612]],\n              \n                      [[-0.4365]],\n              \n                      [[-0.1613]],\n              \n                      [[ 0.1671]],\n              \n                      [[ 0.1384]],\n              \n                      [[-0.1567]],\n              \n                      [[ 0.1717]],\n              \n                      [[ 0.1788]],\n              \n                      [[ 0.1793]],\n              \n                      [[-0.1773]],\n              \n                      [[ 0.1615]],\n              \n                      [[ 0.1504]],\n              \n                      [[ 0.6465]],\n              \n                      [[-0.1681]],\n              \n                      [[ 0.2448]],\n              \n                      [[ 0.2139]],\n              \n                      [[ 0.1687]],\n              \n                      [[ 0.1618]],\n              \n                      [[ 0.1431]],\n              \n                      [[-0.1690]],\n              \n                      [[ 0.1611]],\n              \n                      [[-0.1478]],\n              \n                      [[ 0.1715]],\n              \n                      [[ 0.1730]],\n              \n                      [[ 0.1639]],\n              \n                      [[ 0.1860]],\n              \n                      [[ 0.1706]],\n              \n                      [[ 0.1720]],\n              \n                      [[-0.5158]],\n              \n                      [[ 0.1449]],\n              \n                      [[-0.1608]],\n              \n                      [[-0.1808]],\n              \n                      [[ 0.1881]],\n              \n                      [[-0.1738]],\n              \n                      [[-0.1725]],\n              \n                      [[ 0.1758]],\n              \n                      [[ 0.1564]],\n              \n                      [[ 0.1743]],\n              \n                      [[ 0.1588]],\n              \n                      [[-0.2619]],\n              \n                      [[-0.1680]],\n              \n                      [[-0.1531]],\n              \n                      [[-0.8549]],\n              \n                      [[-0.8595]],\n              \n                      [[ 0.1527]],\n              \n                      [[ 0.1793]],\n              \n                      [[ 0.1664]],\n              \n                      [[-0.1546]],\n              \n                      [[-0.2186]],\n              \n                      [[ 0.1509]],\n              \n                      [[ 0.1612]],\n              \n                      [[ 0.1748]],\n              \n                      [[-0.1657]],\n              \n                      [[-0.1621]],\n              \n                      [[-0.5216]],\n              \n                      [[ 0.1575]],\n              \n                      [[ 0.1453]],\n              \n                      [[-0.1769]],\n              \n                      [[ 0.1789]],\n              \n                      [[-0.1729]],\n              \n                      [[-0.1498]],\n              \n                      [[-0.1660]],\n              \n                      [[-0.1637]],\n              \n                      [[ 0.1637]],\n              \n                      [[-0.8775]],\n              \n                      [[-0.1606]],\n              \n                      [[-0.8518]],\n              \n                      [[-0.1546]],\n              \n                      [[ 0.1578]],\n              \n                      [[ 0.1701]],\n              \n                      [[ 0.2293]],\n              \n                      [[-0.3614]],\n              \n                      [[ 0.1779]],\n              \n                      [[ 0.7858]],\n              \n                      [[-0.1829]],\n              \n                      [[-0.1688]],\n              \n                      [[ 0.1537]],\n              \n                      [[-0.1637]],\n              \n                      [[ 0.1564]],\n              \n                      [[ 0.1632]],\n              \n                      [[ 0.4679]],\n              \n                      [[-0.1642]],\n              \n                      [[-0.1634]],\n              \n                      [[-0.1736]],\n              \n                      [[ 0.1702]],\n              \n                      [[-0.3724]],\n              \n                      [[ 0.1728]],\n              \n                      [[ 0.1678]],\n              \n                      [[-0.1579]],\n              \n                      [[-0.5990]],\n              \n                      [[-0.1835]],\n              \n                      [[ 0.2034]],\n              \n                      [[ 0.1696]],\n              \n                      [[-0.1728]],\n              \n                      [[ 0.1602]],\n              \n                      [[-0.1470]],\n              \n                      [[ 0.4419]],\n              \n                      [[-0.1928]],\n              \n                      [[-0.1468]],\n              \n                      [[-0.1585]],\n              \n                      [[ 0.8756]],\n              \n                      [[-0.1506]],\n              \n                      [[ 0.2387]],\n              \n                      [[ 0.1871]],\n              \n                      [[-0.1468]],\n              \n                      [[ 0.1812]],\n              \n                      [[ 0.1572]],\n              \n                      [[ 0.1607]],\n              \n                      [[ 0.1636]],\n              \n                      [[ 0.8599]],\n              \n                      [[-0.1616]],\n              \n                      [[ 0.1628]],\n              \n                      [[ 0.1596]],\n              \n                      [[ 0.1821]],\n              \n                      [[-0.1493]],\n              \n                      [[ 0.8111]],\n              \n                      [[-0.2025]],\n              \n                      [[-0.1677]],\n              \n                      [[ 0.2168]],\n              \n                      [[-0.4545]],\n              \n                      [[ 0.1655]],\n              \n                      [[-0.6278]],\n              \n                      [[ 0.2091]],\n              \n                      [[-0.2064]],\n              \n                      [[-0.1526]],\n              \n                      [[ 0.1737]],\n              \n                      [[-0.1959]],\n              \n                      [[-0.1828]],\n              \n                      [[ 0.1564]],\n              \n                      [[ 0.1592]],\n              \n                      [[-0.1529]],\n              \n                      [[-0.2026]],\n              \n                      [[ 0.1681]],\n              \n                      [[-0.2035]],\n              \n                      [[ 0.1659]],\n              \n                      [[-0.1665]],\n              \n                      [[ 0.1760]],\n              \n                      [[-0.1668]],\n              \n                      [[ 0.1877]],\n              \n                      [[ 0.1531]],\n              \n                      [[ 0.1778]],\n              \n                      [[ 0.1602]],\n              \n                      [[-0.1649]],\n              \n                      [[-0.1689]],\n              \n                      [[-0.8289]],\n              \n                      [[ 0.1643]],\n              \n                      [[-0.1530]],\n              \n                      [[ 0.1745]],\n              \n                      [[-0.1752]],\n              \n                      [[-0.1589]],\n              \n                      [[-0.3412]],\n              \n                      [[ 0.1496]],\n              \n                      [[-0.1522]],\n              \n                      [[ 0.1612]],\n              \n                      [[-0.1683]],\n              \n                      [[-0.1705]],\n              \n                      [[ 0.1760]],\n              \n                      [[ 0.1563]],\n              \n                      [[-0.1747]],\n              \n                      [[-0.2180]],\n              \n                      [[-0.1746]],\n              \n                      [[-0.1856]],\n              \n                      [[ 0.1666]],\n              \n                      [[-0.1712]],\n              \n                      [[ 0.1420]],\n              \n                      [[-0.1683]],\n              \n                      [[-0.6524]],\n              \n                      [[ 0.1620]],\n              \n                      [[ 0.1437]],\n              \n                      [[-0.1638]],\n              \n                      [[-0.1659]],\n              \n                      [[ 0.1577]],\n              \n                      [[ 0.1633]],\n              \n                      [[ 0.1672]],\n              \n                      [[ 0.1465]],\n              \n                      [[-0.1667]],\n              \n                      [[-0.1684]],\n              \n                      [[-0.1734]],\n              \n                      [[-0.1532]],\n              \n                      [[-0.1592]],\n              \n                      [[ 0.1635]],\n              \n                      [[ 0.1544]],\n              \n                      [[ 0.1531]],\n              \n                      [[-0.1613]],\n              \n                      [[ 0.1665]],\n              \n                      [[ 0.1474]],\n              \n                      [[ 0.1373]],\n              \n                      [[-0.1612]],\n              \n                      [[ 0.1749]],\n              \n                      [[-0.1631]],\n              \n                      [[-0.1657]],\n              \n                      [[ 0.3241]],\n              \n                      [[-0.1650]],\n              \n                      [[-0.1785]],\n              \n                      [[-0.1660]],\n              \n                      [[-0.1983]],\n              \n                      [[-0.1662]],\n              \n                      [[ 0.1483]],\n              \n                      [[-0.1426]],\n              \n                      [[-0.1552]],\n              \n                      [[-0.1515]],\n              \n                      [[-0.2002]],\n              \n                      [[ 0.1444]],\n              \n                      [[-0.2539]],\n              \n                      [[-0.2888]],\n              \n                      [[-0.1388]],\n              \n                      [[ 0.1615]],\n              \n                      [[-0.1754]],\n              \n                      [[-0.1952]],\n              \n                      [[-0.1511]],\n              \n                      [[ 0.1772]],\n              \n                      [[-0.1599]],\n              \n                      [[-0.1489]],\n              \n                      [[ 0.1871]],\n              \n                      [[ 0.1894]],\n              \n                      [[ 0.1623]],\n              \n                      [[ 0.1387]],\n              \n                      [[-0.1513]],\n              \n                      [[ 0.1660]],\n              \n                      [[-0.1993]],\n              \n                      [[-0.1494]],\n              \n                      [[-0.1599]],\n              \n                      [[ 0.1778]],\n              \n                      [[-0.1473]],\n              \n                      [[-0.8233]],\n              \n                      [[ 0.1694]],\n              \n                      [[-0.1576]]])),\n             ('features.7.2.block.0.weight',\n              tensor([[[[-0.0165, -0.0041,  0.0185,  ...,  0.0082,  0.0062, -0.0165],\n                        [ 0.0062,  0.0144,  0.0185,  ...,  0.0144,  0.0103,  0.0062],\n                        [ 0.0247,  0.0247, -0.0082,  ..., -0.0082,  0.0247,  0.0247],\n                        ...,\n                        [ 0.0144,  0.0206, -0.0144,  ..., -0.0185,  0.0144,  0.0288],\n                        [-0.0021,  0.0103,  0.0165,  ...,  0.0247,  0.0062, -0.0041],\n                        [-0.0041, -0.0021,  0.0226,  ...,  0.0062,  0.0165, -0.0226]]],\n              \n              \n                      [[[-0.0887, -0.0377, -0.0266,  ..., -0.0155, -0.0244, -0.1064],\n                        [-0.0200,  0.0244,  0.0044,  ...,  0.0044,  0.0155, -0.0133],\n                        [ 0.0111,  0.0022, -0.0643,  ..., -0.0554,  0.0089,  0.0177],\n                        ...,\n                        [ 0.0067, -0.0044, -0.0643,  ..., -0.0554,  0.0177,  0.0111],\n                        [-0.0266, -0.0044, -0.0089,  ..., -0.0266, -0.0044, -0.0266],\n                        [-0.0954, -0.0111, -0.0111,  ...,  0.0177, -0.0111, -0.0732]]],\n              \n              \n                      [[[-0.0111,  0.0063, -0.0032,  ...,  0.0158, -0.0047, -0.0190],\n                        [-0.0205, -0.0079,  0.0111,  ...,  0.0000,  0.0000, -0.0079],\n                        [ 0.0032,  0.0142, -0.0205,  ..., -0.0269,  0.0190, -0.0079],\n                        ...,\n                        [ 0.0063,  0.0095, -0.0253,  ..., -0.0237,  0.0111, -0.0095],\n                        [-0.0079,  0.0047,  0.0095,  ...,  0.0111,  0.0142, -0.0126],\n                        [-0.0111,  0.0111, -0.0095,  ..., -0.0190,  0.0000, -0.0016]]],\n              \n              \n                      ...,\n              \n              \n                      [[[ 0.0729,  0.0194,  0.0254,  ...,  0.0294,  0.0334,  0.0662],\n                        [ 0.0154,  0.0107,  0.0227,  ...,  0.0268,  0.0000,  0.0247],\n                        [ 0.0087,  0.0207,  0.0468,  ...,  0.0455,  0.0194,  0.0140],\n                        ...,\n                        [ 0.0361,  0.0161,  0.0441,  ...,  0.0455,  0.0207,  0.0395],\n                        [ 0.0207,  0.0114,  0.0221,  ...,  0.0308,  0.0060,  0.0201],\n                        [ 0.0796,  0.0174,  0.0254,  ...,  0.0241,  0.0181,  0.0682]]],\n              \n              \n                      [[[ 0.0099,  0.0099,  0.0099,  ...,  0.0082,  0.0000,  0.0181],\n                        [ 0.0000,  0.0033,  0.0148,  ...,  0.0148,  0.0099, -0.0049],\n                        [ 0.0214,  0.0164,  0.0049,  ...,  0.0049,  0.0181,  0.0181],\n                        ...,\n                        [ 0.0066,  0.0148,  0.0049,  ...,  0.0049,  0.0164,  0.0066],\n                        [ 0.0099, -0.0033,  0.0131,  ...,  0.0115, -0.0082,  0.0082],\n                        [ 0.0033,  0.0115,  0.0099,  ...,  0.0099,  0.0082,  0.0082]]],\n              \n              \n                      [[[ 0.0776,  0.0080,  0.0358,  ...,  0.0199,  0.0080,  0.0856],\n                        [ 0.0319, -0.0279,  0.0020,  ...,  0.0179, -0.0199,  0.0398],\n                        [-0.0020, -0.0239,  0.0518,  ...,  0.0338, -0.0159, -0.0219],\n                        ...,\n                        [ 0.0219, -0.0080,  0.0538,  ...,  0.0577, -0.0119, -0.0080],\n                        [ 0.0139, -0.0040,  0.0040,  ...,  0.0219,  0.0000,  0.0299],\n                        [ 0.0697, -0.0020, -0.0159,  ..., -0.0219, -0.0040,  0.0816]]]],\n                     size=(768, 1, 7, 7), dtype=torch.qint8,\n                     quantization_scheme=torch.per_channel_affine,\n                     scale=tensor([0.0021, 0.0022, 0.0016, 0.0016, 0.0013, 0.0008, 0.0017, 0.0020, 0.0017,\n                      0.0008, 0.0019, 0.0011, 0.0015, 0.0007, 0.0045, 0.0009, 0.0008, 0.0016,\n                      0.0018, 0.0006, 0.0011, 0.0014, 0.0018, 0.0006, 0.0010, 0.0007, 0.0014,\n                      0.0011, 0.0010, 0.0006, 0.0008, 0.0005, 0.0028, 0.0012, 0.0010, 0.0014,\n                      0.0009, 0.0008, 0.0013, 0.0004, 0.0017, 0.0017, 0.0017, 0.0013, 0.0014,\n                      0.0006, 0.0005, 0.0010, 0.0014, 0.0015, 0.0018, 0.0012, 0.0023, 0.0017,\n                      0.0006, 0.0017, 0.0013, 0.0015, 0.0006, 0.0014, 0.0018, 0.0009, 0.0015,\n                      0.0017, 0.0005, 0.0016, 0.0007, 0.0014, 0.0009, 0.0009, 0.0010, 0.0020,\n                      0.0016, 0.0012, 0.0019, 0.0024, 0.0011, 0.0011, 0.0017, 0.0005, 0.0013,\n                      0.0005, 0.0008, 0.0006, 0.0010, 0.0008, 0.0011, 0.0015, 0.0012, 0.0008,\n                      0.0009, 0.0009, 0.0014, 0.0016, 0.0016, 0.0013, 0.0011, 0.0016, 0.0016,\n                      0.0020, 0.0009, 0.0016, 0.0006, 0.0019, 0.0015, 0.0004, 0.0013, 0.0015,\n                      0.0015, 0.0009, 0.0013, 0.0015, 0.0017, 0.0019, 0.0016, 0.0014, 0.0017,\n                      0.0008, 0.0015, 0.0014, 0.0007, 0.0008, 0.0013, 0.0027, 0.0008, 0.0018,\n                      0.0006, 0.0019, 0.0012, 0.0026, 0.0031, 0.0017, 0.0006, 0.0009, 0.0016,\n                      0.0015, 0.0018, 0.0016, 0.0006, 0.0016, 0.0011, 0.0014, 0.0023, 0.0013,\n                      0.0008, 0.0007, 0.0018, 0.0010, 0.0016, 0.0017, 0.0017, 0.0014, 0.0011,\n                      0.0013, 0.0012, 0.0006, 0.0017, 0.0010, 0.0013, 0.0008, 0.0018, 0.0005,\n                      0.0016, 0.0017, 0.0014, 0.0016, 0.0016, 0.0013, 0.0010, 0.0016, 0.0010,\n                      0.0023, 0.0016, 0.0025, 0.0018, 0.0008, 0.0018, 0.0020, 0.0009, 0.0017,\n                      0.0011, 0.0009, 0.0012, 0.0008, 0.0015, 0.0017, 0.0008, 0.0006, 0.0012,\n                      0.0011, 0.0016, 0.0020, 0.0006, 0.0008, 0.0006, 0.0007, 0.0008, 0.0005,\n                      0.0014, 0.0017, 0.0008, 0.0011, 0.0013, 0.0007, 0.0013, 0.0013, 0.0009,\n                      0.0012, 0.0005, 0.0025, 0.0010, 0.0009, 0.0006, 0.0008, 0.0013, 0.0015,\n                      0.0010, 0.0011, 0.0022, 0.0019, 0.0011, 0.0016, 0.0016, 0.0018, 0.0008,\n                      0.0015, 0.0014, 0.0009, 0.0005, 0.0013, 0.0026, 0.0017, 0.0011, 0.0019,\n                      0.0012, 0.0015, 0.0026, 0.0014, 0.0016, 0.0012, 0.0008, 0.0019, 0.0015,\n                      0.0017, 0.0009, 0.0017, 0.0014, 0.0019, 0.0016, 0.0031, 0.0019, 0.0008,\n                      0.0017, 0.0010, 0.0007, 0.0019, 0.0015, 0.0015, 0.0016, 0.0011, 0.0010,\n                      0.0013, 0.0018, 0.0011, 0.0013, 0.0009, 0.0019, 0.0009, 0.0021, 0.0013,\n                      0.0013, 0.0018, 0.0005, 0.0028, 0.0018, 0.0015, 0.0015, 0.0020, 0.0015,\n                      0.0018, 0.0012, 0.0011, 0.0020, 0.0015, 0.0015, 0.0020, 0.0016, 0.0018,\n                      0.0018, 0.0017, 0.0008, 0.0008, 0.0008, 0.0013, 0.0017, 0.0015, 0.0010,\n                      0.0011, 0.0016, 0.0026, 0.0018, 0.0010, 0.0007, 0.0022, 0.0019, 0.0010,\n                      0.0017, 0.0010, 0.0016, 0.0008, 0.0006, 0.0009, 0.0009, 0.0019, 0.0018,\n                      0.0018, 0.0006, 0.0026, 0.0014, 0.0019, 0.0011, 0.0019, 0.0012, 0.0014,\n                      0.0012, 0.0009, 0.0009, 0.0013, 0.0010, 0.0016, 0.0019, 0.0018, 0.0007,\n                      0.0009, 0.0008, 0.0020, 0.0009, 0.0019, 0.0014, 0.0006, 0.0007, 0.0017,\n                      0.0015, 0.0008, 0.0017, 0.0006, 0.0020, 0.0018, 0.0017, 0.0018, 0.0013,\n                      0.0013, 0.0009, 0.0017, 0.0014, 0.0015, 0.0014, 0.0011, 0.0004, 0.0018,\n                      0.0009, 0.0014, 0.0016, 0.0009, 0.0010, 0.0010, 0.0013, 0.0010, 0.0023,\n                      0.0019, 0.0013, 0.0005, 0.0019, 0.0008, 0.0019, 0.0021, 0.0014, 0.0010,\n                      0.0019, 0.0013, 0.0006, 0.0011, 0.0008, 0.0016, 0.0012, 0.0010, 0.0006,\n                      0.0006, 0.0016, 0.0006, 0.0016, 0.0009, 0.0018, 0.0012, 0.0009, 0.0016,\n                      0.0013, 0.0008, 0.0013, 0.0007, 0.0018, 0.0004, 0.0009, 0.0028, 0.0011,\n                      0.0016, 0.0019, 0.0008, 0.0011, 0.0007, 0.0008, 0.0007, 0.0018, 0.0014,\n                      0.0004, 0.0016, 0.0013, 0.0012, 0.0016, 0.0019, 0.0031, 0.0018, 0.0012,\n                      0.0013, 0.0004, 0.0018, 0.0010, 0.0017, 0.0005, 0.0005, 0.0011, 0.0017,\n                      0.0008, 0.0010, 0.0012, 0.0017, 0.0009, 0.0008, 0.0009, 0.0018, 0.0005,\n                      0.0008, 0.0020, 0.0022, 0.0015, 0.0019, 0.0019, 0.0012, 0.0016, 0.0029,\n                      0.0008, 0.0018, 0.0018, 0.0026, 0.0012, 0.0007, 0.0009, 0.0008, 0.0017,\n                      0.0009, 0.0012, 0.0018, 0.0022, 0.0013, 0.0016, 0.0007, 0.0008, 0.0016,\n                      0.0004, 0.0012, 0.0010, 0.0014, 0.0016, 0.0014, 0.0008, 0.0007, 0.0016,\n                      0.0019, 0.0008, 0.0015, 0.0013, 0.0009, 0.0018, 0.0007, 0.0005, 0.0009,\n                      0.0017, 0.0008, 0.0012, 0.0007, 0.0009, 0.0014, 0.0008, 0.0017, 0.0019,\n                      0.0006, 0.0023, 0.0008, 0.0008, 0.0005, 0.0018, 0.0016, 0.0010, 0.0014,\n                      0.0014, 0.0012, 0.0007, 0.0017, 0.0011, 0.0010, 0.0009, 0.0019, 0.0018,\n                      0.0005, 0.0025, 0.0017, 0.0015, 0.0017, 0.0017, 0.0017, 0.0015, 0.0026,\n                      0.0016, 0.0007, 0.0017, 0.0018, 0.0007, 0.0011, 0.0007, 0.0017, 0.0014,\n                      0.0010, 0.0026, 0.0013, 0.0017, 0.0016, 0.0006, 0.0023, 0.0016, 0.0014,\n                      0.0017, 0.0020, 0.0008, 0.0029, 0.0017, 0.0007, 0.0017, 0.0016, 0.0007,\n                      0.0012, 0.0008, 0.0017, 0.0010, 0.0015, 0.0017, 0.0013, 0.0013, 0.0017,\n                      0.0018, 0.0018, 0.0006, 0.0008, 0.0018, 0.0006, 0.0020, 0.0013, 0.0016,\n                      0.0015, 0.0006, 0.0007, 0.0007, 0.0012, 0.0016, 0.0015, 0.0007, 0.0020,\n                      0.0010, 0.0008, 0.0017, 0.0017, 0.0011, 0.0007, 0.0009, 0.0012, 0.0017,\n                      0.0009, 0.0016, 0.0016, 0.0021, 0.0027, 0.0011, 0.0007, 0.0019, 0.0017,\n                      0.0005, 0.0008, 0.0017, 0.0013, 0.0014, 0.0018, 0.0013, 0.0011, 0.0014,\n                      0.0007, 0.0007, 0.0007, 0.0011, 0.0018, 0.0018, 0.0012, 0.0016, 0.0013,\n                      0.0010, 0.0012, 0.0030, 0.0018, 0.0017, 0.0015, 0.0012, 0.0012, 0.0004,\n                      0.0009, 0.0022, 0.0007, 0.0007, 0.0016, 0.0009, 0.0004, 0.0016, 0.0016,\n                      0.0013, 0.0016, 0.0008, 0.0024, 0.0017, 0.0008, 0.0013, 0.0008, 0.0005,\n                      0.0014, 0.0011, 0.0019, 0.0007, 0.0026, 0.0034, 0.0011, 0.0011, 0.0014,\n                      0.0017, 0.0020, 0.0014, 0.0005, 0.0011, 0.0005, 0.0011, 0.0017, 0.0014,\n                      0.0017, 0.0018, 0.0018, 0.0010, 0.0017, 0.0018, 0.0017, 0.0009, 0.0011,\n                      0.0017, 0.0017, 0.0010, 0.0007, 0.0019, 0.0010, 0.0015, 0.0006, 0.0018,\n                      0.0019, 0.0006, 0.0007, 0.0007, 0.0013, 0.0011, 0.0006, 0.0012, 0.0021,\n                      0.0009, 0.0014, 0.0016, 0.0016, 0.0007, 0.0020, 0.0006, 0.0012, 0.0015,\n                      0.0016, 0.0011, 0.0007, 0.0012, 0.0029, 0.0009, 0.0012, 0.0003, 0.0012,\n                      0.0007, 0.0018, 0.0007, 0.0010, 0.0005, 0.0011, 0.0006, 0.0015, 0.0007,\n                      0.0015, 0.0006, 0.0007, 0.0010, 0.0018, 0.0008, 0.0010, 0.0007, 0.0007,\n                      0.0013, 0.0019, 0.0017, 0.0018, 0.0006, 0.0016, 0.0022, 0.0017, 0.0017,\n                      0.0016, 0.0018, 0.0009, 0.0011, 0.0006, 0.0018, 0.0006, 0.0009, 0.0006,\n                      0.0020, 0.0016, 0.0006, 0.0018, 0.0036, 0.0018, 0.0011, 0.0012, 0.0015,\n                      0.0018, 0.0013, 0.0007, 0.0008, 0.0008, 0.0016, 0.0007, 0.0016, 0.0019,\n                      0.0005, 0.0018, 0.0010, 0.0015, 0.0012, 0.0013, 0.0009, 0.0007, 0.0018,\n                      0.0007, 0.0016, 0.0020], dtype=torch.float64),\n                     zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n                     axis=0)),\n             ('features.7.2.block.0.bias',\n              Parameter containing:\n              tensor([-2.3386e-02, -1.6742e-02, -3.0241e-02,  4.0047e-03, -1.5069e-02,\n                      -2.4585e-03,  6.2066e-04, -1.6876e-03, -1.0012e-02, -4.8606e-02,\n                      -1.7274e-02,  4.1002e-02,  1.7821e-02, -2.2384e-02, -4.0866e-01,\n                       1.2701e-01,  7.6496e-02,  1.8867e-02, -1.6216e-03,  6.4651e-03,\n                       2.7351e-04, -2.2966e-02,  3.3643e-02, -3.9579e-03,  8.7411e-02,\n                      -1.4658e-02, -4.4825e-03, -2.9202e-03,  1.3141e-02,  4.1247e-02,\n                       1.7732e-02, -6.4625e-03, -1.9163e-02,  1.3529e-02,  7.4473e-03,\n                       4.5644e-02, -9.6869e-03, -8.0727e-03,  3.4006e-03,  2.6428e-03,\n                      -3.6945e-03, -1.7633e-03, -1.1604e-02,  2.3652e-02,  5.5238e-02,\n                      -8.3555e-04, -2.4086e-03, -1.6740e-03, -3.3027e-02, -9.7118e-03,\n                      -3.9678e-02, -1.7285e-02, -9.1872e-03, -1.3672e-02,  3.2389e-03,\n                       4.1600e-03,  1.2031e-02, -1.4728e-02,  1.6490e-03, -1.2685e-02,\n                       1.3395e-03, -6.0278e-04,  4.4323e-02,  6.3423e-03, -2.0139e-03,\n                      -3.4834e-03,  8.4701e-03,  7.9127e-03,  1.5955e-01, -2.3783e-03,\n                      -1.1927e-03, -1.8832e-02,  3.0153e-03,  1.2187e-02,  2.8128e-03,\n                      -2.1994e-02, -4.5309e-03,  1.0561e-02, -4.3658e-03,  9.5570e-03,\n                      -1.2548e-02, -3.0324e-03, -5.5145e-03, -1.4778e-03,  6.6084e-04,\n                      -5.7752e-03,  8.1834e-03,  1.4602e-01, -2.0030e-02, -2.8391e-03,\n                      -6.6100e-03,  4.0640e-03, -1.4340e-02,  1.3110e-02, -7.3537e-03,\n                      -1.6819e-03, -2.4834e-03,  5.8255e-03, -8.9623e-03, -1.1019e-02,\n                      -2.2103e-02,  6.2498e-03, -1.2721e-03, -7.4104e-03, -5.0415e-03,\n                       2.7549e-02,  8.0753e-03, -2.1500e-03, -1.3034e-03, -5.7878e-03,\n                      -2.1149e-03,  1.7114e-03, -6.0860e-03, -8.2982e-03,  2.3769e-02,\n                      -8.3340e-04, -1.3884e-02,  1.6042e-04, -4.0522e-04, -1.7200e-02,\n                       5.1969e-02,  3.3201e-02,  2.8973e-02, -2.6348e-02,  8.6212e-03,\n                      -9.7148e-03,  1.2309e-02, -1.4291e-02,  2.5246e-03, -3.4260e-02,\n                       2.1856e-02, -1.2482e-02, -5.9112e-03, -9.0754e-04,  1.8341e-03,\n                       2.2249e-02,  5.4454e-02,  1.3524e-04,  5.0747e-03,  3.2510e-02,\n                       2.4673e-02,  4.1394e-02, -1.9541e-02, -1.6171e-02,  3.0062e-02,\n                       7.3744e-03, -3.5286e-03, -5.0337e-04,  4.5662e-03,  1.4959e-02,\n                      -8.4976e-03, -9.3480e-03, -6.1166e-04,  1.1289e-02,  1.7656e-02,\n                       4.4939e-03, -5.1694e-03, -1.8569e-03,  1.2156e-02, -5.3120e-04,\n                      -1.3933e-02,  6.0059e-03,  4.4744e-03, -6.0292e-03,  2.1955e-02,\n                      -3.5169e-03, -2.1163e-03,  1.2386e-02,  7.1433e-03,  5.8908e-03,\n                       1.0147e-01,  3.8879e-03, -1.0800e-02, -1.4589e-02, -1.8035e-02,\n                       1.2354e-02,  3.6151e-03, -3.5562e-03,  3.4923e-03, -1.1237e-02,\n                      -2.7326e-02, -8.3439e-03,  1.4310e-02, -1.6903e-02, -3.1178e-03,\n                      -1.2121e-02,  1.7580e-01,  3.9375e-03, -1.9853e-02, -2.3109e-02,\n                       1.5252e-03, -1.5780e-02,  1.2359e-02,  1.7792e-02,  1.9885e-02,\n                       6.9884e-03,  3.1397e-02,  1.1911e-02, -4.8376e-03, -5.3279e-03,\n                       9.1195e-03,  5.3351e-02, -1.4665e-02,  4.9075e-03, -3.0258e-02,\n                       2.9956e-02, -3.6984e-03,  1.7857e-02, -2.4862e-02, -8.7368e-03,\n                       1.6980e-03,  2.8716e-03,  1.2413e-02,  2.6889e-04, -2.9609e-02,\n                      -1.7498e-02, -2.2032e-02,  3.6662e-02, -3.2921e-03, -1.5838e-02,\n                       2.7940e-02, -2.7165e-03,  1.4295e-03, -1.2923e-02,  4.1116e-02,\n                       1.5396e-03, -2.6293e-03, -6.2748e-03, -8.0450e-04,  8.2052e-03,\n                      -6.8635e-03, -7.8814e-03, -4.3780e-04, -1.4372e-02,  3.0992e-02,\n                       2.6292e-02,  5.2272e-02, -3.1791e-03,  6.1674e-02,  7.9858e-05,\n                       2.9031e-03, -1.7444e-02, -1.6332e-03, -2.2709e-02,  6.9713e-03,\n                      -4.5887e-04,  3.0755e-02, -8.6307e-03, -3.4587e-03, -9.5783e-02,\n                      -1.2537e-02, -8.6067e-03,  1.0055e-02, -3.5812e-02, -2.7577e-03,\n                      -1.6938e-02,  2.7063e-02,  6.4889e-03, -1.8411e-02,  3.2289e-02,\n                       1.0125e-03,  4.4844e-02,  1.4148e-03,  3.0114e-02,  3.9705e-02,\n                       6.5198e-03, -2.2666e-01, -6.7247e-03, -2.0895e-02,  1.1267e-02,\n                       4.7161e-03, -7.3841e-03, -1.1044e-03,  3.8716e-03, -2.2632e-04,\n                       7.6260e-02, -4.5129e-03, -1.9164e-02, -1.1284e-02, -6.7234e-03,\n                       4.5162e-02, -7.7193e-03,  3.7456e-02, -6.7467e-04,  1.4882e-02,\n                      -1.0609e-02, -2.0252e-02,  2.1701e-02,  5.9987e-02, -7.3031e-03,\n                       3.4820e-02,  2.0108e-01,  7.5601e-03, -2.7995e-03, -2.6797e-03,\n                       3.1976e-02,  3.6612e-03,  2.2483e-02, -2.0018e-02, -7.5004e-03,\n                       9.1744e-03,  1.7345e-02,  8.9437e-04, -4.9834e-02, -1.3909e-02,\n                      -1.3284e-02, -1.4854e-02,  3.0224e-02, -2.7196e-02, -3.1346e-03,\n                       1.0364e-03,  3.2489e-03,  1.7871e-04,  2.9231e-03, -7.8413e-03,\n                      -7.1927e-03, -3.5950e-03,  3.1994e-02, -9.9976e-03, -5.4105e-03,\n                       9.6196e-03, -1.2280e-02, -1.7786e-03,  9.5889e-03,  9.3689e-03,\n                       1.1341e-01, -7.5911e-03, -3.8061e-03, -1.1096e-03, -1.0812e-03,\n                       3.6629e-02, -1.4745e-04,  6.7671e-03, -5.8552e-03, -3.5099e-03,\n                      -1.1785e-02,  7.0874e-02,  1.8447e-02,  3.4835e-02,  9.1864e-03,\n                       4.1629e-02, -1.0232e-03, -8.5416e-03,  1.0491e-02,  1.3874e-02,\n                       9.3912e-03, -5.6912e-03, -2.9552e-03, -1.1197e-03, -1.8554e-03,\n                      -1.2263e-02, -3.3006e-02,  8.4251e-03, -8.1151e-03, -1.8112e-03,\n                      -1.3703e-02,  6.9836e-04,  4.3029e-02,  1.5466e-02, -8.8337e-03,\n                       1.3041e-01, -1.8630e-03, -1.6803e-02,  4.2357e-02,  5.2025e-02,\n                      -1.0787e-02,  6.3144e-02, -1.1213e-03, -3.4519e-02, -5.7133e-03,\n                      -1.2030e-03, -7.2878e-03,  3.3721e-02, -6.6897e-03, -1.0863e-02,\n                      -2.1684e-02,  2.3632e-03, -6.1848e-03,  9.4332e-03,  9.0929e-03,\n                      -2.5090e-03,  2.2711e-02,  1.6089e-01,  3.2042e-03,  1.3359e-02,\n                      -6.0771e-03, -8.2486e-04, -3.4878e-03,  8.9375e-03, -6.9279e-03,\n                       1.4864e-03,  4.3823e-03, -5.9406e-03,  2.6425e-02,  3.8639e-03,\n                      -3.0558e-03,  2.8845e-02,  1.7028e-02,  3.7186e-03, -1.3279e-03,\n                       4.0819e-02,  4.6048e-03, -7.8102e-04, -3.4743e-03,  2.9886e-02,\n                       6.8738e-03,  6.6116e-04, -4.9532e-04, -1.7666e-02, -1.1900e-02,\n                       1.6675e-02, -6.8928e-03, -4.6631e-03,  6.6529e-03,  1.4342e-02,\n                       9.4272e-03,  5.7664e-03,  1.1302e-02, -5.1836e-03,  4.6905e-03,\n                      -1.3044e-02,  1.2329e-02,  1.6084e-02, -4.6183e-03,  1.0578e-02,\n                      -1.6767e-02,  1.0197e-02, -8.1941e-03, -2.0546e-02, -5.5482e-03,\n                       2.8417e-03,  1.3566e-02, -2.2870e-02, -2.3774e-03, -1.3312e-02,\n                       2.7923e-03,  1.5296e-01,  7.1469e-03,  2.4247e-02,  8.7240e-03,\n                      -1.2968e-02,  1.0302e-02,  4.9545e-03,  2.3775e-02,  6.5560e-04,\n                      -1.0772e-02, -8.1302e-02, -4.1976e-02, -6.7251e-03,  1.3470e-01,\n                       5.2655e-04,  7.7429e-03, -2.4567e-02, -4.7803e-02,  2.1770e-04,\n                      -9.8594e-03, -6.4477e-03, -4.3920e-04, -1.3946e-02, -5.3602e-03,\n                       1.6263e-02,  2.7255e-03, -4.1813e-03, -6.6177e-03, -1.4006e-02,\n                       3.4015e-02, -7.8499e-04,  2.2432e-02, -5.2516e-03,  6.2946e-03,\n                       4.3599e-03,  3.2384e-03, -1.4302e-02, -8.7417e-04,  1.5065e-01,\n                      -6.6921e-03,  4.1588e-03, -3.1942e-03,  5.1236e-02, -1.1268e-02,\n                       3.9094e-03,  2.5253e-02,  1.5080e-03,  3.3516e-03, -6.6086e-03,\n                       7.3737e-03, -3.0767e-02, -2.5675e-03,  6.1145e-03, -1.4022e-02,\n                       2.5795e-02,  1.7985e-02,  1.9802e-01, -4.4977e-03,  1.2753e-02,\n                       2.0058e-02, -1.5878e-02, -1.8529e-02,  5.5424e-03, -2.0641e-03,\n                      -2.7407e-03, -1.1656e-03,  3.8091e-04,  1.8999e-03,  4.0775e-03,\n                      -1.0903e-02, -3.8229e-03, -1.2247e-02,  8.0185e-03,  2.9623e-03,\n                       3.6019e-03,  3.0276e-02, -6.8611e-03, -4.3989e-03,  4.4496e-03,\n                      -1.8875e-03, -1.0289e-02,  2.2221e-03,  4.9286e-03,  5.7834e-03,\n                       2.2978e-02, -7.5755e-03, -9.1207e-03,  2.9817e-02, -1.0619e-02,\n                      -1.1059e-02, -1.3585e-03,  2.1628e-02,  8.2151e-04, -3.5315e-03,\n                       3.7715e-03, -2.2585e-02,  3.9794e-02,  2.8425e-02,  1.4459e-03,\n                      -2.8918e-03,  1.3944e-03, -1.3040e-02,  6.0423e-03, -3.6147e-03,\n                       1.5793e-03, -1.8416e-02,  1.8428e-01, -2.2422e-02, -1.3028e-02,\n                       3.4445e-03, -1.4189e-02, -2.7436e-03, -5.4790e-03,  2.4889e-03,\n                       3.3741e-02, -8.2928e-03,  5.4377e-02,  4.3495e-02,  3.0812e-02,\n                       2.5633e-02,  5.7236e-03,  7.2832e-03, -1.4490e-02, -1.3464e-02,\n                       3.1589e-02,  4.9292e-03, -1.0723e-02, -4.9884e-03, -8.1272e-03,\n                       2.7864e-02, -1.6790e-02,  1.9312e-02,  2.9845e-03,  1.9000e-01,\n                       6.4137e-02,  5.1474e-03, -2.0525e-04, -2.4279e-03, -3.7379e-02,\n                       5.4233e-03,  8.1831e-03,  1.2086e-04, -5.0924e-03,  4.1624e-04,\n                       1.0237e-02,  4.1743e-02,  3.8107e-02,  4.2034e-02, -6.1874e-02,\n                       2.5574e-03, -8.6053e-04, -8.3430e-03, -1.1731e-02, -9.1475e-03,\n                       4.1110e-02,  6.5635e-04, -9.7632e-03, -1.1398e-02, -8.4110e-05,\n                      -4.5047e-03, -6.4644e-03, -9.8329e-03,  1.1489e-02,  3.0836e-03,\n                      -2.5173e-02,  1.2013e-02, -4.0468e-02,  1.9426e-03,  9.9481e-04,\n                       4.3395e-02,  8.5937e-04, -7.3566e-03, -1.5926e-02,  1.4392e-02,\n                      -1.1872e-02,  7.8740e-02,  6.4103e-02,  1.5127e-02,  1.3950e-01,\n                      -1.6998e-02, -3.1530e-03,  6.4943e-03, -9.8860e-03,  3.0439e-02,\n                      -1.2572e-03,  1.3867e-01, -1.3889e-02,  1.8155e-02, -4.3653e-03,\n                       4.6846e-03,  1.1299e-01, -1.8552e-03, -8.9964e-03, -7.3377e-04,\n                       4.5604e-02, -2.5064e-04, -1.7574e-02, -1.2642e-02,  1.0446e-03,\n                       4.6248e-02,  7.6520e-04,  1.4404e-01,  2.7240e-03, -6.2917e-03,\n                      -2.2823e-02, -3.9441e-03,  2.1031e-02, -3.0461e-02, -1.5481e-03,\n                       4.8994e-02,  3.8429e-03,  8.2649e-03, -3.1769e-03,  2.2626e-03,\n                      -5.1523e-04,  9.5422e-03,  2.7138e-03,  3.5483e-02,  2.1028e-02,\n                       2.5200e-02, -5.0817e-03, -1.6510e-02,  9.9281e-03,  1.4603e-02,\n                       9.8072e-02, -6.6837e-03,  1.4965e-01, -8.4073e-03,  5.5838e-02,\n                       5.9080e-03,  8.7539e-03, -1.4807e-02, -1.2732e-02,  2.5541e-03,\n                       3.4972e-03, -3.0233e-03, -2.7874e-02, -3.3850e-02, -1.1808e-02,\n                      -1.2814e-02,  6.0542e-03, -4.6552e-03, -8.7006e-03,  5.6184e-03,\n                       1.2597e-02, -7.5663e-03,  2.7161e-02, -1.6824e-02, -1.7426e-02,\n                      -9.5106e-03, -2.3424e-02, -2.6549e-02,  6.4228e-03, -7.5600e-03,\n                      -4.0869e-03,  8.7141e-02, -4.3536e-03, -6.3223e-03,  1.6697e-02,\n                       6.4342e-03,  4.8885e-02, -6.0177e-02,  2.3015e-05,  1.4943e-02,\n                       7.4181e-03,  2.3613e-02, -9.4826e-03,  7.2141e-03,  9.2681e-03,\n                       3.5325e-02,  3.2934e-03,  1.3715e-01, -4.5680e-03,  1.8668e-02,\n                       6.0013e-04,  1.2113e-02,  5.9544e-03,  2.2576e-02, -8.2797e-03,\n                      -8.4721e-03,  2.6940e-02,  1.0853e-02,  6.2257e-03,  3.6652e-02,\n                      -4.6519e-04, -8.2477e-03, -2.7256e-03, -1.5369e-02,  4.7785e-03,\n                      -1.0222e-02, -1.5395e-02, -2.5908e-03, -9.2070e-05,  2.0504e-02,\n                      -1.2866e-02,  2.9811e-02,  8.0254e-02, -3.4247e-03, -2.1694e-03,\n                       2.0181e-03, -3.9035e-03, -4.2982e-04, -1.5905e-02,  3.3278e-03,\n                       1.2064e-02, -6.9997e-03, -2.9031e-03, -3.8036e-03,  3.8207e-03,\n                       6.7220e-02,  8.9945e-03,  5.3260e-03, -2.1981e-02,  1.7671e-02,\n                       3.7833e-02,  9.9746e-03, -1.2641e-02,  4.0618e-02, -7.1216e-03,\n                      -2.0914e-02,  2.4647e-04, -2.3847e-03,  3.2669e-02, -3.5094e-04,\n                       2.2664e-02, -2.2742e-02,  4.5965e-03,  9.4916e-03, -1.9723e-02,\n                       1.6934e-03,  1.3161e-03, -1.4804e-02], requires_grad=True)),\n             ('features.7.2.block.0.scale', tensor(0.0376)),\n             ('features.7.2.block.0.zero_point', tensor(72)),\n             ('features.7.2.block.2.weight',\n              tensor([3.4308, 1.1512, 3.0019, 1.9162, 1.0769, 3.0620, 1.7673, 1.0373, 3.6913,\n                      6.3137, 5.4349, 1.2546, 5.2874, 2.4565, 0.8325, 0.3463, 0.3629, 4.4127,\n                      5.0915, 3.0939, 2.9419, 3.0812, 3.7055, 3.3551, 0.4651, 3.3729, 4.1969,\n                      1.1526, 1.3107, 1.3472, 2.3624, 3.0898, 1.9323, 4.8825, 3.0201, 0.6105,\n                      3.5176, 1.9855, 1.8153, 2.4676, 4.5616, 4.8515, 3.3843, 0.7235, 0.6066,\n                      1.9130, 3.0528, 3.5560, 2.2571, 4.6578, 0.9530, 4.6746, 0.8046, 3.6915,\n                      3.4138, 4.7079, 0.9047, 3.0221, 3.0919, 1.0193, 4.5676, 3.7641, 1.0888,\n                      3.4920, 2.8569, 4.7484, 1.4363, 1.3363, 0.3631, 3.8996, 3.8073, 2.1051,\n                      3.9628, 0.6411, 1.7522, 3.5968, 4.8076, 1.2024, 4.5115, 1.9205, 4.2899,\n                      1.6594, 3.0931, 3.1917, 3.6882, 2.9689, 0.9204, 0.3163, 1.6064, 2.9895,\n                      2.7186, 3.6083, 1.6972, 1.8374, 3.4687, 4.6678, 1.2626, 4.3864, 4.6509,\n                      1.2961, 3.2885, 1.2291, 3.2464, 4.6812, 3.1820, 1.7157, 1.2356, 4.6561,\n                      2.0053, 1.4630, 4.6842, 2.1740, 4.8535, 2.2835, 2.2800, 7.2961, 3.4485,\n                      3.5389, 1.8072, 4.3084, 1.4728, 1.1993, 1.3476, 1.0964, 1.3915, 4.0109,\n                      3.6942, 2.5821, 4.7572, 1.1544, 1.0454, 4.1470, 2.9506, 3.3543, 1.2251,\n                      1.8004, 0.7169, 5.1059, 3.2842, 0.7344, 0.9744, 0.7184, 3.6758, 4.6844,\n                      4.1911, 3.8725, 3.8151, 3.2214, 5.3045, 4.6631, 3.5284, 2.4084, 1.4724,\n                      1.6658, 1.4082, 3.3690, 3.7949, 1.4967, 1.7163, 2.8283, 4.4828, 3.1028,\n                      3.3458, 3.9452, 2.4165, 3.3780, 5.1768, 0.9150, 2.9260, 4.1513, 0.3062,\n                      1.0683, 5.0314, 1.8165, 0.8749, 1.3170, 3.8620, 4.5009, 3.7330, 5.0475,\n                      2.8964, 3.3013, 1.1288, 3.6202, 4.4279, 3.9738, 0.3419, 3.2428, 1.7044,\n                      1.0346, 4.3614, 2.1430, 2.8242, 1.1370, 1.7863, 3.7694, 1.2612, 2.6277,\n                      1.5883, 4.5143, 3.1357, 0.8934, 1.4483, 2.2605, 3.3354, 0.6577, 3.4487,\n                      0.7823, 1.8182, 3.0755, 4.3833, 3.6084, 3.5722, 3.9245, 3.4831, 5.6456,\n                      1.8255, 4.0207, 2.6578, 1.4737, 1.1051, 4.9512, 5.0935, 2.9853, 1.4422,\n                      3.8182, 4.0050, 2.6779, 3.3383, 1.4956, 3.7990, 3.9824, 4.8440, 3.3218,\n                      1.3572, 1.8460, 0.9077, 4.8065, 0.4335, 1.8640, 2.8948, 3.7252, 1.8369,\n                      1.6227, 2.9741, 4.6130, 1.2843, 2.1269, 2.6956, 1.1999, 2.0300, 3.5248,\n                      3.5335, 1.6210, 3.2813, 3.6909, 0.6576, 2.4408, 2.2287, 1.9886, 4.3025,\n                      1.4127, 3.9316, 0.8515, 4.6158, 3.9296, 0.7093, 3.2732, 3.9156, 1.0039,\n                      1.8862, 3.2942, 1.9779, 0.9421, 1.9252, 0.3841, 3.3050, 4.2875, 4.7513,\n                      2.0787, 2.7144, 1.4166, 0.6428, 2.4258, 1.1410, 4.7308, 4.4383, 2.4043,\n                      4.8952, 4.5922, 1.1263, 0.3031, 1.9771, 0.9757, 5.3629, 0.6007, 1.4242,\n                      1.1438, 5.2757, 3.5909, 3.7862, 1.2713, 2.5423, 2.6018, 3.8239, 3.4107,\n                      4.1729, 3.5833, 2.5692, 3.8660, 1.6541, 3.5375, 3.8854, 1.7271, 3.5571,\n                      4.0051, 3.3099, 1.1904, 2.8288, 1.7247, 0.9567, 4.0753, 4.2855, 4.9547,\n                      1.9913, 0.4380, 3.0616, 2.8319, 0.9804, 4.3114, 0.7217, 4.5950, 1.4586,\n                      1.2439, 2.6501, 3.3924, 2.0346, 3.7694, 1.2014, 3.2067, 3.1203, 4.7005,\n                      1.8542, 2.7908, 4.1885, 3.2074, 4.9435, 4.4470, 4.1918, 4.6748, 1.6081,\n                      1.4004, 3.7540, 4.3213, 1.8304, 0.9771, 5.3499, 1.2860, 2.8113, 4.3867,\n                      0.3736, 3.3438, 1.4193, 1.1949, 1.1443, 3.1541, 0.6114, 3.2530, 2.6740,\n                      4.1164, 4.5656, 2.7600, 0.9382, 1.3507, 4.0322, 4.3164, 0.9152, 3.5166,\n                      3.6108, 4.3653, 1.9944, 0.5464, 0.2951, 4.9356, 0.9499, 4.1107, 1.7494,\n                      3.5864, 5.3389, 3.1703, 4.0627, 4.4654, 4.2090, 1.3932, 2.7949, 5.8620,\n                      1.3720, 2.4404, 1.1164, 3.6430, 0.6567, 2.3280, 3.8269, 1.0987, 0.4753,\n                      4.5817, 3.3620, 2.0570, 1.7750, 2.6036, 1.3784, 3.6172, 4.5238, 1.5718,\n                      1.9541, 4.0975, 2.0337, 4.5705, 3.8879, 2.1819, 3.0633, 2.0654, 1.1303,\n                      1.2159, 2.5015, 4.3210, 1.0129, 2.0439, 2.7764, 3.2600, 1.3604, 3.4956,\n                      1.9728, 3.5321, 0.8799, 3.8111, 0.3379, 2.8327, 1.3401, 2.7244, 2.9828,\n                      2.7511, 0.7883, 3.5320, 1.3533, 2.3590, 0.5822, 2.5782, 5.2855, 0.4519,\n                      3.5719, 5.4526, 3.8856, 0.9502, 4.2774, 3.4208, 7.0307, 2.7814, 2.0773,\n                      3.0258, 1.5693, 3.4547, 2.7054, 2.4926, 3.9651, 1.6089, 1.3166, 0.9445,\n                      2.3033, 5.1400, 2.9692, 4.5474, 4.3385, 2.4608, 0.3566, 2.9294, 2.7676,\n                      4.2543, 1.9479, 4.9844, 1.2381, 1.0286, 3.6369, 3.3714, 3.1918, 2.2746,\n                      3.1029, 3.1210, 4.2421, 3.2779, 1.2317, 1.3553, 0.2707, 4.2905, 2.6549,\n                      3.0835, 1.8321, 2.9151, 3.7016, 2.6008, 5.7022, 4.2532, 1.4492, 5.1975,\n                      2.7459, 3.4667, 3.0732, 2.9269, 3.0674, 1.3493, 3.2653, 1.0794, 2.6604,\n                      3.3395, 0.9148, 4.7171, 1.0941, 4.6402, 4.3658, 4.1607, 2.7089, 0.9933,\n                      1.3114, 1.7790, 4.4487, 2.1227, 3.6548, 1.4669, 3.4425, 3.1166, 5.0770,\n                      4.3530, 0.9709, 0.9183, 4.3537, 5.0459, 1.3388, 1.2689, 3.3091, 3.0193,\n                      4.6545, 3.5362, 0.3068, 1.5258, 1.1180, 2.4372, 1.7345, 3.7259, 3.4260,\n                      1.0537, 1.9293, 4.8289, 1.4584, 0.4426, 3.0415, 0.6440, 4.8682, 4.0088,\n                      1.1661, 2.1777, 1.5265, 3.3607, 4.2960, 2.8815, 3.5060, 1.0686, 1.0148,\n                      0.9323, 3.1887, 0.2356, 1.5190, 1.4528, 4.4603, 2.9862, 1.9634, 1.1150,\n                      1.0881, 1.4505, 4.4638, 3.6359, 3.2144, 1.4852, 1.4054, 2.8650, 8.6888,\n                      2.7830, 3.9923, 3.5452, 4.3332, 1.5844, 1.6896, 3.4331, 4.3830, 1.5113,\n                      2.9244, 4.3343, 2.9406, 4.6565, 1.0561, 4.1427, 1.5534, 1.2903, 2.0879,\n                      3.4795, 3.5283, 4.2681, 1.6214, 5.7266, 1.4467, 1.4223, 2.8748, 0.9637,\n                      0.5195, 0.8739, 0.3792, 3.2396, 4.4935, 5.1634, 1.7068, 1.1608, 3.0486,\n                      0.3265, 1.0903, 1.3846, 3.4517, 3.9733, 0.3982, 2.1042, 4.2567, 4.5789,\n                      4.5623, 4.5008, 3.5845, 3.3883, 3.9656, 1.4411, 1.9847, 0.3108, 3.2375,\n                      1.8126, 1.2431, 5.6604, 1.2910, 3.0633, 1.5600, 1.3428, 4.3345, 3.8683,\n                      4.0518, 3.4794, 5.0592, 3.0257, 1.3475, 2.9655, 0.9012, 1.3809, 5.4147,\n                      2.5438, 4.2056, 3.8654, 0.3599, 4.5609, 0.3405, 4.1139, 2.4819, 1.6295,\n                      4.6787, 3.8570, 3.0999, 1.2669, 5.2702, 1.4535, 3.5552, 2.4020, 4.6847,\n                      2.1524, 3.4415, 2.2149, 2.7215, 5.1008, 1.6547, 3.2576, 1.1303, 3.3943,\n                      3.4488, 4.6437, 1.3618, 2.5605, 3.2136, 4.2797, 3.3822, 0.4228, 1.9639,\n                      1.4508, 1.1407, 3.2677, 0.8623, 1.5456, 2.6498, 0.9670, 2.6485, 0.9388,\n                      3.0827, 0.9235, 3.6322, 1.7161, 2.9896, 1.1547, 1.9208, 2.3681, 3.3122,\n                      3.1115, 3.3816, 3.3495, 4.4585, 4.9179, 1.1270, 1.1587, 3.3778, 1.5348,\n                      3.9467, 4.0974, 1.9655, 4.0134, 3.2955, 2.5972, 3.5819, 2.7558, 4.5723,\n                      0.9327, 4.0522, 1.2343, 0.5207, 3.2703, 4.8624, 1.9154, 3.2617, 1.9640,\n                      4.0281, 2.3886, 3.5045, 3.2675, 1.1398, 1.9745, 3.3854, 0.5896, 1.7193,\n                      3.6284, 1.0655, 4.4691, 1.1682, 1.9601, 2.9290, 1.5029, 4.4189, 0.7704,\n                      2.9808, 2.6020, 1.4603, 1.0759, 0.8611, 1.3005, 3.7007, 1.9679, 3.1890,\n                      2.3768, 4.9952, 1.2964])),\n             ('features.7.2.block.2.bias',\n              tensor([ 1.3660e+00,  1.1301e+00,  1.6708e+00,  1.9071e-01,  4.2029e-01,\n                       3.9904e-01,  8.0607e-01,  3.5637e-01,  8.1370e-01,  3.4036e+00,\n                       1.7011e+00, -2.8647e-01, -3.1438e-01,  6.7062e-01,  7.0545e+00,\n                       1.0679e-01,  1.8684e-01, -3.4161e-01,  8.8229e-01,  8.8694e-02,\n                       3.6341e-01,  1.0007e+00, -7.8762e-01,  3.9323e-01,  2.9319e-01,\n                       6.9597e-01,  5.3242e-01,  3.3649e-01,  1.6349e-01, -3.8440e-01,\n                      -1.7262e-01,  4.2312e-01,  2.7919e+00, -1.9282e-01,  1.7030e-01,\n                       9.5486e-02,  7.1648e-01,  3.0529e-01,  3.1910e-01,  1.6535e-01,\n                       6.0286e-01,  5.1576e-01,  7.9464e-01,  2.5323e-01,  4.0065e-01,\n                       2.4384e-01,  3.4546e-01,  3.8907e-01,  1.2530e+00,  7.1648e-01,\n                       9.6442e-01,  1.2089e+00,  9.9605e-01,  1.1051e+00,  2.1392e-01,\n                       1.3160e-01,  3.3967e-01,  1.0000e+00,  2.7329e-01,  4.2406e-01,\n                       4.6101e-01,  3.5371e-01, -3.5339e-01,  1.9646e-01,  3.4320e-01,\n                       7.4476e-01,  8.9286e-02,  3.0707e-01, -1.2747e-01,  4.0526e-01,\n                       3.4454e-01,  1.0519e+00,  2.7705e-01,  2.3513e-01,  4.3417e-01,\n                       1.6842e+00,  5.6992e-01,  1.2992e-01,  6.9124e-01,  6.7201e-02,\n                       8.3979e-01,  4.8465e-01,  5.5025e-01,  3.4332e-01,  3.9866e-01,\n                       4.1434e-01,  1.8786e-01, -6.3196e-02,  8.2417e-01,  4.6251e-01,\n                       4.9944e-01,  1.7946e-01,  5.4609e-01,  1.0011e-01,  8.7396e-01,\n                       4.8277e-01,  3.2627e-01,  2.9959e-01,  7.5736e-01,  6.5530e-01,\n                       8.8097e-01,  5.0605e-01,  3.9381e-01,  9.5271e-01,  5.1429e-01,\n                      -2.4591e-01,  3.2610e-01,  6.6585e-01,  4.0978e-01,  4.5421e-01,\n                       5.1631e-01,  4.0096e-01,  6.9171e-01,  7.2761e-01, -2.2558e-01,\n                       5.5178e-01,  8.6088e-01,  3.1088e-01,  4.9872e-01,  1.3645e+00,\n                      -1.0125e+00, -3.0385e-01, -1.6947e-02,  2.0701e+00,  1.0347e-01,\n                       8.9920e-01,  2.7828e-01,  8.0434e-01,  3.1417e-01,  1.8341e+00,\n                       2.3452e+00,  9.8787e-01,  4.6968e-01,  3.9171e-01,  5.3033e-01,\n                      -2.0204e-03, -8.8268e-02,  4.7597e-01,  2.5997e-01,  3.5392e-01,\n                       1.2240e-01,  3.4223e-01,  2.6945e+00,  6.1152e-01, -4.4058e+00,\n                       2.7302e-01,  5.5421e-01,  2.9123e-01,  3.1447e-01, -3.0954e-01,\n                       6.5420e-01,  9.0322e-01,  4.6587e-01,  2.9438e-01,  5.1172e-02,\n                       2.0928e-01,  6.7949e-01,  3.4989e-01, -2.5960e-01,  2.6958e-01,\n                       1.1675e+00,  1.7717e-01,  7.4829e-02,  6.3583e-01, -2.9476e-01,\n                       6.2084e-01,  7.2872e-01,  1.0762e-01,  1.6119e-01,  2.8178e-01,\n                      -4.2891e-02,  1.1605e+00,  9.3414e-01,  1.5913e+00,  4.3880e-01,\n                       8.2001e-02,  3.8694e-01,  1.3553e+00,  2.3519e-01,  9.2161e-01,\n                       9.6740e-01,  5.6834e-01,  1.6644e-01,  9.6071e-01,  6.4105e-01,\n                       8.8188e-01, -7.9581e-02,  2.3049e-01,  8.1258e-01,  3.9452e-01,\n                       2.8440e-01,  1.1002e+00, -8.6024e-02, -1.6128e-01, -1.6798e-01,\n                       1.0263e-01, -8.0725e-02, -2.1567e-02,  2.7590e-01,  7.8673e-01,\n                       2.3424e-02, -2.0806e-01,  5.5010e-01,  2.1088e-01,  1.4337e+00,\n                       1.3450e-01,  4.7947e-01,  2.9637e-01,  7.3104e-01,  1.9014e+00,\n                       2.7831e-01,  2.6651e-01,  6.9003e-03,  1.6119e+00,  2.8313e+00,\n                       1.1584e+00,  7.1716e-01, -6.2639e+00,  1.5485e+00,  1.0407e+00,\n                       1.1918e-01,  6.4307e-01,  3.0881e-01,  9.2008e-01, -1.2898e+00,\n                       3.8486e-01,  4.1822e-01,  4.8888e-01,  4.0168e-01,  2.5199e-02,\n                       3.4947e+00,  8.1569e-01,  3.4391e-01,  9.8314e-01, -4.2049e-02,\n                      -1.6429e-01,  1.1453e+00,  5.3241e-01,  6.1812e-01,  2.5996e-01,\n                       2.1389e-01,  1.4383e+00,  5.8171e-01,  7.1110e-01,  4.6705e-02,\n                       5.5893e-01, -1.7845e+00,  6.8817e-01,  3.3069e-01, -2.2409e+00,\n                       7.5093e-01,  5.5849e-01,  4.1879e-02,  9.0760e-01,  3.7682e-01,\n                       1.1427e+00,  1.9528e-01,  2.3853e-01,  9.2344e-01, -4.1585e-01,\n                       3.9581e-01, -2.9939e-01,  7.9377e-01, -8.5302e-02, -2.1763e+00,\n                       1.5530e-01,  3.8506e+00,  5.4481e-01,  1.7546e+00,  2.9611e-01,\n                       1.5346e-01,  4.7851e-01,  1.3032e-01,  1.4316e+00,  4.8774e-01,\n                       3.6138e-01,  2.4802e-01,  1.6345e+00,  8.3177e-01,  7.3318e-01,\n                      -2.0901e+00,  7.0962e-01,  1.1597e+00,  3.6607e-01,  1.4172e-01,\n                       1.2679e+00,  1.3254e+00, -2.2623e-01, -2.5005e+00,  8.1274e-01,\n                      -1.9913e-01, -1.0459e-01,  1.5390e-01,  3.1270e-01,  7.3641e-01,\n                       5.3429e-01,  3.3239e-01,  1.5593e-01,  1.3005e+00,  2.3376e+00,\n                       1.3845e-01,  1.6981e-01,  1.9023e-01,  2.4147e+00,  1.1766e+00,\n                       7.2194e-01,  1.0408e+00, -4.3530e+00,  1.1594e+00,  5.6337e-01,\n                       2.1379e-01,  3.6791e-01,  3.8688e-01,  3.3744e-01,  6.0803e-01,\n                       6.8201e-01,  4.1860e-01, -4.5676e-01,  7.2411e-01,  6.7983e-01,\n                       2.1782e-01,  9.1074e-01,  4.5121e-01,  1.1155e-01,  1.5281e-01,\n                       1.0894e-01,  5.1958e-01,  4.2218e-01,  3.7926e-01,  5.4469e-01,\n                       3.2655e-01,  4.5610e-01, -2.7190e-01,  2.5806e-01,  4.0409e-01,\n                       1.0252e+00, -1.8609e+00, -1.9350e-01, -1.9872e-02,  3.4286e-01,\n                      -1.3042e+00,  4.8157e-01,  5.2636e-01,  8.6399e-02,  5.8927e-02,\n                       2.5546e-02,  1.1997e+00,  6.6226e-01,  6.7904e-01,  4.8407e-01,\n                       2.4847e-01,  8.7897e-01,  1.2734e-01,  7.8634e-01,  4.2007e-01,\n                       4.7098e-01,  3.0869e-01, -5.0906e-01, -1.5137e-01,  9.0777e-01,\n                       1.5332e-02,  4.8945e-01,  6.3284e-01, -8.7873e-01, -7.4036e-01,\n                       5.2238e-01,  1.3120e-01,  3.3518e-01,  1.7927e+00,  8.3166e-01,\n                       4.3353e-01,  3.5307e-01,  6.9389e-01,  2.2422e-01,  9.0608e-01,\n                       1.7283e+00,  3.7899e-01,  5.1050e-01, -3.2918e-01,  1.1794e-01,\n                       4.9507e-01,  2.2472e-01, -6.0797e-03,  1.3045e-01,  1.3562e-01,\n                       5.6420e-01,  2.9335e-01,  2.5002e-01,  1.0122e-01,  4.5614e-01,\n                       3.6862e-01,  1.2290e-01,  6.6667e-01,  1.2754e-01,  1.2464e-01,\n                       8.6443e-01, -1.3871e-01, -2.3557e+00,  2.8646e-01,  4.4393e-01,\n                       2.5154e-01, -8.3714e-03,  3.1575e-01,  2.1736e+00,  6.4019e-01,\n                       3.0625e-01,  6.1744e-01,  2.2939e-01,  7.2105e-01,  4.5218e-01,\n                      -6.7511e-02,  5.7515e-01,  6.6552e-01,  2.0738e-02,  9.4712e-02,\n                       9.1801e-02,  1.7604e-01,  6.3718e-02,  5.0387e-01,  5.0915e-01,\n                       3.2158e+00,  1.9476e-02,  8.2867e-02,  6.3467e-01, -6.2520e-02,\n                       1.2392e+00,  1.8281e-01,  3.8623e-01,  7.2676e-01,  4.3946e-01,\n                       2.8812e-01, -4.5196e-02,  8.5257e-01,  4.8132e-01,  4.1604e-01,\n                       4.9718e-01,  3.5977e-02,  8.4736e-02, -2.1239e-01,  2.5723e-01,\n                       5.9814e-01,  9.4040e-03,  1.2890e+00, -1.7402e+00,  5.5475e-01,\n                       7.3581e-01,  9.7935e-01,  1.8584e+00,  9.2704e-01, -1.7630e+00,\n                       2.7023e-01,  1.3737e-01,  1.6736e+00,  2.2310e+00,  3.9218e-01,\n                       6.1922e-01,  4.4449e+00,  3.2032e-01,  8.2171e-01,  3.9068e-01,\n                      -7.2223e-02,  3.3007e-01,  1.9744e+00,  1.2415e+00,  9.9021e-01,\n                      -4.6561e-01,  2.6472e-01, -1.1638e-03,  2.9123e-01, -1.6782e-01,\n                       7.6471e-02,  2.9925e-01,  9.6626e-01,  3.3780e-01,  9.2105e-02,\n                       4.6743e-01,  4.6693e-01,  6.4637e-01, -1.7396e+00,  9.0839e-01,\n                       2.4178e-01, -1.1202e-01,  4.1813e-01,  2.5642e-01,  4.9402e-01,\n                       4.3744e-01,  1.4420e+00,  4.7186e-01,  2.9066e-01,  7.5220e-01,\n                      -1.7826e-01,  2.2928e-01, -9.5641e-02,  7.4907e-01,  1.2604e-01,\n                      -4.3696e-01,  8.4807e-01,  9.1080e-01,  1.5068e-01,  2.6226e-01,\n                       6.7003e-01,  4.9095e-01,  3.2049e-01,  4.0388e-01,  1.9385e-01,\n                       5.6651e-01,  4.1557e-01,  8.3553e-01,  7.4425e-02,  3.6263e-01,\n                       1.8019e-01,  5.5452e-02,  8.3665e-01,  4.1004e-01,  1.1900e+00,\n                       7.1135e-01,  5.5201e-01,  2.3328e-01,  3.6973e-01,  3.1068e-01,\n                      -2.6567e-01,  1.9465e+00,  6.7773e-01, -1.1521e+00,  8.6839e-01,\n                       6.6243e-01,  2.8996e-01, -6.5345e-04,  2.6155e-01,  4.7197e-01,\n                       4.3642e-01,  1.6796e+00,  6.0239e-01, -6.3700e-03,  3.7030e-01,\n                       5.1196e-01,  3.6643e-01,  9.1807e-01,  2.2765e-01,  4.4926e-01,\n                       5.8381e-01,  1.8999e+00, -4.3425e-02,  2.1354e+00,  7.2464e-01,\n                       1.2636e-01,  7.5869e-01,  4.4503e-01,  4.4689e-01,  2.3912e-01,\n                      -7.9415e-01,  9.2901e-01, -8.0684e-01,  7.5088e-01, -9.4211e-01,\n                       4.0474e-01,  2.2680e-01,  2.1461e-01,  8.1225e-01,  7.3727e-01,\n                      -4.4453e-01,  2.0480e-01,  1.0166e+00,  4.3562e-01,  1.4452e+00,\n                      -1.7542e-01,  8.0445e-01,  1.9005e-01,  2.0898e-01, -1.6606e-01,\n                      -1.1867e+00,  3.7597e-01,  5.1333e-01,  5.4782e-01,  9.8062e-01,\n                       8.2353e-01,  2.9969e-01,  2.4363e-01,  8.3494e-01,  6.0135e-01,\n                      -2.0161e-01, -5.6103e-01, -3.5467e-01, -1.1779e+00,  8.2187e+00,\n                       2.3604e-01,  4.6356e-01,  6.2385e-01,  1.4759e+00,  1.6453e+00,\n                      -4.7614e-01,  3.6754e-01,  1.1697e+00,  1.2048e+00,  2.7212e-01,\n                       7.0952e-01,  7.9846e-01,  7.4936e-01,  2.5603e-01,  5.0803e-01,\n                       7.5661e-01,  2.1848e-01,  1.9491e+00,  2.1058e-01,  2.8083e-01,\n                      -2.3347e+00,  3.2887e-01,  9.2196e-01,  7.5825e-01,  1.5579e-01,\n                       8.3074e-01, -7.6920e-01,  3.3866e-01,  2.2257e-01, -3.8000e-01,\n                       9.3967e-01,  5.7758e-01,  1.8954e-01,  7.0331e-01, -3.6179e-02,\n                       2.9852e-01,  1.5417e-01,  1.1888e+00, -2.3397e-01,  4.2922e-01,\n                       3.1994e-01,  4.9620e-02,  2.5698e-01,  8.7107e-01,  4.0797e-01,\n                      -2.4669e+00,  4.9936e-01,  9.9933e-01,  1.7942e+00,  4.1198e-01,\n                      -2.9972e-01,  4.4954e-01, -3.1116e-02,  2.8955e-01,  4.6492e-01,\n                       6.4671e-01,  8.9639e-01, -1.5531e-01,  2.5336e+00,  2.6426e+00,\n                      -6.0331e-01,  2.8123e-01,  1.2050e-01,  6.1414e-01,  5.0689e-01,\n                       4.1880e-01, -2.3643e-02,  2.8383e-01, -1.5434e+00,  7.6365e-02,\n                       2.5602e-02,  1.0462e+00,  1.5116e+00,  9.5528e-02, -1.8142e-01,\n                       4.0717e-01,  9.0440e-01, -1.8972e-01,  7.4369e-01, -2.7385e+00,\n                       2.6094e-01,  5.0478e-02,  1.0571e+00,  5.9436e-01,  1.4352e-01,\n                       6.7692e-01,  2.9253e-01,  1.3316e+00,  1.6140e+00,  1.1602e+00,\n                       1.6084e+00,  2.8744e-01,  2.9128e-01,  4.7038e-01,  1.8789e-01,\n                      -3.3869e-01,  5.3548e-01, -1.9597e-03,  1.2959e+00,  9.2018e-01,\n                       8.2602e-01,  1.2570e+00,  1.0864e+00,  2.3613e-01,  1.3699e+00,\n                       4.6808e-01,  1.0637e-01,  5.0853e-01,  4.3669e-01,  1.6070e-01,\n                       1.5791e-01, -6.4536e-01,  2.4229e+00,  2.4960e-01,  1.5951e-01,\n                       8.3428e-02,  2.3876e-01,  5.5448e-01,  6.6356e-01,  8.5007e-02,\n                      -1.1726e+00,  1.8803e-01, -3.7475e+00,  5.2127e-01, -3.5622e-01,\n                       2.9844e-01, -4.6922e-02,  1.3735e-01, -3.4754e-01,  7.1456e-01,\n                       8.5714e-01, -2.1135e-01,  1.7802e-01,  2.0291e-01, -5.2460e-01,\n                       3.5987e-01,  8.8758e-01,  4.9087e-01,  1.1683e+00,  1.6479e-01,\n                       9.1169e-01,  1.0722e+00,  4.2081e-01,  5.1132e-01,  3.4587e-01,\n                       1.0153e+00,  7.9589e-02,  2.0383e-01,  4.2114e-01,  6.1772e-01,\n                       1.8742e-01,  3.8310e-01,  2.2903e-01,  1.2850e+00,  1.7853e-01,\n                       7.1100e-02,  6.1408e-01,  3.2113e+00,  5.5376e-01,  2.3563e-01,\n                       3.6191e-01,  2.3230e-01,  2.8066e-01,  6.5755e-01,  5.2730e-02,\n                      -3.5538e-01,  9.3607e-02,  9.3709e-01, -8.5644e-01,  7.4306e-01,\n                       6.6745e-01,  2.5316e-01,  5.5819e-01, -9.4724e-01,  6.9434e-01,\n                       2.1773e-01,  8.5505e-01,  1.8747e-01,  1.0959e-01,  1.1297e+00,\n                      -3.2595e-01,  4.9776e-01,  7.5062e-01])),\n             ('features.7.2.block.2.scale', tensor(0.2292)),\n             ('features.7.2.block.2.zero_point', tensor(48)),\n             ('features.7.2.block.3.scale', tensor(0.1873)),\n             ('features.7.2.block.3.zero_point', tensor(81)),\n             ('features.7.2.block.3._packed_params.dtype', torch.qint8),\n             ('features.7.2.block.3._packed_params._packed_params',\n              (tensor([[ 0.0115, -0.1199, -0.0197,  ...,  0.0361, -0.1068, -0.0263],\n                       [ 0.0013, -0.0301,  0.0226,  ...,  0.0427,  0.0728,  0.0176],\n                       [-0.0639, -0.0013, -0.0625,  ...,  0.0546,  0.0519, -0.0692],\n                       ...,\n                       [-0.0103, -0.0296, -0.0437,  ..., -0.0450, -0.0219, -0.0489],\n                       [-0.0383, -0.0548, -0.0575,  ...,  0.0205, -0.0575, -0.0219],\n                       [-0.1275, -0.0104, -0.0059,  ...,  0.0934,  0.0000,  0.0623]],\n                      size=(3072, 768), dtype=torch.qint8,\n                      quantization_scheme=torch.per_channel_affine,\n                      scale=tensor([0.0016, 0.0013, 0.0013,  ..., 0.0013, 0.0014, 0.0015],\n                      dtype=torch.float64),\n                      zero_point=tensor([0, 0, 0,  ..., 0, 0, 0]), axis=0),\n               Parameter containing:\n               tensor([-0.0250, -0.0195, -0.0324,  ..., -0.0180, -0.0277, -0.0299],\n                      requires_grad=True))),\n             ('features.7.2.block.5.scale', tensor(0.1508)),\n             ('features.7.2.block.5.zero_point', tensor(78)),\n             ('features.7.2.block.5._packed_params.dtype', torch.qint8),\n             ('features.7.2.block.5._packed_params._packed_params',\n              (tensor([[-0.0583, -0.0117, -0.0070,  ...,  0.0955, -0.0070, -0.0303],\n                       [ 0.0199,  0.0077, -0.0720,  ..., -0.0046,  0.0812, -0.0367],\n                       [-0.0826,  0.0064,  0.0000,  ..., -0.0275, -0.0127, -0.0402],\n                       ...,\n                       [ 0.1262,  0.0310,  0.0797,  ...,  0.0199,  0.0421, -0.0266],\n                       [ 0.0697, -0.0746, -0.0924,  ..., -0.0032, -0.0762, -0.0616],\n                       [ 0.0424, -0.0581, -0.0863,  ..., -0.0377, -0.1036, -0.0644]],\n                      size=(768, 3072), dtype=torch.qint8,\n                      quantization_scheme=torch.per_channel_affine,\n                      scale=tensor([0.0023, 0.0015, 0.0021, 0.0015, 0.0014, 0.0016, 0.0014, 0.0016, 0.0015,\n                       0.0025, 0.0016, 0.0017, 0.0020, 0.0016, 0.0030, 0.0020, 0.0019, 0.0026,\n                       0.0016, 0.0016, 0.0016, 0.0024, 0.0025, 0.0015, 0.0016, 0.0014, 0.0016,\n                       0.0015, 0.0018, 0.0019, 0.0017, 0.0014, 0.0015, 0.0014, 0.0022, 0.0017,\n                       0.0017, 0.0016, 0.0014, 0.0015, 0.0016, 0.0018, 0.0016, 0.0014, 0.0013,\n                       0.0015, 0.0017, 0.0016, 0.0017, 0.0021, 0.0015, 0.0015, 0.0020, 0.0017,\n                       0.0017, 0.0015, 0.0016, 0.0017, 0.0016, 0.0016, 0.0023, 0.0018, 0.0015,\n                       0.0019, 0.0016, 0.0014, 0.0016, 0.0014, 0.0018, 0.0013, 0.0014, 0.0017,\n                       0.0016, 0.0017, 0.0014, 0.0015, 0.0014, 0.0014, 0.0016, 0.0013, 0.0021,\n                       0.0016, 0.0017, 0.0014, 0.0018, 0.0019, 0.0017, 0.0023, 0.0014, 0.0019,\n                       0.0016, 0.0017, 0.0017, 0.0015, 0.0016, 0.0014, 0.0020, 0.0016, 0.0016,\n                       0.0016, 0.0014, 0.0016, 0.0014, 0.0017, 0.0016, 0.0015, 0.0016, 0.0018,\n                       0.0016, 0.0015, 0.0016, 0.0016, 0.0025, 0.0016, 0.0016, 0.0018, 0.0016,\n                       0.0015, 0.0017, 0.0022, 0.0015, 0.0015, 0.0015, 0.0015, 0.0016, 0.0016,\n                       0.0022, 0.0017, 0.0015, 0.0014, 0.0017, 0.0015, 0.0017, 0.0017, 0.0017,\n                       0.0017, 0.0018, 0.0020, 0.0014, 0.0015, 0.0016, 0.0015, 0.0017, 0.0014,\n                       0.0022, 0.0021, 0.0015, 0.0015, 0.0016, 0.0016, 0.0017, 0.0017, 0.0016,\n                       0.0017, 0.0015, 0.0016, 0.0016, 0.0015, 0.0017, 0.0016, 0.0016, 0.0014,\n                       0.0015, 0.0015, 0.0015, 0.0018, 0.0022, 0.0015, 0.0017, 0.0019, 0.0016,\n                       0.0015, 0.0018, 0.0017, 0.0015, 0.0016, 0.0013, 0.0043, 0.0014, 0.0015,\n                       0.0017, 0.0015, 0.0014, 0.0018, 0.0016, 0.0016, 0.0017, 0.0014, 0.0016,\n                       0.0015, 0.0022, 0.0014, 0.0020, 0.0017, 0.0016, 0.0018, 0.0016, 0.0015,\n                       0.0016, 0.0017, 0.0020, 0.0016, 0.0018, 0.0017, 0.0016, 0.0015, 0.0015,\n                       0.0015, 0.0017, 0.0017, 0.0015, 0.0015, 0.0015, 0.0021, 0.0021, 0.0017,\n                       0.0018, 0.0015, 0.0024, 0.0015, 0.0016, 0.0019, 0.0019, 0.0015, 0.0015,\n                       0.0015, 0.0015, 0.0021, 0.0015, 0.0017, 0.0016, 0.0020, 0.0016, 0.0015,\n                       0.0016, 0.0014, 0.0015, 0.0018, 0.0023, 0.0019, 0.0018, 0.0014, 0.0020,\n                       0.0017, 0.0016, 0.0018, 0.0015, 0.0022, 0.0016, 0.0016, 0.0015, 0.0014,\n                       0.0018, 0.0014, 0.0015, 0.0019, 0.0014, 0.0015, 0.0014, 0.0019, 0.0016,\n                       0.0015, 0.0025, 0.0015, 0.0020, 0.0018, 0.0033, 0.0018, 0.0020, 0.0014,\n                       0.0017, 0.0017, 0.0017, 0.0016, 0.0020, 0.0021, 0.0022, 0.0016, 0.0017,\n                       0.0014, 0.0019, 0.0016, 0.0014, 0.0014, 0.0017, 0.0023, 0.0017, 0.0017,\n                       0.0024, 0.0016, 0.0014, 0.0018, 0.0016, 0.0014, 0.0017, 0.0014, 0.0016,\n                       0.0017, 0.0020, 0.0017, 0.0016, 0.0017, 0.0016, 0.0015, 0.0016, 0.0022,\n                       0.0017, 0.0016, 0.0025, 0.0017, 0.0017, 0.0025, 0.0015, 0.0015, 0.0014,\n                       0.0016, 0.0016, 0.0034, 0.0017, 0.0017, 0.0019, 0.0014, 0.0016, 0.0017,\n                       0.0015, 0.0016, 0.0014, 0.0019, 0.0015, 0.0015, 0.0014, 0.0015, 0.0017,\n                       0.0016, 0.0016, 0.0016, 0.0014, 0.0016, 0.0017, 0.0014, 0.0022, 0.0017,\n                       0.0014, 0.0014, 0.0014, 0.0015, 0.0017, 0.0017, 0.0016, 0.0017, 0.0014,\n                       0.0014, 0.0016, 0.0015, 0.0016, 0.0016, 0.0018, 0.0017, 0.0019, 0.0016,\n                       0.0017, 0.0015, 0.0017, 0.0016, 0.0020, 0.0016, 0.0016, 0.0019, 0.0019,\n                       0.0016, 0.0018, 0.0014, 0.0015, 0.0017, 0.0017, 0.0015, 0.0015, 0.0018,\n                       0.0015, 0.0016, 0.0016, 0.0015, 0.0019, 0.0015, 0.0017, 0.0017, 0.0015,\n                       0.0016, 0.0021, 0.0016, 0.0015, 0.0016, 0.0018, 0.0016, 0.0020, 0.0027,\n                       0.0015, 0.0014, 0.0018, 0.0015, 0.0018, 0.0030, 0.0016, 0.0017, 0.0015,\n                       0.0018, 0.0021, 0.0018, 0.0015, 0.0017, 0.0016, 0.0015, 0.0017, 0.0016,\n                       0.0016, 0.0015, 0.0014, 0.0018, 0.0015, 0.0015, 0.0019, 0.0018, 0.0015,\n                       0.0017, 0.0014, 0.0021, 0.0015, 0.0017, 0.0017, 0.0016, 0.0015, 0.0016,\n                       0.0016, 0.0022, 0.0017, 0.0016, 0.0017, 0.0018, 0.0015, 0.0018, 0.0016,\n                       0.0020, 0.0016, 0.0016, 0.0014, 0.0014, 0.0016, 0.0015, 0.0015, 0.0016,\n                       0.0016, 0.0021, 0.0015, 0.0016, 0.0013, 0.0015, 0.0022, 0.0015, 0.0015,\n                       0.0017, 0.0015, 0.0015, 0.0018, 0.0016, 0.0015, 0.0013, 0.0015, 0.0016,\n                       0.0016, 0.0023, 0.0018, 0.0016, 0.0017, 0.0020, 0.0019, 0.0016, 0.0015,\n                       0.0018, 0.0016, 0.0017, 0.0020, 0.0018, 0.0016, 0.0014, 0.0016, 0.0017,\n                       0.0016, 0.0019, 0.0024, 0.0016, 0.0015, 0.0016, 0.0017, 0.0015, 0.0016,\n                       0.0014, 0.0015, 0.0025, 0.0016, 0.0020, 0.0024, 0.0016, 0.0016, 0.0016,\n                       0.0017, 0.0015, 0.0014, 0.0019, 0.0022, 0.0015, 0.0018, 0.0016, 0.0015,\n                       0.0014, 0.0016, 0.0014, 0.0017, 0.0022, 0.0018, 0.0018, 0.0016, 0.0017,\n                       0.0014, 0.0016, 0.0014, 0.0015, 0.0017, 0.0015, 0.0016, 0.0017, 0.0015,\n                       0.0017, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0020, 0.0019,\n                       0.0015, 0.0016, 0.0019, 0.0016, 0.0015, 0.0017, 0.0015, 0.0016, 0.0015,\n                       0.0017, 0.0016, 0.0017, 0.0015, 0.0033, 0.0017, 0.0016, 0.0015, 0.0016,\n                       0.0017, 0.0014, 0.0014, 0.0016, 0.0015, 0.0016, 0.0016, 0.0016, 0.0017,\n                       0.0014, 0.0014, 0.0020, 0.0014, 0.0017, 0.0014, 0.0023, 0.0014, 0.0014,\n                       0.0017, 0.0016, 0.0018, 0.0017, 0.0016, 0.0015, 0.0016, 0.0021, 0.0024,\n                       0.0018, 0.0017, 0.0014, 0.0014, 0.0014, 0.0017, 0.0015, 0.0017, 0.0014,\n                       0.0014, 0.0018, 0.0013, 0.0016, 0.0015, 0.0020, 0.0017, 0.0017, 0.0015,\n                       0.0014, 0.0015, 0.0024, 0.0016, 0.0021, 0.0016, 0.0014, 0.0014, 0.0016,\n                       0.0015, 0.0015, 0.0033, 0.0016, 0.0018, 0.0016, 0.0018, 0.0014, 0.0015,\n                       0.0019, 0.0015, 0.0015, 0.0014, 0.0014, 0.0015, 0.0015, 0.0017, 0.0015,\n                       0.0017, 0.0016, 0.0015, 0.0014, 0.0015, 0.0015, 0.0019, 0.0016, 0.0013,\n                       0.0015, 0.0015, 0.0021, 0.0017, 0.0019, 0.0016, 0.0016, 0.0017, 0.0015,\n                       0.0015, 0.0016, 0.0021, 0.0014, 0.0016, 0.0017, 0.0018, 0.0018, 0.0020,\n                       0.0017, 0.0018, 0.0017, 0.0020, 0.0020, 0.0020, 0.0017, 0.0017, 0.0019,\n                       0.0018, 0.0015, 0.0017, 0.0016, 0.0018, 0.0014, 0.0015, 0.0016, 0.0017,\n                       0.0016, 0.0016, 0.0020, 0.0016, 0.0016, 0.0015, 0.0015, 0.0016, 0.0015,\n                       0.0017, 0.0020, 0.0015, 0.0016, 0.0016, 0.0015, 0.0015, 0.0018, 0.0017,\n                       0.0015, 0.0016, 0.0013, 0.0017, 0.0021, 0.0014, 0.0020, 0.0018, 0.0017,\n                       0.0014, 0.0015, 0.0017, 0.0014, 0.0015, 0.0049, 0.0017, 0.0017, 0.0016,\n                       0.0015, 0.0015, 0.0015, 0.0016, 0.0018, 0.0015, 0.0015, 0.0015, 0.0015,\n                       0.0016, 0.0016, 0.0019, 0.0015, 0.0015, 0.0017, 0.0014, 0.0018, 0.0017,\n                       0.0016, 0.0015, 0.0015, 0.0016, 0.0019, 0.0014, 0.0015, 0.0018, 0.0016,\n                       0.0019, 0.0017, 0.0019, 0.0015, 0.0020, 0.0021, 0.0016, 0.0016, 0.0015,\n                       0.0021, 0.0016, 0.0024, 0.0016, 0.0017, 0.0016, 0.0017, 0.0017, 0.0015,\n                       0.0014, 0.0016, 0.0016, 0.0014, 0.0015, 0.0014, 0.0016, 0.0016, 0.0015,\n                       0.0022, 0.0016, 0.0016], dtype=torch.float64),\n                      zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n                      axis=0),\n               Parameter containing:\n               tensor([ 4.1702e-03, -1.0969e-03, -3.0834e-02, -1.0825e-02, -1.6850e-02,\n                       -1.2338e-02,  2.3214e-02,  9.8791e-03,  7.9888e-03,  2.6072e-02,\n                       -2.9971e-03,  2.2410e-03,  4.7728e-02, -3.5498e-03,  1.7286e-01,\n                       -3.3023e-02,  1.9206e-03, -1.4100e-02, -1.2232e-02, -1.7985e-02,\n                        1.9281e-02,  1.2960e-02, -3.7366e-02, -7.3629e-04, -2.1766e-02,\n                        8.4640e-03,  1.0002e-02,  5.6224e-03, -1.9165e-02, -2.3320e-02,\n                        1.6791e-02,  1.4355e-02, -2.8739e-02,  1.0064e-02,  1.1748e-02,\n                       -6.6543e-03, -1.0004e-02, -1.5016e-03, -7.7356e-03,  1.1623e-02,\n                       -1.3115e-02, -3.9860e-03,  1.4352e-02, -3.3420e-03,  2.8120e-02,\n                       -1.2411e-02,  2.9969e-03, -1.3565e-02,  1.3456e-02,  3.2038e-02,\n                        1.6838e-02,  1.8098e-02, -2.9228e-02, -5.2232e-03, -9.9052e-03,\n                        5.2822e-03,  2.0874e-02,  1.7642e-03,  1.2148e-02, -4.0724e-03,\n                       -6.3149e-03,  2.1463e-02,  1.1370e-02, -2.8129e-02,  2.6966e-03,\n                        1.6162e-02, -5.3597e-03,  2.5217e-02,  3.4784e-02, -6.5216e-03,\n                        4.1301e-03, -6.9967e-03,  3.1272e-03, -9.4289e-03,  1.1922e-02,\n                       -1.8362e-02,  2.4038e-02, -1.4683e-02, -6.0449e-03,  1.3080e-02,\n                        2.5142e-02, -1.0242e-02,  9.4260e-03, -7.1658e-03, -1.3535e-02,\n                       -6.7776e-03, -1.6006e-02, -8.9546e-02, -2.3484e-04, -2.5093e-03,\n                       -1.0914e-02,  3.9286e-02,  9.2612e-03, -2.1133e-03,  1.1287e-02,\n                       -2.1184e-02, -1.0369e-02,  7.4836e-03,  1.1157e-02, -1.4954e-02,\n                       -8.3642e-03,  2.7381e-02,  1.3032e-03,  1.2909e-02, -6.3205e-03,\n                       -7.4393e-03, -6.5027e-03,  4.1237e-02,  2.6858e-03, -3.2770e-03,\n                       -1.9945e-02,  2.5239e-02, -6.4409e-03, -1.4678e-02, -9.3682e-03,\n                       -1.1522e-03,  3.6058e-03,  4.2924e-03,  1.4318e-02, -2.2730e-02,\n                       -8.4291e-03,  1.7043e-02,  2.4321e-02,  4.7124e-03,  4.1112e-02,\n                        3.0380e-02,  4.1351e-02,  5.9690e-06, -1.7553e-02, -5.1685e-04,\n                        2.3727e-03,  1.7355e-02,  2.4972e-02,  3.1227e-03, -1.6665e-03,\n                        3.3959e-02, -1.7701e-02, -4.5488e-03, -6.6770e-03,  2.1271e-02,\n                        3.9049e-04,  7.1423e-03, -2.2006e-02,  2.0885e-02,  1.5323e-02,\n                       -4.2062e-02, -6.8658e-05,  5.6881e-03, -3.1408e-02, -1.4183e-02,\n                        1.1267e-02,  3.2223e-03,  3.4709e-02,  3.3977e-03,  4.8681e-03,\n                        1.7174e-03,  4.3303e-03, -1.2370e-02,  2.5550e-02,  6.6452e-03,\n                       -1.1551e-02,  1.4893e-02, -1.2220e-02, -1.9944e-03,  8.2128e-03,\n                       -6.9543e-03, -3.6279e-02, -2.1602e-02,  1.3635e-02, -2.6220e-03,\n                        5.4107e-02, -2.1159e-02,  4.6858e-03,  1.4203e-02,  6.1124e-03,\n                       -8.6227e-04,  2.3360e-02,  8.6127e-04,  1.1765e-02, -1.0635e-02,\n                       -7.7084e-03, -5.1917e-03,  1.4150e-02,  5.9793e-03, -2.9229e-03,\n                       -4.5928e-03,  1.1093e-03,  3.1034e-03, -1.6177e-02, -1.5702e-03,\n                        2.3017e-02,  7.4020e-03,  1.4130e-02, -1.7787e-02, -5.5999e-03,\n                        7.7713e-03, -1.3625e-02, -6.0283e-03,  1.6378e-02, -1.3034e-03,\n                        1.3811e-02,  1.3157e-03, -2.1882e-03,  3.1120e-02, -1.8887e-02,\n                       -4.5594e-03,  1.2246e-02,  3.0131e-02, -9.6278e-03, -1.2858e-04,\n                        4.9421e-03, -2.6371e-02,  2.6350e-02, -9.0437e-02, -6.1861e-02,\n                        2.0002e-03, -1.6434e-03, -4.7439e-02,  2.7686e-02,  1.5868e-02,\n                        3.5464e-03, -5.5070e-03,  8.6034e-03,  2.1646e-02, -2.7190e-02,\n                        6.7236e-03, -8.7302e-03,  9.7319e-03, -2.3074e-02,  6.6269e-03,\n                       -1.7284e-02,  8.1414e-03, -1.3881e-02, -1.0834e-02, -8.3371e-03,\n                       -1.3391e-02,  4.9076e-02, -7.8664e-03, -3.3616e-02, -8.5705e-03,\n                       -7.6165e-03, -2.1877e-02, -7.6237e-03,  1.2284e-03, -1.6612e-02,\n                       -2.1843e-03,  1.5986e-03, -2.8099e-04, -1.7318e-03, -7.1488e-02,\n                       -8.9317e-03, -1.2273e-03, -7.3791e-03, -1.6322e-02, -2.1099e-02,\n                       -4.8510e-03, -1.4393e-02, -3.5393e-02,  5.0442e-03,  2.3286e-02,\n                        1.0232e-03, -3.0046e-02,  2.1930e-02,  6.7276e-03, -2.7331e-03,\n                       -1.7666e-03,  7.3298e-01,  5.8941e-03,  5.1923e-02, -2.0573e-02,\n                       -2.5516e-02, -1.8658e-02,  2.4258e-03,  4.0828e-03,  1.2155e-02,\n                        1.5690e-02,  4.7138e-03, -6.5202e-03,  1.2917e-03,  1.5207e-02,\n                        3.0020e-02, -1.0201e-02,  1.9378e-02, -2.0492e-02, -1.2925e-02,\n                        2.3819e-02, -1.3294e-02,  1.2758e-02,  2.8720e-02, -1.4665e-02,\n                        8.8061e-03, -2.1407e-02,  1.7540e-02, -8.2318e-03,  6.6251e-03,\n                        1.3405e-03,  2.4241e-03, -1.4710e-02,  5.7427e-03,  2.9697e-02,\n                        2.9953e-03, -1.2299e-02, -1.1023e-02,  5.2037e-03, -2.8821e-03,\n                        4.1096e-04, -9.1333e-03,  6.9965e-02,  1.5103e-02, -1.5668e-02,\n                       -1.2952e-02, -1.7069e-02,  4.3880e-03, -1.0352e-02, -1.6436e-02,\n                        2.5546e-02,  1.1665e-02,  8.8263e-02,  1.7556e-03, -4.2041e-03,\n                       -1.3051e-02, -8.3524e-03,  7.8939e-03, -1.6453e-02,  8.4463e-03,\n                        3.2620e-02, -8.0769e-03, -2.7852e-04,  1.7764e-03, -7.2355e-03,\n                        4.3786e-03, -6.5894e-04,  3.6518e-02,  1.5195e-03,  1.5306e-03,\n                        9.5620e-03,  1.3745e-02,  1.6252e-02,  2.6780e-02, -1.0059e-03,\n                       -3.7082e-02, -3.2844e-03, -1.2327e-02, -1.4225e-04, -1.4187e-02,\n                       -3.8657e-02, -2.6963e-02, -1.6898e-02,  3.9998e-03,  1.2554e-02,\n                        1.9264e-02, -1.4612e-02, -7.7191e-03,  2.5847e-02, -1.5340e-02,\n                        6.2997e-04,  2.5303e-02, -3.1915e-02,  1.0667e-02, -2.7648e-02,\n                       -2.6047e-02,  6.4434e-03, -6.1746e-03,  1.5960e-02,  3.4641e-03,\n                       -1.1963e-02,  5.5144e-03, -1.4829e-02, -9.4710e-03,  3.3643e-03,\n                        9.2921e-04, -2.2623e-02,  1.2536e-03,  9.5009e-03,  9.5276e-03,\n                        5.9615e-03, -3.7776e-02, -1.4147e-02, -6.0146e-04,  5.9655e-03,\n                       -1.2229e-02,  1.5952e-02, -1.4061e-03, -9.2990e-03,  2.3066e-02,\n                       -2.0118e-03, -4.3867e-03, -6.9050e-03,  4.6486e-03, -6.4201e-03,\n                        6.2890e-03, -2.3814e-03, -1.1446e-02, -1.0718e-02, -2.3428e-02,\n                       -9.6020e-03, -1.6563e-02,  1.0744e-02,  9.0067e-03, -2.3112e-02,\n                       -3.5792e-03,  3.6316e-02,  8.7012e-03, -3.0034e-02,  4.7362e-03,\n                       -1.3217e-02, -1.0302e-02, -1.0369e-02,  8.2324e-03, -8.6154e-04,\n                        7.6577e-03, -2.3930e-02,  5.0441e-03, -4.2698e-02,  1.0189e-02,\n                        5.9625e-03,  5.7063e-03,  1.5431e-02,  1.6253e-02, -4.7660e-03,\n                       -3.0996e-02,  1.3363e-02,  3.1290e-03, -9.1102e-03,  1.7292e-02,\n                       -6.3867e-03, -4.2917e-03, -1.7457e-02,  7.7320e-03,  1.3084e-02,\n                        4.8887e-02, -2.8534e-02, -2.3935e-02,  1.3760e-02, -2.6905e-03,\n                       -8.7941e-03,  9.8763e-03,  4.6727e-03,  1.0438e-02,  2.8763e-02,\n                       -5.8356e-03,  3.9894e-04,  1.8201e-02,  2.0346e-02,  8.0060e-03,\n                        9.3942e-03, -5.7279e-03,  3.2251e-02, -6.7157e-03, -1.7309e-02,\n                        7.8343e-03, -1.0594e-02,  1.0287e-03,  8.5613e-03,  5.3190e-03,\n                        1.1421e-02,  1.1329e-01,  6.4631e-03,  2.8864e-02,  1.7157e-02,\n                       -8.2079e-03,  1.2965e-02,  9.3252e-03, -8.1532e-04, -1.5149e-02,\n                        1.8619e-02,  2.1262e-03, -3.7437e-03,  1.2894e-02, -4.7046e-02,\n                        1.7876e-02, -7.8684e-03,  6.1898e-04, -6.2390e-03, -2.4732e-02,\n                       -5.8755e-03, -2.7508e-03,  3.1876e-04, -2.9777e-03,  1.4716e-02,\n                        7.1749e-03,  9.4898e-04, -1.4768e-02, -1.2191e-02, -1.3533e-02,\n                       -5.2888e-03, -2.2677e-03, -1.8175e-02, -1.3328e-03, -8.7145e-03,\n                        4.7038e-03, -4.4999e-03, -3.2359e-02, -1.3954e-03,  5.9809e-03,\n                       -5.3012e-02, -1.1536e-02, -1.1112e-02, -9.7703e-03, -2.4923e-03,\n                       -4.8835e-04,  1.2498e-02, -1.9400e-02, -9.3083e-03, -1.6175e-03,\n                       -1.0867e-02,  6.0643e-04, -1.9547e-02, -3.7290e-03,  1.7872e-02,\n                        1.8805e-02, -9.2983e-03, -4.1146e-03,  1.5957e-03, -4.6679e-02,\n                        1.6280e-02,  9.5765e-03, -3.8634e-02, -2.2471e-02, -8.0973e-03,\n                       -6.7793e-03, -1.6161e-03,  7.1097e-03,  2.5247e-02,  8.9905e-03,\n                        1.3583e-02,  2.6155e-03, -1.6197e-02, -4.7775e-03,  1.3741e-02,\n                       -6.7330e-03, -2.3601e-03,  2.0371e-02, -7.3316e-03,  1.7005e-02,\n                        2.0426e-03, -5.0149e-02, -2.0898e-03, -1.7171e-02, -1.3198e-02,\n                        1.1690e-02, -1.0269e-03,  6.3581e-03,  1.6616e-02, -1.8474e-02,\n                        1.3524e-02, -7.5415e-03,  2.5669e-03,  2.1337e-02,  5.8831e-03,\n                       -1.2921e-03, -5.8441e-03, -2.1050e-02,  1.1603e-02, -1.4154e-02,\n                       -1.7076e-02, -5.5688e-03, -1.4780e-02, -2.8030e-02, -2.6112e-04,\n                        6.3928e-03, -1.5493e-02,  2.8369e-02, -9.0730e-04,  2.0828e-02,\n                        2.4636e-02, -7.1271e-03, -3.5428e-03,  1.0442e-02, -2.4561e-02,\n                       -1.6022e-02,  2.5045e-03, -5.0016e-03, -3.2576e-03, -6.1017e-03,\n                        4.9562e-04,  2.1927e-02, -2.3554e-02, -1.2498e-02, -9.7534e-03,\n                        7.8661e-03, -7.4466e-03,  1.0879e-02,  2.6755e-02,  3.3828e-02,\n                       -8.4321e-03, -5.3525e-03,  6.9793e-03,  5.5772e-03, -1.1279e-02,\n                       -5.2440e-03,  2.6156e-02, -1.0841e-02,  1.5560e-02,  2.0072e-02,\n                       -2.0405e-03, -1.4919e-02, -9.7582e-03, -7.2396e-03, -1.1139e-02,\n                        5.1298e-03,  5.2287e-03, -2.7862e-02,  1.8040e-02, -3.2361e-03,\n                        1.0979e-01,  1.0106e-02, -3.7980e-03,  3.0906e-02, -5.3303e-03,\n                        1.2772e-03,  9.2685e-04, -2.2788e-02, -2.1660e-02,  5.0367e-02,\n                       -3.0487e-02,  1.3516e-02, -3.7727e-02, -1.7500e-03,  1.3388e-02,\n                        8.9992e-03,  4.4718e-02,  1.1848e-02,  1.9424e-02, -2.1408e-02,\n                       -4.4702e-03,  1.2496e-03, -1.4535e-02, -4.9708e-03,  1.6012e-02,\n                       -3.2999e-02,  9.5266e-04,  8.1194e-03, -6.5381e-03,  1.9640e-03,\n                        9.7120e-03,  6.4723e-03,  3.2090e-02, -1.1414e-02,  1.6973e-04,\n                       -8.7230e-04,  2.5039e-04,  9.3902e-03,  3.3801e-04, -2.0414e-02,\n                        4.2968e-04, -2.8949e-02,  7.7784e-03, -1.7199e-02,  6.2561e-03,\n                       -3.1884e-02,  3.6624e-02, -7.3697e-03,  9.8994e-03, -3.0195e-02,\n                        1.8017e-02,  3.4337e-02, -1.1148e-02,  1.9564e-03, -3.6195e-03,\n                        5.0014e-02, -8.2329e-03, -1.4804e-02,  1.3161e-03,  1.5367e-02,\n                        4.9698e-03,  2.0178e-03,  1.3321e-02, -2.6816e-03, -6.7544e-03,\n                       -4.5894e-03, -2.6025e-02, -1.2216e-02,  9.3850e-03, -1.4718e-03,\n                        1.4750e-02, -5.6431e-03, -2.0150e-02,  1.2387e-02,  1.0213e-02,\n                        2.4715e-03,  6.7870e-03,  2.2641e-02,  2.6613e-02, -2.5389e-02,\n                        4.6736e-03, -2.8603e-03, -1.8727e-02,  8.2459e-04, -1.0926e-02,\n                       -1.0409e-02, -6.2334e-03, -1.2090e-03,  1.2573e-02, -2.9571e-02,\n                       -5.1601e-03, -7.7635e-03, -1.9593e-03,  2.1616e-03,  6.0461e-03,\n                       -2.3224e-03,  1.0920e-02,  1.7763e-02, -2.0033e-02,  8.2557e-03,\n                        2.2517e-02, -6.5773e-03, -6.2424e-01, -9.9564e-04, -2.6252e-02,\n                       -6.5096e-03,  4.8722e-03, -1.7928e-02,  4.4146e-03, -1.0024e-02,\n                       -3.1144e-03,  1.5937e-02, -1.5190e-02,  2.9805e-02,  1.2833e-02,\n                       -7.4022e-03, -3.1063e-03,  1.1966e-02, -1.9725e-02, -2.1278e-02,\n                        1.2720e-04, -6.4014e-03, -1.3185e-02, -2.2771e-03, -1.0217e-02,\n                       -5.4068e-04,  7.6041e-03,  9.3647e-03,  2.1860e-02,  4.1921e-03,\n                       -9.7881e-04, -1.0174e-03,  1.1897e-02, -1.0584e-02,  4.8606e-03,\n                       -5.4483e-02, -9.1126e-03, -3.6875e-03, -1.7030e-02, -1.1693e-02,\n                        1.0200e-04,  6.1351e-03, -1.0764e-02,  8.1029e-03, -7.5078e-02,\n                        7.2437e-03, -9.2770e-03,  1.7909e-02,  7.7964e-03,  6.7032e-03,\n                        6.4848e-03, -1.9910e-02,  1.9043e-02,  1.4924e-02,  8.0531e-03,\n                        6.7166e-03,  5.8178e-03, -7.4979e-03,  1.3358e-02, -1.0353e-02,\n                        9.1220e-02, -3.5227e-03, -6.1988e-03], requires_grad=True))),\n             ('classifier.0.weight',\n              tensor([7.3772e-01, 9.5534e-01, 1.6403e-01, 9.6778e-01, 7.8367e-01, 7.6791e-01,\n                      1.0037e+00, 1.0594e+00, 7.6229e-01, 1.3843e-01, 5.3102e-01, 8.9521e-01,\n                      1.9909e-01, 8.1013e-01, 1.1140e-01, 2.3112e-01, 2.4490e-01, 7.4051e-01,\n                      7.4488e-01, 8.5912e-01, 7.9977e-01, 1.6022e-01, 1.5258e-01, 1.0654e+00,\n                      3.5132e-01, 1.0229e+00, 8.0660e-01, 8.0964e-01, 8.1881e-01, 1.0015e+00,\n                      8.4787e-01, 9.2572e-01, 5.8667e-01, 1.0057e+00, 1.5784e-01, 6.5660e-01,\n                      7.6501e-01, 8.0097e-01, 8.7037e-01, 1.0213e+00, 7.1956e-01, 6.5524e-01,\n                      7.0380e-01, 5.9944e-01, 5.3181e-01, 7.8998e-01, 8.2314e-01, 6.2251e-01,\n                      8.5466e-01, 1.5357e-01, 1.0316e+00, 8.5235e-01, 1.1883e+00, 7.1969e-01,\n                      1.3624e+00, 7.0910e-01, 7.9169e-01, 6.2282e-01, 9.9911e-01, 8.1228e-01,\n                      1.5338e-01, 1.3264e+00, 9.2895e-01, 8.6014e-01, 8.5909e-01, 8.5045e-01,\n                      7.4340e-01, 9.2214e-01, 2.8399e-01, 9.4751e-01, 9.4973e-01, 6.5538e-01,\n                      7.3101e-01, 5.1927e-01, 9.0288e-01, 7.8880e-01, 9.2052e-01, 8.6571e-01,\n                      7.5080e-01, 9.0537e-01, 1.5593e-01, 8.9002e-01, 7.4617e-01, 8.9917e-01,\n                      8.8804e-01, 7.3127e-01, 7.8173e-01, 1.8621e-01, 8.7678e-01, 7.7849e-01,\n                      7.0636e-01, 8.2085e-01, 8.4976e-01, 9.3698e-01, 8.5306e-01, 1.0169e+00,\n                      8.1759e-01, 7.6686e-01, 8.4470e-01, 9.1027e-01, 7.3508e-01, 1.0372e+00,\n                      1.0865e+00, 6.9484e-01, 8.2191e-01, 9.6754e-01, 8.6308e-01, 8.3671e-01,\n                      9.2368e-01, 8.1858e-01, 6.8071e-01, 9.0329e-01, 1.6335e-01, 9.5897e-01,\n                      9.1959e-01, 2.1562e-01, 6.9726e-01, 1.0748e+00, 9.4548e-01, 1.5490e-01,\n                      1.1058e+00, 8.2165e-01, 9.3975e-01, 1.1561e+00, 9.3748e-01, 7.7642e-01,\n                      2.1021e+00, 9.6006e-01, 9.6105e-01, 1.1267e+00, 1.0303e+00, 7.3038e-01,\n                      1.0235e+00, 7.9823e-01, 8.9167e-01, 9.9565e-01, 1.0946e+00, 6.5491e-01,\n                      1.1006e+00, 6.6384e-01, 7.1721e-01, 6.2936e-01, 8.1411e-01, 7.0409e-01,\n                      1.5122e-01, 2.0727e+00, 7.0293e-01, 8.8734e-01, 8.3732e-01, 7.2689e-01,\n                      7.6603e-01, 8.6552e-01, 9.3144e-01, 9.4759e-01, 8.6345e-01, 1.4011e+00,\n                      7.5396e-01, 8.3045e-01, 9.0262e-01, 8.3527e-01, 7.7216e-01, 1.0830e+00,\n                      7.8922e-01, 7.8832e-01, 9.2062e-01, 7.3897e-01, 1.5858e-01, 9.7104e-01,\n                      7.4600e-01, 6.9301e-01, 2.4566e-01, 1.2622e+00, 8.2141e-01, 1.0052e+00,\n                      8.4756e-01, 7.2178e-01, 6.9980e-01, 1.0944e+00, 1.0367e+00, 7.1240e-01,\n                      7.3483e-01, 8.5486e-01, 8.7162e-01, 5.3432e-01, 7.2807e-01, 9.0548e-01,\n                      2.8404e-01, 9.8753e-01, 9.1929e-01, 8.6425e-01, 1.6326e-01, 9.7457e-01,\n                      9.2483e-01, 9.2771e-01, 8.5389e-01, 7.5764e-01, 8.4075e-01, 8.3412e-01,\n                      8.8601e-01, 6.6310e-01, 7.2683e-01, 7.1476e-01, 8.5522e-01, 7.8679e-01,\n                      6.2851e-01, 6.3642e-01, 6.8578e-01, 8.0981e-01, 7.7590e-01, 6.6649e-01,\n                      8.4976e-01, 8.3193e-01, 1.4036e+00, 1.4443e-01, 1.4607e-01, 5.5714e-01,\n                      8.3780e-01, 3.7392e-01, 1.5971e-01, 1.0224e+00, 8.2178e-01, 7.4779e-01,\n                      7.2623e-01, 6.8880e-01, 1.0053e+00, 7.6061e-01, 7.4127e-01, 1.5618e-01,\n                      1.1330e+00, 8.8976e-01, 3.4602e-01, 8.6195e-01, 1.0893e+00, 7.4916e-01,\n                      8.8822e-01, 9.1454e-01, 1.0973e+00, 8.0526e-01, 1.5079e-01, 8.9391e-01,\n                      7.2512e-01, 8.2476e-01, 8.7057e-01, 9.4400e-01, 6.8805e-01, 8.3267e-01,\n                      9.6267e-01, 6.6830e-01, 9.1886e-01, 4.2861e-01, 9.1742e-01, 1.0858e+00,\n                      6.9847e-01, 8.4638e-01, 8.5995e-01, 7.6687e-01, 5.8470e-01, 9.1827e-01,\n                      8.6444e-01, 9.5037e-01, 8.8342e-01, 9.1705e-01, 1.4988e-01, 8.2339e-01,\n                      1.4433e-01, 1.6475e+00, 8.0305e-05, 7.1576e-01, 1.4801e-01, 8.7109e-01,\n                      8.5302e-01, 8.4943e-01, 7.8454e-01, 7.7927e-01, 9.6798e-01, 1.4875e-01,\n                      1.5746e-01, 7.4276e-01, 8.1805e-01, 9.3701e-01, 7.9260e-01, 8.5962e-01,\n                      5.6227e-01, 9.3538e-01, 1.0114e+00, 1.5344e-01, 6.6474e-01, 9.5285e-01,\n                      1.5223e-01, 7.9908e-01, 9.3609e-01, 2.5051e-01, 7.8697e-01, 7.7192e-01,\n                      8.1997e-01, 5.4623e-01, 8.5574e-01, 8.3198e-01, 1.5101e-01, 1.0080e+00,\n                      6.6798e-01, 7.5711e-01, 6.5865e-01, 8.2014e-01, 6.7673e-01, 1.5793e-01,\n                      7.0997e-01, 5.0644e-01, 1.5947e-01, 1.5762e+00, 9.1507e-01, 1.5533e-01,\n                      1.1562e+00, 9.7863e-01, 6.8083e-01, 7.7361e-01, 9.1309e-01, 1.0006e-01,\n                      8.4677e-01, 9.3928e-01, 7.5193e-01, 6.3439e-01, 8.8251e-01, 1.4653e+00,\n                      8.9947e-01, 3.0852e-01, 7.6368e-01, 8.5303e-01, 7.9466e-01, 8.5916e-01,\n                      7.9124e-01, 7.6433e-01, 8.6982e-01, 7.8939e-01, 7.3755e-01, 6.2527e-01,\n                      9.5677e-01, 6.6822e-01, 8.9960e-01, 8.8085e-01, 1.5265e-01, 7.0789e-01,\n                      9.5434e-01, 7.4953e-01, 7.6653e-01, 1.3716e+00, 8.6308e-01, 8.0161e-01,\n                      7.4189e-01, 7.9962e-01, 8.5192e-01, 9.2939e-01, 1.0507e+00, 7.1305e-01,\n                      9.0433e-01, 8.4158e-01, 6.6604e-01, 9.4676e-01, 9.0973e-01, 8.2025e-01,\n                      2.6459e-01, 8.4976e-01, 8.8904e-01, 9.9424e-01, 9.6945e-01, 8.0740e-01,\n                      5.8708e-01, 6.9396e-01, 7.3251e-01, 7.2299e-01, 8.0346e-01, 9.2786e-01,\n                      9.4910e-01, 8.2001e-01, 8.6602e-01, 7.4130e-01, 9.2962e-01, 7.9246e-01,\n                      8.4788e-01, 9.2037e-01, 6.9986e-01, 4.7759e-01, 2.0913e-01, 6.9695e-01,\n                      8.1530e-01, 9.8633e-01, 8.5692e-01, 1.8845e+00, 1.5849e-01, 9.9150e-01,\n                      6.2412e-01, 3.7306e-01, 8.2096e-01, 7.4575e-01, 8.1718e-01, 1.5408e-01,\n                      9.3956e-01, 7.4753e-01, 8.0449e-01, 1.2203e+00, 8.3868e-01, 2.1431e+00,\n                      1.0993e+00, 1.1327e+00, 3.4974e-01, 8.0004e-01, 8.2468e-01, 7.9420e-01,\n                      8.2583e-01, 7.1573e-01, 8.6468e-01, 1.0718e+00, 7.6036e-01, 8.4107e-01,\n                      5.9936e-01, 7.2212e-01, 8.8552e-01, 8.2125e-01, 8.9237e-01, 9.4392e-01,\n                      9.3767e-01, 9.8206e-01, 1.0236e+00, 8.3902e-01, 1.1110e+00, 1.6882e-01,\n                      8.9525e-01, 9.8968e-01, 7.4166e-01, 1.1060e+00, 9.2926e-01, 7.6751e-01,\n                      8.7601e-01, 1.5989e-01, 8.3836e-01, 8.6838e-01, 2.8384e-01, 8.2463e-01,\n                      8.6932e-01, 9.5160e-01, 1.1657e+00, 7.9034e-01, 7.8340e-01, 1.0757e+00,\n                      9.9952e-01, 9.6746e-01, 5.3459e-01, 6.9137e-01, 9.3292e-01, 5.3351e-01,\n                      9.0915e-01, 1.5849e-01, 7.2161e-01, 8.6309e-01, 8.9751e-01, 1.0972e+00,\n                      1.4368e-01, 7.6695e-01, 9.3251e-01, 7.0877e-01, 9.2184e-01, 8.6969e-01,\n                      7.9294e-01, 8.3850e-01, 7.1694e-01, 9.5784e-01, 1.0216e+00, 1.0017e+00,\n                      8.6471e-01, 1.5060e-01, 6.8105e-01, 7.0981e-01, 7.3628e-01, 8.3631e-01,\n                      2.0879e-01, 8.7501e-01, 8.1381e-01, 7.1439e-01, 9.2617e-01, 8.1122e-01,\n                      8.4006e-01, 7.9515e-01, 7.7329e-01, 1.0419e+00, 1.1009e+00, 7.0488e-01,\n                      7.7379e-01, 6.9477e-01, 1.5535e-01, 7.0915e-01, 7.9855e-01, 9.0580e-01,\n                      2.2154e-01, 6.9206e-01, 5.9343e-01, 1.2794e+00, 1.0641e+00, 1.6090e-01,\n                      9.9113e-01, 8.1892e-01, 1.4411e-01, 6.6649e-01, 8.9928e-01, 9.2868e-01,\n                      8.8867e-01, 6.2923e-01, 8.1394e-01, 7.8295e-01, 7.7522e-01, 8.2105e-01,\n                      7.8830e-01, 8.8075e-01, 8.2646e-01, 1.0930e+00, 1.1985e+00, 7.1430e-01,\n                      8.6768e-01, 1.4929e-01, 7.0856e-01, 7.4483e-01, 8.2675e-01, 1.1818e+00,\n                      8.9099e-01, 9.8894e-01, 7.8749e-01, 9.4937e-01, 1.2035e+00, 8.2506e-01,\n                      8.6263e-01, 8.1419e-01, 7.9512e-01, 5.6526e-01, 1.0691e+00, 7.8672e-01,\n                      7.4053e-01, 6.2272e-01, 8.0612e-01, 1.0234e+00, 8.7344e-01, 8.0143e-01,\n                      7.2789e-01, 5.6596e-01, 2.5085e-01, 1.1452e+00, 9.0728e-01, 8.7525e-01,\n                      9.3356e-01, 7.8085e-01, 8.6965e-01, 7.5121e-01, 7.6398e-01, 7.6836e-01,\n                      9.9935e-01, 1.3990e-01, 8.1603e-01, 5.8175e-01, 7.3543e-01, 8.2097e-01,\n                      9.8192e-01, 1.0144e+00, 9.1685e-01, 9.9223e-01, 8.2588e-01, 7.8819e-01,\n                      7.9430e-01, 9.2870e-01, 8.1176e-01, 8.9999e-01, 1.0059e+00, 1.8540e-01,\n                      1.0013e+00, 8.7948e-01, 7.4739e-01, 7.2103e-01, 7.8242e-01, 9.4824e-01,\n                      7.6302e-01, 7.8977e-01, 7.8882e-01, 8.3555e-01, 5.6502e-01, 8.5072e-01,\n                      9.5091e-01, 1.5142e-01, 1.4162e-01, 8.1835e-01, 7.6618e-01, 7.9266e-01,\n                      9.3772e-01, 6.6680e-01, 8.5967e-01, 1.3908e+00, 7.8127e-01, 8.7015e-01,\n                      1.0575e+00, 2.8039e-01, 8.9266e-01, 1.0547e+00, 8.4540e-01, 7.3557e-01,\n                      8.4293e-01, 9.5575e-01, 8.4984e-01, 1.0841e+00, 1.0533e+00, 1.3660e-01,\n                      8.2660e-01, 1.6167e-01, 9.2775e-01, 9.2187e-01, 7.8273e-01, 6.1231e-01,\n                      3.2681e-01, 8.0191e-01, 5.8379e-02, 7.4358e-01, 7.9991e-01, 1.0029e+00,\n                      8.5835e-01, 8.8429e-01, 1.3660e+00, 2.2117e-01, 1.1430e+00, 8.3827e-01,\n                      9.5502e-01, 7.7703e-01, 3.2278e-01, 8.4113e-01, 8.0270e-01, 8.2279e-01,\n                      2.3588e-01, 7.4226e-01, 6.9445e-01, 7.2121e-01, 7.6015e-01, 8.2235e-01,\n                      9.0110e-01, 2.2615e-01, 9.4960e-01, 9.4993e-01, 9.0075e-01, 1.4886e-01,\n                      8.7231e-01, 6.3978e-01, 1.0723e+00, 9.5956e-01, 8.1105e-01, 9.8299e-01,\n                      7.6930e-01, 7.4609e-01, 1.5242e-01, 1.2771e+00, 8.4507e-01, 8.5973e-01,\n                      7.9045e-01, 9.7375e-01, 1.6862e-01, 6.8629e-01, 7.6003e-01, 5.9968e-01,\n                      2.4011e-01, 7.1494e-01, 1.4851e-01, 6.6760e-01, 7.4580e-01, 8.7211e-01,\n                      7.8136e-01, 7.0018e-01, 6.8245e-01, 9.7571e-01, 1.0434e+00, 9.0843e-01,\n                      6.9873e-01, 8.2494e-01, 6.6209e-01, 9.0133e-01, 1.2845e+00, 8.1898e-01,\n                      8.2167e-01, 8.5562e-01, 9.2520e-01, 1.1550e+00, 9.0437e-01, 8.2707e-01,\n                      7.6544e-01, 1.6320e-01, 9.4520e-01, 8.6338e-01, 9.7512e-01, 7.3450e-01,\n                      1.2349e+00, 3.8991e-01, 9.3357e-01, 9.3891e-01, 8.0723e-01, 9.5624e-01,\n                      8.0486e-01, 1.0909e+00, 8.0482e-01, 7.8842e-01, 7.7258e-01, 8.7254e-01,\n                      7.4742e-01, 8.8087e-01, 1.0862e+00, 9.5893e-01, 1.0801e+00, 2.2339e-02,\n                      8.0052e-01, 9.1247e-01, 1.0806e+00, 8.3278e-01, 1.1529e+00, 9.6511e-01,\n                      1.4826e+00, 8.4467e-01, 8.8580e-01, 8.6545e-01, 9.2751e-01, 9.9161e-01,\n                      7.5202e-01, 7.2276e-01, 9.6394e-01, 8.6796e-01, 1.0059e+00, 8.3045e-01,\n                      8.1219e-01, 9.1970e-01, 7.4432e-01, 8.6989e-01, 8.1080e-01, 8.5387e-01,\n                      3.7764e-01, 1.6978e+00, 7.5775e-01, 8.4035e-01, 6.8410e-01, 8.7193e-01,\n                      8.6494e-01, 9.6125e-01, 1.7720e+00, 9.3374e-01, 1.3391e+00, 9.5915e-01,\n                      5.8964e-01, 4.6680e-01, 9.5838e-01, 7.5172e-01, 8.1246e-01, 3.5643e+00,\n                      9.2638e-01, 7.3592e-01, 8.3920e-01, 1.0208e+00, 7.5987e-01, 8.6163e-01,\n                      1.0826e+00, 9.7716e-01, 9.2985e-01, 9.6789e-01, 7.1870e-01, 1.0020e+00,\n                      1.0373e+00, 7.8951e-01, 8.4171e-01, 1.5790e-01, 7.2400e-01, 9.8272e-01])),\n             ('classifier.0.bias',\n              tensor([ 1.3162e-02, -4.5757e-03,  8.5066e-03,  5.4144e-02, -5.0175e-02,\n                       3.2888e-03, -7.6518e-03, -5.4559e-02, -1.8740e-03,  9.3408e-03,\n                       3.1297e-02, -5.3252e-02,  4.1706e-02,  1.3194e-02, -2.4276e-01,\n                      -9.0777e-02, -4.6016e-02,  1.8443e-02,  1.1859e-02,  5.6158e-02,\n                       5.2980e-03, -3.5076e-03,  1.3894e-02, -3.5657e-02, -2.4428e-02,\n                       2.2916e-03,  5.3786e-03, -6.4147e-03,  3.4028e-02, -7.7360e-03,\n                      -1.2372e-02, -5.8917e-03, -1.0147e-01,  1.9262e-02,  5.3407e-03,\n                      -1.1055e-01, -1.5243e-02,  4.5724e-02,  1.0403e-02,  2.4214e-02,\n                       2.2265e-02, -2.4031e-03, -2.5105e-03, -4.2711e-02,  7.9856e-02,\n                      -3.0926e-03,  1.5540e-02, -1.8428e-02, -5.5708e-02, -2.3866e-02,\n                      -1.1789e-02,  7.4184e-02, -1.4045e-01,  5.0611e-02,  5.9309e-02,\n                      -1.3105e-02,  4.9902e-02,  2.3086e-02,  5.8682e-03,  5.0681e-02,\n                       1.5524e-02,  3.1064e-02, -9.9651e-02,  2.2584e-02,  2.2839e-02,\n                       4.0086e-02,  6.7235e-03,  4.4488e-02, -7.9902e-02, -1.1270e-02,\n                       6.2879e-02,  6.9835e-03, -1.5039e-02, -7.2467e-02,  8.4949e-03,\n                       5.9872e-02,  2.8477e-02,  1.0925e-01,  2.9684e-02, -1.7541e-02,\n                       2.3344e-02,  2.0566e-03,  2.6901e-02, -1.2851e-02, -1.6429e-02,\n                       2.8497e-02, -4.6311e-02, -6.5817e-02,  7.0897e-02, -2.3097e-02,\n                      -3.7657e-02,  2.8413e-02, -2.1520e-02,  8.3825e-02, -3.2419e-02,\n                       4.1441e-02,  3.2510e-02, -1.4501e-02,  3.9485e-03, -6.4858e-03,\n                      -4.3948e-03, -2.4827e-02, -2.5805e-02, -1.3573e-02,  8.5818e-03,\n                       7.8971e-02,  1.5874e-02,  1.1808e-02,  2.5687e-02,  2.2531e-02,\n                      -3.7583e-03,  1.9675e-02,  6.1295e-03,  4.6255e-02, -3.7620e-02,\n                       3.6511e-02,  5.6162e-03,  4.8389e-02,  7.5228e-03,  1.1245e-02,\n                      -2.3445e-01,  1.0084e-01,  1.0574e-02, -3.4315e-02,  1.5626e-02,\n                       4.7754e-02, -2.5784e-01,  2.8380e-02, -4.8820e-02, -3.2883e-02,\n                      -1.0001e-01,  3.3409e-03,  6.7398e-02, -6.1343e-03,  5.3925e-02,\n                      -2.1213e-02, -5.1243e-02,  3.3459e-03, -2.6147e-02, -1.1009e-02,\n                       3.4652e-02,  4.5333e-02,  9.1442e-02, -4.0460e-02, -1.0340e-01,\n                      -2.2560e-01,  2.0266e-02,  2.1013e-02,  2.5928e-02, -1.4933e-02,\n                       7.1670e-03, -6.7671e-02,  1.0284e-01,  4.3386e-02,  6.3485e-02,\n                       4.9399e-02,  1.7237e-02,  2.8211e-02, -1.2534e-01, -1.2258e-03,\n                       2.1749e-02,  3.5510e-02, -4.9517e-02,  2.1879e-02,  7.6842e-02,\n                       3.4864e-02,  4.3665e-02,  1.5710e-01,  3.1314e-02,  4.4110e-03,\n                       1.3462e-01,  1.5398e-02,  1.2058e-02, -4.5534e-02, -4.7864e-02,\n                      -2.9485e-02,  1.9477e-02,  7.8372e-02, -5.0073e-02,  9.6025e-04,\n                       3.7977e-02,  2.8448e-02, -1.9987e-02, -2.1662e-02,  1.3869e-02,\n                       5.3015e-02, -1.2034e-02,  3.2322e-02, -5.5249e-02,  7.8461e-02,\n                      -1.6396e-03, -1.6677e-02, -1.2289e-01, -1.0247e-01,  7.6225e-02,\n                       4.2367e-02,  2.1203e-03, -9.0887e-03, -1.8923e-02,  2.0676e-02,\n                       2.2332e-02,  8.3485e-02,  2.0502e-02,  7.7412e-04, -3.5661e-02,\n                      -5.3330e-02, -2.1147e-02,  3.2925e-02,  1.6039e-02, -6.3843e-05,\n                       6.7970e-03,  3.1056e-02,  7.8384e-03, -1.1278e-02, -1.6130e-03,\n                       3.9251e-03, -1.1701e-04,  2.4702e-01,  4.1413e-02,  6.3378e-02,\n                       2.4581e-02, -2.7410e-02,  2.1866e-02,  1.0060e-02, -3.1439e-01,\n                       3.9543e-02,  7.6631e-03,  1.6269e-02, -8.4637e-03, -3.5633e-02,\n                      -6.5777e-02, -6.6035e-03, -3.3091e-02,  7.8348e-03,  6.1008e-02,\n                       6.4732e-02,  1.2018e-01, -1.3824e-02, -5.4408e-02, -3.8690e-03,\n                       1.1035e-02,  5.2662e-02, -1.2889e-02,  3.5576e-02,  4.1329e-02,\n                      -3.3378e-03,  3.8447e-01,  4.0247e-02, -3.8355e-02,  1.0918e-01,\n                       1.5486e-02, -5.4712e-02,  2.0957e-02,  7.3332e-02,  4.3853e-02,\n                       5.9934e-02,  1.0184e-01, -6.8820e-04,  7.3294e-02,  1.2381e-01,\n                       2.0947e-02,  8.2969e-02,  2.1728e-02, -8.9096e-02,  6.3647e-03,\n                       3.2521e-02,  5.0868e-04,  4.9859e-02,  1.4480e-02,  2.6441e-02,\n                      -3.9395e-02, -2.5266e-02, -1.6217e-02,  1.8380e-02,  3.6737e-02,\n                      -1.2431e-02, -5.4440e-04,  3.7082e-02,  4.2044e-03, -2.4826e-03,\n                       1.8510e-01,  7.2853e-02,  7.8480e-02,  4.6878e-02,  1.0822e-01,\n                      -2.0854e-02, -2.3785e-02,  8.9001e-02,  3.2752e-03,  7.4387e-03,\n                       5.5126e-02, -6.5401e-02,  5.9020e-02,  5.5813e-02,  5.3199e-02,\n                      -5.6525e-02, -5.5515e-03,  2.2340e-02,  1.2586e-02, -4.6668e-02,\n                       1.4303e-02,  4.5700e-02,  4.5846e-02,  1.0460e-01, -5.3658e-03,\n                      -2.0427e-03,  2.0354e-02, -3.2289e-01, -2.5391e-04,  3.8205e-02,\n                      -1.8761e-02,  1.0845e-02, -1.7226e-02,  5.2174e-02, -1.7663e-02,\n                       3.7441e-02,  3.8724e-02, -3.4572e-02, -1.2826e-02,  6.0225e-02,\n                      -1.2512e-02,  1.6032e-02,  2.0035e-02,  3.0689e-02,  3.5256e-02,\n                      -5.1627e-03, -2.0522e-02,  3.6812e-03, -1.0080e-02,  6.2072e-03,\n                      -1.2721e-01,  1.9650e-02,  1.7869e-01, -1.3021e-02,  2.9703e-02,\n                       3.0870e-02, -3.2677e-01,  7.8243e-04,  6.1509e-02,  9.9511e-02,\n                      -5.0674e-02, -9.5311e-04,  2.8112e-03,  7.2221e-02,  1.6948e-02,\n                       8.7351e-02,  1.5906e-02,  1.1135e-02,  1.7879e-02,  1.7120e-02,\n                      -5.9320e-02,  3.1353e-02, -9.9457e-03,  1.4733e-02,  7.8046e-03,\n                       3.9057e-03,  1.1394e-01,  1.3825e-01, -3.3349e-02, -1.1039e-02,\n                      -8.0793e-02,  3.4300e-02,  3.6258e-02, -2.2124e-01, -1.6490e-01,\n                       7.6051e-02,  5.8837e-02, -1.6014e-02, -2.2985e-02,  4.8639e-03,\n                       4.2323e-02,  9.2466e-02, -4.0465e-02, -6.6449e-02,  3.6405e-02,\n                       7.2913e-02,  6.6515e-02, -4.0359e-02,  5.0818e-02, -4.6471e-03,\n                      -2.4475e-02, -8.3275e-02, -2.7407e-02,  1.4331e-02, -8.4324e-02,\n                       5.0847e-02,  2.8586e-04, -1.2348e-01,  9.6838e-03, -9.8696e-03,\n                       2.6070e-02, -2.6100e-02,  1.4923e-02,  1.4745e-02, -3.2869e-02,\n                      -9.8967e-03, -4.1720e-02, -3.7278e-01, -2.7131e-02,  7.2855e-03,\n                      -5.3830e-02, -1.3309e-01,  8.9295e-02,  2.4540e-02, -4.1671e-02,\n                       2.1449e-02,  4.0936e-02,  5.5314e-03,  3.6259e-02,  2.6088e-02,\n                       9.4415e-02,  5.0403e-03, -2.4457e-03, -3.8831e-02, -4.8016e-02,\n                       1.8170e-02,  5.2154e-02, -1.6122e-02, -1.4990e-02,  2.0800e-02,\n                      -5.6851e-02, -2.0921e-02, -3.2548e-02,  2.0279e-02,  1.2013e-01,\n                      -9.2210e-03, -5.3525e-02, -2.7500e-02, -5.5574e-03,  2.4676e-02,\n                       9.6007e-02,  5.1405e-02,  2.2374e-02, -1.0359e-02,  8.7482e-02,\n                       2.5666e-02,  5.8568e-02,  6.0215e-04,  1.0268e-01,  5.7749e-02,\n                       3.3179e-02,  1.8569e-03,  9.3849e-02, -2.8929e-02, -2.2365e-02,\n                       3.5035e-02, -9.6994e-02,  7.2453e-02, -2.8293e-02, -3.7981e-01,\n                      -1.6062e-02,  9.9269e-03, -2.9361e-02,  1.3345e-01,  1.6328e-02,\n                       6.7332e-02, -4.3524e-02,  1.1064e-02,  2.9657e-02,  1.4123e-02,\n                      -6.0619e-02,  7.5688e-03,  1.0078e-01,  1.3592e-01, -2.9844e-03,\n                      -1.0032e-01, -1.6626e-02, -5.4917e-02, -4.7152e-02, -3.6363e-02,\n                       2.1602e-02, -1.9936e-03,  1.7960e-02,  9.0165e-03,  5.2723e-02,\n                       5.7136e-02, -8.1856e-03,  3.4372e-02, -3.0325e-01,  4.0031e-02,\n                      -2.5555e-02, -7.8350e-02,  3.6172e-03,  2.1868e-02,  2.0095e-02,\n                      -5.3467e-03,  6.1986e-02,  1.3188e-02, -3.0935e-03,  5.7999e-02,\n                      -7.6329e-02,  1.9752e-02,  8.1899e-02,  1.5794e-03,  1.3506e-02,\n                      -2.2969e-01,  7.7108e-03,  1.1787e-02,  6.7971e-02,  6.7175e-02,\n                       1.8422e-03,  1.7087e-02,  6.6910e-03,  6.1152e-02, -8.2093e-03,\n                       1.5103e-03,  1.1005e-03,  2.9468e-02, -1.0670e-02, -1.3258e-02,\n                       3.2262e-02,  9.3777e-02, -5.5661e-04,  8.0929e-02,  5.1635e-02,\n                       9.6637e-03,  3.1711e-02,  7.9770e-03,  1.8987e-02,  1.5052e-02,\n                       3.4529e-02,  3.4560e-02,  1.2200e-02, -3.0909e-01,  3.0814e-02,\n                       7.1846e-03,  1.4501e-02,  3.7871e-02,  3.8912e-02,  1.2127e-02,\n                       3.1358e-02,  2.9373e-02,  1.0071e-01, -6.4050e-02,  9.3103e-03,\n                       9.4490e-03,  6.0977e-02,  1.8474e-02,  1.5903e-02,  3.5325e-02,\n                       7.8024e-03, -5.7380e-02, -3.3863e-02, -6.5964e-02,  2.3000e-02,\n                      -7.0440e-03,  3.0996e-02,  1.6426e-02,  1.5494e-02,  2.4427e-02,\n                      -1.2081e-01,  9.4572e-03,  1.9035e-01,  2.2067e-02,  1.1661e-01,\n                       6.0902e-02,  2.6758e-02,  5.3077e-03,  2.7876e-02,  6.0847e-03,\n                       1.6832e-01,  3.0909e-02,  7.4642e-03,  3.8551e-02,  4.4175e-02,\n                       9.9971e-02,  6.9072e-02, -6.7228e-02, -2.1259e-02, -5.9426e-02,\n                      -1.6854e-01,  6.0114e-02,  2.3022e-02,  4.7235e-02,  5.5552e-02,\n                      -3.8166e-02,  3.4772e-02, -1.2903e-02,  3.3862e-02,  5.2938e-02,\n                      -6.5133e-02,  1.3778e-01,  1.0414e-01,  3.1960e-02, -5.4566e-02,\n                      -2.0499e-02,  3.4737e-03,  9.2329e-03, -1.2487e-02, -1.5689e-02,\n                       9.9989e-02,  2.0139e-02,  5.7133e-02,  7.8374e-02,  7.3707e-04,\n                      -3.9717e-04, -1.3932e-03,  6.7144e-03,  6.3150e-02,  2.5271e-02,\n                       1.0046e-02,  6.9682e-02, -1.8809e-01,  5.6428e-02,  3.6135e-02,\n                       7.7155e-02,  4.0798e-02, -6.5317e-03,  3.9437e-02,  9.2184e-02,\n                       3.0563e-02, -1.1969e-01, -6.6243e-02, -2.4550e-02, -3.3593e-02,\n                      -5.5062e-05,  1.9123e-02,  2.0762e-02, -1.2120e-02,  5.6960e-02,\n                      -6.1295e-02, -6.7489e-02, -5.1148e-02,  1.1329e-01,  2.6766e-02,\n                       1.7299e-02,  6.3799e-02,  1.8032e-02,  1.8135e-02,  8.7203e-03,\n                       4.7461e-02,  1.0835e-02, -3.0922e-02,  5.2819e-02,  1.6907e-02,\n                       3.9511e-02,  4.6311e-02, -7.7078e-02,  4.0459e-02,  2.3204e-02,\n                       1.9821e-03, -6.3643e-03, -2.8302e-02, -2.0987e-02, -1.2838e-02,\n                      -7.1666e-02,  1.9202e-02,  3.6450e-02,  1.4802e-02,  3.0110e-02,\n                       1.6556e-02, -1.6889e-02,  1.6718e-02, -3.1535e-01,  9.6381e-02,\n                      -8.5590e-03, -1.9059e-02,  1.0145e-01,  4.1442e-02,  4.0336e-02,\n                       7.8390e-02, -2.1039e-03,  2.7474e-03, -5.7407e-03, -2.8102e-01,\n                       4.1831e-02,  1.6984e-02, -8.8275e-03, -3.6985e-02, -6.7097e-02,\n                       2.7197e-02, -4.4762e-02,  2.0237e-02,  1.3979e-01,  3.9483e-02,\n                       1.4164e-01,  3.7231e-04,  1.4367e-02,  2.6068e-02,  5.2987e-03,\n                       1.2977e-01, -3.0794e-02, -6.5308e-02, -1.2599e-02,  3.0265e-02,\n                       6.1589e-03,  1.0422e-01, -3.4080e-02,  1.0114e-01, -2.2199e-02,\n                      -6.7313e-02, -6.6309e-02, -2.8454e-02,  7.2560e-02,  7.8043e-02,\n                       3.4935e-02,  1.9228e-01, -4.4334e-02, -5.0990e-02, -4.0314e-02,\n                      -4.4631e-02, -1.3654e-02, -2.3701e-03,  1.5554e-02,  4.8776e-03,\n                       2.4264e-01,  7.7429e-02, -1.9766e-01, -1.2704e-02,  1.1417e-01,\n                      -1.2128e-03, -1.3823e-02,  2.1259e-02, -6.9612e-02,  6.1268e-02,\n                      -1.5244e-02, -7.1450e-02,  2.2016e-02,  3.9417e-02, -1.2946e-01,\n                       2.6186e-02,  1.2256e-02,  2.7089e-02, -2.8943e-02, -1.6506e-02,\n                      -4.7981e-02, -8.0960e-03,  3.1092e-02,  4.2023e-02, -2.0545e-02,\n                       2.6498e-02,  2.1309e-02, -3.9332e-02, -2.2922e-02,  8.4353e-03,\n                      -8.0438e-03,  6.3090e-02,  1.0025e-02, -1.7523e-02,  3.8928e-04,\n                      -1.0576e-01, -1.2710e-02, -5.3959e-03,  3.0707e-02,  7.2261e-03,\n                       3.2340e-02,  1.4883e-02,  1.4885e-02,  1.2794e-02, -8.7165e-01,\n                      -5.7513e-02,  4.4035e-02, -1.5663e-02, -1.9344e-01,  1.5484e-02,\n                       8.4768e-02,  5.5334e-02,  5.8453e-03,  2.6799e-01, -2.2038e-03,\n                      -2.5796e-02, -2.7830e-02,  6.5207e-02, -4.7342e-02, -5.1217e-02,\n                       2.7322e-02,  2.3410e-02,  6.5954e-03])),\n             ('classifier.2.scale', tensor(0.1548)),\n             ('classifier.2.zero_point', tensor(48)),\n             ('classifier.2._packed_params.dtype', torch.qint8),\n             ('classifier.2._packed_params._packed_params',\n              (tensor([[-0.0158,  0.0727, -0.0158,  ...,  0.0285,  0.0395,  0.0000],\n                       [ 0.0083, -0.1320,  0.0214,  ..., -0.0273, -0.0392,  0.0012],\n                       [ 0.0000,  0.0224,  0.0530,  ..., -0.0102,  0.0490,  0.0367],\n                       ...,\n                       [ 0.0354, -0.0616,  0.0236,  ...,  0.0315,  0.0682,  0.0170],\n                       [ 0.0356,  0.0034, -0.0424,  ...,  0.0390, -0.0560, -0.0356],\n                       [ 0.0473,  0.0129, -0.0107,  ...,  0.0086, -0.0666,  0.0279]],\n                      size=(10, 768), dtype=torch.qint8,\n                      quantization_scheme=torch.per_channel_affine,\n                      scale=tensor([0.0016, 0.0012, 0.0020, 0.0013, 0.0012, 0.0014, 0.0012, 0.0013, 0.0017,\n                       0.0021], dtype=torch.float64),\n                      zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), axis=0),\n               Parameter containing:\n               tensor([ 0.0077, -0.0487, -0.0305,  0.0125, -0.0433, -0.0539, -0.0047,  0.0128,\n                       -0.0222,  0.0180], requires_grad=True)))])"},"metadata":{}}]},{"cell_type":"code","source":"model_quantized_static","metadata":{"execution":{"iopub.status.busy":"2024-04-09T10:15:27.718373Z","iopub.execute_input":"2024-04-09T10:15:27.718762Z","iopub.status.idle":"2024-04-09T10:15:27.894529Z","shell.execute_reply.started":"2024-04-09T10:15:27.718731Z","shell.execute_reply":"2024-04-09T10:15:27.893544Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":86,"outputs":[{"execution_count":86,"output_type":"execute_result","data":{"text/plain":"GraphModule(\n  (features): Module(\n    (0): Module(\n      (0): QuantizedConv2d(3, 96, kernel_size=(4, 4), stride=(4, 4), scale=0.016453946009278297, zero_point=58)\n      (1): Module()\n    )\n    (1): Module(\n      (0): Module(\n        (block): Module(\n          (0): QuantizedConv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), scale=0.0018453479278832674, zero_point=66, padding=(3, 3), groups=96)\n          (2): QuantizedLayerNorm((96,), eps=1e-06, elementwise_affine=True)\n          (3): QuantizedLinear(in_features=96, out_features=384, scale=0.11345763504505157, zero_point=67, qscheme=torch.per_channel_affine)\n          (4): GELU(approximate='none')\n          (5): QuantizedLinear(in_features=384, out_features=96, scale=0.04554577171802521, zero_point=64, qscheme=torch.per_channel_affine)\n        )\n      )\n      (1): Module(\n        (block): Module(\n          (0): QuantizedConv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), scale=0.0043882462196052074, zero_point=70, padding=(3, 3), groups=96)\n          (2): QuantizedLayerNorm((96,), eps=1e-06, elementwise_affine=True)\n          (3): QuantizedLinear(in_features=96, out_features=384, scale=0.11349708586931229, zero_point=74, qscheme=torch.per_channel_affine)\n          (4): GELU(approximate='none')\n          (5): QuantizedLinear(in_features=384, out_features=96, scale=0.050522129982709885, zero_point=93, qscheme=torch.per_channel_affine)\n        )\n      )\n      (2): Module(\n        (block): Module(\n          (0): QuantizedConv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), scale=0.0017921760445460677, zero_point=66, padding=(3, 3), groups=96)\n          (2): QuantizedLayerNorm((96,), eps=1e-06, elementwise_affine=True)\n          (3): QuantizedLinear(in_features=96, out_features=384, scale=0.0809139609336853, zero_point=78, qscheme=torch.per_channel_affine)\n          (4): GELU(approximate='none')\n          (5): QuantizedLinear(in_features=384, out_features=96, scale=0.04812664911150932, zero_point=80, qscheme=torch.per_channel_affine)\n        )\n      )\n    )\n    (2): Module(\n      (0): Module()\n      (1): QuantizedConv2d(96, 192, kernel_size=(2, 2), stride=(2, 2), scale=0.008915295824408531, zero_point=91)\n    )\n    (3): Module(\n      (0): Module(\n        (block): Module(\n          (0): QuantizedConv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), scale=0.003366074524819851, zero_point=65, padding=(3, 3), groups=192)\n          (2): QuantizedLayerNorm((192,), eps=1e-06, elementwise_affine=True)\n          (3): QuantizedLinear(in_features=192, out_features=768, scale=0.07458765804767609, zero_point=75, qscheme=torch.per_channel_affine)\n          (4): GELU(approximate='none')\n          (5): QuantizedLinear(in_features=768, out_features=192, scale=0.07030438631772995, zero_point=97, qscheme=torch.per_channel_affine)\n        )\n      )\n      (1): Module(\n        (block): Module(\n          (0): QuantizedConv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), scale=0.0033094657119363546, zero_point=80, padding=(3, 3), groups=192)\n          (2): QuantizedLayerNorm((192,), eps=1e-06, elementwise_affine=True)\n          (3): QuantizedLinear(in_features=192, out_features=768, scale=0.07660354673862457, zero_point=70, qscheme=torch.per_channel_affine)\n          (4): GELU(approximate='none')\n          (5): QuantizedLinear(in_features=768, out_features=192, scale=0.060053180903196335, zero_point=39, qscheme=torch.per_channel_affine)\n        )\n      )\n      (2): Module(\n        (block): Module(\n          (0): QuantizedConv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), scale=0.005890395492315292, zero_point=80, padding=(3, 3), groups=192)\n          (2): QuantizedLayerNorm((192,), eps=1e-06, elementwise_affine=True)\n          (3): QuantizedLinear(in_features=192, out_features=768, scale=0.0679759532213211, zero_point=83, qscheme=torch.per_channel_affine)\n          (4): GELU(approximate='none')\n          (5): QuantizedLinear(in_features=768, out_features=192, scale=0.08103502541780472, zero_point=63, qscheme=torch.per_channel_affine)\n        )\n      )\n    )\n    (4): Module(\n      (0): Module()\n      (1): QuantizedConv2d(192, 384, kernel_size=(2, 2), stride=(2, 2), scale=0.004985204432159662, zero_point=59)\n    )\n    (5): Module(\n      (0): Module(\n        (block): Module(\n          (0): QuantizedConv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), scale=0.0017072366317734122, zero_point=61, padding=(3, 3), groups=384)\n          (2): QuantizedLayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (3): QuantizedLinear(in_features=384, out_features=1536, scale=0.07582294940948486, zero_point=83, qscheme=torch.per_channel_affine)\n          (4): GELU(approximate='none')\n          (5): QuantizedLinear(in_features=1536, out_features=384, scale=0.04450752213597298, zero_point=64, qscheme=torch.per_channel_affine)\n        )\n      )\n      (1): Module(\n        (block): Module(\n          (0): QuantizedConv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), scale=0.0030318479984998703, zero_point=63, padding=(3, 3), groups=384)\n          (2): QuantizedLayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (3): QuantizedLinear(in_features=384, out_features=1536, scale=0.07559102773666382, zero_point=85, qscheme=torch.per_channel_affine)\n          (4): GELU(approximate='none')\n          (5): QuantizedLinear(in_features=1536, out_features=384, scale=0.04001267999410629, zero_point=66, qscheme=torch.per_channel_affine)\n        )\n      )\n      (2): Module(\n        (block): Module(\n          (0): QuantizedConv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), scale=0.004143495112657547, zero_point=57, padding=(3, 3), groups=384)\n          (2): QuantizedLayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (3): QuantizedLinear(in_features=384, out_features=1536, scale=0.09310123324394226, zero_point=93, qscheme=torch.per_channel_affine)\n          (4): GELU(approximate='none')\n          (5): QuantizedLinear(in_features=1536, out_features=384, scale=0.047522954642772675, zero_point=48, qscheme=torch.per_channel_affine)\n        )\n      )\n      (3): Module(\n        (block): Module(\n          (0): QuantizedConv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), scale=0.0031323255971074104, zero_point=63, padding=(3, 3), groups=384)\n          (2): QuantizedLayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (3): QuantizedLinear(in_features=384, out_features=1536, scale=0.08077171444892883, zero_point=82, qscheme=torch.per_channel_affine)\n          (4): GELU(approximate='none')\n          (5): QuantizedLinear(in_features=1536, out_features=384, scale=0.04662035405635834, zero_point=62, qscheme=torch.per_channel_affine)\n        )\n      )\n      (4): Module(\n        (block): Module(\n          (0): QuantizedConv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), scale=0.004711270332336426, zero_point=64, padding=(3, 3), groups=384)\n          (2): QuantizedLayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (3): QuantizedLinear(in_features=384, out_features=1536, scale=0.08873225748538971, zero_point=86, qscheme=torch.per_channel_affine)\n          (4): GELU(approximate='none')\n          (5): QuantizedLinear(in_features=1536, out_features=384, scale=0.04283386096358299, zero_point=72, qscheme=torch.per_channel_affine)\n        )\n      )\n      (5): Module(\n        (block): Module(\n          (0): QuantizedConv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), scale=0.009607778862118721, zero_point=81, padding=(3, 3), groups=384)\n          (2): QuantizedLayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (3): QuantizedLinear(in_features=384, out_features=1536, scale=0.08675136417150497, zero_point=82, qscheme=torch.per_channel_affine)\n          (4): GELU(approximate='none')\n          (5): QuantizedLinear(in_features=1536, out_features=384, scale=0.0629814937710762, zero_point=57, qscheme=torch.per_channel_affine)\n        )\n      )\n      (6): Module(\n        (block): Module(\n          (0): QuantizedConv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), scale=0.007385149132460356, zero_point=59, padding=(3, 3), groups=384)\n          (2): QuantizedLayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (3): QuantizedLinear(in_features=384, out_features=1536, scale=0.09389615803956985, zero_point=83, qscheme=torch.per_channel_affine)\n          (4): GELU(approximate='none')\n          (5): QuantizedLinear(in_features=1536, out_features=384, scale=0.11043699830770493, zero_point=98, qscheme=torch.per_channel_affine)\n        )\n      )\n      (7): Module(\n        (block): Module(\n          (0): QuantizedConv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), scale=0.01470804214477539, zero_point=61, padding=(3, 3), groups=384)\n          (2): QuantizedLayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (3): QuantizedLinear(in_features=384, out_features=1536, scale=0.0992111936211586, zero_point=90, qscheme=torch.per_channel_affine)\n          (4): GELU(approximate='none')\n          (5): QuantizedLinear(in_features=1536, out_features=384, scale=0.12755073606967926, zero_point=14, qscheme=torch.per_channel_affine)\n        )\n      )\n      (8): Module(\n        (block): Module(\n          (0): QuantizedConv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), scale=0.014010815881192684, zero_point=58, padding=(3, 3), groups=384)\n          (2): QuantizedLayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (3): QuantizedLinear(in_features=384, out_features=1536, scale=0.09175347536802292, zero_point=83, qscheme=torch.per_channel_affine)\n          (4): GELU(approximate='none')\n          (5): QuantizedLinear(in_features=1536, out_features=384, scale=0.11368975043296814, zero_point=94, qscheme=torch.per_channel_affine)\n        )\n      )\n    )\n    (6): Module(\n      (0): Module()\n      (1): QuantizedConv2d(384, 768, kernel_size=(2, 2), stride=(2, 2), scale=0.025593668222427368, zero_point=70)\n    )\n    (7): Module(\n      (0): Module(\n        (block): Module(\n          (0): QuantizedConv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), scale=0.022649643942713737, zero_point=86, padding=(3, 3), groups=768)\n          (2): QuantizedLayerNorm((768,), eps=1e-06, elementwise_affine=True)\n          (3): QuantizedLinear(in_features=768, out_features=3072, scale=0.24729260802268982, zero_point=94, qscheme=torch.per_channel_affine)\n          (4): GELU(approximate='none')\n          (5): QuantizedLinear(in_features=3072, out_features=768, scale=0.08845265209674835, zero_point=74, qscheme=torch.per_channel_affine)\n        )\n      )\n      (1): Module(\n        (block): Module(\n          (0): QuantizedConv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), scale=0.038697436451911926, zero_point=68, padding=(3, 3), groups=768)\n          (2): QuantizedLayerNorm((768,), eps=1e-06, elementwise_affine=True)\n          (3): QuantizedLinear(in_features=768, out_features=3072, scale=0.137152761220932, zero_point=89, qscheme=torch.per_channel_affine)\n          (4): GELU(approximate='none')\n          (5): QuantizedLinear(in_features=3072, out_features=768, scale=0.05288971960544586, zero_point=64, qscheme=torch.per_channel_affine)\n        )\n      )\n      (2): Module(\n        (block): Module(\n          (0): QuantizedConv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), scale=0.03757606819272041, zero_point=72, padding=(3, 3), groups=768)\n          (2): QuantizedLayerNorm((768,), eps=1e-06, elementwise_affine=True)\n          (3): QuantizedLinear(in_features=768, out_features=3072, scale=0.1872970163822174, zero_point=81, qscheme=torch.per_channel_affine)\n          (4): GELU(approximate='none')\n          (5): QuantizedLinear(in_features=3072, out_features=768, scale=0.15079057216644287, zero_point=78, qscheme=torch.per_channel_affine)\n        )\n      )\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=1)\n  (classifier): Module(\n    (0): Module()\n    (1): Flatten(start_dim=1, end_dim=-1)\n    (2): QuantizedLinear(in_features=768, out_features=10, scale=0.1548439860343933, zero_point=48, qscheme=torch.per_channel_affine)\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"model_quantized_static.to('cpu')","metadata":{"execution":{"iopub.status.busy":"2024-04-09T10:15:36.604361Z","iopub.execute_input":"2024-04-09T10:15:36.604751Z","iopub.status.idle":"2024-04-09T10:15:36.784717Z","shell.execute_reply.started":"2024-04-09T10:15:36.604720Z","shell.execute_reply":"2024-04-09T10:15:36.783687Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":87,"outputs":[{"execution_count":87,"output_type":"execute_result","data":{"text/plain":"GraphModule(\n  (features): Module(\n    (0): Module(\n      (0): QuantizedConv2d(3, 96, kernel_size=(4, 4), stride=(4, 4), scale=0.016453946009278297, zero_point=58)\n      (1): Module()\n    )\n    (1): Module(\n      (0): Module(\n        (block): Module(\n          (0): QuantizedConv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), scale=0.0018453479278832674, zero_point=66, padding=(3, 3), groups=96)\n          (2): QuantizedLayerNorm((96,), eps=1e-06, elementwise_affine=True)\n          (3): QuantizedLinear(in_features=96, out_features=384, scale=0.11345763504505157, zero_point=67, qscheme=torch.per_channel_affine)\n          (4): GELU(approximate='none')\n          (5): QuantizedLinear(in_features=384, out_features=96, scale=0.04554577171802521, zero_point=64, qscheme=torch.per_channel_affine)\n        )\n      )\n      (1): Module(\n        (block): Module(\n          (0): QuantizedConv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), scale=0.0043882462196052074, zero_point=70, padding=(3, 3), groups=96)\n          (2): QuantizedLayerNorm((96,), eps=1e-06, elementwise_affine=True)\n          (3): QuantizedLinear(in_features=96, out_features=384, scale=0.11349708586931229, zero_point=74, qscheme=torch.per_channel_affine)\n          (4): GELU(approximate='none')\n          (5): QuantizedLinear(in_features=384, out_features=96, scale=0.050522129982709885, zero_point=93, qscheme=torch.per_channel_affine)\n        )\n      )\n      (2): Module(\n        (block): Module(\n          (0): QuantizedConv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), scale=0.0017921760445460677, zero_point=66, padding=(3, 3), groups=96)\n          (2): QuantizedLayerNorm((96,), eps=1e-06, elementwise_affine=True)\n          (3): QuantizedLinear(in_features=96, out_features=384, scale=0.0809139609336853, zero_point=78, qscheme=torch.per_channel_affine)\n          (4): GELU(approximate='none')\n          (5): QuantizedLinear(in_features=384, out_features=96, scale=0.04812664911150932, zero_point=80, qscheme=torch.per_channel_affine)\n        )\n      )\n    )\n    (2): Module(\n      (0): Module()\n      (1): QuantizedConv2d(96, 192, kernel_size=(2, 2), stride=(2, 2), scale=0.008915295824408531, zero_point=91)\n    )\n    (3): Module(\n      (0): Module(\n        (block): Module(\n          (0): QuantizedConv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), scale=0.003366074524819851, zero_point=65, padding=(3, 3), groups=192)\n          (2): QuantizedLayerNorm((192,), eps=1e-06, elementwise_affine=True)\n          (3): QuantizedLinear(in_features=192, out_features=768, scale=0.07458765804767609, zero_point=75, qscheme=torch.per_channel_affine)\n          (4): GELU(approximate='none')\n          (5): QuantizedLinear(in_features=768, out_features=192, scale=0.07030438631772995, zero_point=97, qscheme=torch.per_channel_affine)\n        )\n      )\n      (1): Module(\n        (block): Module(\n          (0): QuantizedConv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), scale=0.0033094657119363546, zero_point=80, padding=(3, 3), groups=192)\n          (2): QuantizedLayerNorm((192,), eps=1e-06, elementwise_affine=True)\n          (3): QuantizedLinear(in_features=192, out_features=768, scale=0.07660354673862457, zero_point=70, qscheme=torch.per_channel_affine)\n          (4): GELU(approximate='none')\n          (5): QuantizedLinear(in_features=768, out_features=192, scale=0.060053180903196335, zero_point=39, qscheme=torch.per_channel_affine)\n        )\n      )\n      (2): Module(\n        (block): Module(\n          (0): QuantizedConv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), scale=0.005890395492315292, zero_point=80, padding=(3, 3), groups=192)\n          (2): QuantizedLayerNorm((192,), eps=1e-06, elementwise_affine=True)\n          (3): QuantizedLinear(in_features=192, out_features=768, scale=0.0679759532213211, zero_point=83, qscheme=torch.per_channel_affine)\n          (4): GELU(approximate='none')\n          (5): QuantizedLinear(in_features=768, out_features=192, scale=0.08103502541780472, zero_point=63, qscheme=torch.per_channel_affine)\n        )\n      )\n    )\n    (4): Module(\n      (0): Module()\n      (1): QuantizedConv2d(192, 384, kernel_size=(2, 2), stride=(2, 2), scale=0.004985204432159662, zero_point=59)\n    )\n    (5): Module(\n      (0): Module(\n        (block): Module(\n          (0): QuantizedConv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), scale=0.0017072366317734122, zero_point=61, padding=(3, 3), groups=384)\n          (2): QuantizedLayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (3): QuantizedLinear(in_features=384, out_features=1536, scale=0.07582294940948486, zero_point=83, qscheme=torch.per_channel_affine)\n          (4): GELU(approximate='none')\n          (5): QuantizedLinear(in_features=1536, out_features=384, scale=0.04450752213597298, zero_point=64, qscheme=torch.per_channel_affine)\n        )\n      )\n      (1): Module(\n        (block): Module(\n          (0): QuantizedConv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), scale=0.0030318479984998703, zero_point=63, padding=(3, 3), groups=384)\n          (2): QuantizedLayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (3): QuantizedLinear(in_features=384, out_features=1536, scale=0.07559102773666382, zero_point=85, qscheme=torch.per_channel_affine)\n          (4): GELU(approximate='none')\n          (5): QuantizedLinear(in_features=1536, out_features=384, scale=0.04001267999410629, zero_point=66, qscheme=torch.per_channel_affine)\n        )\n      )\n      (2): Module(\n        (block): Module(\n          (0): QuantizedConv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), scale=0.004143495112657547, zero_point=57, padding=(3, 3), groups=384)\n          (2): QuantizedLayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (3): QuantizedLinear(in_features=384, out_features=1536, scale=0.09310123324394226, zero_point=93, qscheme=torch.per_channel_affine)\n          (4): GELU(approximate='none')\n          (5): QuantizedLinear(in_features=1536, out_features=384, scale=0.047522954642772675, zero_point=48, qscheme=torch.per_channel_affine)\n        )\n      )\n      (3): Module(\n        (block): Module(\n          (0): QuantizedConv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), scale=0.0031323255971074104, zero_point=63, padding=(3, 3), groups=384)\n          (2): QuantizedLayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (3): QuantizedLinear(in_features=384, out_features=1536, scale=0.08077171444892883, zero_point=82, qscheme=torch.per_channel_affine)\n          (4): GELU(approximate='none')\n          (5): QuantizedLinear(in_features=1536, out_features=384, scale=0.04662035405635834, zero_point=62, qscheme=torch.per_channel_affine)\n        )\n      )\n      (4): Module(\n        (block): Module(\n          (0): QuantizedConv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), scale=0.004711270332336426, zero_point=64, padding=(3, 3), groups=384)\n          (2): QuantizedLayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (3): QuantizedLinear(in_features=384, out_features=1536, scale=0.08873225748538971, zero_point=86, qscheme=torch.per_channel_affine)\n          (4): GELU(approximate='none')\n          (5): QuantizedLinear(in_features=1536, out_features=384, scale=0.04283386096358299, zero_point=72, qscheme=torch.per_channel_affine)\n        )\n      )\n      (5): Module(\n        (block): Module(\n          (0): QuantizedConv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), scale=0.009607778862118721, zero_point=81, padding=(3, 3), groups=384)\n          (2): QuantizedLayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (3): QuantizedLinear(in_features=384, out_features=1536, scale=0.08675136417150497, zero_point=82, qscheme=torch.per_channel_affine)\n          (4): GELU(approximate='none')\n          (5): QuantizedLinear(in_features=1536, out_features=384, scale=0.0629814937710762, zero_point=57, qscheme=torch.per_channel_affine)\n        )\n      )\n      (6): Module(\n        (block): Module(\n          (0): QuantizedConv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), scale=0.007385149132460356, zero_point=59, padding=(3, 3), groups=384)\n          (2): QuantizedLayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (3): QuantizedLinear(in_features=384, out_features=1536, scale=0.09389615803956985, zero_point=83, qscheme=torch.per_channel_affine)\n          (4): GELU(approximate='none')\n          (5): QuantizedLinear(in_features=1536, out_features=384, scale=0.11043699830770493, zero_point=98, qscheme=torch.per_channel_affine)\n        )\n      )\n      (7): Module(\n        (block): Module(\n          (0): QuantizedConv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), scale=0.01470804214477539, zero_point=61, padding=(3, 3), groups=384)\n          (2): QuantizedLayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (3): QuantizedLinear(in_features=384, out_features=1536, scale=0.0992111936211586, zero_point=90, qscheme=torch.per_channel_affine)\n          (4): GELU(approximate='none')\n          (5): QuantizedLinear(in_features=1536, out_features=384, scale=0.12755073606967926, zero_point=14, qscheme=torch.per_channel_affine)\n        )\n      )\n      (8): Module(\n        (block): Module(\n          (0): QuantizedConv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), scale=0.014010815881192684, zero_point=58, padding=(3, 3), groups=384)\n          (2): QuantizedLayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (3): QuantizedLinear(in_features=384, out_features=1536, scale=0.09175347536802292, zero_point=83, qscheme=torch.per_channel_affine)\n          (4): GELU(approximate='none')\n          (5): QuantizedLinear(in_features=1536, out_features=384, scale=0.11368975043296814, zero_point=94, qscheme=torch.per_channel_affine)\n        )\n      )\n    )\n    (6): Module(\n      (0): Module()\n      (1): QuantizedConv2d(384, 768, kernel_size=(2, 2), stride=(2, 2), scale=0.025593668222427368, zero_point=70)\n    )\n    (7): Module(\n      (0): Module(\n        (block): Module(\n          (0): QuantizedConv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), scale=0.022649643942713737, zero_point=86, padding=(3, 3), groups=768)\n          (2): QuantizedLayerNorm((768,), eps=1e-06, elementwise_affine=True)\n          (3): QuantizedLinear(in_features=768, out_features=3072, scale=0.24729260802268982, zero_point=94, qscheme=torch.per_channel_affine)\n          (4): GELU(approximate='none')\n          (5): QuantizedLinear(in_features=3072, out_features=768, scale=0.08845265209674835, zero_point=74, qscheme=torch.per_channel_affine)\n        )\n      )\n      (1): Module(\n        (block): Module(\n          (0): QuantizedConv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), scale=0.038697436451911926, zero_point=68, padding=(3, 3), groups=768)\n          (2): QuantizedLayerNorm((768,), eps=1e-06, elementwise_affine=True)\n          (3): QuantizedLinear(in_features=768, out_features=3072, scale=0.137152761220932, zero_point=89, qscheme=torch.per_channel_affine)\n          (4): GELU(approximate='none')\n          (5): QuantizedLinear(in_features=3072, out_features=768, scale=0.05288971960544586, zero_point=64, qscheme=torch.per_channel_affine)\n        )\n      )\n      (2): Module(\n        (block): Module(\n          (0): QuantizedConv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), scale=0.03757606819272041, zero_point=72, padding=(3, 3), groups=768)\n          (2): QuantizedLayerNorm((768,), eps=1e-06, elementwise_affine=True)\n          (3): QuantizedLinear(in_features=768, out_features=3072, scale=0.1872970163822174, zero_point=81, qscheme=torch.per_channel_affine)\n          (4): GELU(approximate='none')\n          (5): QuantizedLinear(in_features=3072, out_features=768, scale=0.15079057216644287, zero_point=78, qscheme=torch.per_channel_affine)\n        )\n      )\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=1)\n  (classifier): Module(\n    (0): Module()\n    (1): Flatten(start_dim=1, end_dim=-1)\n    (2): QuantizedLinear(in_features=768, out_features=10, scale=0.1548439860343933, zero_point=48, qscheme=torch.per_channel_affine)\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"all_predictions_int8 = []\nall_labels_int8 = []\ncorrect_pred = 0\ntotal_pred = 0\nstart_time_int8 = time()\nwith torch.no_grad():\n    model_quantized_static.eval()\n    for data in testloader:\n        images, labels = data\n        all_labels_int8.extend(labels.numpy())\n        #images, labels = images.to(device), labels.to(device)\n        outputs = model_quantized_static(images.to('cpu'))\n        _, predicted = torch.max(outputs.data, 1)\n        total_pred += labels.size(0)\n        correct_pred += (predicted == labels).sum().item()\n        predicted_tensor_cpu = predicted.to('cpu')\n        all_predictions_int8.extend(predicted_tensor_cpu.numpy())\nend_time_int8 = time()\nprint(\"Time: \",end_time_int8 - start_time_int8)\nprint('Accuracy achieved by the network on test images is: %d%%' % (100 * correct_pred / total_pred))","metadata":{"execution":{"iopub.status.busy":"2024-04-09T10:15:44.986822Z","iopub.execute_input":"2024-04-09T10:15:44.987256Z","iopub.status.idle":"2024-04-09T10:25:31.385726Z","shell.execute_reply.started":"2024-04-09T10:15:44.987226Z","shell.execute_reply":"2024-04-09T10:25:31.384401Z"},"trusted":true},"execution_count":88,"outputs":[{"name":"stdout","text":"Time:  586.3894338607788\nAccuracy achieved by the network on test images is: 64%\n","output_type":"stream"}]},{"cell_type":"code","source":"metrics(all_predictions_int8,all_labels_int8)","metadata":{"execution":{"iopub.status.busy":"2024-04-09T10:25:32.297050Z","iopub.execute_input":"2024-04-09T10:25:32.297359Z","iopub.status.idle":"2024-04-09T10:25:33.086770Z","shell.execute_reply.started":"2024-04-09T10:25:32.297332Z","shell.execute_reply":"2024-04-09T10:25:33.085644Z"},"trusted":true},"execution_count":90,"outputs":[{"name":"stdout","text":"Confusion Matrix:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 800x600 with 2 Axes>","image/png":"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"},"metadata":{"image/png":{"width":793,"height":689}}},{"name":"stdout","text":"----------------------------------------------------------------\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.90      0.52      0.66       982\n           1       0.25      0.80      0.38       193\n           2       0.87      0.51      0.64      1073\n           3       0.66      0.67      0.67       450\n           4       0.76      0.90      0.82       415\n           5       0.61      0.62      0.62       410\n           6       0.58      0.68      0.62       436\n           7       0.45      0.99      0.62       281\n           8       0.35      0.91      0.50       181\n           9       0.95      0.60      0.74       979\n\n    accuracy                           0.64      5400\n   macro avg       0.64      0.72      0.63      5400\nweighted avg       0.76      0.64      0.66      5400\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### QAT","metadata":{}},{"cell_type":"markdown","source":"Load a new vgg model which is not trained before starting qat","metadata":{}},{"cell_type":"code","source":"input_data = next(iter(trainloader))[0]\ncalibrate_data = input_data.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-04-09T10:25:44.350747Z","iopub.execute_input":"2024-04-09T10:25:44.351183Z","iopub.status.idle":"2024-04-09T10:25:44.492480Z","shell.execute_reply.started":"2024-04-09T10:25:44.351151Z","shell.execute_reply":"2024-04-09T10:25:44.491458Z"},"trusted":true},"execution_count":91,"outputs":[]},{"cell_type":"code","source":"model.eval()\nmodel.qconfig = torch.ao.quantization.get_default_qat_qconfig('x86')\nmodel.train()","metadata":{"execution":{"iopub.status.busy":"2024-04-09T10:25:47.689160Z","iopub.execute_input":"2024-04-09T10:25:47.689779Z","iopub.status.idle":"2024-04-09T10:25:47.702978Z","shell.execute_reply.started":"2024-04-09T10:25:47.689746Z","shell.execute_reply":"2024-04-09T10:25:47.701883Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":92,"outputs":[{"execution_count":92,"output_type":"execute_result","data":{"text/plain":"ConvNeXt(\n  (features): Sequential(\n    (0): Conv2dNormActivation(\n      (0): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\n      (1): LayerNorm2d((96,), eps=1e-06, elementwise_affine=True)\n    )\n    (1): Sequential(\n      (0): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n          (1): Permute()\n          (2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=96, out_features=384, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=384, out_features=96, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n      )\n      (1): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n          (1): Permute()\n          (2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=96, out_features=384, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=384, out_features=96, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.0058823529411764705, mode=row)\n      )\n      (2): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n          (1): Permute()\n          (2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=96, out_features=384, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=384, out_features=96, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.011764705882352941, mode=row)\n      )\n    )\n    (2): Sequential(\n      (0): LayerNorm2d((96,), eps=1e-06, elementwise_affine=True)\n      (1): Conv2d(96, 192, kernel_size=(2, 2), stride=(2, 2))\n    )\n    (3): Sequential(\n      (0): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n          (1): Permute()\n          (2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=192, out_features=768, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=768, out_features=192, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.017647058823529415, mode=row)\n      )\n      (1): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n          (1): Permute()\n          (2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=192, out_features=768, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=768, out_features=192, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.023529411764705882, mode=row)\n      )\n      (2): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n          (1): Permute()\n          (2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=192, out_features=768, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=768, out_features=192, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.029411764705882353, mode=row)\n      )\n    )\n    (4): Sequential(\n      (0): LayerNorm2d((192,), eps=1e-06, elementwise_affine=True)\n      (1): Conv2d(192, 384, kernel_size=(2, 2), stride=(2, 2))\n    )\n    (5): Sequential(\n      (0): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n          (1): Permute()\n          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=384, out_features=1536, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=1536, out_features=384, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.03529411764705883, mode=row)\n      )\n      (1): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n          (1): Permute()\n          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=384, out_features=1536, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=1536, out_features=384, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.0411764705882353, mode=row)\n      )\n      (2): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n          (1): Permute()\n          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=384, out_features=1536, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=1536, out_features=384, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.047058823529411764, mode=row)\n      )\n      (3): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n          (1): Permute()\n          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=384, out_features=1536, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=1536, out_features=384, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.052941176470588235, mode=row)\n      )\n      (4): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n          (1): Permute()\n          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=384, out_features=1536, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=1536, out_features=384, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.058823529411764705, mode=row)\n      )\n      (5): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n          (1): Permute()\n          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=384, out_features=1536, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=1536, out_features=384, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.06470588235294118, mode=row)\n      )\n      (6): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n          (1): Permute()\n          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=384, out_features=1536, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=1536, out_features=384, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.07058823529411766, mode=row)\n      )\n      (7): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n          (1): Permute()\n          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=384, out_features=1536, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=1536, out_features=384, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.07647058823529412, mode=row)\n      )\n      (8): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n          (1): Permute()\n          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=384, out_features=1536, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=1536, out_features=384, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.0823529411764706, mode=row)\n      )\n    )\n    (6): Sequential(\n      (0): LayerNorm2d((384,), eps=1e-06, elementwise_affine=True)\n      (1): Conv2d(384, 768, kernel_size=(2, 2), stride=(2, 2))\n    )\n    (7): Sequential(\n      (0): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n          (1): Permute()\n          (2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=768, out_features=3072, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=3072, out_features=768, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.08823529411764706, mode=row)\n      )\n      (1): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n          (1): Permute()\n          (2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=768, out_features=3072, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=3072, out_features=768, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.09411764705882353, mode=row)\n      )\n      (2): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n          (1): Permute()\n          (2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=768, out_features=3072, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=3072, out_features=768, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.1, mode=row)\n      )\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=1)\n  (classifier): Sequential(\n    (0): LayerNorm2d((768,), eps=1e-06, elementwise_affine=True)\n    (1): Flatten(start_dim=1, end_dim=-1)\n    (2): Linear(in_features=768, out_features=10, bias=True)\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"qconfig_mapping = get_default_qat_qconfig_mapping(\"x86\")\nmodel_prepared = quantize_fx.prepare_qat_fx(model, qconfig_mapping, calibrate_data)","metadata":{"execution":{"iopub.status.busy":"2024-04-09T10:25:53.693184Z","iopub.execute_input":"2024-04-09T10:25:53.693591Z","iopub.status.idle":"2024-04-09T10:25:56.688447Z","shell.execute_reply.started":"2024-04-09T10:25:53.693558Z","shell.execute_reply":"2024-04-09T10:25:56.687537Z"},"trusted":true},"execution_count":93,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/ao/quantization/observer.py:214: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"model_prepared.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-04-09T10:25:58.754248Z","iopub.execute_input":"2024-04-09T10:25:58.755058Z","iopub.status.idle":"2024-04-09T10:25:59.271424Z","shell.execute_reply.started":"2024-04-09T10:25:58.755023Z","shell.execute_reply":"2024-04-09T10:25:59.270327Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":94,"outputs":[{"execution_count":94,"output_type":"execute_result","data":{"text/plain":"GraphModule(\n  (activation_post_process_0): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (features): Module(\n    (0): Module(\n      (0): Conv2d(\n        3, 96, kernel_size=(4, 4), stride=(4, 4)\n        (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n          fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n        )\n      )\n      (1): Module()\n    )\n    (1): Module(\n      (0): Module(\n        (block): Module(\n          (0): Conv2d(\n            96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n            )\n          )\n          (2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(\n            in_features=96, out_features=384, bias=True\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n            )\n          )\n          (4): GELU(approximate='none')\n          (5): Linear(\n            in_features=384, out_features=96, bias=True\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n            )\n          )\n        )\n      )\n      (1): Module(\n        (block): Module(\n          (0): Conv2d(\n            96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n            )\n          )\n          (2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(\n            in_features=96, out_features=384, bias=True\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n            )\n          )\n          (4): GELU(approximate='none')\n          (5): Linear(\n            in_features=384, out_features=96, bias=True\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n            )\n          )\n        )\n      )\n      (2): Module(\n        (block): Module(\n          (0): Conv2d(\n            96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n            )\n          )\n          (2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(\n            in_features=96, out_features=384, bias=True\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n            )\n          )\n          (4): GELU(approximate='none')\n          (5): Linear(\n            in_features=384, out_features=96, bias=True\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n            )\n          )\n        )\n      )\n    )\n    (2): Module(\n      (0): Module()\n      (1): Conv2d(\n        96, 192, kernel_size=(2, 2), stride=(2, 2)\n        (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n          fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n        )\n      )\n    )\n    (3): Module(\n      (0): Module(\n        (block): Module(\n          (0): Conv2d(\n            192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n            )\n          )\n          (2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(\n            in_features=192, out_features=768, bias=True\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n            )\n          )\n          (4): GELU(approximate='none')\n          (5): Linear(\n            in_features=768, out_features=192, bias=True\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n            )\n          )\n        )\n      )\n      (1): Module(\n        (block): Module(\n          (0): Conv2d(\n            192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n            )\n          )\n          (2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(\n            in_features=192, out_features=768, bias=True\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n            )\n          )\n          (4): GELU(approximate='none')\n          (5): Linear(\n            in_features=768, out_features=192, bias=True\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n            )\n          )\n        )\n      )\n      (2): Module(\n        (block): Module(\n          (0): Conv2d(\n            192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n            )\n          )\n          (2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(\n            in_features=192, out_features=768, bias=True\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n            )\n          )\n          (4): GELU(approximate='none')\n          (5): Linear(\n            in_features=768, out_features=192, bias=True\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n            )\n          )\n        )\n      )\n    )\n    (4): Module(\n      (0): Module()\n      (1): Conv2d(\n        192, 384, kernel_size=(2, 2), stride=(2, 2)\n        (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n          fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n        )\n      )\n    )\n    (5): Module(\n      (0): Module(\n        (block): Module(\n          (0): Conv2d(\n            384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n            )\n          )\n          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(\n            in_features=384, out_features=1536, bias=True\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n            )\n          )\n          (4): GELU(approximate='none')\n          (5): Linear(\n            in_features=1536, out_features=384, bias=True\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n            )\n          )\n        )\n      )\n      (1): Module(\n        (block): Module(\n          (0): Conv2d(\n            384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n            )\n          )\n          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(\n            in_features=384, out_features=1536, bias=True\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n            )\n          )\n          (4): GELU(approximate='none')\n          (5): Linear(\n            in_features=1536, out_features=384, bias=True\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n            )\n          )\n        )\n      )\n      (2): Module(\n        (block): Module(\n          (0): Conv2d(\n            384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n            )\n          )\n          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(\n            in_features=384, out_features=1536, bias=True\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n            )\n          )\n          (4): GELU(approximate='none')\n          (5): Linear(\n            in_features=1536, out_features=384, bias=True\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n            )\n          )\n        )\n      )\n      (3): Module(\n        (block): Module(\n          (0): Conv2d(\n            384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n            )\n          )\n          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(\n            in_features=384, out_features=1536, bias=True\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n            )\n          )\n          (4): GELU(approximate='none')\n          (5): Linear(\n            in_features=1536, out_features=384, bias=True\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n            )\n          )\n        )\n      )\n      (4): Module(\n        (block): Module(\n          (0): Conv2d(\n            384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n            )\n          )\n          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(\n            in_features=384, out_features=1536, bias=True\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n            )\n          )\n          (4): GELU(approximate='none')\n          (5): Linear(\n            in_features=1536, out_features=384, bias=True\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n            )\n          )\n        )\n      )\n      (5): Module(\n        (block): Module(\n          (0): Conv2d(\n            384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n            )\n          )\n          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(\n            in_features=384, out_features=1536, bias=True\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n            )\n          )\n          (4): GELU(approximate='none')\n          (5): Linear(\n            in_features=1536, out_features=384, bias=True\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n            )\n          )\n        )\n      )\n      (6): Module(\n        (block): Module(\n          (0): Conv2d(\n            384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n            )\n          )\n          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(\n            in_features=384, out_features=1536, bias=True\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n            )\n          )\n          (4): GELU(approximate='none')\n          (5): Linear(\n            in_features=1536, out_features=384, bias=True\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n            )\n          )\n        )\n      )\n      (7): Module(\n        (block): Module(\n          (0): Conv2d(\n            384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n            )\n          )\n          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(\n            in_features=384, out_features=1536, bias=True\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n            )\n          )\n          (4): GELU(approximate='none')\n          (5): Linear(\n            in_features=1536, out_features=384, bias=True\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n            )\n          )\n        )\n      )\n      (8): Module(\n        (block): Module(\n          (0): Conv2d(\n            384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n            )\n          )\n          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(\n            in_features=384, out_features=1536, bias=True\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n            )\n          )\n          (4): GELU(approximate='none')\n          (5): Linear(\n            in_features=1536, out_features=384, bias=True\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n            )\n          )\n        )\n      )\n    )\n    (6): Module(\n      (0): Module()\n      (1): Conv2d(\n        384, 768, kernel_size=(2, 2), stride=(2, 2)\n        (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n          fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n        )\n      )\n    )\n    (7): Module(\n      (0): Module(\n        (block): Module(\n          (0): Conv2d(\n            768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n            )\n          )\n          (2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(\n            in_features=768, out_features=3072, bias=True\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n            )\n          )\n          (4): GELU(approximate='none')\n          (5): Linear(\n            in_features=3072, out_features=768, bias=True\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n            )\n          )\n        )\n      )\n      (1): Module(\n        (block): Module(\n          (0): Conv2d(\n            768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n            )\n          )\n          (2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(\n            in_features=768, out_features=3072, bias=True\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n            )\n          )\n          (4): GELU(approximate='none')\n          (5): Linear(\n            in_features=3072, out_features=768, bias=True\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n            )\n          )\n        )\n      )\n      (2): Module(\n        (block): Module(\n          (0): Conv2d(\n            768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n            )\n          )\n          (2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(\n            in_features=768, out_features=3072, bias=True\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n            )\n          )\n          (4): GELU(approximate='none')\n          (5): Linear(\n            in_features=3072, out_features=768, bias=True\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n            )\n          )\n        )\n      )\n    )\n  )\n  (activation_post_process_1): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_2): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_3): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_4): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_11): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_5): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_6): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_7): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_8): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_9): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_10): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_12): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_13): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_14): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_15): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_22): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_16): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_17): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_18): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_19): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_20): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_21): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_23): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_24): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_25): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_26): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_33): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_27): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_28): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_29): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_30): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_31): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_32): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_34): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_35): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_36): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_37): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_38): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_39): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_40): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_41): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_48): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_42): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_43): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_44): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_45): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_46): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_47): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_49): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_50): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_51): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_52): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_59): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_53): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_54): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_55): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_56): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_57): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_58): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_60): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_61): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_62): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_63): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_70): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_64): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_65): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_66): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_67): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_68): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_69): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_71): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_72): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_73): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_74): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_75): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_76): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_77): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_78): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_85): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_79): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_80): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_81): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_82): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_83): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_84): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_86): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_87): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_88): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_89): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_96): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_90): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_91): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_92): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_93): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_94): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_95): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_97): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_98): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_99): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_100): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_107): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_101): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_102): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_103): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_104): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_105): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_106): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_108): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_109): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_110): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_111): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_118): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_112): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_113): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_114): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_115): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_116): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_117): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_119): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_120): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_121): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_122): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_129): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_123): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_124): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_125): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_126): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_127): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_128): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_130): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_131): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_132): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_133): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_140): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_134): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_135): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_136): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_137): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_138): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_139): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_141): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_142): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_143): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_144): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_151): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_145): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_146): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_147): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_148): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_149): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_150): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_152): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_153): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_154): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_155): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_162): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_156): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_157): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_158): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_159): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_160): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_161): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_163): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_164): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_165): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_166): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_173): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_167): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_168): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_169): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_170): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_171): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_172): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_174): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_175): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_176): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_177): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_178): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_179): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_180): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_181): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_188): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_182): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_183): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_184): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_185): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_186): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_187): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_189): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_190): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_191): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_192): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_199): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_193): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_194): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_195): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_196): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_197): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_198): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_200): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_201): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_202): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_203): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_210): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_204): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_205): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_206): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_207): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_208): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_209): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_211): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_212): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_213): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_214): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=1)\n  (activation_post_process_215): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_216): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (classifier): Module(\n    (0): Module()\n    (1): Flatten(start_dim=1, end_dim=-1)\n    (2): Linear(\n      in_features=768, out_features=10, bias=True\n      (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n        fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n        (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n      )\n    )\n  )\n  (activation_post_process_217): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_218): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_219): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_220): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"model = train(model,trainloader, validloader,criterion, optimizer,'cuda', epochs,steps, print_every)","metadata":{"execution":{"iopub.status.busy":"2024-04-09T10:26:05.346387Z","iopub.execute_input":"2024-04-09T10:26:05.347157Z","iopub.status.idle":"2024-04-09T10:26:35.275242Z","shell.execute_reply.started":"2024-04-09T10:26:05.347125Z","shell.execute_reply":"2024-04-09T10:26:35.273283Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":95,"outputs":[{"name":"stdout","text":"Training process initializing .....\n\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[95], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43msteps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_every\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[55], line 27\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, trainloader, validloader, criterion, optimizer, device, epochs, steps, print_every)\u001b[0m\n\u001b[1;32m     24\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 27\u001b[0m     valid_loss, accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mvalidation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m | \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(e\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, epochs),\n\u001b[1;32m     30\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining Loss: \u001b[39m\u001b[38;5;132;01m{:.4f}\u001b[39;00m\u001b[38;5;124m | \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(running_loss\u001b[38;5;241m/\u001b[39mprint_every),\n\u001b[1;32m     31\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation Loss: \u001b[39m\u001b[38;5;132;01m{:.4f}\u001b[39;00m\u001b[38;5;124m | \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(valid_loss\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(validloader)),\n\u001b[1;32m     32\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation Accuracy: \u001b[39m\u001b[38;5;132;01m{:.4f}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(accuracy\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(validloader)))\n\u001b[1;32m     34\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n","Cell \u001b[0;32mIn[54], line 5\u001b[0m, in \u001b[0;36mvalidation\u001b[0;34m(model, testloader, criterion, device)\u001b[0m\n\u001b[1;32m      2\u001b[0m test_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      3\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ii, (inputs, labels) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(testloader):\n\u001b[1;32m      7\u001b[0m     inputs, labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      9\u001b[0m     output \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mforward(inputs)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataset.py:364\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 364\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataset.py:364\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 364\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/datasets/folder.py:231\u001b[0m, in \u001b[0;36mDatasetFolder.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    229\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader(path)\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 231\u001b[0m     sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/transforms/transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[1;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:166\u001b[0m, in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;66;03m# handle PIL Image\u001b[39;00m\n\u001b[1;32m    165\u001b[0m mode_to_nptype \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mint32, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI;16\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mint16, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mfloat32}\n\u001b[0;32m--> 166\u001b[0m img \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode_to_nptype\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muint8\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m)\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pic\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    169\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m255\u001b[39m \u001b[38;5;241m*\u001b[39m img\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/PIL/Image.py:696\u001b[0m, in \u001b[0;36mImage.__array_interface__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    694\u001b[0m         new[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtobytes(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    695\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 696\u001b[0m         new[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtobytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    698\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, (\u001b[38;5;167;01mMemoryError\u001b[39;00m, \u001b[38;5;167;01mRecursionError\u001b[39;00m)):\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/PIL/Image.py:767\u001b[0m, in \u001b[0;36mImage.tobytes\u001b[0;34m(self, encoder_name, *args)\u001b[0m\n\u001b[1;32m    765\u001b[0m output \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    766\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 767\u001b[0m     bytes_consumed, errcode, data \u001b[38;5;241m=\u001b[39m \u001b[43me\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbufsize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    768\u001b[0m     output\u001b[38;5;241m.\u001b[39mappend(data)\n\u001b[1;32m    769\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errcode:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"model_prepared = train(model_prepared,trainloader, validloader,criterion, optimizer,device,epochs,steps,print_every)","metadata":{"execution":{"iopub.status.busy":"2024-04-09T10:26:41.953544Z","iopub.execute_input":"2024-04-09T10:26:41.953940Z","iopub.status.idle":"2024-04-09T11:07:54.480585Z","shell.execute_reply.started":"2024-04-09T10:26:41.953910Z","shell.execute_reply":"2024-04-09T11:07:54.479587Z"},"trusted":true},"execution_count":96,"outputs":[{"name":"stdout","text":"Training process initializing .....\n\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/ao/quantization/fake_quantize.py:343: UserWarning: _aminmax is deprecated as of PyTorch 1.11 and will be removed in a future release. Use aminmax instead. This warning will only appear once per process. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/native/ReduceAllOps.cpp:72.)\n  return torch.fused_moving_avg_obs_fake_quant(\n/opt/conda/lib/python3.10/site-packages/torch/ao/quantization/fake_quantize.py:343: UserWarning: _aminmax is deprecated as of PyTorch 1.11 and will be removed in a future release. Use aminmax instead. This warning will only appear once per process. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/native/TensorCompare.cpp:677.)\n  return torch.fused_moving_avg_obs_fake_quant(\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 1/5 |  Training Loss: 0.6408 |  Validation Loss: 0.6315 |  Validation Accuracy: 0.7904\nEpoch: 1/5 |  Training Loss: 0.5452 |  Validation Loss: 0.5218 |  Validation Accuracy: 0.8281\nEpoch: 1/5 |  Training Loss: 0.4965 |  Validation Loss: 0.5124 |  Validation Accuracy: 0.8411\nEpoch: 1/5 |  Training Loss: 0.5037 |  Validation Loss: 0.5040 |  Validation Accuracy: 0.8433\nEpoch: 1/5 |  Training Loss: 0.4824 |  Validation Loss: 0.4948 |  Validation Accuracy: 0.8444\nEpoch: 1/5 |  Training Loss: 0.4687 |  Validation Loss: 0.5034 |  Validation Accuracy: 0.8485\nEpoch: 1/5 |  Training Loss: 0.6807 |  Validation Loss: 1.4846 |  Validation Accuracy: 0.5500\nEpoch: 1/5 |  Training Loss: 1.4092 |  Validation Loss: 1.4152 |  Validation Accuracy: 0.5915\nEpoch: 1/5 |  Training Loss: 1.4204 |  Validation Loss: 1.3738 |  Validation Accuracy: 0.6363\nEpoch: 1/5 |  Training Loss: 1.3802 |  Validation Loss: 1.3632 |  Validation Accuracy: 0.6574\nEpoch: 1/5 |  Training Loss: 1.3781 |  Validation Loss: 1.3321 |  Validation Accuracy: 0.6444\nEpoch: 1/5 |  Training Loss: 1.3132 |  Validation Loss: 1.3171 |  Validation Accuracy: 0.6544\nEpoch: 2/5 |  Training Loss: 0.5191 |  Validation Loss: 1.3038 |  Validation Accuracy: 0.6726\nEpoch: 2/5 |  Training Loss: 1.2894 |  Validation Loss: 1.2796 |  Validation Accuracy: 0.6715\nEpoch: 2/5 |  Training Loss: 1.2639 |  Validation Loss: 1.2243 |  Validation Accuracy: 0.6804\nEpoch: 2/5 |  Training Loss: 1.2138 |  Validation Loss: 1.2116 |  Validation Accuracy: 0.6985\nEpoch: 2/5 |  Training Loss: 1.2476 |  Validation Loss: 1.2210 |  Validation Accuracy: 0.6819\nEpoch: 2/5 |  Training Loss: 1.1957 |  Validation Loss: 1.2026 |  Validation Accuracy: 0.6874\nEpoch: 2/5 |  Training Loss: 1.2172 |  Validation Loss: 1.2209 |  Validation Accuracy: 0.6711\nEpoch: 2/5 |  Training Loss: 1.1948 |  Validation Loss: 1.1768 |  Validation Accuracy: 0.6926\nEpoch: 2/5 |  Training Loss: 1.2056 |  Validation Loss: 1.1433 |  Validation Accuracy: 0.7044\nEpoch: 2/5 |  Training Loss: 1.1686 |  Validation Loss: 1.1603 |  Validation Accuracy: 0.6985\nEpoch: 2/5 |  Training Loss: 1.1690 |  Validation Loss: 1.1665 |  Validation Accuracy: 0.6852\nEpoch: 2/5 |  Training Loss: 1.1710 |  Validation Loss: 1.1419 |  Validation Accuracy: 0.6870\nEpoch: 2/5 |  Training Loss: 1.1395 |  Validation Loss: 1.1258 |  Validation Accuracy: 0.6967\nEpoch: 3/5 |  Training Loss: 0.8854 |  Validation Loss: 1.1249 |  Validation Accuracy: 0.7052\nEpoch: 3/5 |  Training Loss: 1.0849 |  Validation Loss: 1.1032 |  Validation Accuracy: 0.7022\nEpoch: 3/5 |  Training Loss: 1.1175 |  Validation Loss: 1.0907 |  Validation Accuracy: 0.7167\nEpoch: 3/5 |  Training Loss: 1.1203 |  Validation Loss: 1.0948 |  Validation Accuracy: 0.7141\nEpoch: 3/5 |  Training Loss: 1.0675 |  Validation Loss: 1.0894 |  Validation Accuracy: 0.7070\nEpoch: 3/5 |  Training Loss: 1.0629 |  Validation Loss: 1.0820 |  Validation Accuracy: 0.6993\nEpoch: 3/5 |  Training Loss: 1.0990 |  Validation Loss: 1.0802 |  Validation Accuracy: 0.7111\nEpoch: 3/5 |  Training Loss: 1.0690 |  Validation Loss: 1.0743 |  Validation Accuracy: 0.7070\nEpoch: 3/5 |  Training Loss: 1.0786 |  Validation Loss: 1.0600 |  Validation Accuracy: 0.7252\nEpoch: 3/5 |  Training Loss: 1.0658 |  Validation Loss: 1.0334 |  Validation Accuracy: 0.7148\nEpoch: 3/5 |  Training Loss: 1.0267 |  Validation Loss: 1.0270 |  Validation Accuracy: 0.7256\nEpoch: 3/5 |  Training Loss: 1.0488 |  Validation Loss: 1.0434 |  Validation Accuracy: 0.7122\nEpoch: 4/5 |  Training Loss: 0.2129 |  Validation Loss: 1.0170 |  Validation Accuracy: 0.7322\nEpoch: 4/5 |  Training Loss: 1.0149 |  Validation Loss: 1.0496 |  Validation Accuracy: 0.7111\nEpoch: 4/5 |  Training Loss: 1.0381 |  Validation Loss: 1.0287 |  Validation Accuracy: 0.7230\nEpoch: 4/5 |  Training Loss: 1.0003 |  Validation Loss: 0.9982 |  Validation Accuracy: 0.7233\nEpoch: 4/5 |  Training Loss: 0.9834 |  Validation Loss: 1.0073 |  Validation Accuracy: 0.7263\nEpoch: 4/5 |  Training Loss: 0.9869 |  Validation Loss: 0.9920 |  Validation Accuracy: 0.7281\nEpoch: 4/5 |  Training Loss: 1.0409 |  Validation Loss: 0.9903 |  Validation Accuracy: 0.7226\nEpoch: 4/5 |  Training Loss: 1.0117 |  Validation Loss: 0.9944 |  Validation Accuracy: 0.7222\nEpoch: 4/5 |  Training Loss: 0.9506 |  Validation Loss: 0.9635 |  Validation Accuracy: 0.7404\nEpoch: 4/5 |  Training Loss: 0.9824 |  Validation Loss: 0.9971 |  Validation Accuracy: 0.7307\nEpoch: 4/5 |  Training Loss: 0.9754 |  Validation Loss: 0.9806 |  Validation Accuracy: 0.7296\nEpoch: 4/5 |  Training Loss: 0.9948 |  Validation Loss: 0.9529 |  Validation Accuracy: 0.7400\nEpoch: 4/5 |  Training Loss: 0.9535 |  Validation Loss: 0.9654 |  Validation Accuracy: 0.7252\nEpoch: 5/5 |  Training Loss: 0.5841 |  Validation Loss: 0.9683 |  Validation Accuracy: 0.7304\nEpoch: 5/5 |  Training Loss: 0.9773 |  Validation Loss: 0.9589 |  Validation Accuracy: 0.7259\nEpoch: 5/5 |  Training Loss: 0.9992 |  Validation Loss: 0.9673 |  Validation Accuracy: 0.7281\nEpoch: 5/5 |  Training Loss: 0.9496 |  Validation Loss: 0.9666 |  Validation Accuracy: 0.7348\nEpoch: 5/5 |  Training Loss: 0.9606 |  Validation Loss: 0.9351 |  Validation Accuracy: 0.7489\nEpoch: 5/5 |  Training Loss: 0.9284 |  Validation Loss: 0.9425 |  Validation Accuracy: 0.7348\nEpoch: 5/5 |  Training Loss: 0.9542 |  Validation Loss: 0.9593 |  Validation Accuracy: 0.7281\nEpoch: 5/5 |  Training Loss: 0.9455 |  Validation Loss: 0.9516 |  Validation Accuracy: 0.7256\nEpoch: 5/5 |  Training Loss: 0.9770 |  Validation Loss: 0.9392 |  Validation Accuracy: 0.7407\nEpoch: 5/5 |  Training Loss: 0.9150 |  Validation Loss: 0.9330 |  Validation Accuracy: 0.7356\nEpoch: 5/5 |  Training Loss: 0.9614 |  Validation Loss: 0.9400 |  Validation Accuracy: 0.7281\nEpoch: 5/5 |  Training Loss: 0.9351 |  Validation Loss: 0.9200 |  Validation Accuracy: 0.7385\nEpoch: 5/5 |  Training Loss: 0.9421 |  Validation Loss: 0.9199 |  Validation Accuracy: 0.7470\n\nTraining process is now complete!!\n","output_type":"stream"}]},{"cell_type":"code","source":"model_prepared.to('cpu')","metadata":{"execution":{"iopub.status.busy":"2024-04-09T11:07:54.482404Z","iopub.execute_input":"2024-04-09T11:07:54.482810Z","iopub.status.idle":"2024-04-09T11:07:55.378974Z","shell.execute_reply.started":"2024-04-09T11:07:54.482776Z","shell.execute_reply":"2024-04-09T11:07:55.377992Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":97,"outputs":[{"execution_count":97,"output_type":"execute_result","data":{"text/plain":"GraphModule(\n  (activation_post_process_0): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0347]), zero_point=tensor([51], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.7678688764572144, max_val=2.6396148204803467)\n  )\n  (features): Module(\n    (0): Module(\n      (0): Conv2d(\n        3, 96, kernel_size=(4, 4), stride=(4, 4)\n        (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.0455e-03, 8.8132e-04, 8.6979e-04, 1.1817e-03, 1.0981e-03, 1.4716e-03,\n                  8.0483e-05, 1.7205e-03, 5.9163e-04, 9.9396e-04, 9.2219e-04, 1.3138e-03,\n                  1.6544e-03, 8.4335e-04, 7.4061e-04, 1.5462e-03, 6.3502e-04, 5.8783e-04,\n                  2.4384e-04, 1.5799e-04, 9.6865e-04, 1.0631e-03, 2.0456e-04, 6.4129e-04,\n                  1.3318e-03, 1.1956e-03, 4.3279e-04, 4.7040e-04, 7.2337e-04, 9.0019e-04,\n                  8.7883e-04, 5.6740e-04, 8.6941e-04, 1.4106e-03, 8.6612e-04, 1.0445e-03,\n                  8.6306e-04, 7.3508e-04, 7.7705e-04, 9.0973e-04, 1.9801e-04, 1.0246e-03,\n                  8.6096e-04, 1.2455e-03, 1.4269e-03, 1.1426e-03, 8.9326e-04, 1.2847e-03,\n                  6.2189e-04, 1.1412e-03, 8.2785e-04, 3.8153e-04, 5.8858e-04, 1.2192e-03,\n                  8.8270e-04, 7.8393e-04, 9.2907e-04, 9.1326e-04, 1.0193e-03, 7.8280e-04,\n                  8.9653e-04, 1.0671e-03, 1.4821e-03, 8.6049e-04, 1.5297e-03, 1.0162e-03,\n                  1.6200e-04, 4.9421e-04, 1.0101e-03, 6.4159e-04, 1.5044e-03, 5.9732e-04,\n                  8.3157e-04, 3.9693e-04, 8.2594e-04, 7.7388e-04, 8.1066e-04, 1.2200e-03,\n                  1.4486e-03, 1.4611e-03, 4.0893e-04, 1.3145e-03, 1.3371e-03, 1.1184e-03,\n                  8.0215e-04, 6.8980e-04, 2.2729e-04, 5.7469e-04, 1.0924e-04, 7.0030e-04,\n                  7.9366e-04, 6.9731e-04, 8.1100e-04, 1.6816e-03, 1.0374e-03, 8.9845e-04]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                 dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n          (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n            min_val=tensor([-0.1258, -0.1128, -0.1113, -0.1513, -0.1406, -0.1884, -0.0103, -0.1198,\n                    -0.0403, -0.1272, -0.1180, -0.1682, -0.1423, -0.0968, -0.0525, -0.1979,\n                    -0.0778, -0.0607, -0.0312, -0.0202, -0.1240, -0.1361, -0.0059, -0.0514,\n                    -0.1705, -0.1058, -0.0253, -0.0374, -0.0926, -0.1152, -0.1125, -0.0726,\n                    -0.1052, -0.1806, -0.1089, -0.1337, -0.0777, -0.0858, -0.0995, -0.1164,\n                    -0.0253, -0.1311, -0.0844, -0.1594, -0.1000, -0.1463, -0.1143, -0.1618,\n                    -0.0728, -0.1314, -0.1060, -0.0488, -0.0753, -0.1246, -0.1130, -0.1003,\n                    -0.1189, -0.1131, -0.0987, -0.1002, -0.1060, -0.1022, -0.0894, -0.1101,\n                    -0.1958, -0.0984, -0.0207, -0.0633, -0.1293, -0.0821, -0.1926, -0.0419,\n                    -0.1064, -0.0508, -0.1057, -0.0868, -0.1038, -0.1108, -0.1381, -0.1870,\n                    -0.0464, -0.1464, -0.1184, -0.1432, -0.1027, -0.0883, -0.0291, -0.0698,\n                    -0.0140, -0.0896, -0.1016, -0.0893, -0.1038, -0.2152, -0.1328, -0.1070]), max_val=tensor([0.1328, 0.0895, 0.1074, 0.1089, 0.0871, 0.1443, 0.0082, 0.2185, 0.0751,\n                    0.1056, 0.1022, 0.1227, 0.2101, 0.1071, 0.0941, 0.0609, 0.0806, 0.0747,\n                    0.0067, 0.0105, 0.0987, 0.0853, 0.0260, 0.0814, 0.1089, 0.1518, 0.0550,\n                    0.0597, 0.0889, 0.0763, 0.0879, 0.0720, 0.1104, 0.1495, 0.1100, 0.1061,\n                    0.1096, 0.0934, 0.0861, 0.1071, 0.0078, 0.1028, 0.1093, 0.1414, 0.1812,\n                    0.0986, 0.0929, 0.1632, 0.0790, 0.1449, 0.1031, 0.0457, 0.0703, 0.1548,\n                    0.0996, 0.0743, 0.0957, 0.1160, 0.1295, 0.0933, 0.1139, 0.1355, 0.1882,\n                    0.0928, 0.1845, 0.1291, 0.0090, 0.0590, 0.0614, 0.0767, 0.1157, 0.0759,\n                    0.0963, 0.0484, 0.1024, 0.0983, 0.0796, 0.1549, 0.1840, 0.1455, 0.0519,\n                    0.1669, 0.1698, 0.1337, 0.0722, 0.0822, 0.0223, 0.0730, 0.0133, 0.0704,\n                    0.0984, 0.0770, 0.0992, 0.1759, 0.1081, 0.1141])\n          )\n        )\n      )\n      (1): Module()\n    )\n    (1): Module(\n      (0): Module(\n        (block): Module(\n          (0): Conv2d(\n            96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([4.8501e-03, 4.4650e-03, 3.1704e-03, 3.0663e-03, 2.6425e-03, 1.1395e-03,\n                      1.1965e-03, 4.0018e-03, 1.0573e-04, 2.6788e-03, 3.0864e-03, 2.7441e-04,\n                      2.8020e-03, 1.7160e-03, 3.1027e-03, 3.5288e-03, 2.8921e-03, 2.8028e-03,\n                      2.1473e-03, 3.6461e-04, 2.4699e-03, 3.6556e-03, 1.8559e-03, 6.7027e-05,\n                      1.9542e-03, 2.9339e-03, 2.3967e-03, 2.5305e-03, 5.2983e-03, 1.7425e-03,\n                      4.0197e-03, 2.4349e-03, 3.5159e-03, 3.4394e-04, 3.7036e-03, 3.9601e-03,\n                      2.8925e-03, 1.7842e-03, 5.4639e-03, 2.4884e-03, 1.8765e-03, 3.1720e-03,\n                      2.8640e-03, 3.0840e-04, 4.2905e-03, 5.4097e-04, 6.5954e-04, 8.0638e-04,\n                      2.3077e-03, 8.3587e-04, 2.5303e-03, 3.1387e-03, 2.2885e-03, 2.7463e-04,\n                      2.9617e-03, 3.7353e-03, 3.0982e-03, 4.1804e-03, 1.1999e-03, 4.4531e-03,\n                      1.0581e-03, 3.5559e-03, 5.1136e-03, 2.3825e-03, 4.0663e-03, 9.6317e-04,\n                      1.5869e-03, 2.2338e-03, 3.9353e-03, 2.9483e-03, 4.2394e-03, 1.3738e-03,\n                      1.2719e-03, 2.5142e-03, 1.1791e-03, 3.3434e-03, 3.0350e-03, 3.1183e-03,\n                      4.7750e-03, 3.0621e-03, 1.1832e-03, 2.8737e-03, 3.8193e-03, 1.5386e-03,\n                      4.2834e-04, 1.8597e-03, 2.4172e-03, 2.0792e-03, 2.0099e-03, 2.2227e-03,\n                      2.2760e-03, 2.1942e-03, 3.5565e-03, 1.2337e-03, 4.0415e-03, 3.5943e-03]), zero_point=tensor([   0,    0,    0,    0,    0, -128,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0, -128,  127,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                       127,    0,    0,    0,    0, -128,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n                     dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n                min_val=tensor([-0.0426, -0.5715, -0.4058, -0.1495, -0.3382,  0.0424, -0.1205, -0.5122,\n                        -0.0079, -0.3429, -0.3951, -0.0349, -0.0798, -0.2197, -0.0543, -0.4517,\n                        -0.0049, -0.3588, -0.2749, -0.0467, -0.3162, -0.4679, -0.2376, -0.0076,\n                        -0.2501, -0.3755, -0.1676, -0.3239, -0.6782, -0.2230, -0.0283, -0.0258,\n                        -0.4500, -0.0292, -0.4741, -0.0344, -0.0087, -0.2284, -0.6994, -0.3185,\n                        -0.2090, -0.0112, -0.3666, -0.0374, -0.5492,  0.0660, -0.1682, -0.0320,\n                        -0.1989, -0.0305, -0.0091, -0.1439, -0.2711, -0.0352, -0.3791, -0.4781,\n                        -0.3966, -0.1049, -0.1536, -0.0810, -0.2698, -0.4552, -0.6545, -0.3050,\n                        -0.5205,  0.0409, -0.2031, -0.2859, -0.2344, -0.3774, -0.5426, -0.1385,\n                        -0.0624, -0.3218, -0.1509, -0.4279, -0.3885, -0.1117, -0.0576, -0.3919,\n                        -0.1514, -0.0545, -0.1833, -0.0382, -0.0187, -0.0434, -0.0435, -0.2661,\n                        -0.2573, -0.2845, -0.2913, -0.0443, -0.0318, -0.0914, -0.0179, -0.4601]), max_val=tensor([ 0.6160,  0.0191,  0.0191,  0.3894,  0.0149,  0.2906,  0.1520,  0.1280,\n                         0.0134,  0.0132,  0.0227,  0.0348,  0.3559,  0.1022,  0.3940,  0.1289,\n                         0.3673,  0.2858,  0.1847,  0.0323,  0.0161,  0.1062,  0.0157,  0.0085,\n                         0.0055,  0.0175,  0.3044,  0.2647,  0.1681,  0.0052,  0.5105,  0.3092,\n                         0.0315,  0.0437,  0.0377,  0.5029,  0.3673,  0.0056,  0.0904,  0.0560,\n                         0.2383,  0.4028,  0.0103,  0.0392,  0.1769,  0.1379, -0.0452,  0.1024,\n                         0.2931,  0.1062,  0.3214,  0.3986,  0.2906,  0.0311,  0.0185,  0.0805,\n                         0.1397,  0.5309,  0.0763,  0.5655, -0.0433,  0.0462,  0.1586,  0.0107,\n                         0.1684,  0.2456,  0.0832,  0.1963,  0.4998,  0.0134,  0.1778,  0.1745,\n                         0.1615,  0.1258,  0.1350,  0.0518,  0.0597,  0.3960,  0.6064,  0.1293,\n                         0.1259,  0.3650,  0.4851,  0.1954,  0.0544,  0.2362,  0.3070,  0.0347,\n                         0.1348,  0.2065,  0.0392,  0.2787,  0.4517,  0.1567,  0.5133,  0.0326])\n              )\n            )\n          )\n          (2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(\n            in_features=96, out_features=384, bias=True\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0009, 0.0016, 0.0016, 0.0019, 0.0027, 0.0017, 0.0014, 0.0013, 0.0014,\n                      0.0015, 0.0022, 0.0025, 0.0012, 0.0029, 0.0032, 0.0010, 0.0011, 0.0019,\n                      0.0021, 0.0015, 0.0010, 0.0018, 0.0019, 0.0009, 0.0014, 0.0015, 0.0029,\n                      0.0013, 0.0013, 0.0014, 0.0014, 0.0025, 0.0015, 0.0012, 0.0021, 0.0023,\n                      0.0014, 0.0029, 0.0010, 0.0026, 0.0021, 0.0016, 0.0014, 0.0015, 0.0014,\n                      0.0008, 0.0027, 0.0019, 0.0012, 0.0011, 0.0011, 0.0023, 0.0014, 0.0011,\n                      0.0011, 0.0012, 0.0011, 0.0019, 0.0014, 0.0013, 0.0026, 0.0015, 0.0021,\n                      0.0013, 0.0023, 0.0010, 0.0017, 0.0014, 0.0022, 0.0021, 0.0013, 0.0015,\n                      0.0020, 0.0015, 0.0022, 0.0019, 0.0023, 0.0011, 0.0016, 0.0018, 0.0010,\n                      0.0011, 0.0028, 0.0019, 0.0024, 0.0013, 0.0012, 0.0013, 0.0021, 0.0016,\n                      0.0014, 0.0021, 0.0024, 0.0024, 0.0022, 0.0023, 0.0010, 0.0015, 0.0008,\n                      0.0014, 0.0020, 0.0012, 0.0022, 0.0010, 0.0040, 0.0019, 0.0040, 0.0017,\n                      0.0018, 0.0023, 0.0017, 0.0023, 0.0014, 0.0013, 0.0023, 0.0024, 0.0016,\n                      0.0018, 0.0013, 0.0020, 0.0018, 0.0009, 0.0008, 0.0013, 0.0011, 0.0015,\n                      0.0014, 0.0015, 0.0027, 0.0013, 0.0010, 0.0020, 0.0014, 0.0031, 0.0014,\n                      0.0014, 0.0012, 0.0017, 0.0025, 0.0022, 0.0008, 0.0032, 0.0025, 0.0012,\n                      0.0021, 0.0016, 0.0013, 0.0024, 0.0027, 0.0010, 0.0019, 0.0022, 0.0015,\n                      0.0022, 0.0021, 0.0012, 0.0014, 0.0019, 0.0010, 0.0010, 0.0012, 0.0013,\n                      0.0027, 0.0012, 0.0009, 0.0015, 0.0024, 0.0020, 0.0026, 0.0011, 0.0011,\n                      0.0014, 0.0013, 0.0012, 0.0028, 0.0013, 0.0023, 0.0012, 0.0018, 0.0020,\n                      0.0010, 0.0012, 0.0008, 0.0014, 0.0011, 0.0022, 0.0016, 0.0014, 0.0013,\n                      0.0011, 0.0022, 0.0015, 0.0019, 0.0012, 0.0011, 0.0024, 0.0020, 0.0017,\n                      0.0020, 0.0027, 0.0009, 0.0020, 0.0021, 0.0011, 0.0013, 0.0028, 0.0013,\n                      0.0014, 0.0022, 0.0014, 0.0015, 0.0022, 0.0010, 0.0011, 0.0017, 0.0012,\n                      0.0028, 0.0023, 0.0013, 0.0020, 0.0011, 0.0016, 0.0017, 0.0024, 0.0009,\n                      0.0018, 0.0012, 0.0014, 0.0017, 0.0010, 0.0017, 0.0025, 0.0016, 0.0016,\n                      0.0016, 0.0013, 0.0015, 0.0011, 0.0024, 0.0021, 0.0014, 0.0028, 0.0019,\n                      0.0014, 0.0015, 0.0028, 0.0010, 0.0019, 0.0008, 0.0014, 0.0023, 0.0025,\n                      0.0013, 0.0020, 0.0013, 0.0022, 0.0013, 0.0011, 0.0013, 0.0014, 0.0014,\n                      0.0011, 0.0012, 0.0028, 0.0018, 0.0011, 0.0015, 0.0009, 0.0021, 0.0020,\n                      0.0014, 0.0021, 0.0023, 0.0032, 0.0012, 0.0016, 0.0014, 0.0012, 0.0014,\n                      0.0017, 0.0011, 0.0021, 0.0020, 0.0024, 0.0019, 0.0015, 0.0009, 0.0011,\n                      0.0014, 0.0010, 0.0014, 0.0011, 0.0016, 0.0014, 0.0010, 0.0010, 0.0009,\n                      0.0015, 0.0010, 0.0013, 0.0011, 0.0020, 0.0011, 0.0023, 0.0024, 0.0015,\n                      0.0020, 0.0023, 0.0020, 0.0026, 0.0021, 0.0015, 0.0014, 0.0009, 0.0015,\n                      0.0012, 0.0014, 0.0012, 0.0015, 0.0013, 0.0015, 0.0024, 0.0017, 0.0009,\n                      0.0012, 0.0015, 0.0022, 0.0012, 0.0009, 0.0018, 0.0013, 0.0013, 0.0019,\n                      0.0021, 0.0020, 0.0023, 0.0011, 0.0019, 0.0018, 0.0011, 0.0015, 0.0008,\n                      0.0018, 0.0029, 0.0023, 0.0014, 0.0019, 0.0030, 0.0021, 0.0013, 0.0011,\n                      0.0013, 0.0012, 0.0020, 0.0029, 0.0024, 0.0010, 0.0024, 0.0016, 0.0015,\n                      0.0015, 0.0020, 0.0016, 0.0021, 0.0013, 0.0012, 0.0010, 0.0013, 0.0010,\n                      0.0016, 0.0008, 0.0012, 0.0012, 0.0021, 0.0027, 0.0017, 0.0015, 0.0010,\n                      0.0017, 0.0015, 0.0016, 0.0012, 0.0019, 0.0014]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                     dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n                min_val=tensor([-0.1133, -0.1989, -0.2038, -0.2427, -0.3508, -0.0950, -0.1741, -0.1628,\n                        -0.1828, -0.1973, -0.2862, -0.3167, -0.1547, -0.1866, -0.2541, -0.1276,\n                        -0.1101, -0.1696, -0.1581, -0.1515, -0.1221, -0.2241, -0.1683, -0.1095,\n                        -0.1449, -0.1887, -0.3172, -0.1671, -0.1693, -0.1796, -0.1780, -0.3199,\n                        -0.1918, -0.1486, -0.1674, -0.1870, -0.1838, -0.2625, -0.1047, -0.3337,\n                        -0.2187, -0.2077, -0.1824, -0.1921, -0.1813, -0.1044, -0.2870, -0.2486,\n                        -0.1534, -0.1413, -0.1389, -0.1862, -0.1160, -0.1351, -0.1427, -0.1367,\n                        -0.1434, -0.1930, -0.1823, -0.1308, -0.1925, -0.1884, -0.2713, -0.1433,\n                        -0.2072, -0.1274, -0.1621, -0.1742, -0.2811, -0.2453, -0.1624, -0.1628,\n                        -0.2570, -0.1868, -0.2755, -0.2378, -0.2302, -0.1426, -0.2094, -0.2267,\n                        -0.1233, -0.1406, -0.3102, -0.2423, -0.2679, -0.1601, -0.1549, -0.1638,\n                        -0.1832, -0.2107, -0.1804, -0.2049, -0.1407, -0.3122, -0.2611, -0.2705,\n                        -0.1298, -0.1903, -0.1009, -0.1781, -0.1852, -0.1503, -0.2722, -0.1220,\n                        -0.3152, -0.2431, -0.5137, -0.1654, -0.2260, -0.2554, -0.1775, -0.1566,\n                        -0.1750, -0.1603, -0.2925, -0.1681, -0.1851, -0.1646, -0.1204, -0.2605,\n                        -0.1984, -0.1201, -0.1008, -0.1618, -0.1373, -0.1176, -0.1826, -0.0973,\n                        -0.3404, -0.1696, -0.1290, -0.2434, -0.1455, -0.1195, -0.1772, -0.1783,\n                        -0.1472, -0.2141, -0.1738, -0.2735, -0.1015, -0.4048, -0.3222, -0.1589,\n                        -0.2169, -0.2106, -0.1403, -0.2509, -0.3394, -0.1298, -0.2149, -0.2801,\n                        -0.1262, -0.2816, -0.1828, -0.1496, -0.1821, -0.2454, -0.1289, -0.1268,\n                        -0.0984, -0.1201, -0.3425, -0.1593, -0.1121, -0.1925, -0.3064, -0.2533,\n                        -0.3339, -0.1378, -0.1280, -0.1711, -0.1202, -0.1588, -0.3606, -0.1627,\n                        -0.3002, -0.1380, -0.2317, -0.2554, -0.1277, -0.1592, -0.0974, -0.1647,\n                        -0.1192, -0.1394, -0.1549, -0.1688, -0.1632, -0.1403, -0.2767, -0.1981,\n                        -0.1819, -0.1541, -0.1396, -0.2777, -0.2011, -0.2219, -0.2498, -0.3479,\n                        -0.1202, -0.1488, -0.2742, -0.1348, -0.1465, -0.3531, -0.1636, -0.1458,\n                        -0.2800, -0.1766, -0.0915, -0.2722, -0.1223, -0.1379, -0.2199, -0.1547,\n                        -0.0741, -0.1705, -0.1533, -0.2521, -0.1391, -0.2097, -0.2211, -0.3105,\n                        -0.1151, -0.2305, -0.1516, -0.1828, -0.2196, -0.1239, -0.1455, -0.3141,\n                        -0.2101, -0.2082, -0.2096, -0.1254, -0.1901, -0.1397, -0.2758, -0.2730,\n                        -0.1748, -0.2662, -0.1527, -0.1771, -0.1962, -0.3563, -0.1294, -0.1401,\n                        -0.1023, -0.1851, -0.2977, -0.1470, -0.1704, -0.1980, -0.1634, -0.2777,\n                        -0.1666, -0.1450, -0.1624, -0.1537, -0.1790, -0.1454, -0.1527, -0.3643,\n                        -0.2304, -0.1391, -0.1900, -0.1144, -0.2715, -0.1569, -0.1159, -0.2620,\n                        -0.2261, -0.4111, -0.1569, -0.2026, -0.1847, -0.1518, -0.1789, -0.0955,\n                        -0.1291, -0.2138, -0.2329, -0.3106, -0.2363, -0.1724, -0.1126, -0.1130,\n                        -0.1628, -0.1285, -0.1796, -0.1248, -0.1195, -0.1794, -0.1217, -0.1054,\n                        -0.1154, -0.1556, -0.1340, -0.1612, -0.1345, -0.2547, -0.1403, -0.2996,\n                        -0.1347, -0.1911, -0.1845, -0.1706, -0.2506, -0.1483, -0.2598, -0.1909,\n                        -0.1753, -0.1165, -0.1866, -0.1532, -0.1773, -0.1483, -0.1898, -0.1700,\n                        -0.1753, -0.3047, -0.2169, -0.1164, -0.1549, -0.1871, -0.2425, -0.1580,\n                        -0.1111, -0.1638, -0.1656, -0.1724, -0.1729, -0.2662, -0.2540, -0.2947,\n                        -0.1387, -0.2338, -0.2299, -0.1386, -0.1899, -0.0998, -0.2293, -0.2397,\n                        -0.2928, -0.1307, -0.2475, -0.3805, -0.2644, -0.1691, -0.1389, -0.1605,\n                        -0.1594, -0.1947, -0.3730, -0.0922, -0.1088, -0.3121, -0.1889, -0.1926,\n                        -0.1965, -0.2587, -0.1119, -0.2697, -0.1651, -0.1499, -0.1214, -0.1613,\n                        -0.1238, -0.2066, -0.1082, -0.1550, -0.1425, -0.2625, -0.1870, -0.2145,\n                        -0.1907, -0.1283, -0.2115, -0.1726, -0.2047, -0.1525, -0.2414, -0.1534]), max_val=tensor([0.1003, 0.1048, 0.0760, 0.2344, 0.2522, 0.2114, 0.0988, 0.1586, 0.1220,\n                        0.0687, 0.2247, 0.2626, 0.1433, 0.3741, 0.4034, 0.1127, 0.1383, 0.2375,\n                        0.2638, 0.1928, 0.1185, 0.1249, 0.2378, 0.0940, 0.1780, 0.1180, 0.3707,\n                        0.1100, 0.1227, 0.1394, 0.1499, 0.1697, 0.1893, 0.1497, 0.2723, 0.2933,\n                        0.1332, 0.3710, 0.1325, 0.2718, 0.2624, 0.1512, 0.1455, 0.1357, 0.1292,\n                        0.1068, 0.3392, 0.1696, 0.1100, 0.1058, 0.0997, 0.2879, 0.1715, 0.1086,\n                        0.0937, 0.1484, 0.1304, 0.2448, 0.1110, 0.1669, 0.3332, 0.1809, 0.1597,\n                        0.1688, 0.2910, 0.0970, 0.2097, 0.1222, 0.1511, 0.2683, 0.0994, 0.1939,\n                        0.1716, 0.1406, 0.2158, 0.1920, 0.2907, 0.0823, 0.1987, 0.2045, 0.1159,\n                        0.1118, 0.3513, 0.1130, 0.3081, 0.1058, 0.1585, 0.1048, 0.2616, 0.1188,\n                        0.1769, 0.2607, 0.3015, 0.2106, 0.2790, 0.2960, 0.0989, 0.1920, 0.1002,\n                        0.1464, 0.2589, 0.0944, 0.2767, 0.0881, 0.5124, 0.1714, 0.2074, 0.2102,\n                        0.2126, 0.2927, 0.2195, 0.2908, 0.1730, 0.1512, 0.1063, 0.3063, 0.2047,\n                        0.2274, 0.1696, 0.2260, 0.2229, 0.1043, 0.0938, 0.1238, 0.1181, 0.1884,\n                        0.0930, 0.1960, 0.3164, 0.1179, 0.1295, 0.2562, 0.1809, 0.3953, 0.1024,\n                        0.1548, 0.1290, 0.0969, 0.3139, 0.2813, 0.1020, 0.2704, 0.2401, 0.1135,\n                        0.2631, 0.1988, 0.1708, 0.3052, 0.2018, 0.1276, 0.2450, 0.1730, 0.1948,\n                        0.2217, 0.2620, 0.1141, 0.1537, 0.1333, 0.0995, 0.1294, 0.1470, 0.1590,\n                        0.1454, 0.1385, 0.1015, 0.1479, 0.1935, 0.1393, 0.1516, 0.1459, 0.1376,\n                        0.1719, 0.1711, 0.1378, 0.2929, 0.1038, 0.1824, 0.1570, 0.2131, 0.1579,\n                        0.1021, 0.1582, 0.1064, 0.1735, 0.1418, 0.2763, 0.1981, 0.1805, 0.1588,\n                        0.1046, 0.2307, 0.1289, 0.2390, 0.1421, 0.1371, 0.3073, 0.2483, 0.2197,\n                        0.1870, 0.1296, 0.1041, 0.2511, 0.2570, 0.0885, 0.1599, 0.0846, 0.0656,\n                        0.1822, 0.1511, 0.1155, 0.1879, 0.2836, 0.0980, 0.1062, 0.1828, 0.1242,\n                        0.3501, 0.2889, 0.1629, 0.1331, 0.1151, 0.1455, 0.1521, 0.1685, 0.1042,\n                        0.1789, 0.1127, 0.1306, 0.1005, 0.1107, 0.2209, 0.2806, 0.1936, 0.1795,\n                        0.2057, 0.1713, 0.0922, 0.1172, 0.3004, 0.2492, 0.1753, 0.3499, 0.2442,\n                        0.1001, 0.1592, 0.1830, 0.1000, 0.2453, 0.0963, 0.0920, 0.1737, 0.3187,\n                        0.1644, 0.2492, 0.1130, 0.1638, 0.1037, 0.1151, 0.1335, 0.1825, 0.1789,\n                        0.0987, 0.1214, 0.3117, 0.1372, 0.1308, 0.1393, 0.1168, 0.1091, 0.2585,\n                        0.1811, 0.2687, 0.2923, 0.1788, 0.1043, 0.1497, 0.1195, 0.1169, 0.0867,\n                        0.2138, 0.1440, 0.2666, 0.2512, 0.1866, 0.2452, 0.1939, 0.1079, 0.1348,\n                        0.1806, 0.1257, 0.1409, 0.1396, 0.2042, 0.0710, 0.1239, 0.1327, 0.1178,\n                        0.1956, 0.1282, 0.1009, 0.1039, 0.2161, 0.1362, 0.1525, 0.3017, 0.1040,\n                        0.2545, 0.2875, 0.1975, 0.3310, 0.2642, 0.1612, 0.0874, 0.1070, 0.1772,\n                        0.0791, 0.1086, 0.1209, 0.1773, 0.1654, 0.1842, 0.1701, 0.0794, 0.1009,\n                        0.1534, 0.1323, 0.2824, 0.0863, 0.1073, 0.2296, 0.1332, 0.1394, 0.2428,\n                        0.2694, 0.2575, 0.2085, 0.1024, 0.2441, 0.0796, 0.0980, 0.1202, 0.0995,\n                        0.1714, 0.3705, 0.1827, 0.1716, 0.1866, 0.3836, 0.2497, 0.1485, 0.0970,\n                        0.1371, 0.1187, 0.2488, 0.1926, 0.3066, 0.1261, 0.1589, 0.2090, 0.1081,\n                        0.1590, 0.2227, 0.2005, 0.1690, 0.1369, 0.1213, 0.1272, 0.1191, 0.1308,\n                        0.1684, 0.0995, 0.1161, 0.1570, 0.1369, 0.3449, 0.1569, 0.1745, 0.1018,\n                        0.0765, 0.1921, 0.1071, 0.1449, 0.2115, 0.1727])\n              )\n            )\n          )\n          (4): GELU(approximate='none')\n          (5): Linear(\n            in_features=384, out_features=96, bias=True\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0016, 0.0023, 0.0032, 0.0024, 0.0023, 0.0012, 0.0020, 0.0030, 0.0005,\n                      0.0019, 0.0018, 0.0024, 0.0019, 0.0034, 0.0026, 0.0028, 0.0024, 0.0010,\n                      0.0010, 0.0029, 0.0035, 0.0024, 0.0027, 0.0010, 0.0015, 0.0030, 0.0014,\n                      0.0017, 0.0016, 0.0022, 0.0012, 0.0024, 0.0019, 0.0023, 0.0021, 0.0026,\n                      0.0023, 0.0018, 0.0031, 0.0019, 0.0023, 0.0022, 0.0014, 0.0016, 0.0033,\n                      0.0014, 0.0017, 0.0017, 0.0018, 0.0033, 0.0024, 0.0023, 0.0025, 0.0018,\n                      0.0018, 0.0041, 0.0016, 0.0014, 0.0012, 0.0011, 0.0018, 0.0022, 0.0032,\n                      0.0024, 0.0035, 0.0015, 0.0006, 0.0020, 0.0019, 0.0021, 0.0029, 0.0006,\n                      0.0041, 0.0030, 0.0027, 0.0011, 0.0025, 0.0012, 0.0026, 0.0023, 0.0007,\n                      0.0033, 0.0022, 0.0038, 0.0017, 0.0016, 0.0013, 0.0011, 0.0020, 0.0019,\n                      0.0034, 0.0038, 0.0019, 0.0013, 0.0022, 0.0017]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                     dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n                min_val=tensor([-0.1995, -0.2933, -0.3793, -0.3101, -0.2811, -0.1491, -0.2214, -0.2849,\n                        -0.0532, -0.2238, -0.1638, -0.3059, -0.1985, -0.2756, -0.2598, -0.3634,\n                        -0.3101, -0.1215, -0.1341, -0.2386, -0.4341, -0.2591, -0.3453, -0.1260,\n                        -0.1910, -0.3480, -0.1739, -0.1292, -0.1712, -0.2788, -0.1456, -0.2160,\n                        -0.2408, -0.1545, -0.2682, -0.2953, -0.2948, -0.2225, -0.1727, -0.2470,\n                        -0.2967, -0.2811, -0.1809, -0.1595, -0.3995, -0.1784, -0.1576, -0.2125,\n                        -0.2280, -0.4224, -0.3037, -0.2974, -0.3234, -0.2291, -0.2032, -0.5231,\n                        -0.1986, -0.1808, -0.1076, -0.1469, -0.2332, -0.2880, -0.4068, -0.2097,\n                        -0.4091, -0.1943, -0.0811, -0.2407, -0.2475, -0.2678, -0.3640, -0.0786,\n                        -0.5287, -0.3829, -0.3459, -0.1455, -0.2061, -0.1413, -0.3290, -0.2945,\n                        -0.0877, -0.3306, -0.2721, -0.3629, -0.0851, -0.1683, -0.1703, -0.1393,\n                        -0.2520, -0.2324, -0.4173, -0.4812, -0.2369, -0.1692, -0.2430, -0.1726]), max_val=tensor([0.1852, 0.1719, 0.4109, 0.2916, 0.2938, 0.1210, 0.2529, 0.3789, 0.0627,\n                        0.2440, 0.2341, 0.2054, 0.2414, 0.4328, 0.3281, 0.3589, 0.1998, 0.1250,\n                        0.1269, 0.3732, 0.4460, 0.3066, 0.2742, 0.0924, 0.1873, 0.3872, 0.1501,\n                        0.2160, 0.1981, 0.2109, 0.1479, 0.3003, 0.1405, 0.2984, 0.2397, 0.3321,\n                        0.2579, 0.2280, 0.3967, 0.2369, 0.2404, 0.2222, 0.1390, 0.2095, 0.4159,\n                        0.1123, 0.2151, 0.1203, 0.1907, 0.3848, 0.2605, 0.2763, 0.2729, 0.2181,\n                        0.2246, 0.3941, 0.1662, 0.1549, 0.1518, 0.1420, 0.1304, 0.2259, 0.3964,\n                        0.3063, 0.4489, 0.1585, 0.0783, 0.2574, 0.2375, 0.2477, 0.3710, 0.0569,\n                        0.5163, 0.2376, 0.3380, 0.1270, 0.3233, 0.1511, 0.3161, 0.2885, 0.0633,\n                        0.4227, 0.2786, 0.4855, 0.2212, 0.2087, 0.1158, 0.1280, 0.2294, 0.2421,\n                        0.4358, 0.3843, 0.2111, 0.1532, 0.2806, 0.2105])\n              )\n            )\n          )\n        )\n      )\n      (1): Module(\n        (block): Module(\n          (0): Conv2d(\n            96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([8.6653e-04, 8.3951e-04, 1.4754e-03, 1.6104e-03, 8.1286e-04, 3.8478e-03,\n                      1.5043e-03, 2.6681e-03, 3.2571e-03, 1.5966e-03, 1.4399e-03, 5.9601e-04,\n                      1.8810e-03, 1.1300e-03, 2.1530e-03, 2.1805e-03, 1.0801e-03, 2.4975e-03,\n                      1.2883e-03, 2.1469e-03, 1.2157e-03, 2.8353e-03, 2.5601e-03, 2.9218e-03,\n                      1.1647e-03, 1.1321e-03, 1.9633e-03, 2.7114e-03, 4.4687e-04, 2.3271e-03,\n                      1.2934e-03, 4.8419e-04, 3.1400e-03, 1.1372e-03, 1.1230e-03, 2.0482e-03,\n                      1.7704e-03, 2.6267e-03, 1.5569e-03, 1.3804e-03, 1.0069e-03, 8.0180e-04,\n                      1.3301e-03, 7.1910e-05, 2.0341e-03, 6.4700e-04, 5.8093e-04, 2.6678e-03,\n                      3.2378e-03, 4.8812e-03, 1.6218e-03, 1.0609e-03, 2.7337e-03, 3.5838e-04,\n                      9.0062e-04, 2.3282e-03, 1.7203e-03, 1.9999e-03, 1.1152e-03, 1.5240e-03,\n                      4.3755e-04, 9.7436e-04, 1.3683e-03, 1.9608e-03, 2.2669e-03, 2.4025e-04,\n                      2.8886e-03, 2.6662e-03, 2.5775e-03, 7.7173e-04, 1.9759e-03, 6.6920e-04,\n                      1.8764e-03, 1.4680e-03, 1.1341e-03, 1.7881e-03, 2.0495e-03, 7.1713e-04,\n                      1.5129e-03, 2.1633e-03, 9.1196e-04, 9.5969e-04, 2.3174e-03, 2.1077e-03,\n                      3.3835e-03, 6.0968e-04, 2.8689e-03, 1.6272e-03, 1.7349e-03, 2.0522e-03,\n                      9.7096e-04, 1.6572e-03, 1.0845e-03, 1.3704e-03, 1.0924e-03, 1.3611e-03]), zero_point=tensor([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,  127,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,  127,    0,    0,  127,    0,  127,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,  127, -128,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                       127,    0, -128,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0, -128, -128,    0,    0,    0,    0,    0,\n                         0,    0,    0,  127,    0,    0,    0,    0,    0,  127,    0,    0],\n                     dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n                min_val=tensor([-0.0252, -0.1075, -0.1889, -0.2061, -0.0921, -0.4925, -0.0326, -0.0636,\n                        -0.0274, -0.2044, -0.0699, -0.1520, -0.2408, -0.1446, -0.2756, -0.1201,\n                        -0.1383, -0.0213, -0.1649, -0.0632, -0.0201, -0.3629, -0.3277, -0.3740,\n                        -0.1491, -0.0524, -0.2513, -0.3471, -0.1140, -0.2979, -0.0015, -0.1235,\n                        -0.0340, -0.2900, -0.1437, -0.2622, -0.0470, -0.0662, -0.0577, -0.1209,\n                        -0.1289, -0.0924, -0.1703, -0.0087, -0.0566, -0.1650,  0.0093, -0.0063,\n                        -0.0313, -0.0955, -0.0358, -0.0238, -0.0405, -0.0031, -0.1153, -0.2980,\n                        -0.1019, -0.2560, -0.1427, -0.1951, -0.1116, -0.0968,  0.0053, -0.2510,\n                        -0.0193, -0.0231, -0.3697, -0.0177, -0.3299, -0.0988, -0.1460, -0.0624,\n                        -0.0334, -0.0935, -0.1452, -0.2289, -0.2623,  0.0051,  0.0027, -0.0498,\n                        -0.1167, -0.0881, -0.1691, -0.2698, -0.4331, -0.0738, -0.0203, -0.4149,\n                        -0.0251, -0.0910, -0.1115, -0.2121, -0.1341, -0.3494, -0.0643, -0.1062]), max_val=tensor([ 1.1005e-01,  3.8840e-02,  6.5381e-02,  5.2562e-02,  1.0323e-01,\n                         4.9546e-02,  1.9105e-01,  3.3885e-01,  4.1365e-01,  3.6856e-02,\n                         1.8287e-01, -1.4645e-02,  4.8802e-02,  1.3083e-01,  4.3066e-02,\n                         2.7692e-01,  7.4196e-02,  3.1718e-01,  4.8351e-05,  2.7266e-01,\n                         1.5440e-01,  1.9454e-02,  6.1048e-02,  1.5846e-02,  1.0120e-03,\n                         1.4378e-01,  9.8160e-03,  2.4717e-02, -1.4373e-03,  3.1816e-02,\n                         1.6426e-01, -1.1143e-02,  3.9879e-01, -3.9175e-02,  1.3831e-01,\n                         7.1708e-02,  2.2484e-01,  3.3359e-01,  1.9772e-01,  1.7532e-01,\n                         4.5729e-02,  1.0183e-01,  5.6767e-03,  9.1326e-03,  2.5833e-01,\n                        -3.3539e-03,  1.4814e-01,  3.3881e-01,  4.1120e-01,  6.1991e-01,\n                         2.0597e-01,  1.3474e-01,  3.4719e-01,  4.5514e-02,  1.0954e-02,\n                         6.6148e-02,  2.1848e-01,  1.2049e-01,  9.1167e-02,  3.4135e-02,\n                        -5.9473e-04,  1.2374e-01,  3.4890e-01,  3.4183e-02,  2.8789e-01,\n                         3.0512e-02,  1.9050e-02,  3.3861e-01,  2.3446e-01,  9.2974e-02,\n                         2.5094e-01,  8.4988e-02,  2.3830e-01,  1.8644e-01,  1.3714e-01,\n                         3.2294e-02,  2.6101e-02,  1.8287e-01,  3.8579e-01,  2.7474e-01,\n                         7.6882e-02,  1.2188e-01,  2.9431e-01,  3.5500e-02,  3.0928e-02,\n                         7.7430e-02,  3.6435e-01, -6.5507e-03,  2.2034e-01,  2.6063e-01,\n                         1.2331e-01,  4.8502e-02,  1.3774e-01, -4.4904e-03,  1.3874e-01,\n                         1.7286e-01])\n              )\n            )\n          )\n          (2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(\n            in_features=96, out_features=384, bias=True\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0013, 0.0014, 0.0010, 0.0013, 0.0016, 0.0019, 0.0018, 0.0019, 0.0010,\n                      0.0016, 0.0019, 0.0012, 0.0010, 0.0018, 0.0023, 0.0013, 0.0015, 0.0026,\n                      0.0017, 0.0018, 0.0018, 0.0014, 0.0012, 0.0014, 0.0025, 0.0012, 0.0031,\n                      0.0015, 0.0014, 0.0019, 0.0014, 0.0014, 0.0012, 0.0012, 0.0011, 0.0012,\n                      0.0012, 0.0018, 0.0016, 0.0011, 0.0011, 0.0016, 0.0016, 0.0019, 0.0014,\n                      0.0010, 0.0011, 0.0014, 0.0009, 0.0015, 0.0011, 0.0018, 0.0008, 0.0015,\n                      0.0018, 0.0015, 0.0012, 0.0023, 0.0018, 0.0021, 0.0036, 0.0022, 0.0013,\n                      0.0012, 0.0009, 0.0029, 0.0013, 0.0015, 0.0013, 0.0013, 0.0010, 0.0018,\n                      0.0008, 0.0010, 0.0010, 0.0010, 0.0014, 0.0011, 0.0017, 0.0013, 0.0012,\n                      0.0012, 0.0020, 0.0021, 0.0013, 0.0020, 0.0026, 0.0012, 0.0021, 0.0014,\n                      0.0024, 0.0021, 0.0011, 0.0015, 0.0018, 0.0010, 0.0016, 0.0012, 0.0021,\n                      0.0013, 0.0017, 0.0011, 0.0021, 0.0022, 0.0019, 0.0014, 0.0011, 0.0021,\n                      0.0019, 0.0017, 0.0018, 0.0014, 0.0015, 0.0014, 0.0009, 0.0022, 0.0024,\n                      0.0024, 0.0021, 0.0023, 0.0019, 0.0014, 0.0015, 0.0022, 0.0010, 0.0018,\n                      0.0016, 0.0012, 0.0010, 0.0022, 0.0012, 0.0018, 0.0016, 0.0026, 0.0011,\n                      0.0014, 0.0015, 0.0015, 0.0020, 0.0012, 0.0023, 0.0011, 0.0016, 0.0017,\n                      0.0010, 0.0016, 0.0014, 0.0015, 0.0009, 0.0009, 0.0010, 0.0021, 0.0034,\n                      0.0018, 0.0016, 0.0019, 0.0021, 0.0024, 0.0019, 0.0010, 0.0021, 0.0016,\n                      0.0008, 0.0012, 0.0012, 0.0011, 0.0017, 0.0015, 0.0011, 0.0014, 0.0018,\n                      0.0010, 0.0012, 0.0021, 0.0019, 0.0019, 0.0022, 0.0027, 0.0018, 0.0013,\n                      0.0022, 0.0014, 0.0013, 0.0018, 0.0009, 0.0012, 0.0011, 0.0019, 0.0015,\n                      0.0010, 0.0009, 0.0018, 0.0011, 0.0022, 0.0023, 0.0012, 0.0020, 0.0021,\n                      0.0019, 0.0016, 0.0024, 0.0013, 0.0017, 0.0011, 0.0013, 0.0014, 0.0021,\n                      0.0018, 0.0015, 0.0013, 0.0016, 0.0013, 0.0013, 0.0015, 0.0013, 0.0019,\n                      0.0021, 0.0019, 0.0010, 0.0009, 0.0013, 0.0017, 0.0024, 0.0015, 0.0008,\n                      0.0015, 0.0009, 0.0017, 0.0017, 0.0014, 0.0012, 0.0015, 0.0011, 0.0018,\n                      0.0028, 0.0017, 0.0011, 0.0013, 0.0026, 0.0020, 0.0011, 0.0014, 0.0014,\n                      0.0010, 0.0017, 0.0022, 0.0020, 0.0012, 0.0013, 0.0011, 0.0016, 0.0016,\n                      0.0031, 0.0015, 0.0013, 0.0013, 0.0012, 0.0017, 0.0018, 0.0017, 0.0017,\n                      0.0013, 0.0021, 0.0010, 0.0013, 0.0015, 0.0015, 0.0021, 0.0011, 0.0015,\n                      0.0012, 0.0019, 0.0013, 0.0017, 0.0017, 0.0017, 0.0012, 0.0017, 0.0015,\n                      0.0014, 0.0020, 0.0023, 0.0021, 0.0012, 0.0013, 0.0013, 0.0010, 0.0031,\n                      0.0017, 0.0018, 0.0011, 0.0027, 0.0020, 0.0011, 0.0009, 0.0015, 0.0022,\n                      0.0009, 0.0024, 0.0018, 0.0011, 0.0016, 0.0016, 0.0026, 0.0013, 0.0012,\n                      0.0019, 0.0013, 0.0023, 0.0022, 0.0014, 0.0014, 0.0014, 0.0012, 0.0018,\n                      0.0010, 0.0015, 0.0019, 0.0016, 0.0017, 0.0010, 0.0012, 0.0018, 0.0010,\n                      0.0012, 0.0020, 0.0013, 0.0009, 0.0022, 0.0012, 0.0016, 0.0011, 0.0012,\n                      0.0015, 0.0013, 0.0023, 0.0017, 0.0019, 0.0023, 0.0011, 0.0016, 0.0009,\n                      0.0012, 0.0013, 0.0013, 0.0012, 0.0015, 0.0013, 0.0011, 0.0011, 0.0010,\n                      0.0022, 0.0016, 0.0022, 0.0017, 0.0026, 0.0013, 0.0016, 0.0026, 0.0013,\n                      0.0028, 0.0020, 0.0028, 0.0010, 0.0016, 0.0010, 0.0025, 0.0015, 0.0022,\n                      0.0016, 0.0031, 0.0014, 0.0017, 0.0029, 0.0016, 0.0026, 0.0014, 0.0013,\n                      0.0017, 0.0013, 0.0014, 0.0010, 0.0017, 0.0017]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                     dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n                min_val=tensor([-0.1705, -0.1395, -0.1233, -0.1686, -0.1998, -0.2495, -0.2170, -0.1559,\n                        -0.0982, -0.2074, -0.0993, -0.0946, -0.1298, -0.2355, -0.3004, -0.1501,\n                        -0.1978, -0.3313, -0.2168, -0.2330, -0.1425, -0.1337, -0.1146, -0.1408,\n                        -0.3184, -0.1198, -0.1945, -0.1886, -0.1141, -0.1873, -0.1744, -0.1280,\n                        -0.1322, -0.1098, -0.1446, -0.1585, -0.1536, -0.1276, -0.1895, -0.1086,\n                        -0.0682, -0.1451, -0.1994, -0.1855, -0.1835, -0.1233, -0.1333, -0.1593,\n                        -0.1067, -0.1911, -0.1181, -0.2199, -0.0848, -0.1954, -0.2282, -0.1909,\n                        -0.1502, -0.2944, -0.2184, -0.2649, -0.1494, -0.2809, -0.1293, -0.1544,\n                        -0.1096, -0.3773, -0.1670, -0.1975, -0.1708, -0.1683, -0.1237, -0.1676,\n                        -0.1010, -0.1281, -0.1303, -0.1298, -0.1845, -0.1278, -0.2003, -0.1433,\n                        -0.1162, -0.1559, -0.2517, -0.2708, -0.0963, -0.1453, -0.3375, -0.1464,\n                        -0.2111, -0.1748, -0.1604, -0.2655, -0.1293, -0.1125, -0.2078, -0.1327,\n                        -0.2019, -0.1537, -0.2740, -0.1625, -0.1116, -0.1469, -0.2723, -0.2877,\n                        -0.2437, -0.1814, -0.1373, -0.2658, -0.2446, -0.1732, -0.2324, -0.1551,\n                        -0.1588, -0.1476, -0.1144, -0.2834, -0.1625, -0.2019, -0.2735, -0.2965,\n                        -0.2414, -0.1741, -0.1929, -0.2855, -0.1292, -0.2354, -0.2005, -0.1551,\n                        -0.1224, -0.1970, -0.1599, -0.2336, -0.1834, -0.1407, -0.1051, -0.1245,\n                        -0.1970, -0.1746, -0.2612, -0.1287, -0.2921, -0.1358, -0.0808, -0.2211,\n                        -0.1080, -0.1386, -0.1152, -0.1720, -0.1145, -0.1190, -0.1262, -0.1905,\n                        -0.2776, -0.2264, -0.1590, -0.1473, -0.1344, -0.2333, -0.1167, -0.1196,\n                        -0.2000, -0.2034, -0.1064, -0.1478, -0.1574, -0.1400, -0.2181, -0.1895,\n                        -0.1077, -0.1441, -0.1550, -0.1306, -0.1068, -0.1570, -0.1930, -0.1742,\n                        -0.2822, -0.3422, -0.1750, -0.1686, -0.1975, -0.1687, -0.1252, -0.2291,\n                        -0.1155, -0.1600, -0.1431, -0.2417, -0.1922, -0.1200, -0.1061, -0.1884,\n                        -0.1259, -0.1123, -0.2426, -0.1492, -0.2571, -0.2661, -0.1214, -0.2066,\n                        -0.3099, -0.1622, -0.0856, -0.1450, -0.1602, -0.1772, -0.2718, -0.1806,\n                        -0.1003, -0.1605, -0.1746, -0.1405, -0.1493, -0.1530, -0.1279, -0.2450,\n                        -0.2660, -0.1267, -0.1258, -0.1011, -0.1513, -0.2204, -0.3085, -0.1864,\n                        -0.1009, -0.1605, -0.1196, -0.2156, -0.2122, -0.1807, -0.1360, -0.1875,\n                        -0.1430, -0.2306, -0.2899, -0.1201, -0.1457, -0.1312, -0.3307, -0.1726,\n                        -0.1008, -0.1821, -0.1693, -0.1229, -0.2227, -0.2781, -0.2527, -0.1525,\n                        -0.1671, -0.1352, -0.1709, -0.2080, -0.2429, -0.1212, -0.1676, -0.1623,\n                        -0.1044, -0.2204, -0.2010, -0.2118, -0.2167, -0.1309, -0.2728, -0.1195,\n                        -0.1241, -0.1312, -0.1372, -0.1262, -0.1221, -0.1567, -0.1382, -0.1980,\n                        -0.1341, -0.2157, -0.1844, -0.2083, -0.1426, -0.2205, -0.1222, -0.1804,\n                        -0.2109, -0.2917, -0.2723, -0.1487, -0.1723, -0.1636, -0.1159, -0.3971,\n                        -0.1187, -0.1951, -0.1083, -0.2523, -0.2517, -0.1417, -0.1158, -0.1647,\n                        -0.2859, -0.1205, -0.3057, -0.1784, -0.1276, -0.2034, -0.1687, -0.3293,\n                        -0.1717, -0.1600, -0.2367, -0.1641, -0.2889, -0.2312, -0.1388, -0.1774,\n                        -0.1729, -0.1183, -0.2259, -0.1267, -0.1911, -0.1235, -0.1096, -0.1853,\n                        -0.1258, -0.1524, -0.1956, -0.1310, -0.1507, -0.2501, -0.1694, -0.1133,\n                        -0.2843, -0.1495, -0.1888, -0.1380, -0.1495, -0.1946, -0.0844, -0.2725,\n                        -0.2009, -0.0988, -0.2090, -0.1345, -0.2020, -0.0907, -0.1400, -0.1714,\n                        -0.1357, -0.1217, -0.0623, -0.1037, -0.1429, -0.1411, -0.1247, -0.2404,\n                        -0.2046, -0.2793, -0.2158, -0.3265, -0.1619, -0.2065, -0.1224, -0.1718,\n                        -0.3610, -0.2549, -0.3592, -0.0933, -0.2098, -0.1282, -0.3203, -0.1887,\n                        -0.2618, -0.2075, -0.0954, -0.1174, -0.2118, -0.2463, -0.1688, -0.3377,\n                        -0.1777, -0.1310, -0.1198, -0.1696, -0.1736, -0.1245, -0.2075, -0.2128]), max_val=tensor([0.1623, 0.1734, 0.1041, 0.1374, 0.2085, 0.2032, 0.2272, 0.2351, 0.1218,\n                        0.1281, 0.2405, 0.1461, 0.1079, 0.2109, 0.1034, 0.1707, 0.1924, 0.2185,\n                        0.1447, 0.1507, 0.2252, 0.1734, 0.1517, 0.1794, 0.1582, 0.1565, 0.3957,\n                        0.1923, 0.1833, 0.2400, 0.1354, 0.1725, 0.1505, 0.1474, 0.0964, 0.1229,\n                        0.1322, 0.2276, 0.2060, 0.1451, 0.1393, 0.2080, 0.2014, 0.2400, 0.1482,\n                        0.1124, 0.1401, 0.1778, 0.1157, 0.0770, 0.1351, 0.2271, 0.1016, 0.1611,\n                        0.1586, 0.1418, 0.1406, 0.2580, 0.2287, 0.1932, 0.4525, 0.2016, 0.1617,\n                        0.0880, 0.0978, 0.2676, 0.1269, 0.1695, 0.1310, 0.1441, 0.1290, 0.2283,\n                        0.0981, 0.1120, 0.0967, 0.0923, 0.1337, 0.1355, 0.2211, 0.1700, 0.1578,\n                        0.1240, 0.1592, 0.2366, 0.1664, 0.2501, 0.2877, 0.1474, 0.2686, 0.1153,\n                        0.3067, 0.1720, 0.1338, 0.1962, 0.2333, 0.0873, 0.1629, 0.1105, 0.2640,\n                        0.1602, 0.2159, 0.1387, 0.2144, 0.1929, 0.1878, 0.0753, 0.1306, 0.2191,\n                        0.1787, 0.2200, 0.1823, 0.1746, 0.1906, 0.1758, 0.1128, 0.0939, 0.3006,\n                        0.3035, 0.1803, 0.2266, 0.1960, 0.1424, 0.1540, 0.1613, 0.1085, 0.0920,\n                        0.2049, 0.1290, 0.1255, 0.2756, 0.1354, 0.2036, 0.2020, 0.3289, 0.1448,\n                        0.1726, 0.1178, 0.1843, 0.2281, 0.1533, 0.2121, 0.1240, 0.2063, 0.1768,\n                        0.1254, 0.2041, 0.1814, 0.1846, 0.0941, 0.0994, 0.1042, 0.2617, 0.4287,\n                        0.1253, 0.2044, 0.2407, 0.2696, 0.3017, 0.2411, 0.1324, 0.2727, 0.1330,\n                        0.0919, 0.1225, 0.1513, 0.1132, 0.1067, 0.1757, 0.1459, 0.1830, 0.2249,\n                        0.1306, 0.1545, 0.2729, 0.2369, 0.2450, 0.1269, 0.1284, 0.2263, 0.1469,\n                        0.2753, 0.1763, 0.1609, 0.2296, 0.1039, 0.1467, 0.1288, 0.1998, 0.1450,\n                        0.1216, 0.1098, 0.2262, 0.1416, 0.2830, 0.2930, 0.1107, 0.1983, 0.2018,\n                        0.2465, 0.1817, 0.1920, 0.1347, 0.2136, 0.0974, 0.1241, 0.1795, 0.2474,\n                        0.2319, 0.1847, 0.1464, 0.1973, 0.1692, 0.1634, 0.1867, 0.1692, 0.1345,\n                        0.1200, 0.2416, 0.1211, 0.1138, 0.1619, 0.1033, 0.2094, 0.1297, 0.1030,\n                        0.1960, 0.0940, 0.1145, 0.1560, 0.1541, 0.1473, 0.1702, 0.1218, 0.2063,\n                        0.3611, 0.2189, 0.1311, 0.1591, 0.3116, 0.2568, 0.1445, 0.1332, 0.1835,\n                        0.1157, 0.1300, 0.1219, 0.2248, 0.1234, 0.1366, 0.1002, 0.2081, 0.1830,\n                        0.3893, 0.1876, 0.1244, 0.1497, 0.1467, 0.1723, 0.2294, 0.1241, 0.1500,\n                        0.1674, 0.1491, 0.1238, 0.1591, 0.1874, 0.1931, 0.2639, 0.1460, 0.1879,\n                        0.1532, 0.2410, 0.1608, 0.0921, 0.2212, 0.2180, 0.1467, 0.2032, 0.1915,\n                        0.1228, 0.2495, 0.2327, 0.1797, 0.1251, 0.0969, 0.1248, 0.1329, 0.1431,\n                        0.2101, 0.2340, 0.1384, 0.3464, 0.1502, 0.1284, 0.1168, 0.1897, 0.0998,\n                        0.1076, 0.1123, 0.2332, 0.1347, 0.1982, 0.2055, 0.1303, 0.0671, 0.1437,\n                        0.2393, 0.1301, 0.2224, 0.2783, 0.1770, 0.1363, 0.1200, 0.1468, 0.0986,\n                        0.0956, 0.1849, 0.2421, 0.2083, 0.2121, 0.1064, 0.0785, 0.2225, 0.0829,\n                        0.1563, 0.2367, 0.1615, 0.0973, 0.2307, 0.1011, 0.2023, 0.1415, 0.1363,\n                        0.1119, 0.1606, 0.2859, 0.2134, 0.2435, 0.2916, 0.1448, 0.1349, 0.1202,\n                        0.1483, 0.1053, 0.1627, 0.1483, 0.1877, 0.1633, 0.1172, 0.0887, 0.1052,\n                        0.2804, 0.1665, 0.1471, 0.0904, 0.1745, 0.1010, 0.0785, 0.3246, 0.1595,\n                        0.1727, 0.1875, 0.1431, 0.1321, 0.1783, 0.0893, 0.2519, 0.1576, 0.2767,\n                        0.1328, 0.3923, 0.1738, 0.1942, 0.3668, 0.2038, 0.0956, 0.1327, 0.1614,\n                        0.2175, 0.1087, 0.1010, 0.0881, 0.2198, 0.1733])\n              )\n            )\n          )\n          (4): GELU(approximate='none')\n          (5): Linear(\n            in_features=384, out_features=96, bias=True\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0023, 0.0032, 0.0024, 0.0023, 0.0023, 0.0032, 0.0005, 0.0025, 0.0018,\n                      0.0025, 0.0019, 0.0026, 0.0018, 0.0024, 0.0020, 0.0026, 0.0027, 0.0029,\n                      0.0021, 0.0029, 0.0024, 0.0016, 0.0033, 0.0020, 0.0025, 0.0030, 0.0035,\n                      0.0027, 0.0018, 0.0018, 0.0029, 0.0022, 0.0029, 0.0023, 0.0026, 0.0018,\n                      0.0021, 0.0020, 0.0017, 0.0016, 0.0022, 0.0018, 0.0017, 0.0006, 0.0017,\n                      0.0018, 0.0022, 0.0018, 0.0016, 0.0032, 0.0023, 0.0026, 0.0028, 0.0014,\n                      0.0018, 0.0018, 0.0020, 0.0019, 0.0024, 0.0025, 0.0019, 0.0023, 0.0036,\n                      0.0022, 0.0022, 0.0019, 0.0040, 0.0028, 0.0018, 0.0028, 0.0017, 0.0025,\n                      0.0030, 0.0019, 0.0029, 0.0014, 0.0020, 0.0016, 0.0030, 0.0022, 0.0022,\n                      0.0019, 0.0017, 0.0023, 0.0021, 0.0024, 0.0021, 0.0020, 0.0020, 0.0021,\n                      0.0031, 0.0021, 0.0021, 0.0018, 0.0024, 0.0022]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                     dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n                min_val=tensor([-0.2135, -0.4141, -0.2786, -0.2975, -0.2297, -0.2308, -0.0588, -0.3173,\n                        -0.2334, -0.3253, -0.1648, -0.3286, -0.2287, -0.3122, -0.2533, -0.3392,\n                        -0.3401, -0.3706, -0.2091, -0.1984, -0.2363, -0.1907, -0.4178, -0.2562,\n                        -0.1434, -0.3760, -0.2943, -0.3432, -0.2172, -0.2316, -0.2880, -0.1832,\n                        -0.3652, -0.2563, -0.2979, -0.1949, -0.2627, -0.2531, -0.1881, -0.1889,\n                        -0.2469, -0.2368, -0.1883, -0.0559, -0.1928, -0.2256, -0.2851, -0.1587,\n                        -0.1950, -0.3902, -0.2757, -0.3315, -0.3466, -0.1824, -0.2263, -0.2252,\n                        -0.1987, -0.2461, -0.3043, -0.3152, -0.2188, -0.2931, -0.3761, -0.2783,\n                        -0.2879, -0.1876, -0.2447, -0.3537, -0.2115, -0.3282, -0.2207, -0.3176,\n                        -0.3901, -0.2452, -0.3495, -0.1643, -0.2596, -0.1757, -0.3227, -0.2850,\n                        -0.2848, -0.2493, -0.2193, -0.2413, -0.2707, -0.2363, -0.2695, -0.2502,\n                        -0.2074, -0.2644, -0.3905, -0.2698, -0.2421, -0.2314, -0.3109, -0.2468]), max_val=tensor([0.2876, 0.1892, 0.3101, 0.2269, 0.2913, 0.4125, 0.0648, 0.2435, 0.2273,\n                        0.3155, 0.2417, 0.2981, 0.2181, 0.2589, 0.1705, 0.3049, 0.3366, 0.2524,\n                        0.2630, 0.3695, 0.3042, 0.1971, 0.3976, 0.2431, 0.3201, 0.3774, 0.4419,\n                        0.2219, 0.2281, 0.2090, 0.3721, 0.2853, 0.3109, 0.2860, 0.3318, 0.2330,\n                        0.2368, 0.2178, 0.2203, 0.2086, 0.2738, 0.2290, 0.2164, 0.0753, 0.2181,\n                        0.2092, 0.1607, 0.2338, 0.2047, 0.4101, 0.2975, 0.2423, 0.3579, 0.1584,\n                        0.1738, 0.2181, 0.2503, 0.1878, 0.2859, 0.3115, 0.2429, 0.2055, 0.4528,\n                        0.2099, 0.2153, 0.2430, 0.5059, 0.2989, 0.2340, 0.3522, 0.1982, 0.3176,\n                        0.3134, 0.2180, 0.3653, 0.1747, 0.2119, 0.2018, 0.3828, 0.2061, 0.2304,\n                        0.2110, 0.1774, 0.2912, 0.1589, 0.3011, 0.2337, 0.1898, 0.2504, 0.2117,\n                        0.3483, 0.2195, 0.2717, 0.2042, 0.2818, 0.2836])\n              )\n            )\n          )\n        )\n      )\n      (2): Module(\n        (block): Module(\n          (0): Conv2d(\n            96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([2.8848e-03, 2.3993e-03, 1.9360e-03, 2.7080e-03, 1.6093e-03, 1.3961e-03,\n                      1.1617e-03, 2.7918e-03, 2.1507e-03, 2.0213e-03, 1.4062e-03, 5.1930e-04,\n                      2.4394e-03, 2.5986e-03, 2.3620e-03, 2.8162e-03, 3.0403e-03, 1.7687e-03,\n                      8.4109e-04, 1.7495e-03, 1.6895e-03, 3.2651e-03, 2.6924e-03, 2.3188e-03,\n                      1.6386e-03, 1.9205e-03, 7.4859e-05, 5.5790e-04, 2.6775e-03, 2.7435e-03,\n                      9.4097e-04, 6.6828e-04, 1.3829e-03, 6.7139e-04, 1.9279e-03, 2.6984e-03,\n                      1.9946e-03, 2.8041e-03, 2.1170e-03, 2.4259e-03, 2.2496e-03, 1.8237e-03,\n                      2.8194e-03, 9.2939e-04, 2.9747e-03, 2.9553e-03, 8.5015e-04, 2.8357e-03,\n                      2.5580e-03, 2.6962e-03, 1.8082e-03, 3.2053e-03, 2.4011e-03, 1.0202e-04,\n                      1.6637e-03, 2.3907e-03, 2.1462e-03, 2.1167e-03, 2.0769e-03, 2.1939e-03,\n                      3.2034e-03, 2.3417e-03, 2.6795e-03, 2.7178e-03, 2.3405e-03, 4.2805e-03,\n                      3.5487e-04, 1.7123e-03, 3.4683e-03, 2.8953e-03, 2.7227e-03, 1.7112e-04,\n                      2.0472e-03, 2.5760e-03, 2.2733e-03, 1.2568e-03, 2.4460e-03, 2.7447e-03,\n                      8.4755e-04, 2.6596e-03, 6.5712e-04, 2.2692e-03, 2.9803e-03, 1.9625e-03,\n                      1.4044e-03, 2.6063e-03, 1.9831e-03, 2.7444e-03, 2.2920e-03, 2.6997e-03,\n                      1.8336e-03, 2.0284e-03, 1.9782e-03, 2.9664e-03, 2.5056e-03, 2.0522e-03]), zero_point=tensor([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0, 127,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n                     dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n                min_val=tensor([-0.3693, -0.0308, -0.0461, -0.0417, -0.2060, -0.0027, -0.1487, -0.3574,\n                        -0.2753, -0.0300, -0.1800, -0.0218, -0.0508, -0.0973, -0.3023, -0.0506,\n                        -0.3892, -0.1936, -0.1050, -0.0504, -0.0528, -0.4179, -0.3446, -0.2968,\n                        -0.1887, -0.2458, -0.0096, -0.0244, -0.3427, -0.0247, -0.0057, -0.0855,\n                        -0.1770, -0.1712, -0.1289, -0.0559, -0.0318, -0.0417, -0.2233, -0.0795,\n                        -0.1537, -0.0407, -0.3609, -0.1190, -0.1247, -0.3783, -0.0045, -0.3630,\n                        -0.3274, -0.3451, -0.2315, -0.0316, -0.3073, -0.0115, -0.0178, -0.0664,\n                        -0.2143, -0.0563, -0.2658, -0.2808, -0.0347, -0.0324, -0.0772, -0.0281,\n                        -0.2996, -0.0451, -0.0016, -0.0724, -0.4439, -0.0337, -0.0396, -0.0153,\n                        -0.0922, -0.0558, -0.2910, -0.0287, -0.3131, -0.3513, -0.1085, -0.0300,\n                        -0.0808, -0.0818, -0.0709, -0.2421, -0.1798, -0.3336, -0.0723, -0.3513,\n                        -0.0639, -0.0949, -0.2347, -0.2596, -0.0685, -0.0415, -0.0405, -0.2627]), max_val=tensor([ 0.0284,  0.3047,  0.2459,  0.3439,  0.0266,  0.1773,  0.1154,  0.0278,\n                         0.1847,  0.2567,  0.0880,  0.0660,  0.3098,  0.3300,  0.0265,  0.3577,\n                         0.0321,  0.2246,  0.1068,  0.2222,  0.2146,  0.0549,  0.0477,  0.1896,\n                         0.2081,  0.0415,  0.0091,  0.0709,  0.0643,  0.3484,  0.1195,  0.0451,\n                         0.0760, -0.0011,  0.2448,  0.3427,  0.2533,  0.3561,  0.2689,  0.3081,\n                         0.2857,  0.2316,  0.0307,  0.0148,  0.3778,  0.0371,  0.1080,  0.0316,\n                         0.1077,  0.0104,  0.0272,  0.4071,  0.0371,  0.0130,  0.2113,  0.3036,\n                         0.2726,  0.2688,  0.0204,  0.0641,  0.4068,  0.2974,  0.3403,  0.3452,\n                         0.0289,  0.5436,  0.0451,  0.2175,  0.0565,  0.3677,  0.3458,  0.0217,\n                         0.2600,  0.3271,  0.1461,  0.1596,  0.0306,  0.0356,  0.0954,  0.3378,\n                         0.0835,  0.2882,  0.3785,  0.2492,  0.0132,  0.0313,  0.2519,  0.0233,\n                         0.2911,  0.3429,  0.1327,  0.0173,  0.2512,  0.3767,  0.3182,  0.0603])\n              )\n            )\n          )\n          (2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(\n            in_features=96, out_features=384, bias=True\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0010, 0.0017, 0.0019, 0.0011, 0.0011, 0.0016, 0.0015, 0.0016, 0.0021,\n                      0.0010, 0.0028, 0.0019, 0.0014, 0.0011, 0.0012, 0.0009, 0.0019, 0.0012,\n                      0.0010, 0.0012, 0.0013, 0.0009, 0.0011, 0.0017, 0.0013, 0.0012, 0.0014,\n                      0.0011, 0.0015, 0.0020, 0.0013, 0.0016, 0.0017, 0.0016, 0.0017, 0.0030,\n                      0.0013, 0.0010, 0.0022, 0.0012, 0.0014, 0.0013, 0.0013, 0.0012, 0.0015,\n                      0.0010, 0.0017, 0.0011, 0.0009, 0.0014, 0.0021, 0.0014, 0.0013, 0.0020,\n                      0.0009, 0.0015, 0.0016, 0.0015, 0.0010, 0.0011, 0.0011, 0.0011, 0.0015,\n                      0.0010, 0.0011, 0.0016, 0.0012, 0.0014, 0.0021, 0.0009, 0.0009, 0.0010,\n                      0.0009, 0.0014, 0.0009, 0.0016, 0.0015, 0.0011, 0.0010, 0.0018, 0.0011,\n                      0.0020, 0.0015, 0.0012, 0.0012, 0.0019, 0.0020, 0.0025, 0.0018, 0.0012,\n                      0.0016, 0.0009, 0.0014, 0.0013, 0.0010, 0.0016, 0.0010, 0.0011, 0.0016,\n                      0.0016, 0.0019, 0.0015, 0.0013, 0.0016, 0.0016, 0.0014, 0.0014, 0.0016,\n                      0.0015, 0.0018, 0.0014, 0.0010, 0.0011, 0.0010, 0.0022, 0.0017, 0.0017,\n                      0.0015, 0.0011, 0.0009, 0.0018, 0.0013, 0.0011, 0.0012, 0.0012, 0.0014,\n                      0.0010, 0.0011, 0.0014, 0.0017, 0.0017, 0.0014, 0.0010, 0.0016, 0.0022,\n                      0.0016, 0.0008, 0.0012, 0.0009, 0.0020, 0.0010, 0.0018, 0.0018, 0.0009,\n                      0.0014, 0.0013, 0.0009, 0.0010, 0.0013, 0.0014, 0.0013, 0.0014, 0.0019,\n                      0.0011, 0.0011, 0.0022, 0.0012, 0.0011, 0.0011, 0.0013, 0.0009, 0.0013,\n                      0.0014, 0.0013, 0.0016, 0.0013, 0.0020, 0.0023, 0.0013, 0.0015, 0.0017,\n                      0.0014, 0.0011, 0.0019, 0.0012, 0.0010, 0.0013, 0.0014, 0.0020, 0.0015,\n                      0.0017, 0.0019, 0.0015, 0.0022, 0.0019, 0.0011, 0.0014, 0.0009, 0.0016,\n                      0.0013, 0.0015, 0.0011, 0.0012, 0.0013, 0.0011, 0.0023, 0.0014, 0.0017,\n                      0.0008, 0.0019, 0.0016, 0.0015, 0.0018, 0.0016, 0.0013, 0.0013, 0.0016,\n                      0.0013, 0.0010, 0.0014, 0.0019, 0.0016, 0.0010, 0.0012, 0.0014, 0.0008,\n                      0.0012, 0.0014, 0.0011, 0.0011, 0.0009, 0.0014, 0.0012, 0.0013, 0.0016,\n                      0.0019, 0.0013, 0.0012, 0.0010, 0.0013, 0.0012, 0.0011, 0.0012, 0.0014,\n                      0.0012, 0.0011, 0.0014, 0.0010, 0.0012, 0.0009, 0.0022, 0.0010, 0.0015,\n                      0.0009, 0.0010, 0.0014, 0.0011, 0.0018, 0.0017, 0.0021, 0.0011, 0.0016,\n                      0.0030, 0.0018, 0.0010, 0.0012, 0.0011, 0.0017, 0.0023, 0.0009, 0.0015,\n                      0.0015, 0.0014, 0.0015, 0.0013, 0.0014, 0.0012, 0.0016, 0.0010, 0.0014,\n                      0.0015, 0.0013, 0.0009, 0.0013, 0.0014, 0.0017, 0.0014, 0.0025, 0.0020,\n                      0.0010, 0.0010, 0.0019, 0.0020, 0.0011, 0.0011, 0.0011, 0.0016, 0.0024,\n                      0.0017, 0.0016, 0.0015, 0.0011, 0.0014, 0.0013, 0.0011, 0.0015, 0.0012,\n                      0.0019, 0.0013, 0.0016, 0.0009, 0.0020, 0.0012, 0.0010, 0.0020, 0.0022,\n                      0.0016, 0.0013, 0.0010, 0.0019, 0.0014, 0.0020, 0.0012, 0.0012, 0.0018,\n                      0.0025, 0.0012, 0.0012, 0.0014, 0.0011, 0.0015, 0.0015, 0.0015, 0.0011,\n                      0.0014, 0.0011, 0.0013, 0.0021, 0.0010, 0.0011, 0.0016, 0.0019, 0.0012,\n                      0.0019, 0.0011, 0.0014, 0.0014, 0.0013, 0.0016, 0.0012, 0.0036, 0.0015,\n                      0.0013, 0.0016, 0.0013, 0.0017, 0.0011, 0.0014, 0.0013, 0.0015, 0.0012,\n                      0.0016, 0.0014, 0.0012, 0.0013, 0.0011, 0.0011, 0.0014, 0.0015, 0.0012,\n                      0.0013, 0.0010, 0.0016, 0.0021, 0.0013, 0.0013, 0.0015, 0.0015, 0.0009,\n                      0.0012, 0.0015, 0.0013, 0.0013, 0.0021, 0.0009, 0.0011, 0.0011, 0.0012,\n                      0.0017, 0.0009, 0.0020, 0.0014, 0.0014, 0.0014]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                     dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n                min_val=tensor([-0.1245, -0.1765, -0.1253, -0.1422, -0.1271, -0.1987, -0.1904, -0.2063,\n                        -0.0995, -0.0901, -0.3530, -0.2401, -0.1475, -0.1442, -0.1491, -0.1198,\n                        -0.0831, -0.1502, -0.0960, -0.1570, -0.1084, -0.1080, -0.1379, -0.2144,\n                        -0.1648, -0.1503, -0.1359, -0.1416, -0.1687, -0.1222, -0.1529, -0.2061,\n                        -0.2167, -0.2061, -0.1851, -0.3865, -0.1609, -0.1308, -0.2877, -0.1243,\n                        -0.1741, -0.1642, -0.1185, -0.1513, -0.0966, -0.1318, -0.1651, -0.1451,\n                        -0.1125, -0.0737, -0.2743, -0.1802, -0.1634, -0.2590, -0.1066, -0.1925,\n                        -0.1364, -0.1786, -0.1338, -0.1464, -0.1369, -0.1424, -0.1904, -0.1172,\n                        -0.1452, -0.1389, -0.1515, -0.1780, -0.2200, -0.0982, -0.1085, -0.1234,\n                        -0.0865, -0.1741, -0.0908, -0.1341, -0.1589, -0.1358, -0.1270, -0.2342,\n                        -0.1411, -0.1651, -0.1872, -0.1027, -0.1218, -0.2337, -0.2504, -0.2545,\n                        -0.1074, -0.1555, -0.2033, -0.1149, -0.1354, -0.1689, -0.1046, -0.2001,\n                        -0.1302, -0.1406, -0.1987, -0.1672, -0.1488, -0.1013, -0.1536, -0.2099,\n                        -0.2039, -0.1662, -0.1364, -0.2104, -0.1303, -0.2268, -0.0870, -0.0954,\n                        -0.1459, -0.1236, -0.1480, -0.1684, -0.1309, -0.1898, -0.1373, -0.1140,\n                        -0.1893, -0.1613, -0.1286, -0.1308, -0.1105, -0.1725, -0.1339, -0.1379,\n                        -0.1005, -0.1247, -0.1114, -0.1103, -0.1221, -0.2066, -0.2859, -0.1203,\n                        -0.1060, -0.1564, -0.1179, -0.2563, -0.1276, -0.2299, -0.2282, -0.1066,\n                        -0.1156, -0.1692, -0.1156, -0.1330, -0.1208, -0.1133, -0.1698, -0.1767,\n                        -0.2481, -0.1205, -0.1370, -0.1535, -0.1481, -0.1349, -0.1304, -0.1666,\n                        -0.1204, -0.1208, -0.1843, -0.1285, -0.1619, -0.1296, -0.2576, -0.1239,\n                        -0.1238, -0.1870, -0.2136, -0.1811, -0.1410, -0.1248, -0.1385, -0.1185,\n                        -0.1589, -0.0970, -0.2604, -0.1869, -0.1145, -0.2481, -0.1650, -0.2845,\n                        -0.1548, -0.1450, -0.1855, -0.1016, -0.2105, -0.1123, -0.1108, -0.1395,\n                        -0.1544, -0.1627, -0.1383, -0.0905, -0.1616, -0.2233, -0.1049, -0.1134,\n                        -0.2089, -0.1876, -0.2322, -0.1573, -0.1610, -0.1637, -0.1986, -0.1086,\n                        -0.1088, -0.1272, -0.2116, -0.1774, -0.1292, -0.1347, -0.1123, -0.1058,\n                        -0.1100, -0.1775, -0.1371, -0.1174, -0.1133, -0.1762, -0.1556, -0.1550,\n                        -0.1728, -0.2379, -0.1605, -0.0825, -0.1316, -0.1707, -0.1370, -0.1390,\n                        -0.1569, -0.1249, -0.1493, -0.1366, -0.1026, -0.1281, -0.1147, -0.1091,\n                        -0.2811, -0.1100, -0.1789, -0.1210, -0.1262, -0.1407, -0.1436, -0.2244,\n                        -0.2223, -0.1553, -0.1419, -0.1613, -0.1397, -0.2263, -0.1278, -0.1517,\n                        -0.1415, -0.1257, -0.2888, -0.0982, -0.1601, -0.1897, -0.1778, -0.1776,\n                        -0.1628, -0.1587, -0.0897, -0.1742, -0.1340, -0.1781, -0.1964, -0.1662,\n                        -0.1197, -0.1626, -0.1853, -0.2187, -0.1754, -0.1529, -0.2062, -0.1254,\n                        -0.1119, -0.2434, -0.2568, -0.1386, -0.1308, -0.1378, -0.2054, -0.3016,\n                        -0.1799, -0.1325, -0.1902, -0.1443, -0.1607, -0.1597, -0.1307, -0.1932,\n                        -0.1181, -0.1312, -0.1578, -0.2059, -0.1191, -0.1180, -0.1576, -0.1303,\n                        -0.1314, -0.2768, -0.2053, -0.1644, -0.1323, -0.0827, -0.1379, -0.1477,\n                        -0.1508, -0.0895, -0.2257, -0.3219, -0.1579, -0.1486, -0.1839, -0.1081,\n                        -0.1879, -0.1927, -0.1468, -0.1057, -0.1729, -0.1134, -0.1708, -0.2658,\n                        -0.1264, -0.1237, -0.1074, -0.2378, -0.1158, -0.2437, -0.1242, -0.1743,\n                        -0.1807, -0.1608, -0.2062, -0.1555, -0.4014, -0.1917, -0.1289, -0.2059,\n                        -0.1624, -0.1438, -0.1449, -0.1795, -0.1345, -0.1442, -0.1498, -0.1985,\n                        -0.1800, -0.1559, -0.1194, -0.1375, -0.1108, -0.1244, -0.1961, -0.1530,\n                        -0.1610, -0.1269, -0.1567, -0.2485, -0.1380, -0.1716, -0.1940, -0.1863,\n                        -0.1190, -0.1081, -0.1571, -0.1653, -0.1335, -0.2721, -0.1215, -0.1301,\n                        -0.1460, -0.1101, -0.2178, -0.1196, -0.1088, -0.1800, -0.1580, -0.1774]), max_val=tensor([0.1166, 0.2197, 0.2366, 0.1181, 0.1342, 0.1745, 0.1890, 0.1414, 0.2605,\n                        0.1216, 0.2468, 0.1222, 0.1747, 0.1078, 0.1439, 0.1184, 0.2424, 0.1439,\n                        0.1270, 0.1142, 0.1611, 0.1200, 0.0896, 0.1876, 0.1277, 0.1492, 0.1740,\n                        0.1454, 0.1861, 0.2515, 0.1647, 0.1166, 0.1956, 0.1374, 0.2146, 0.1514,\n                        0.1169, 0.0974, 0.1373, 0.1538, 0.0959, 0.1089, 0.1608, 0.1332, 0.1880,\n                        0.1185, 0.2209, 0.1063, 0.1153, 0.1805, 0.1324, 0.1471, 0.1620, 0.1792,\n                        0.1100, 0.1198, 0.1995, 0.1915, 0.0974, 0.1386, 0.0991, 0.1449, 0.1627,\n                        0.1291, 0.1050, 0.2034, 0.1162, 0.1589, 0.2652, 0.1156, 0.1180, 0.1310,\n                        0.1191, 0.1567, 0.1181, 0.2049, 0.1885, 0.1442, 0.1262, 0.1504, 0.0969,\n                        0.2497, 0.1334, 0.1477, 0.1558, 0.2435, 0.1323, 0.3112, 0.2256, 0.1301,\n                        0.1528, 0.1102, 0.1819, 0.1525, 0.1221, 0.1516, 0.1267, 0.1374, 0.1478,\n                        0.2031, 0.2355, 0.1900, 0.1691, 0.1692, 0.2031, 0.1786, 0.1715, 0.1977,\n                        0.1929, 0.1414, 0.1797, 0.1329, 0.0910, 0.1168, 0.2839, 0.2099, 0.2114,\n                        0.1011, 0.1266, 0.1002, 0.2324, 0.1421, 0.1366, 0.1552, 0.1517, 0.1834,\n                        0.1206, 0.1284, 0.1809, 0.2165, 0.2195, 0.1819, 0.1130, 0.1571, 0.1367,\n                        0.2086, 0.1079, 0.1343, 0.0987, 0.0565, 0.0827, 0.1949, 0.1544, 0.1158,\n                        0.1821, 0.1150, 0.1179, 0.1100, 0.1608, 0.1807, 0.1456, 0.1207, 0.1899,\n                        0.1414, 0.0989, 0.2773, 0.1551, 0.1378, 0.1451, 0.1363, 0.0913, 0.1650,\n                        0.1819, 0.1676, 0.2033, 0.1622, 0.1024, 0.2928, 0.1647, 0.1228, 0.2129,\n                        0.1495, 0.1273, 0.2460, 0.1549, 0.1230, 0.1660, 0.1796, 0.1530, 0.1717,\n                        0.2193, 0.1864, 0.1947, 0.0954, 0.2440, 0.1088, 0.1557, 0.1112, 0.1237,\n                        0.1590, 0.1943, 0.1246, 0.1443, 0.1228, 0.1187, 0.2966, 0.1807, 0.2069,\n                        0.0945, 0.2451, 0.1947, 0.0928, 0.1470, 0.2023, 0.1524, 0.1091, 0.1975,\n                        0.1691, 0.1245, 0.1736, 0.2434, 0.2004, 0.1007, 0.1502, 0.1726, 0.1052,\n                        0.1498, 0.1512, 0.1072, 0.1348, 0.1046, 0.1803, 0.1428, 0.1598, 0.1993,\n                        0.2195, 0.1145, 0.1502, 0.1092, 0.1600, 0.1474, 0.1101, 0.1375, 0.1799,\n                        0.1017, 0.1199, 0.1836, 0.1321, 0.1576, 0.1105, 0.1352, 0.1288, 0.1955,\n                        0.0977, 0.1107, 0.1818, 0.1387, 0.1843, 0.1171, 0.2673, 0.0844, 0.1993,\n                        0.3750, 0.1876, 0.1258, 0.1326, 0.1057, 0.2120, 0.2067, 0.1206, 0.1885,\n                        0.1041, 0.1668, 0.1851, 0.1397, 0.1769, 0.1529, 0.2072, 0.1282, 0.1798,\n                        0.1444, 0.1054, 0.1018, 0.1626, 0.1363, 0.2051, 0.1347, 0.3160, 0.2498,\n                        0.1212, 0.1326, 0.1881, 0.1187, 0.1055, 0.1336, 0.1355, 0.1368, 0.2253,\n                        0.2123, 0.2093, 0.1068, 0.1301, 0.1725, 0.1592, 0.1445, 0.1082, 0.1562,\n                        0.2408, 0.1646, 0.1383, 0.1059, 0.2510, 0.1099, 0.1284, 0.2507, 0.1247,\n                        0.1092, 0.1205, 0.0986, 0.2372, 0.1839, 0.2590, 0.1477, 0.1481, 0.1998,\n                        0.1806, 0.1510, 0.0820, 0.1650, 0.1452, 0.1277, 0.1628, 0.1847, 0.1435,\n                        0.1159, 0.1427, 0.1127, 0.1617, 0.0934, 0.1426, 0.2021, 0.1673, 0.1478,\n                        0.1911, 0.1402, 0.1440, 0.0965, 0.0712, 0.1310, 0.1143, 0.4565, 0.1525,\n                        0.1694, 0.0991, 0.1559, 0.2151, 0.1293, 0.1397, 0.1703, 0.1845, 0.0859,\n                        0.1723, 0.1632, 0.1551, 0.1640, 0.1067, 0.1427, 0.1776, 0.1251, 0.0910,\n                        0.1520, 0.1012, 0.2032, 0.2655, 0.1635, 0.1558, 0.0917, 0.1487, 0.1176,\n                        0.1473, 0.1953, 0.1233, 0.1692, 0.1566, 0.1104, 0.1449, 0.1256, 0.1551,\n                        0.1179, 0.1181, 0.2524, 0.1199, 0.1766, 0.0945])\n              )\n            )\n          )\n          (4): GELU(approximate='none')\n          (5): Linear(\n            in_features=384, out_features=96, bias=True\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0015, 0.0024, 0.0019, 0.0029, 0.0040, 0.0015, 0.0007, 0.0016, 0.0018,\n                      0.0019, 0.0027, 0.0024, 0.0033, 0.0028, 0.0021, 0.0023, 0.0023, 0.0022,\n                      0.0006, 0.0023, 0.0042, 0.0017, 0.0019, 0.0009, 0.0005, 0.0031, 0.0042,\n                      0.0048, 0.0018, 0.0023, 0.0014, 0.0025, 0.0025, 0.0022, 0.0018, 0.0022,\n                      0.0019, 0.0021, 0.0015, 0.0022, 0.0017, 0.0014, 0.0015, 0.0017, 0.0016,\n                      0.0011, 0.0015, 0.0016, 0.0017, 0.0035, 0.0031, 0.0018, 0.0016, 0.0014,\n                      0.0027, 0.0028, 0.0015, 0.0014, 0.0024, 0.0016, 0.0017, 0.0022, 0.0020,\n                      0.0022, 0.0025, 0.0016, 0.0037, 0.0019, 0.0019, 0.0023, 0.0028, 0.0007,\n                      0.0019, 0.0038, 0.0017, 0.0011, 0.0014, 0.0015, 0.0020, 0.0021, 0.0005,\n                      0.0041, 0.0018, 0.0008, 0.0013, 0.0019, 0.0020, 0.0017, 0.0016, 0.0023,\n                      0.0020, 0.0020, 0.0018, 0.0019, 0.0042, 0.0016]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                     dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n                min_val=tensor([-0.1705, -0.2017, -0.2452, -0.2333, -0.2015, -0.1876, -0.0630, -0.1725,\n                        -0.2264, -0.2422, -0.2068, -0.3075, -0.4285, -0.2590, -0.1759, -0.2704,\n                        -0.2418, -0.2752, -0.0696, -0.1899, -0.5387, -0.2189, -0.2253, -0.1160,\n                        -0.0600, -0.1889, -0.5392, -0.6110, -0.2270, -0.2941, -0.1329, -0.2025,\n                        -0.1749, -0.1262, -0.1981, -0.2281, -0.2484, -0.2658, -0.1722, -0.2797,\n                        -0.1840, -0.1773, -0.1290, -0.2136, -0.2098, -0.1463, -0.1936, -0.2076,\n                        -0.1303, -0.4469, -0.3930, -0.2355, -0.1869, -0.1759, -0.3518, -0.3526,\n                        -0.1596, -0.1845, -0.2309, -0.2077, -0.2172, -0.2841, -0.2609, -0.2759,\n                        -0.2962, -0.1996, -0.3520, -0.2088, -0.2327, -0.2374, -0.3632, -0.0653,\n                        -0.2295, -0.3268, -0.1292, -0.1445, -0.1709, -0.1946, -0.2536, -0.2702,\n                        -0.0646, -0.5215, -0.2171, -0.1037, -0.1582, -0.1906, -0.1577, -0.1753,\n                        -0.1496, -0.2896, -0.2601, -0.2571, -0.2171, -0.1751, -0.5431, -0.2065]), max_val=tensor([0.1941, 0.2993, 0.2445, 0.3728, 0.5086, 0.1782, 0.0850, 0.1979, 0.1311,\n                        0.2266, 0.3413, 0.2722, 0.2914, 0.3596, 0.2637, 0.2941, 0.2902, 0.2182,\n                        0.0701, 0.2904, 0.3461, 0.1817, 0.2401, 0.1177, 0.0630, 0.3895, 0.4919,\n                        0.1881, 0.1640, 0.1906, 0.1744, 0.3141, 0.3165, 0.2848, 0.2272, 0.2731,\n                        0.1802, 0.1908, 0.1893, 0.1976, 0.2138, 0.1528, 0.1932, 0.2079, 0.1894,\n                        0.1448, 0.1143, 0.1438, 0.2212, 0.2087, 0.2120, 0.2324, 0.2004, 0.1625,\n                        0.2832, 0.2140, 0.1967, 0.1822, 0.3103, 0.1715, 0.1387, 0.1844, 0.2462,\n                        0.1871, 0.3129, 0.1803, 0.4660, 0.2367, 0.2350, 0.2946, 0.2392, 0.0855,\n                        0.2465, 0.4765, 0.2159, 0.1365, 0.1773, 0.1599, 0.2568, 0.2046, 0.0645,\n                        0.2624, 0.2236, 0.0897, 0.1682, 0.2384, 0.2539, 0.2216, 0.2062, 0.1845,\n                        0.1453, 0.1723, 0.2275, 0.2406, 0.1600, 0.2049])\n              )\n            )\n          )\n        )\n      )\n    )\n    (2): Module(\n      (0): Module()\n      (1): Conv2d(\n        96, 192, kernel_size=(2, 2), stride=(2, 2)\n        (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0014, 0.0019, 0.0017, 0.0009, 0.0013, 0.0013, 0.0021, 0.0012, 0.0023,\n                  0.0016, 0.0013, 0.0014, 0.0011, 0.0018, 0.0018, 0.0014, 0.0012, 0.0012,\n                  0.0015, 0.0021, 0.0017, 0.0020, 0.0017, 0.0009, 0.0015, 0.0021, 0.0014,\n                  0.0018, 0.0010, 0.0012, 0.0015, 0.0013, 0.0015, 0.0018, 0.0016, 0.0009,\n                  0.0008, 0.0014, 0.0025, 0.0013, 0.0009, 0.0010, 0.0018, 0.0016, 0.0018,\n                  0.0009, 0.0010, 0.0014, 0.0012, 0.0011, 0.0017, 0.0014, 0.0011, 0.0019,\n                  0.0015, 0.0015, 0.0016, 0.0013, 0.0020, 0.0013, 0.0012, 0.0012, 0.0014,\n                  0.0016, 0.0014, 0.0014, 0.0009, 0.0015, 0.0012, 0.0013, 0.0014, 0.0014,\n                  0.0013, 0.0018, 0.0015, 0.0018, 0.0025, 0.0013, 0.0011, 0.0013, 0.0014,\n                  0.0013, 0.0014, 0.0010, 0.0012, 0.0013, 0.0009, 0.0012, 0.0011, 0.0017,\n                  0.0018, 0.0011, 0.0019, 0.0014, 0.0021, 0.0017, 0.0014, 0.0016, 0.0008,\n                  0.0022, 0.0012, 0.0015, 0.0019, 0.0011, 0.0015, 0.0024, 0.0016, 0.0020,\n                  0.0014, 0.0019, 0.0013, 0.0010, 0.0012, 0.0021, 0.0019, 0.0012, 0.0016,\n                  0.0019, 0.0022, 0.0023, 0.0013, 0.0016, 0.0015, 0.0010, 0.0012, 0.0012,\n                  0.0013, 0.0024, 0.0025, 0.0014, 0.0010, 0.0017, 0.0011, 0.0015, 0.0019,\n                  0.0021, 0.0008, 0.0022, 0.0021, 0.0015, 0.0018, 0.0013, 0.0022, 0.0013,\n                  0.0014, 0.0017, 0.0009, 0.0012, 0.0010, 0.0010, 0.0013, 0.0014, 0.0017,\n                  0.0013, 0.0019, 0.0015, 0.0010, 0.0021, 0.0011, 0.0015, 0.0023, 0.0012,\n                  0.0015, 0.0012, 0.0014, 0.0024, 0.0004, 0.0011, 0.0019, 0.0014, 0.0011,\n                  0.0020, 0.0013, 0.0018, 0.0017, 0.0008, 0.0028, 0.0014, 0.0020, 0.0019,\n                  0.0012, 0.0012, 0.0015, 0.0014, 0.0019, 0.0009, 0.0016, 0.0011, 0.0013,\n                  0.0015, 0.0026, 0.0021]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                 dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n          (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n            min_val=tensor([-0.1835, -0.2392, -0.1196, -0.1182, -0.1571, -0.1622, -0.2210, -0.1489,\n                    -0.2995, -0.1263, -0.1261, -0.1829, -0.1360, -0.2270, -0.2317, -0.1787,\n                    -0.1300, -0.1512, -0.1802, -0.2712, -0.2238, -0.2534, -0.2177, -0.0868,\n                    -0.1676, -0.2714, -0.0958, -0.2275, -0.1329, -0.1369, -0.1450, -0.1542,\n                    -0.1958, -0.2314, -0.0920, -0.1206, -0.1023, -0.1587, -0.3229, -0.1660,\n                    -0.1170, -0.1189, -0.1801, -0.2092, -0.2054, -0.1124, -0.1173, -0.1844,\n                    -0.1546, -0.1371, -0.1602, -0.1500, -0.1431, -0.2446, -0.1961, -0.1640,\n                    -0.0821, -0.1315, -0.2619, -0.1504, -0.1589, -0.1518, -0.1809, -0.1167,\n                    -0.1473, -0.1762, -0.1164, -0.1214, -0.1561, -0.1311, -0.1524, -0.1636,\n                    -0.1650, -0.2272, -0.1873, -0.1196, -0.1423, -0.1631, -0.1378, -0.1455,\n                    -0.1809, -0.1486, -0.1735, -0.1066, -0.1473, -0.1187, -0.1160, -0.1384,\n                    -0.1292, -0.2229, -0.1766, -0.1033, -0.2440, -0.1839, -0.2679, -0.1810,\n                    -0.1760, -0.1532, -0.1025, -0.2493, -0.1088, -0.1868, -0.2397, -0.1135,\n                    -0.1294, -0.0803, -0.2024, -0.2507, -0.1813, -0.2375, -0.1699, -0.1260,\n                    -0.1505, -0.1830, -0.1764, -0.0996, -0.2054, -0.2410, -0.2383, -0.0820,\n                    -0.1414, -0.0940, -0.1911, -0.1306, -0.1243, -0.1500, -0.1697, -0.2569,\n                    -0.2290, -0.1835, -0.1280, -0.2153, -0.1456, -0.1761, -0.1011, -0.2666,\n                    -0.0664, -0.2857, -0.1899, -0.1973, -0.2265, -0.1345, -0.2862, -0.1602,\n                    -0.1600, -0.2166, -0.1205, -0.1214, -0.1291, -0.1317, -0.1598, -0.1201,\n                    -0.2086, -0.1270, -0.1736, -0.1928, -0.1019, -0.2475, -0.1219, -0.1645,\n                    -0.1517, -0.1566, -0.1929, -0.1508, -0.1827, -0.1782, -0.0458, -0.1309,\n                    -0.1996, -0.1749, -0.1420, -0.1584, -0.1269, -0.2246, -0.2199, -0.0676,\n                    -0.3579, -0.1801, -0.2227, -0.2440, -0.1487, -0.1515, -0.1290, -0.1750,\n                    -0.2414, -0.1099, -0.2066, -0.1359, -0.1691, -0.1754, -0.3311, -0.2666]), max_val=tensor([0.1304, 0.1918, 0.2142, 0.0853, 0.1711, 0.1232, 0.2658, 0.1507, 0.2019,\n                    0.2092, 0.1602, 0.1697, 0.0999, 0.1865, 0.1943, 0.1199, 0.1536, 0.1399,\n                    0.1878, 0.1210, 0.1835, 0.2141, 0.2171, 0.1196, 0.1900, 0.1493, 0.1806,\n                    0.2020, 0.1297, 0.1532, 0.1861, 0.1706, 0.1502, 0.2017, 0.2030, 0.1131,\n                    0.0885, 0.1831, 0.2102, 0.1493, 0.0966, 0.1222, 0.2343, 0.1926, 0.2239,\n                    0.0726, 0.1289, 0.1506, 0.1202, 0.1301, 0.2178, 0.1736, 0.1093, 0.1719,\n                    0.1833, 0.1881, 0.2047, 0.1611, 0.1882, 0.1644, 0.1261, 0.1264, 0.1670,\n                    0.2051, 0.1721, 0.1480, 0.0870, 0.1919, 0.1338, 0.1672, 0.1783, 0.1765,\n                    0.1506, 0.2036, 0.1628, 0.2263, 0.3149, 0.1409, 0.1291, 0.1691, 0.1662,\n                    0.1677, 0.1120, 0.1235, 0.1150, 0.1657, 0.1176, 0.1545, 0.1412, 0.1624,\n                    0.2241, 0.1345, 0.2131, 0.1398, 0.1415, 0.2127, 0.1075, 0.2010, 0.1056,\n                    0.2747, 0.1575, 0.1857, 0.2069, 0.1342, 0.1852, 0.3040, 0.1908, 0.1725,\n                    0.1711, 0.1492, 0.1502, 0.1173, 0.1388, 0.2699, 0.2357, 0.1518, 0.1836,\n                    0.1864, 0.2851, 0.2953, 0.1681, 0.2093, 0.1434, 0.1302, 0.1568, 0.1498,\n                    0.1241, 0.3048, 0.3117, 0.1322, 0.1011, 0.1517, 0.1428, 0.1941, 0.2350,\n                    0.1792, 0.1027, 0.0814, 0.2640, 0.1501, 0.1481, 0.1653, 0.0660, 0.1557,\n                    0.1800, 0.2018, 0.0991, 0.1490, 0.1134, 0.1178, 0.1680, 0.1785, 0.2138,\n                    0.1702, 0.2402, 0.1870, 0.1262, 0.2610, 0.1451, 0.1929, 0.2894, 0.1580,\n                    0.1265, 0.1526, 0.1062, 0.2995, 0.0402, 0.1412, 0.2406, 0.1433, 0.1050,\n                    0.2540, 0.1677, 0.1978, 0.1916, 0.1026, 0.2188, 0.1668, 0.2563, 0.2282,\n                    0.1439, 0.1147, 0.1958, 0.1332, 0.1749, 0.0955, 0.2000, 0.1391, 0.1640,\n                    0.1957, 0.2128, 0.1843])\n          )\n        )\n      )\n    )\n    (3): Module(\n      (0): Module(\n        (block): Module(\n          (0): Conv2d(\n            192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([2.1531e-03, 2.4138e-03, 6.3264e-04, 2.5325e-03, 2.5807e-03, 2.8599e-03,\n                      1.9035e-03, 2.5243e-03, 2.9790e-03, 8.1319e-04, 2.8379e-03, 1.5535e-03,\n                      3.1300e-03, 2.3474e-03, 4.3843e-05, 1.0389e-03, 3.0629e-03, 2.6445e-03,\n                      6.2925e-04, 2.6061e-03, 2.8497e-03, 2.9541e-03, 2.8129e-03, 4.9699e-04,\n                      2.9732e-03, 2.0476e-03, 1.9617e-03, 2.6686e-03, 2.4848e-04, 2.5464e-03,\n                      2.1461e-03, 2.6998e-03, 8.7453e-04, 2.5147e-03, 4.5753e-04, 2.4350e-03,\n                      2.0907e-03, 1.6402e-03, 2.8770e-03, 1.8486e-03, 2.9101e-04, 3.1449e-03,\n                      2.5444e-03, 2.6973e-03, 2.2998e-03, 6.3635e-04, 3.0440e-03, 1.9657e-03,\n                      2.1240e-03, 5.2992e-04, 2.6686e-03, 2.9820e-03, 2.1331e-03, 2.8608e-03,\n                      2.3759e-03, 7.2804e-04, 4.4930e-04, 2.6115e-03, 2.9213e-03, 1.7220e-03,\n                      2.4264e-03, 1.5637e-03, 1.6375e-03, 2.0879e-03, 2.2444e-03, 9.4619e-04,\n                      1.7289e-03, 1.5574e-03, 2.6616e-03, 1.1286e-03, 2.6752e-03, 2.0325e-03,\n                      2.8050e-03, 2.0769e-03, 1.9402e-03, 2.4654e-03, 1.9755e-03, 6.7442e-04,\n                      1.4000e-03, 2.4172e-03, 2.5959e-03, 2.8471e-03, 3.0247e-03, 2.9202e-03,\n                      2.7264e-03, 2.5281e-03, 2.5854e-03, 5.7228e-04, 6.3271e-04, 9.0891e-04,\n                      2.7159e-03, 2.7961e-03, 2.2056e-03, 8.8447e-04, 2.1368e-03, 2.5833e-03,\n                      2.5514e-03, 2.7348e-03, 9.4670e-04, 2.4133e-03, 2.6688e-03, 1.2372e-03,\n                      2.4034e-03, 3.0720e-03, 2.3243e-03, 3.8962e-04, 2.6447e-03, 1.7457e-03,\n                      2.4186e-03, 1.4333e-03, 5.1402e-04, 1.9128e-03, 2.9023e-03, 2.9645e-03,\n                      2.4787e-03, 1.3965e-03, 2.5944e-03, 2.5314e-03, 2.6522e-03, 1.8937e-03,\n                      2.1775e-03, 2.4305e-03, 1.2269e-03, 1.3862e-03, 8.7683e-04, 1.6035e-03,\n                      3.0292e-03, 1.9883e-03, 2.2971e-03, 6.4283e-04, 2.6339e-03, 2.4860e-03,\n                      2.7004e-03, 2.8485e-03, 1.9479e-03, 2.3500e-03, 3.2058e-03, 1.9442e-03,\n                      2.4250e-03, 2.3909e-03, 1.9381e-03, 2.8453e-03, 2.3528e-03, 2.9145e-03,\n                      6.0806e-04, 2.5302e-03, 7.7929e-04, 2.7268e-03, 2.2027e-03, 2.9301e-03,\n                      4.2547e-04, 2.1622e-03, 2.2458e-03, 6.9220e-04, 2.6553e-03, 2.5530e-03,\n                      1.4263e-03, 2.3011e-03, 2.8987e-03, 1.7266e-03, 9.5707e-04, 2.8893e-03,\n                      1.6198e-03, 2.3318e-03, 4.3638e-04, 2.7586e-03, 8.5524e-04, 1.6507e-03,\n                      1.3231e-03, 2.8257e-03, 8.7134e-04, 2.7692e-03, 2.4489e-03, 2.6044e-03,\n                      6.4578e-04, 4.8405e-04, 2.6356e-03, 2.2260e-03, 2.4090e-03, 3.0023e-03,\n                      2.8061e-03, 2.9009e-03, 1.9984e-03, 2.7834e-03, 2.4537e-03, 5.1173e-04,\n                      2.4902e-03, 2.2694e-03, 5.1025e-04, 2.2163e-03, 2.9578e-03, 2.7799e-03]), zero_point=tensor([   0,    0, -128,    0,    0,    0,    0,    0,    0,  127,    0,    0,\n                         0,    0,    0,  127,    0,    0, -128,    0,    0,    0,    0, -128,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0, -128,    0,\n                         0,    0,    0,    0, -128,    0,    0,    0,    0, -128,    0,    0,\n                         0,  127,    0,    0,    0,    0,    0,    0,  127,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,  127,    0,    0,\n                         0,    0,    0,    0,    0,  127,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0, -128, -128,    0,    0,    0,    0, -128,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,  127,    0,    0,\n                         0,    0,  127,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0, -128,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                      -128,    0,  127,    0,    0,    0,    0,    0,    0,  127,    0,    0,\n                         0,    0,    0,    0,  127,    0,    0,    0, -128,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,  127,  127,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0, -128,    0,    0,  127,    0,    0,    0],\n                     dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n                min_val=tensor([-0.2756, -0.3090,  0.0021, -0.1238, -0.3303, -0.3661, -0.2056, -0.0274,\n                        -0.3813, -0.2074, -0.3632, -0.1989, -0.4006, -0.0208, -0.0045, -0.2649,\n                        -0.3920, -0.0238,  0.0030, -0.0464, -0.3648, -0.0820, -0.3601,  0.0019,\n                        -0.0524, -0.0105, -0.2511, -0.0674, -0.0318, -0.0422, -0.2747, -0.3456,\n                        -0.1119, -0.3219,  0.0010, -0.3117, -0.1962, -0.2100, -0.3683, -0.0011,\n                         0.0054, -0.4025, -0.3257, -0.0306, -0.2944,  0.0062, -0.3896, -0.2516,\n                        -0.1276, -0.1351, -0.0907, -0.0681, -0.0502, -0.0643, -0.1393, -0.0481,\n                        -0.1146, -0.3343, -0.0759, -0.0023, -0.3106, -0.0038, -0.1982, -0.1594,\n                        -0.1483, -0.0270, -0.1039, -0.0046, -0.0505, -0.2878, -0.0290, -0.1951,\n                        -0.3590, -0.2305, -0.2484, -0.3156, -0.2529, -0.1720, -0.1792, -0.0176,\n                        -0.3323, -0.3644, -0.0473, -0.3738, -0.0578, -0.3236, -0.2781,  0.0053,\n                         0.0058, -0.1163, -0.0354, -0.0409, -0.0789,  0.0010, -0.2735, -0.3307,\n                        -0.3266, -0.0186, -0.1212, -0.3089, -0.3416, -0.1584, -0.3076, -0.3932,\n                        -0.2975, -0.0994, -0.3385, -0.0522, -0.3096, -0.1196, -0.1311, -0.2448,\n                        -0.3715, -0.0685, -0.0216, -0.1787, -0.1660, -0.1135, -0.2018, -0.2424,\n                        -0.2787, -0.1506, -0.0036, -0.1774,  0.0031, -0.0077, -0.0440, -0.2545,\n                        -0.2940, -0.0136, -0.3371, -0.0730, -0.0134, -0.3646, -0.2493, -0.2465,\n                        -0.4103, -0.2288, -0.0868, -0.3060, -0.2481, -0.3642, -0.2561, -0.0409,\n                         0.0088, -0.3239, -0.1987, -0.3490, -0.2246, -0.0147, -0.0545, -0.2768,\n                        -0.2875, -0.1765, -0.0355, -0.0833, -0.1826, -0.2945, -0.3710, -0.0019,\n                        -0.2441, -0.0340, -0.2073, -0.0105,  0.0013, -0.0622, -0.1095, -0.2113,\n                        -0.1423, -0.3617, -0.1115, -0.3545, -0.0061, -0.0789, -0.1647, -0.1234,\n                        -0.1503, -0.0528, -0.0559, -0.3843, -0.3592, -0.0104, -0.2558, -0.0189,\n                        -0.3141,  0.0074, -0.0464, -0.1273, -0.1301, -0.1542, -0.3786, -0.3558]), max_val=tensor([ 8.6018e-02,  7.1424e-02,  1.6132e-01,  3.2163e-01,  1.4416e-02,\n                         6.9181e-02,  2.4175e-01,  3.2058e-01,  2.3488e-02, -8.0907e-03,\n                         4.5639e-02,  6.0387e-03,  3.9833e-02,  2.9811e-01,  5.5681e-03,\n                        -2.4345e-03,  6.7047e-02,  3.3585e-01,  1.6046e-01,  3.3097e-01,\n                         1.3954e-01,  3.7517e-01,  6.5187e-02,  1.2673e-01,  3.7759e-01,\n                         2.6005e-01,  5.3658e-02,  3.3891e-01,  1.2244e-02,  3.2339e-01,\n                         6.3146e-03,  7.1546e-02,  1.0992e-01,  8.9189e-02,  1.1667e-01,\n                         1.2385e-01,  2.6551e-01,  4.1295e-04,  7.3306e-02,  2.3478e-01,\n                         7.4207e-02,  5.9358e-02,  1.5702e-02,  3.4256e-01,  5.6223e-02,\n                         1.6227e-01,  2.9922e-02,  2.4782e-01,  2.6975e-01, -2.7493e-03,\n                         3.3891e-01,  3.7871e-01,  2.7090e-01,  3.6332e-01,  3.0174e-01,\n                         9.2461e-02, -1.1492e-03,  1.6302e-02,  3.7101e-01,  2.1869e-01,\n                         9.6713e-02,  1.9859e-01,  2.0797e-01,  2.6516e-01,  2.8504e-01,\n                         1.2017e-01,  2.1958e-01,  1.9779e-01,  3.3803e-01, -2.6156e-03,\n                         3.3975e-01,  2.5812e-01,  4.1058e-02,  2.6377e-01,  1.8406e-01,\n                         1.7037e-02,  1.6918e-01, -5.7989e-03,  5.5282e-04,  3.0698e-01,\n                         1.4540e-02,  2.6137e-02,  3.8413e-01,  2.6624e-02,  3.4625e-01,\n                         1.1615e-02,  3.2834e-01,  1.4593e-01,  1.6134e-01,  1.0611e-01,\n                         3.4492e-01,  3.5511e-01,  2.8011e-01,  2.2554e-01,  2.1462e-01,\n                         5.7915e-02,  2.6100e-02,  3.4733e-01,  5.9462e-04,  7.0150e-02,\n                         1.2354e-01,  1.4590e-01,  8.5360e-02,  4.5755e-02,  2.4256e-03,\n                        -4.7889e-04,  8.6727e-02,  2.2171e-01,  9.3100e-02,  1.8202e-01,\n                        -3.2367e-04,  1.6093e-01,  9.4635e-02,  3.7649e-01,  3.1479e-01,\n                         6.3150e-02,  3.2948e-01,  3.2149e-01,  3.3683e-01,  1.5185e-01,\n                         1.3024e-02,  3.0867e-01,  1.5582e-01,  5.5008e-02,  2.2359e-01,\n                         2.0365e-01,  3.8471e-01,  6.8069e-02,  1.7021e-01,  8.1640e-02,\n                         2.3884e-01,  3.1572e-01,  3.4296e-01,  2.3810e-02,  2.0470e-01,\n                         2.9845e-01,  2.9506e-02,  2.4691e-01,  3.0798e-01,  4.8656e-03,\n                         1.9500e-01,  6.2217e-02,  2.9880e-01,  3.7014e-01,  1.5505e-01,\n                         7.4466e-02, -3.5244e-03,  1.8734e-02,  2.7974e-01,  3.7212e-01,\n                         4.7254e-02,  1.6190e-01,  2.8943e-03, -3.8412e-03,  3.3722e-01,\n                         3.2423e-01,  1.3574e-01,  1.4394e-01,  2.0023e-02,  2.1928e-01,\n                        -8.7279e-04,  3.6694e-01,  2.2611e-03,  2.9614e-01,  1.1128e-01,\n                         3.5035e-01,  8.4696e-02,  1.9980e-01,  1.6803e-01,  8.6953e-02,\n                         4.3311e-04,  1.0426e-02,  3.1101e-01,  3.3076e-01, -2.7745e-04,\n                        -1.7281e-03,  3.3472e-01,  2.8270e-01,  3.0595e-01,  8.2227e-02,\n                         5.7673e-02,  3.6841e-01,  2.3725e-03,  3.5349e-01,  1.0034e-01,\n                         1.3049e-01,  3.1626e-01,  2.8822e-01, -4.1169e-03,  2.8148e-01,\n                         8.2076e-02,  3.2000e-02])\n              )\n            )\n          )\n          (2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(\n            in_features=192, out_features=768, bias=True\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0013, 0.0018, 0.0017, 0.0019, 0.0013, 0.0016, 0.0020, 0.0013, 0.0012,\n                      0.0015, 0.0017, 0.0011, 0.0019, 0.0023, 0.0020, 0.0014, 0.0022, 0.0012,\n                      0.0010, 0.0012, 0.0018, 0.0016, 0.0013, 0.0018, 0.0017, 0.0014, 0.0016,\n                      0.0026, 0.0012, 0.0030, 0.0014, 0.0015, 0.0018, 0.0026, 0.0019, 0.0017,\n                      0.0016, 0.0018, 0.0017, 0.0020, 0.0033, 0.0020, 0.0015, 0.0018, 0.0017,\n                      0.0013, 0.0018, 0.0019, 0.0031, 0.0016, 0.0009, 0.0011, 0.0018, 0.0015,\n                      0.0010, 0.0028, 0.0019, 0.0016, 0.0011, 0.0015, 0.0014, 0.0014, 0.0019,\n                      0.0015, 0.0015, 0.0011, 0.0017, 0.0020, 0.0010, 0.0015, 0.0014, 0.0013,\n                      0.0011, 0.0014, 0.0023, 0.0020, 0.0016, 0.0017, 0.0016, 0.0022, 0.0011,\n                      0.0029, 0.0016, 0.0016, 0.0017, 0.0019, 0.0014, 0.0015, 0.0011, 0.0017,\n                      0.0013, 0.0016, 0.0014, 0.0018, 0.0015, 0.0012, 0.0016, 0.0011, 0.0014,\n                      0.0014, 0.0020, 0.0027, 0.0013, 0.0014, 0.0013, 0.0012, 0.0028, 0.0018,\n                      0.0016, 0.0014, 0.0014, 0.0020, 0.0021, 0.0017, 0.0016, 0.0013, 0.0012,\n                      0.0014, 0.0019, 0.0010, 0.0021, 0.0021, 0.0015, 0.0017, 0.0024, 0.0020,\n                      0.0016, 0.0021, 0.0016, 0.0017, 0.0013, 0.0014, 0.0014, 0.0019, 0.0021,\n                      0.0030, 0.0013, 0.0019, 0.0013, 0.0014, 0.0013, 0.0015, 0.0011, 0.0017,\n                      0.0011, 0.0018, 0.0015, 0.0016, 0.0016, 0.0010, 0.0024, 0.0016, 0.0021,\n                      0.0016, 0.0018, 0.0018, 0.0017, 0.0013, 0.0022, 0.0017, 0.0011, 0.0015,\n                      0.0031, 0.0019, 0.0016, 0.0015, 0.0016, 0.0015, 0.0023, 0.0014, 0.0019,\n                      0.0013, 0.0014, 0.0017, 0.0015, 0.0015, 0.0018, 0.0017, 0.0016, 0.0017,\n                      0.0015, 0.0027, 0.0017, 0.0016, 0.0019, 0.0020, 0.0011, 0.0013, 0.0015,\n                      0.0015, 0.0032, 0.0016, 0.0011, 0.0014, 0.0015, 0.0017, 0.0018, 0.0012,\n                      0.0014, 0.0023, 0.0017, 0.0014, 0.0014, 0.0010, 0.0013, 0.0018, 0.0016,\n                      0.0029, 0.0025, 0.0012, 0.0018, 0.0018, 0.0024, 0.0014, 0.0015, 0.0018,\n                      0.0027, 0.0017, 0.0037, 0.0018, 0.0010, 0.0018, 0.0022, 0.0013, 0.0014,\n                      0.0014, 0.0011, 0.0017, 0.0012, 0.0016, 0.0019, 0.0021, 0.0014, 0.0014,\n                      0.0012, 0.0013, 0.0014, 0.0018, 0.0018, 0.0012, 0.0022, 0.0015, 0.0014,\n                      0.0019, 0.0015, 0.0019, 0.0018, 0.0011, 0.0021, 0.0024, 0.0020, 0.0014,\n                      0.0014, 0.0018, 0.0011, 0.0017, 0.0022, 0.0016, 0.0017, 0.0018, 0.0015,\n                      0.0025, 0.0013, 0.0014, 0.0019, 0.0013, 0.0018, 0.0013, 0.0017, 0.0015,\n                      0.0024, 0.0016, 0.0017, 0.0010, 0.0020, 0.0014, 0.0012, 0.0011, 0.0014,\n                      0.0014, 0.0012, 0.0014, 0.0018, 0.0017, 0.0012, 0.0028, 0.0014, 0.0009,\n                      0.0020, 0.0016, 0.0019, 0.0014, 0.0018, 0.0017, 0.0015, 0.0023, 0.0011,\n                      0.0017, 0.0014, 0.0017, 0.0020, 0.0027, 0.0013, 0.0034, 0.0014, 0.0016,\n                      0.0030, 0.0026, 0.0015, 0.0021, 0.0017, 0.0015, 0.0019, 0.0013, 0.0013,\n                      0.0012, 0.0017, 0.0018, 0.0017, 0.0017, 0.0016, 0.0033, 0.0015, 0.0020,\n                      0.0027, 0.0022, 0.0027, 0.0013, 0.0021, 0.0011, 0.0018, 0.0015, 0.0017,\n                      0.0016, 0.0013, 0.0020, 0.0013, 0.0023, 0.0016, 0.0013, 0.0017, 0.0017,\n                      0.0013, 0.0017, 0.0014, 0.0014, 0.0015, 0.0020, 0.0015, 0.0010, 0.0022,\n                      0.0012, 0.0014, 0.0019, 0.0020, 0.0015, 0.0013, 0.0018, 0.0011, 0.0017,\n                      0.0015, 0.0020, 0.0012, 0.0016, 0.0013, 0.0014, 0.0028, 0.0032, 0.0013,\n                      0.0033, 0.0015, 0.0017, 0.0021, 0.0015, 0.0015, 0.0030, 0.0018, 0.0013,\n                      0.0014, 0.0014, 0.0018, 0.0026, 0.0015, 0.0014, 0.0014, 0.0028, 0.0021,\n                      0.0017, 0.0019, 0.0017, 0.0017, 0.0020, 0.0014, 0.0010, 0.0016, 0.0019,\n                      0.0019, 0.0021, 0.0011, 0.0017, 0.0018, 0.0015, 0.0012, 0.0018, 0.0016,\n                      0.0028, 0.0013, 0.0014, 0.0016, 0.0022, 0.0012, 0.0031, 0.0021, 0.0014,\n                      0.0027, 0.0013, 0.0016, 0.0018, 0.0011, 0.0030, 0.0018, 0.0015, 0.0018,\n                      0.0021, 0.0014, 0.0023, 0.0026, 0.0016, 0.0013, 0.0016, 0.0014, 0.0016,\n                      0.0013, 0.0031, 0.0017, 0.0010, 0.0016, 0.0014, 0.0017, 0.0012, 0.0012,\n                      0.0016, 0.0015, 0.0016, 0.0021, 0.0026, 0.0015, 0.0014, 0.0018, 0.0018,\n                      0.0022, 0.0020, 0.0022, 0.0022, 0.0017, 0.0012, 0.0014, 0.0018, 0.0015,\n                      0.0029, 0.0017, 0.0019, 0.0023, 0.0015, 0.0013, 0.0015, 0.0016, 0.0016,\n                      0.0014, 0.0019, 0.0019, 0.0013, 0.0028, 0.0015, 0.0019, 0.0014, 0.0014,\n                      0.0032, 0.0012, 0.0017, 0.0015, 0.0011, 0.0015, 0.0018, 0.0016, 0.0015,\n                      0.0019, 0.0012, 0.0015, 0.0012, 0.0012, 0.0015, 0.0016, 0.0021, 0.0014,\n                      0.0013, 0.0013, 0.0019, 0.0012, 0.0017, 0.0017, 0.0023, 0.0013, 0.0013,\n                      0.0015, 0.0022, 0.0014, 0.0027, 0.0020, 0.0014, 0.0015, 0.0015, 0.0018,\n                      0.0016, 0.0015, 0.0014, 0.0020, 0.0014, 0.0019, 0.0013, 0.0031, 0.0016,\n                      0.0019, 0.0016, 0.0016, 0.0013, 0.0012, 0.0014, 0.0020, 0.0018, 0.0018,\n                      0.0013, 0.0013, 0.0011, 0.0015, 0.0015, 0.0029, 0.0012, 0.0013, 0.0013,\n                      0.0032, 0.0015, 0.0021, 0.0013, 0.0019, 0.0018, 0.0013, 0.0019, 0.0012,\n                      0.0018, 0.0015, 0.0016, 0.0014, 0.0024, 0.0017, 0.0016, 0.0012, 0.0014,\n                      0.0014, 0.0027, 0.0018, 0.0012, 0.0015, 0.0018, 0.0011, 0.0012, 0.0019,\n                      0.0013, 0.0020, 0.0027, 0.0013, 0.0018, 0.0015, 0.0021, 0.0012, 0.0014,\n                      0.0027, 0.0025, 0.0022, 0.0014, 0.0016, 0.0026, 0.0014, 0.0028, 0.0015,\n                      0.0016, 0.0013, 0.0018, 0.0017, 0.0012, 0.0017, 0.0019, 0.0018, 0.0013,\n                      0.0016, 0.0018, 0.0015, 0.0019, 0.0019, 0.0015, 0.0016, 0.0013, 0.0017,\n                      0.0016, 0.0012, 0.0013, 0.0013, 0.0022, 0.0018, 0.0021, 0.0014, 0.0016,\n                      0.0015, 0.0016, 0.0016, 0.0026, 0.0015, 0.0019, 0.0019, 0.0020, 0.0012,\n                      0.0013, 0.0013, 0.0012, 0.0018, 0.0015, 0.0013, 0.0016, 0.0014, 0.0013,\n                      0.0026, 0.0013, 0.0014, 0.0018, 0.0013, 0.0016, 0.0019, 0.0014, 0.0017,\n                      0.0016, 0.0016, 0.0017, 0.0019, 0.0014, 0.0013, 0.0027, 0.0010, 0.0014,\n                      0.0013, 0.0016, 0.0015, 0.0012, 0.0016, 0.0016, 0.0018, 0.0015, 0.0024,\n                      0.0027, 0.0019, 0.0014, 0.0022, 0.0016, 0.0013, 0.0017, 0.0016, 0.0012,\n                      0.0026, 0.0017, 0.0014, 0.0013, 0.0024, 0.0014, 0.0017, 0.0013, 0.0013,\n                      0.0021, 0.0015, 0.0019, 0.0011, 0.0010, 0.0015, 0.0013, 0.0025, 0.0017,\n                      0.0017, 0.0017, 0.0017, 0.0016, 0.0014, 0.0028, 0.0020, 0.0017, 0.0018,\n                      0.0019, 0.0012, 0.0014, 0.0015, 0.0016, 0.0013, 0.0019, 0.0013, 0.0013,\n                      0.0033, 0.0017, 0.0025, 0.0015, 0.0025, 0.0014, 0.0010, 0.0018, 0.0012,\n                      0.0012, 0.0027, 0.0021, 0.0016, 0.0014, 0.0012, 0.0018, 0.0013, 0.0019,\n                      0.0014, 0.0018, 0.0013, 0.0013, 0.0012, 0.0016, 0.0012, 0.0015, 0.0027,\n                      0.0010, 0.0012, 0.0017, 0.0016, 0.0018, 0.0013, 0.0020, 0.0014, 0.0012,\n                      0.0015, 0.0024, 0.0013, 0.0014, 0.0013, 0.0013, 0.0010, 0.0027, 0.0012,\n                      0.0016, 0.0009, 0.0016, 0.0016, 0.0014, 0.0026, 0.0013, 0.0013, 0.0018,\n                      0.0010, 0.0015, 0.0014, 0.0018, 0.0016, 0.0018, 0.0011, 0.0018, 0.0012,\n                      0.0015, 0.0026, 0.0012]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                     dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n                min_val=tensor([-0.1618, -0.2308, -0.1818, -0.2435, -0.1384, -0.1990, -0.2044, -0.1551,\n                        -0.1324, -0.1893, -0.2131, -0.1362, -0.1862, -0.1981, -0.2619, -0.1818,\n                        -0.2452, -0.1553, -0.1264, -0.1048, -0.2281, -0.2010, -0.1681, -0.2244,\n                        -0.2176, -0.1449, -0.1428, -0.1565, -0.1387, -0.1199, -0.1436, -0.1513,\n                        -0.1647, -0.3321, -0.2483, -0.2179, -0.1985, -0.2302, -0.2140, -0.2540,\n                        -0.1326, -0.2529, -0.1457, -0.2220, -0.2210, -0.1413, -0.2334, -0.2395,\n                        -0.4011, -0.1463, -0.1149, -0.1414, -0.1757, -0.1544, -0.1274, -0.3596,\n                        -0.1754, -0.2004, -0.1414, -0.1868, -0.1754, -0.1788, -0.1830, -0.1428,\n                        -0.1955, -0.1398, -0.2224, -0.2599, -0.1172, -0.1404, -0.1607, -0.1489,\n                        -0.1434, -0.1736, -0.2943, -0.2539, -0.1985, -0.2165, -0.2033, -0.2489,\n                        -0.1354, -0.3764, -0.2013, -0.1025, -0.2165, -0.2471, -0.1559, -0.1441,\n                        -0.1322, -0.2125, -0.1706, -0.1745, -0.1537, -0.2264, -0.1940, -0.1519,\n                        -0.2103, -0.1428, -0.1290, -0.1820, -0.2573, -0.3456, -0.1587, -0.1761,\n                        -0.1652, -0.1246, -0.1486, -0.2174, -0.1513, -0.1813, -0.1754, -0.2548,\n                        -0.1249, -0.1788, -0.2039, -0.1701, -0.1449, -0.1743, -0.1931, -0.1234,\n                        -0.1913, -0.2637, -0.1547, -0.1390, -0.1982, -0.2613, -0.1177, -0.1387,\n                        -0.2103, -0.1510, -0.1628, -0.1850, -0.1802, -0.1447, -0.2679, -0.1281,\n                        -0.1283, -0.2494, -0.1634, -0.1808, -0.1247, -0.1305, -0.1371, -0.1431,\n                        -0.1434, -0.2288, -0.1586, -0.1852, -0.2075, -0.1253, -0.3109, -0.1990,\n                        -0.1629, -0.1989, -0.1268, -0.2335, -0.1518, -0.1690, -0.1230, -0.2162,\n                        -0.1415, -0.1438, -0.2017, -0.1834, -0.1359, -0.1980, -0.2001, -0.1867,\n                        -0.2983, -0.1518, -0.2392, -0.1653, -0.1681, -0.1733, -0.1711, -0.1512,\n                        -0.1922, -0.1750, -0.2094, -0.1845, -0.1947, -0.3502, -0.1611, -0.1053,\n                        -0.2422, -0.1667, -0.1442, -0.1650, -0.1717, -0.1943, -0.4035, -0.2020,\n                        -0.1265, -0.1779, -0.1720, -0.2115, -0.2247, -0.1513, -0.1459, -0.2915,\n                        -0.1788, -0.1793, -0.1839, -0.1121, -0.1346, -0.2341, -0.1818, -0.3756,\n                        -0.3215, -0.1579, -0.1428, -0.1451, -0.3103, -0.1842, -0.1695, -0.2247,\n                        -0.2957, -0.2224, -0.4731, -0.1771, -0.1096, -0.2355, -0.2776, -0.1466,\n                        -0.1781, -0.1403, -0.1376, -0.1837, -0.1197, -0.1982, -0.1665, -0.1905,\n                        -0.1757, -0.1609, -0.1462, -0.1728, -0.1746, -0.2326, -0.2283, -0.1249,\n                        -0.2781, -0.1318, -0.1411, -0.2432, -0.1980, -0.2407, -0.2271, -0.1436,\n                        -0.2645, -0.3064, -0.2133, -0.1542, -0.1394, -0.2348, -0.1330, -0.1788,\n                        -0.2624, -0.1230, -0.2033, -0.1776, -0.1379, -0.3256, -0.1655, -0.1674,\n                        -0.1536, -0.1670, -0.2264, -0.1615, -0.2235, -0.1853, -0.3046, -0.2109,\n                        -0.1421, -0.1182, -0.2620, -0.1828, -0.1242, -0.1364, -0.1229, -0.1823,\n                        -0.1580, -0.1415, -0.1803, -0.2027, -0.1173, -0.3591, -0.1593, -0.1177,\n                        -0.2552, -0.2092, -0.1302, -0.1749, -0.2319, -0.1581, -0.1896, -0.0828,\n                        -0.1325, -0.1896, -0.1335, -0.2184, -0.1792, -0.3515, -0.1679, -0.4400,\n                        -0.1228, -0.2060, -0.3816, -0.1951, -0.1426, -0.2454, -0.2152, -0.1382,\n                        -0.2050, -0.1195, -0.1407, -0.1467, -0.2198, -0.2339, -0.2186, -0.1681,\n                        -0.2110, -0.4175, -0.1809, -0.2048, -0.3456, -0.2798, -0.3447, -0.1450,\n                        -0.2679, -0.1466, -0.2058, -0.1873, -0.2164, -0.1625, -0.1556, -0.1980,\n                        -0.1647, -0.2882, -0.1260, -0.1370, -0.2220, -0.1423, -0.1500, -0.2146,\n                        -0.1757, -0.1602, -0.1437, -0.2527, -0.1178, -0.1199, -0.2879, -0.1475,\n                        -0.1795, -0.1520, -0.1797, -0.1492, -0.1503, -0.2279, -0.1465, -0.2007,\n                        -0.1936, -0.1431, -0.1567, -0.2091, -0.1165, -0.1849, -0.3542, -0.2431,\n                        -0.1595, -0.1231, -0.1478, -0.2208, -0.1235, -0.1139, -0.1423, -0.1518,\n                        -0.2259, -0.1643, -0.1663, -0.1687, -0.1887, -0.3304, -0.1888, -0.1836,\n                        -0.1820, -0.1074, -0.1756, -0.2207, -0.2391, -0.1703, -0.2176, -0.2563,\n                        -0.1732, -0.1243, -0.2096, -0.2150, -0.2445, -0.1485, -0.1341, -0.2234,\n                        -0.1994, -0.1871, -0.1576, -0.2246, -0.2003, -0.3625, -0.1679, -0.1515,\n                        -0.1515, -0.2865, -0.1213, -0.1771, -0.1494, -0.1499, -0.3449, -0.1625,\n                        -0.2000, -0.1838, -0.1368, -0.2092, -0.2108, -0.1418, -0.2350, -0.2380,\n                        -0.1363, -0.2943, -0.3329, -0.2097, -0.1624, -0.1738, -0.1802, -0.2054,\n                        -0.1692, -0.1578, -0.1964, -0.1300, -0.2008, -0.1831, -0.2198, -0.1433,\n                        -0.1464, -0.1995, -0.1386, -0.2084, -0.1760, -0.2362, -0.1641, -0.1775,\n                        -0.2128, -0.2094, -0.2841, -0.1906, -0.2814, -0.2788, -0.1753, -0.1485,\n                        -0.1794, -0.1541, -0.1964, -0.1193, -0.2131, -0.2400, -0.2906, -0.1420,\n                        -0.1538, -0.1951, -0.1831, -0.1610, -0.1710, -0.2466, -0.2404, -0.1691,\n                        -0.3627, -0.1925, -0.2449, -0.1340, -0.1123, -0.4047, -0.1537, -0.2161,\n                        -0.1782, -0.1465, -0.1968, -0.2300, -0.1404, -0.1684, -0.2450, -0.1507,\n                        -0.1913, -0.1440, -0.1388, -0.1820, -0.1793, -0.2169, -0.1712, -0.1567,\n                        -0.1538, -0.1109, -0.1478, -0.2160, -0.1593, -0.2237, -0.1211, -0.1394,\n                        -0.1948, -0.1144, -0.1669, -0.1911, -0.2359, -0.1796, -0.1954, -0.1730,\n                        -0.2359, -0.1378, -0.1885, -0.1818, -0.1941, -0.1752, -0.2171, -0.1429,\n                        -0.3933, -0.2020, -0.2414, -0.1667, -0.1580, -0.1622, -0.1416, -0.1795,\n                        -0.2516, -0.2113, -0.1230, -0.1669, -0.1386, -0.1470, -0.1868, -0.1299,\n                        -0.2119, -0.1416, -0.1684, -0.1689, -0.4041, -0.1416, -0.2526, -0.1626,\n                        -0.1738, -0.1656, -0.1534, -0.1346, -0.1309, -0.1836, -0.1906, -0.1334,\n                        -0.1578, -0.2933, -0.2205, -0.1678, -0.1115, -0.1764, -0.1853, -0.3464,\n                        -0.1742, -0.1505, -0.1345, -0.2146, -0.1412, -0.1502, -0.2448, -0.1618,\n                        -0.2575, -0.1511, -0.1630, -0.2292, -0.1670, -0.2750, -0.1282, -0.1675,\n                        -0.1649, -0.3190, -0.2823, -0.1789, -0.1971, -0.1428, -0.1457, -0.2698,\n                        -0.1897, -0.1723, -0.1696, -0.2266, -0.2113, -0.1216, -0.1561, -0.1415,\n                        -0.2308, -0.1612, -0.1260, -0.2015, -0.1499, -0.2441, -0.1299, -0.1688,\n                        -0.2029, -0.1681, -0.1871, -0.1672, -0.1451, -0.1705, -0.1680, -0.2847,\n                        -0.2326, -0.1556, -0.1367, -0.2111, -0.1818, -0.1730, -0.1380, -0.2607,\n                        -0.1975, -0.1798, -0.1951, -0.2115, -0.1568, -0.1469, -0.1689, -0.1475,\n                        -0.2187, -0.1859, -0.1632, -0.1987, -0.1788, -0.1360, -0.3305, -0.1533,\n                        -0.1511, -0.2252, -0.1353, -0.1291, -0.1568, -0.1541, -0.1893, -0.1957,\n                        -0.2038, -0.1854, -0.1182, -0.1738, -0.1474, -0.1250, -0.1248, -0.1818,\n                        -0.1652, -0.2062, -0.1699, -0.1303, -0.1446, -0.2048, -0.2334, -0.1934,\n                        -0.2802, -0.2983, -0.2433, -0.1211, -0.2310, -0.2023, -0.1656, -0.1619,\n                        -0.2034, -0.1554, -0.1473, -0.2100, -0.1738, -0.1651, -0.3124, -0.1559,\n                        -0.2155, -0.1619, -0.1640, -0.2696, -0.1169, -0.2441, -0.1426, -0.1328,\n                        -0.1248, -0.1649, -0.3237, -0.2141, -0.2228, -0.2114, -0.2165, -0.2107,\n                        -0.1505, -0.3615, -0.1946, -0.2181, -0.2258, -0.1848, -0.1489, -0.1501,\n                        -0.1906, -0.2100, -0.1700, -0.2418, -0.1657, -0.1586, -0.1213, -0.2159,\n                        -0.3198, -0.1696, -0.1404, -0.1805, -0.1142, -0.2197, -0.1531, -0.1333,\n                        -0.3441, -0.2743, -0.1948, -0.1323, -0.1059, -0.1514, -0.1464, -0.2233,\n                        -0.1743, -0.1765, -0.1449, -0.1611, -0.1498, -0.1680, -0.1512, -0.1733,\n                        -0.2146, -0.1251, -0.0999, -0.2160, -0.2112, -0.2283, -0.1686, -0.2519,\n                        -0.1585, -0.1547, -0.1883, -0.2288, -0.1711, -0.1577, -0.1603, -0.1589,\n                        -0.1223, -0.3460, -0.1409, -0.1854, -0.1116, -0.1484, -0.2085, -0.1347,\n                        -0.2412, -0.1612, -0.1656, -0.2359, -0.1262, -0.1089, -0.1779, -0.2335,\n                        -0.2057, -0.2279, -0.1467, -0.2206, -0.1215, -0.1966, -0.3338, -0.1013]), max_val=tensor([0.1330, 0.2181, 0.2133, 0.2194, 0.1657, 0.1844, 0.2574, 0.1653, 0.1558,\n                        0.1783, 0.1759, 0.1406, 0.2383, 0.2882, 0.1629, 0.1653, 0.2760, 0.1447,\n                        0.1282, 0.1572, 0.1476, 0.1588, 0.1240, 0.1642, 0.2171, 0.1811, 0.2035,\n                        0.3299, 0.1559, 0.3827, 0.1720, 0.1899, 0.2247, 0.2422, 0.2183, 0.2011,\n                        0.1894, 0.1501, 0.1451, 0.1558, 0.4244, 0.1923, 0.1935, 0.2240, 0.1466,\n                        0.1660, 0.2044, 0.1937, 0.1981, 0.2031, 0.1202, 0.1226, 0.2299, 0.1910,\n                        0.1319, 0.1887, 0.2397, 0.1737, 0.1171, 0.1535, 0.1292, 0.1402, 0.2448,\n                        0.1886, 0.1782, 0.1448, 0.2160, 0.1717, 0.1331, 0.1880, 0.1761, 0.1652,\n                        0.1188, 0.1453, 0.1433, 0.2400, 0.1385, 0.1831, 0.1335, 0.2754, 0.1443,\n                        0.1846, 0.1633, 0.2093, 0.1731, 0.1431, 0.1841, 0.1916, 0.1338, 0.1386,\n                        0.1067, 0.2063, 0.1768, 0.2146, 0.1620, 0.1198, 0.1685, 0.1400, 0.1834,\n                        0.1259, 0.1917, 0.1995, 0.1627, 0.1492, 0.1592, 0.1534, 0.3530, 0.2285,\n                        0.2014, 0.1740, 0.1825, 0.1787, 0.2643, 0.2108, 0.1968, 0.1526, 0.1567,\n                        0.1512, 0.2415, 0.1297, 0.2630, 0.1599, 0.1931, 0.2177, 0.2990, 0.1169,\n                        0.2024, 0.2719, 0.1972, 0.2138, 0.1528, 0.1753, 0.1549, 0.2354, 0.2432,\n                        0.3782, 0.1687, 0.1831, 0.1527, 0.1331, 0.1708, 0.1953, 0.1330, 0.2185,\n                        0.1411, 0.1808, 0.1889, 0.1974, 0.1429, 0.1282, 0.0915, 0.1817, 0.2617,\n                        0.1755, 0.2319, 0.1887, 0.2134, 0.1538, 0.2818, 0.1120, 0.1192, 0.1904,\n                        0.3973, 0.2412, 0.2077, 0.1674, 0.2035, 0.1695, 0.1511, 0.1736, 0.1743,\n                        0.1447, 0.1796, 0.2155, 0.1943, 0.1938, 0.2277, 0.2099, 0.1459, 0.2103,\n                        0.1300, 0.3202, 0.2156, 0.2049, 0.1879, 0.2504, 0.1448, 0.1374, 0.1916,\n                        0.1786, 0.0847, 0.1432, 0.1454, 0.1445, 0.1894, 0.1778, 0.1504, 0.1192,\n                        0.1822, 0.1873, 0.2205, 0.0768, 0.1812, 0.1242, 0.1597, 0.2302, 0.2030,\n                        0.2290, 0.1397, 0.1326, 0.2246, 0.2295, 0.1948, 0.1196, 0.1946, 0.2202,\n                        0.3477, 0.2037, 0.2212, 0.2231, 0.1315, 0.1704, 0.1845, 0.1629, 0.1360,\n                        0.1826, 0.1392, 0.2100, 0.1467, 0.2067, 0.2471, 0.2612, 0.1450, 0.1776,\n                        0.1537, 0.1278, 0.1442, 0.1209, 0.1979, 0.1542, 0.1570, 0.1887, 0.1762,\n                        0.1552, 0.1660, 0.1717, 0.1971, 0.1323, 0.1798, 0.1790, 0.2501, 0.1819,\n                        0.1752, 0.1707, 0.1372, 0.2131, 0.2769, 0.2024, 0.2103, 0.2269, 0.1963,\n                        0.1926, 0.1363, 0.1768, 0.2368, 0.1410, 0.2148, 0.1276, 0.1949, 0.1935,\n                        0.2224, 0.1765, 0.2145, 0.1224, 0.1949, 0.1439, 0.1541, 0.1435, 0.1718,\n                        0.1476, 0.1215, 0.1836, 0.2262, 0.2136, 0.1475, 0.2024, 0.1740, 0.1122,\n                        0.2334, 0.1684, 0.2395, 0.1216, 0.1438, 0.2122, 0.1761, 0.2962, 0.1456,\n                        0.2097, 0.1822, 0.2211, 0.2514, 0.1879, 0.1319, 0.1050, 0.1785, 0.1264,\n                        0.2155, 0.3329, 0.1842, 0.2692, 0.1601, 0.1891, 0.2365, 0.1673, 0.1711,\n                        0.1512, 0.2177, 0.1921, 0.1535, 0.2222, 0.1109, 0.1723, 0.1912, 0.2528,\n                        0.2588, 0.1936, 0.2041, 0.1616, 0.1828, 0.1136, 0.2282, 0.1965, 0.0970,\n                        0.2074, 0.1673, 0.2599, 0.1614, 0.1814, 0.1995, 0.1675, 0.1496, 0.2174,\n                        0.1626, 0.2024, 0.1555, 0.1793, 0.1938, 0.1291, 0.1851, 0.1267, 0.2507,\n                        0.1469, 0.1490, 0.2381, 0.2587, 0.1954, 0.1681, 0.1714, 0.1121, 0.2176,\n                        0.1583, 0.2574, 0.1287, 0.1481, 0.1646, 0.1192, 0.1672, 0.4029, 0.1591,\n                        0.4251, 0.1904, 0.1777, 0.2674, 0.1938, 0.1922, 0.3868, 0.2243, 0.1397,\n                        0.1749, 0.1819, 0.2233, 0.2128, 0.1586, 0.1402, 0.1681, 0.3600, 0.2687,\n                        0.1750, 0.1700, 0.2192, 0.1418, 0.1433, 0.1773, 0.1099, 0.1687, 0.2386,\n                        0.2360, 0.2609, 0.1432, 0.1320, 0.2343, 0.1655, 0.1031, 0.1495, 0.1341,\n                        0.2133, 0.1370, 0.1836, 0.2058, 0.2337, 0.1525, 0.3929, 0.2604, 0.1728,\n                        0.1978, 0.1398, 0.1702, 0.2336, 0.1270, 0.3750, 0.2343, 0.1902, 0.1280,\n                        0.2708, 0.1837, 0.1666, 0.1542, 0.1678, 0.1336, 0.2071, 0.1528, 0.1714,\n                        0.1173, 0.3932, 0.2219, 0.1138, 0.1710, 0.1182, 0.1642, 0.1525, 0.1540,\n                        0.1242, 0.1889, 0.1579, 0.2608, 0.3331, 0.1865, 0.1773, 0.2226, 0.2312,\n                        0.1500, 0.2496, 0.1555, 0.1509, 0.2174, 0.1400, 0.1626, 0.2229, 0.1646,\n                        0.3645, 0.1819, 0.1759, 0.1792, 0.1880, 0.1705, 0.1880, 0.1971, 0.1990,\n                        0.1828, 0.1206, 0.2105, 0.1186, 0.1393, 0.1733, 0.2238, 0.1765, 0.1722,\n                        0.2030, 0.1531, 0.1943, 0.1887, 0.1049, 0.1755, 0.1281, 0.1998, 0.1948,\n                        0.2037, 0.1536, 0.1552, 0.1555, 0.1565, 0.1867, 0.2008, 0.2667, 0.1804,\n                        0.1696, 0.1595, 0.2392, 0.1574, 0.1419, 0.2182, 0.2867, 0.1659, 0.1671,\n                        0.1953, 0.2743, 0.1827, 0.3454, 0.2489, 0.1620, 0.1277, 0.1846, 0.2023,\n                        0.1971, 0.1565, 0.1748, 0.2575, 0.1736, 0.2360, 0.1619, 0.1059, 0.1785,\n                        0.1817, 0.2003, 0.2069, 0.1658, 0.1520, 0.1790, 0.1498, 0.2277, 0.2291,\n                        0.1497, 0.1703, 0.1348, 0.1410, 0.1923, 0.3740, 0.1555, 0.1708, 0.1602,\n                        0.1651, 0.1937, 0.2681, 0.1590, 0.2471, 0.2275, 0.1628, 0.2437, 0.1575,\n                        0.2238, 0.1365, 0.1972, 0.1824, 0.3072, 0.1448, 0.1998, 0.1519, 0.1638,\n                        0.1772, 0.2045, 0.2293, 0.1224, 0.1934, 0.2262, 0.1376, 0.1390, 0.1507,\n                        0.1356, 0.2008, 0.3478, 0.1695, 0.2172, 0.1881, 0.2301, 0.1496, 0.1840,\n                        0.3387, 0.2811, 0.1698, 0.1796, 0.2015, 0.3293, 0.1830, 0.3495, 0.1786,\n                        0.2087, 0.1487, 0.1433, 0.1605, 0.1547, 0.2110, 0.2416, 0.2204, 0.1248,\n                        0.2001, 0.2277, 0.1963, 0.1646, 0.2420, 0.1865, 0.1456, 0.1498, 0.2199,\n                        0.2093, 0.1524, 0.1177, 0.1450, 0.1411, 0.1730, 0.2611, 0.1787, 0.1945,\n                        0.1943, 0.1983, 0.1983, 0.3300, 0.1581, 0.2456, 0.2439, 0.2535, 0.1567,\n                        0.1626, 0.1511, 0.1042, 0.2276, 0.1508, 0.1374, 0.1832, 0.1345, 0.1714,\n                        0.2735, 0.1674, 0.1805, 0.1488, 0.1616, 0.2077, 0.2460, 0.1804, 0.2164,\n                        0.2054, 0.1627, 0.2201, 0.2416, 0.1529, 0.1625, 0.3440, 0.1086, 0.1581,\n                        0.1235, 0.2049, 0.1857, 0.1485, 0.2015, 0.1550, 0.2204, 0.1543, 0.3013,\n                        0.3416, 0.2097, 0.1833, 0.2829, 0.1492, 0.1655, 0.2109, 0.1779, 0.1230,\n                        0.3244, 0.2195, 0.1277, 0.1638, 0.2382, 0.1760, 0.1266, 0.1208, 0.1627,\n                        0.1853, 0.1865, 0.1915, 0.1128, 0.1227, 0.1869, 0.1514, 0.2171, 0.1767,\n                        0.1388, 0.2023, 0.1403, 0.1873, 0.1827, 0.1793, 0.2525, 0.2178, 0.1712,\n                        0.2445, 0.1412, 0.1793, 0.1762, 0.1220, 0.1710, 0.1544, 0.1553, 0.1685,\n                        0.4177, 0.1368, 0.1801, 0.1912, 0.3154, 0.1760, 0.1294, 0.2235, 0.1438,\n                        0.1512, 0.2728, 0.1758, 0.1971, 0.1742, 0.1538, 0.2269, 0.1699, 0.2421,\n                        0.1263, 0.2254, 0.1643, 0.1485, 0.1587, 0.2039, 0.1182, 0.1916, 0.3427,\n                        0.1300, 0.1576, 0.1987, 0.1868, 0.2046, 0.1386, 0.1739, 0.1782, 0.1502,\n                        0.1590, 0.3057, 0.1599, 0.1739, 0.1542, 0.1610, 0.1243, 0.1716, 0.1516,\n                        0.2045, 0.0954, 0.2046, 0.1399, 0.1760, 0.3316, 0.1563, 0.1611, 0.1585,\n                        0.1166, 0.1957, 0.1813, 0.2153, 0.1559, 0.1924, 0.1411, 0.2259, 0.1575,\n                        0.1528, 0.2015, 0.1559])\n              )\n            )\n          )\n          (4): GELU(approximate='none')\n          (5): Linear(\n            in_features=768, out_features=192, bias=True\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0023, 0.0019, 0.0016, 0.0022, 0.0021, 0.0016, 0.0024, 0.0017, 0.0018,\n                      0.0015, 0.0017, 0.0015, 0.0030, 0.0018, 0.0018, 0.0019, 0.0017, 0.0025,\n                      0.0023, 0.0025, 0.0020, 0.0019, 0.0029, 0.0018, 0.0015, 0.0015, 0.0028,\n                      0.0019, 0.0017, 0.0019, 0.0019, 0.0016, 0.0027, 0.0019, 0.0016, 0.0020,\n                      0.0014, 0.0019, 0.0026, 0.0017, 0.0018, 0.0018, 0.0017, 0.0016, 0.0027,\n                      0.0018, 0.0014, 0.0018, 0.0015, 0.0017, 0.0017, 0.0020, 0.0016, 0.0030,\n                      0.0023, 0.0023, 0.0018, 0.0015, 0.0018, 0.0020, 0.0028, 0.0018, 0.0013,\n                      0.0017, 0.0015, 0.0019, 0.0018, 0.0015, 0.0014, 0.0015, 0.0022, 0.0021,\n                      0.0020, 0.0018, 0.0017, 0.0016, 0.0020, 0.0019, 0.0017, 0.0018, 0.0018,\n                      0.0019, 0.0015, 0.0020, 0.0024, 0.0023, 0.0018, 0.0026, 0.0023, 0.0039,\n                      0.0021, 0.0015, 0.0025, 0.0015, 0.0023, 0.0021, 0.0020, 0.0020, 0.0015,\n                      0.0025, 0.0018, 0.0017, 0.0024, 0.0014, 0.0016, 0.0031, 0.0016, 0.0040,\n                      0.0016, 0.0018, 0.0018, 0.0026, 0.0018, 0.0017, 0.0021, 0.0019, 0.0015,\n                      0.0017, 0.0025, 0.0024, 0.0015, 0.0019, 0.0026, 0.0021, 0.0014, 0.0017,\n                      0.0015, 0.0025, 0.0015, 0.0019, 0.0018, 0.0028, 0.0024, 0.0014, 0.0020,\n                      0.0023, 0.0021, 0.0033, 0.0031, 0.0018, 0.0018, 0.0018, 0.0028, 0.0017,\n                      0.0021, 0.0020, 0.0019, 0.0016, 0.0020, 0.0027, 0.0024, 0.0018, 0.0021,\n                      0.0021, 0.0022, 0.0020, 0.0021, 0.0026, 0.0019, 0.0016, 0.0018, 0.0015,\n                      0.0017, 0.0019, 0.0016, 0.0015, 0.0014, 0.0022, 0.0030, 0.0015, 0.0015,\n                      0.0017, 0.0014, 0.0022, 0.0018, 0.0022, 0.0020, 0.0019, 0.0019, 0.0014,\n                      0.0020, 0.0018, 0.0022, 0.0016, 0.0022, 0.0015, 0.0023, 0.0016, 0.0021,\n                      0.0018, 0.0024, 0.0014]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                     dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n                min_val=tensor([-0.2923, -0.2291, -0.1602, -0.2869, -0.2157, -0.2037, -0.3017, -0.2200,\n                        -0.2124, -0.1885, -0.2124, -0.1731, -0.1872, -0.2284, -0.2337, -0.2469,\n                        -0.2193, -0.2355, -0.2463, -0.2880, -0.2219, -0.2465, -0.2758, -0.2119,\n                        -0.1733, -0.1966, -0.3153, -0.1808, -0.1509, -0.2395, -0.2421, -0.1520,\n                        -0.3402, -0.2179, -0.1785, -0.2214, -0.1554, -0.2477, -0.3264, -0.2220,\n                        -0.2235, -0.2347, -0.2115, -0.1954, -0.2414, -0.2040, -0.1856, -0.2307,\n                        -0.1744, -0.2142, -0.2199, -0.2604, -0.1955, -0.3876, -0.2897, -0.2544,\n                        -0.2309, -0.1696, -0.2248, -0.2215, -0.3558, -0.1614, -0.1657, -0.2173,\n                        -0.1718, -0.2462, -0.1792, -0.1945, -0.1826, -0.1933, -0.1619, -0.2732,\n                        -0.2022, -0.2357, -0.2192, -0.2039, -0.2609, -0.2467, -0.1677, -0.1834,\n                        -0.1856, -0.2380, -0.1554, -0.2098, -0.2013, -0.2950, -0.1935, -0.3281,\n                        -0.1890, -0.4347, -0.2687, -0.1932, -0.3202, -0.1833, -0.2687, -0.2450,\n                        -0.1674, -0.2034, -0.1735, -0.3213, -0.2257, -0.1904, -0.2320, -0.1547,\n                        -0.2046, -0.4019, -0.2094, -0.5081, -0.2042, -0.2299, -0.1720, -0.3274,\n                        -0.2296, -0.2210, -0.1982, -0.2035, -0.1923, -0.2203, -0.3194, -0.3111,\n                        -0.1853, -0.1775, -0.3276, -0.2666, -0.1461, -0.1908, -0.1860, -0.2514,\n                        -0.1951, -0.2051, -0.1849, -0.3008, -0.3085, -0.1798, -0.2502, -0.2201,\n                        -0.2538, -0.4169, -0.3466, -0.2295, -0.1799, -0.1483, -0.3571, -0.2216,\n                        -0.1796, -0.1666, -0.2169, -0.2058, -0.2549, -0.3424, -0.3052, -0.1260,\n                        -0.1792, -0.1745, -0.2783, -0.1865, -0.1711, -0.2558, -0.2464, -0.2024,\n                        -0.1797, -0.1831, -0.2193, -0.2479, -0.2061, -0.1755, -0.1669, -0.2844,\n                        -0.3778, -0.1823, -0.1896, -0.2207, -0.1854, -0.2247, -0.2349, -0.1676,\n                        -0.2606, -0.2450, -0.2385, -0.1834, -0.2502, -0.2282, -0.2810, -0.2018,\n                        -0.2275, -0.1768, -0.2985, -0.2111, -0.2580, -0.2002, -0.3126, -0.1818]), max_val=tensor([0.2231, 0.2371, 0.2087, 0.2430, 0.2725, 0.2041, 0.2706, 0.2192, 0.2237,\n                        0.1620, 0.1468, 0.1912, 0.3805, 0.2310, 0.2241, 0.1914, 0.1646, 0.3133,\n                        0.2959, 0.3228, 0.2484, 0.2001, 0.3701, 0.2314, 0.1870, 0.1957, 0.3597,\n                        0.2439, 0.2163, 0.1744, 0.2016, 0.1988, 0.2782, 0.2435, 0.2021, 0.2501,\n                        0.1829, 0.1705, 0.3297, 0.1765, 0.2308, 0.1889, 0.1987, 0.1988, 0.3458,\n                        0.2226, 0.1688, 0.2136, 0.1896, 0.2036, 0.1781, 0.2202, 0.2002, 0.1443,\n                        0.1729, 0.2947, 0.1802, 0.1925, 0.1987, 0.2554, 0.3291, 0.2311, 0.1478,\n                        0.2137, 0.1862, 0.1882, 0.2229, 0.1947, 0.1738, 0.1559, 0.2827, 0.2013,\n                        0.2514, 0.2213, 0.2030, 0.1948, 0.2477, 0.2108, 0.2180, 0.2236, 0.2226,\n                        0.1753, 0.1927, 0.2552, 0.2995, 0.2039, 0.2246, 0.1521, 0.2933, 0.4993,\n                        0.1843, 0.1631, 0.2928, 0.1953, 0.2927, 0.2696, 0.2585, 0.2504, 0.1952,\n                        0.3061, 0.2257, 0.2217, 0.3036, 0.1719, 0.1984, 0.2194, 0.1920, 0.2661,\n                        0.1484, 0.2313, 0.2259, 0.2185, 0.1645, 0.2083, 0.2609, 0.2388, 0.1756,\n                        0.2087, 0.2781, 0.2893, 0.1945, 0.2465, 0.2714, 0.1995, 0.1826, 0.2174,\n                        0.1927, 0.3214, 0.1397, 0.2469, 0.2275, 0.3600, 0.1967, 0.1675, 0.1999,\n                        0.2921, 0.2715, 0.3818, 0.3941, 0.2122, 0.2278, 0.2275, 0.3575, 0.1836,\n                        0.2686, 0.2586, 0.2392, 0.1958, 0.1774, 0.1435, 0.1629, 0.2291, 0.2709,\n                        0.2695, 0.2261, 0.2553, 0.2631, 0.3243, 0.2375, 0.1784, 0.2310, 0.1910,\n                        0.1523, 0.2358, 0.2067, 0.1879, 0.1840, 0.2453, 0.3137, 0.1944, 0.1646,\n                        0.1542, 0.1599, 0.2803, 0.2180, 0.2773, 0.2540, 0.2222, 0.2343, 0.1703,\n                        0.2229, 0.2161, 0.1603, 0.1892, 0.2811, 0.1888, 0.2135, 0.1674, 0.2689,\n                        0.2231, 0.3091, 0.1560])\n              )\n            )\n          )\n        )\n      )\n      (1): Module(\n        (block): Module(\n          (0): Conv2d(\n            192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0023, 0.0026, 0.0027, 0.0015, 0.0023, 0.0028, 0.0025, 0.0025, 0.0027,\n                      0.0022, 0.0027, 0.0023, 0.0020, 0.0020, 0.0026, 0.0017, 0.0027, 0.0019,\n                      0.0017, 0.0016, 0.0026, 0.0026, 0.0027, 0.0013, 0.0030, 0.0023, 0.0014,\n                      0.0025, 0.0028, 0.0026, 0.0023, 0.0027, 0.0023, 0.0026, 0.0023, 0.0027,\n                      0.0021, 0.0019, 0.0024, 0.0020, 0.0012, 0.0030, 0.0022, 0.0024, 0.0023,\n                      0.0001, 0.0028, 0.0024, 0.0025, 0.0019, 0.0026, 0.0029, 0.0005, 0.0017,\n                      0.0027, 0.0025, 0.0008, 0.0024, 0.0025, 0.0012, 0.0021, 0.0018, 0.0030,\n                      0.0023, 0.0026, 0.0023, 0.0017, 0.0021, 0.0027, 0.0029, 0.0023, 0.0025,\n                      0.0027, 0.0026, 0.0025, 0.0021, 0.0022, 0.0021, 0.0023, 0.0027, 0.0023,\n                      0.0025, 0.0031, 0.0029, 0.0029, 0.0023, 0.0009, 0.0029, 0.0028, 0.0001,\n                      0.0028, 0.0025, 0.0023, 0.0022, 0.0015, 0.0026, 0.0024, 0.0025, 0.0027,\n                      0.0023, 0.0029, 0.0024, 0.0026, 0.0029, 0.0021, 0.0011, 0.0026, 0.0026,\n                      0.0026, 0.0024, 0.0016, 0.0010, 0.0027, 0.0026, 0.0025, 0.0004, 0.0026,\n                      0.0025, 0.0015, 0.0025, 0.0024, 0.0026, 0.0016, 0.0014, 0.0027, 0.0025,\n                      0.0030, 0.0019, 0.0026, 0.0029, 0.0030, 0.0021, 0.0026, 0.0026, 0.0022,\n                      0.0022, 0.0010, 0.0014, 0.0019, 0.0021, 0.0027, 0.0028, 0.0015, 0.0026,\n                      0.0017, 0.0027, 0.0013, 0.0023, 0.0029, 0.0017, 0.0027, 0.0025, 0.0021,\n                      0.0028, 0.0025, 0.0021, 0.0025, 0.0024, 0.0024, 0.0021, 0.0020, 0.0029,\n                      0.0022, 0.0022, 0.0008, 0.0023, 0.0003, 0.0011, 0.0025, 0.0026, 0.0018,\n                      0.0027, 0.0026, 0.0025, 0.0021, 0.0010, 0.0021, 0.0025, 0.0023, 0.0025,\n                      0.0027, 0.0028, 0.0030, 0.0024, 0.0015, 0.0006, 0.0024, 0.0027, 0.0021,\n                      0.0022, 0.0023, 0.0024]), zero_point=tensor([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0, -128,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n                     dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n                min_val=tensor([-0.3000, -0.0459, -0.0365, -0.0454, -0.1561, -0.3587, -0.3165, -0.3143,\n                        -0.1216, -0.1313, -0.0326, -0.1697, -0.2451, -0.2513, -0.1083, -0.0033,\n                        -0.0208, -0.2488, -0.2145, -0.1667, -0.0889, -0.3318, -0.0791, -0.0422,\n                        -0.3901, -0.1807, -0.1612, -0.3262, -0.3612, -0.3286, -0.2947, -0.0544,\n                        -0.0130, -0.3272, -0.2970, -0.1266, -0.0858, -0.2453, -0.0555, -0.2551,\n                        -0.0034, -0.0710, -0.2855, -0.3126, -0.0704, -0.0045, -0.3580, -0.3032,\n                        -0.1374, -0.2425, -0.0544, -0.0691, -0.0049, -0.2112, -0.0588, -0.0847,\n                        -0.0972, -0.3096, -0.0797, -0.1553, -0.0449, -0.1998, -0.3868, -0.0562,\n                        -0.0568, -0.2900, -0.2233, -0.2709, -0.0327, -0.1112, -0.1078, -0.1146,\n                        -0.0494, -0.3322, -0.0765, -0.2638, -0.2780, -0.0140, -0.0747, -0.0303,\n                        -0.2921, -0.3243, -0.0528, -0.3729, -0.3735, -0.1766, -0.0042, -0.1472,\n                        -0.0973, -0.0148, -0.3585, -0.1069, -0.0769, -0.1289, -0.1592, -0.3365,\n                        -0.0678, -0.0527, -0.0771, -0.0555, -0.1250, -0.0976, -0.3271, -0.3738,\n                        -0.2038, -0.1359, -0.3289, -0.3290, -0.0301, -0.1026, -0.1957, -0.1217,\n                        -0.1122, -0.3365, -0.0717, -0.0236, -0.1711, -0.0518, -0.1924, -0.0997,\n                        -0.3062, -0.1770, -0.2028, -0.1755, -0.3405, -0.3140, -0.0542, -0.2417,\n                        -0.3334, -0.0171, -0.3859, -0.1676, -0.1087, -0.0304, -0.2222, -0.2143,\n                        -0.0153, -0.1592, -0.2470, -0.2659, -0.0739, -0.0472, -0.1684, -0.3343,\n                        -0.2232, -0.0778, -0.0038, -0.2961, -0.0528, -0.2197, -0.0407, -0.0622,\n                        -0.1900, -0.1164, -0.0126, -0.2748, -0.1605, -0.3022, -0.3132, -0.1972,\n                        -0.2502, -0.3664, -0.2758, -0.2846, -0.1016, -0.1398,  0.0036, -0.1333,\n                        -0.3209, -0.0654, -0.1123, -0.3506, -0.1337, -0.3140, -0.2730, -0.0011,\n                        -0.0177, -0.1008, -0.2964, -0.3234, -0.3491, -0.3536, -0.0830, -0.1990,\n                        -0.1858, -0.0113, -0.3110, -0.0761, -0.2642, -0.2801, -0.2933, -0.3098]), max_val=tensor([0.0315, 0.3279, 0.3443, 0.1963, 0.2962, 0.0576, 0.0827, 0.0212, 0.3413,\n                        0.2787, 0.3460, 0.2907, 0.2561, 0.1672, 0.3260, 0.2160, 0.3406, 0.0508,\n                        0.1224, 0.2049, 0.3260, 0.0272, 0.3431, 0.1635, 0.0376, 0.2965, 0.1724,\n                        0.0760, 0.0840, 0.0313, 0.1566, 0.3400, 0.2907, 0.0335, 0.0229, 0.3490,\n                        0.2709, 0.2048, 0.3000, 0.1816, 0.1584, 0.3755, 0.1901, 0.0778, 0.2983,\n                        0.0164, 0.0697, 0.1855, 0.3138, 0.0134, 0.3334, 0.3623, 0.0661, 0.0468,\n                        0.3407, 0.3170, 0.0045, 0.1603, 0.3129, 0.0185, 0.2641, 0.2225, 0.0393,\n                        0.2903, 0.3357, 0.0073, 0.0068, 0.1519, 0.3479, 0.3656, 0.2924, 0.3229,\n                        0.3393, 0.0577, 0.3139, 0.1861, 0.1132, 0.2622, 0.2877, 0.3434, 0.1495,\n                        0.1310, 0.3983, 0.0348, 0.0653, 0.2968, 0.1177, 0.3674, 0.3519, 0.0151,\n                        0.0071, 0.3113, 0.2944, 0.2753, 0.1851, 0.0673, 0.3045, 0.3172, 0.3436,\n                        0.2983, 0.3730, 0.3095, 0.0285, 0.0884, 0.2617, 0.1082, 0.0257, 0.0577,\n                        0.3364, 0.3086, 0.1982, 0.1331, 0.3376, 0.1664, 0.3229, 0.0554, 0.3300,\n                        0.3137, 0.1289, 0.3227, 0.1327, 0.3251, 0.1800, 0.1730, 0.1526, 0.0379,\n                        0.3831, 0.0268, 0.0670, 0.3688, 0.0639, 0.2709, 0.3249, 0.3339, 0.2798,\n                        0.2739, 0.1319, 0.1752, 0.0796, 0.1761, 0.3415, 0.3561, 0.1886, 0.0506,\n                        0.0406, 0.3457, 0.1674, 0.1761, 0.3660, 0.0111, 0.3425, 0.3208, 0.2707,\n                        0.3597, 0.3193, 0.0406, 0.3238, 0.0775, 0.0119, 0.2659, 0.1553, 0.0767,\n                        0.1083, 0.1826, 0.0499, 0.2969, 0.0729, 0.1387, 0.0923, 0.3334, 0.2250,\n                        0.0138, 0.3306, 0.0247, 0.0165, 0.1263, 0.2728, 0.3194, 0.0669, 0.0768,\n                        0.0332, 0.2005, 0.3757, 0.3093, 0.1725, 0.0768, 0.1420, 0.3426, 0.1129,\n                        0.1338, 0.0428, 0.1638])\n              )\n            )\n          )\n          (2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(\n            in_features=192, out_features=768, bias=True\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0013, 0.0015, 0.0019, 0.0014, 0.0014, 0.0018, 0.0021, 0.0011, 0.0013,\n                      0.0013, 0.0014, 0.0031, 0.0015, 0.0017, 0.0031, 0.0014, 0.0020, 0.0013,\n                      0.0016, 0.0016, 0.0013, 0.0014, 0.0019, 0.0015, 0.0017, 0.0014, 0.0025,\n                      0.0018, 0.0011, 0.0013, 0.0015, 0.0014, 0.0016, 0.0021, 0.0018, 0.0021,\n                      0.0017, 0.0012, 0.0013, 0.0014, 0.0015, 0.0013, 0.0012, 0.0028, 0.0025,\n                      0.0015, 0.0013, 0.0016, 0.0014, 0.0014, 0.0022, 0.0013, 0.0014, 0.0012,\n                      0.0012, 0.0013, 0.0016, 0.0011, 0.0010, 0.0017, 0.0018, 0.0013, 0.0016,\n                      0.0014, 0.0016, 0.0013, 0.0011, 0.0013, 0.0014, 0.0020, 0.0014, 0.0022,\n                      0.0013, 0.0014, 0.0018, 0.0015, 0.0012, 0.0016, 0.0017, 0.0013, 0.0018,\n                      0.0013, 0.0019, 0.0017, 0.0013, 0.0015, 0.0016, 0.0026, 0.0015, 0.0012,\n                      0.0015, 0.0014, 0.0011, 0.0016, 0.0014, 0.0015, 0.0014, 0.0014, 0.0018,\n                      0.0014, 0.0013, 0.0018, 0.0019, 0.0013, 0.0013, 0.0014, 0.0013, 0.0012,\n                      0.0016, 0.0013, 0.0014, 0.0013, 0.0024, 0.0015, 0.0018, 0.0012, 0.0024,\n                      0.0016, 0.0012, 0.0017, 0.0016, 0.0013, 0.0014, 0.0023, 0.0015, 0.0014,\n                      0.0016, 0.0028, 0.0014, 0.0015, 0.0022, 0.0017, 0.0014, 0.0012, 0.0027,\n                      0.0013, 0.0014, 0.0017, 0.0023, 0.0017, 0.0013, 0.0014, 0.0017, 0.0015,\n                      0.0012, 0.0018, 0.0018, 0.0010, 0.0018, 0.0011, 0.0011, 0.0020, 0.0016,\n                      0.0015, 0.0022, 0.0014, 0.0014, 0.0015, 0.0016, 0.0022, 0.0014, 0.0012,\n                      0.0016, 0.0019, 0.0010, 0.0012, 0.0014, 0.0016, 0.0013, 0.0016, 0.0017,\n                      0.0026, 0.0014, 0.0026, 0.0012, 0.0013, 0.0019, 0.0015, 0.0010, 0.0017,\n                      0.0014, 0.0013, 0.0023, 0.0016, 0.0012, 0.0011, 0.0013, 0.0031, 0.0014,\n                      0.0015, 0.0013, 0.0012, 0.0019, 0.0019, 0.0016, 0.0013, 0.0021, 0.0023,\n                      0.0022, 0.0013, 0.0017, 0.0035, 0.0015, 0.0016, 0.0015, 0.0013, 0.0014,\n                      0.0018, 0.0018, 0.0013, 0.0011, 0.0013, 0.0012, 0.0014, 0.0014, 0.0015,\n                      0.0012, 0.0013, 0.0017, 0.0013, 0.0017, 0.0013, 0.0019, 0.0014, 0.0023,\n                      0.0012, 0.0016, 0.0017, 0.0024, 0.0025, 0.0015, 0.0016, 0.0015, 0.0017,\n                      0.0027, 0.0013, 0.0012, 0.0014, 0.0013, 0.0018, 0.0014, 0.0014, 0.0013,\n                      0.0020, 0.0013, 0.0012, 0.0016, 0.0015, 0.0013, 0.0016, 0.0018, 0.0015,\n                      0.0013, 0.0019, 0.0011, 0.0011, 0.0016, 0.0017, 0.0016, 0.0014, 0.0019,\n                      0.0014, 0.0014, 0.0014, 0.0012, 0.0014, 0.0015, 0.0017, 0.0017, 0.0013,\n                      0.0012, 0.0019, 0.0015, 0.0024, 0.0013, 0.0013, 0.0014, 0.0010, 0.0019,\n                      0.0014, 0.0012, 0.0016, 0.0018, 0.0013, 0.0021, 0.0012, 0.0012, 0.0010,\n                      0.0019, 0.0026, 0.0033, 0.0037, 0.0018, 0.0015, 0.0017, 0.0045, 0.0011,\n                      0.0018, 0.0011, 0.0016, 0.0015, 0.0013, 0.0012, 0.0017, 0.0011, 0.0015,\n                      0.0018, 0.0012, 0.0019, 0.0017, 0.0021, 0.0021, 0.0011, 0.0016, 0.0013,\n                      0.0014, 0.0015, 0.0019, 0.0014, 0.0015, 0.0017, 0.0018, 0.0020, 0.0011,\n                      0.0016, 0.0009, 0.0017, 0.0013, 0.0021, 0.0026, 0.0013, 0.0018, 0.0020,\n                      0.0013, 0.0014, 0.0026, 0.0021, 0.0013, 0.0014, 0.0014, 0.0013, 0.0014,\n                      0.0017, 0.0012, 0.0013, 0.0013, 0.0014, 0.0020, 0.0036, 0.0020, 0.0016,\n                      0.0012, 0.0015, 0.0015, 0.0030, 0.0015, 0.0013, 0.0017, 0.0016, 0.0015,\n                      0.0014, 0.0013, 0.0014, 0.0015, 0.0014, 0.0018, 0.0015, 0.0016, 0.0012,\n                      0.0013, 0.0016, 0.0017, 0.0011, 0.0010, 0.0026, 0.0015, 0.0015, 0.0016,\n                      0.0013, 0.0023, 0.0018, 0.0015, 0.0016, 0.0012, 0.0021, 0.0016, 0.0015,\n                      0.0015, 0.0019, 0.0013, 0.0019, 0.0011, 0.0018, 0.0021, 0.0014, 0.0017,\n                      0.0015, 0.0024, 0.0018, 0.0017, 0.0009, 0.0022, 0.0012, 0.0013, 0.0015,\n                      0.0018, 0.0022, 0.0011, 0.0029, 0.0019, 0.0015, 0.0014, 0.0019, 0.0029,\n                      0.0019, 0.0018, 0.0017, 0.0017, 0.0014, 0.0014, 0.0017, 0.0010, 0.0015,\n                      0.0030, 0.0019, 0.0016, 0.0012, 0.0016, 0.0020, 0.0020, 0.0027, 0.0010,\n                      0.0011, 0.0014, 0.0015, 0.0015, 0.0013, 0.0014, 0.0011, 0.0021, 0.0013,\n                      0.0014, 0.0009, 0.0013, 0.0013, 0.0015, 0.0013, 0.0014, 0.0018, 0.0014,\n                      0.0013, 0.0011, 0.0017, 0.0017, 0.0011, 0.0025, 0.0017, 0.0020, 0.0015,\n                      0.0020, 0.0012, 0.0014, 0.0013, 0.0014, 0.0013, 0.0018, 0.0018, 0.0012,\n                      0.0022, 0.0021, 0.0022, 0.0015, 0.0017, 0.0021, 0.0016, 0.0033, 0.0013,\n                      0.0010, 0.0017, 0.0014, 0.0032, 0.0016, 0.0015, 0.0015, 0.0022, 0.0015,\n                      0.0021, 0.0015, 0.0025, 0.0016, 0.0012, 0.0017, 0.0015, 0.0014, 0.0020,\n                      0.0014, 0.0024, 0.0017, 0.0027, 0.0019, 0.0019, 0.0013, 0.0024, 0.0011,\n                      0.0013, 0.0015, 0.0012, 0.0014, 0.0018, 0.0013, 0.0015, 0.0012, 0.0025,\n                      0.0015, 0.0014, 0.0019, 0.0022, 0.0016, 0.0015, 0.0018, 0.0020, 0.0011,\n                      0.0012, 0.0017, 0.0027, 0.0028, 0.0016, 0.0014, 0.0016, 0.0013, 0.0016,\n                      0.0020, 0.0013, 0.0023, 0.0013, 0.0014, 0.0016, 0.0013, 0.0032, 0.0012,\n                      0.0014, 0.0014, 0.0013, 0.0017, 0.0021, 0.0015, 0.0022, 0.0019, 0.0017,\n                      0.0011, 0.0012, 0.0016, 0.0017, 0.0011, 0.0017, 0.0011, 0.0014, 0.0013,\n                      0.0015, 0.0017, 0.0013, 0.0012, 0.0012, 0.0011, 0.0019, 0.0011, 0.0016,\n                      0.0010, 0.0016, 0.0012, 0.0018, 0.0014, 0.0020, 0.0021, 0.0016, 0.0012,\n                      0.0013, 0.0014, 0.0010, 0.0020, 0.0010, 0.0014, 0.0024, 0.0016, 0.0020,\n                      0.0013, 0.0026, 0.0016, 0.0014, 0.0016, 0.0018, 0.0014, 0.0035, 0.0016,\n                      0.0011, 0.0012, 0.0017, 0.0013, 0.0011, 0.0015, 0.0016, 0.0015, 0.0016,\n                      0.0013, 0.0030, 0.0010, 0.0010, 0.0015, 0.0019, 0.0020, 0.0011, 0.0029,\n                      0.0026, 0.0010, 0.0014, 0.0014, 0.0010, 0.0013, 0.0016, 0.0011, 0.0011,\n                      0.0013, 0.0013, 0.0012, 0.0014, 0.0013, 0.0015, 0.0018, 0.0018, 0.0030,\n                      0.0016, 0.0019, 0.0013, 0.0021, 0.0015, 0.0019, 0.0014, 0.0012, 0.0012,\n                      0.0025, 0.0011, 0.0017, 0.0016, 0.0025, 0.0018, 0.0011, 0.0019, 0.0016,\n                      0.0015, 0.0015, 0.0017, 0.0020, 0.0024, 0.0012, 0.0011, 0.0014, 0.0014,\n                      0.0013, 0.0023, 0.0017, 0.0012, 0.0017, 0.0018, 0.0015, 0.0016, 0.0016,\n                      0.0017, 0.0010, 0.0024, 0.0021, 0.0010, 0.0012, 0.0013, 0.0012, 0.0017,\n                      0.0015, 0.0015, 0.0014, 0.0020, 0.0013, 0.0015, 0.0026, 0.0015, 0.0015,\n                      0.0012, 0.0017, 0.0012, 0.0019, 0.0021, 0.0016, 0.0013, 0.0021, 0.0015,\n                      0.0013, 0.0027, 0.0016, 0.0012, 0.0014, 0.0017, 0.0011, 0.0014, 0.0028,\n                      0.0011, 0.0019, 0.0025, 0.0014, 0.0023, 0.0016, 0.0010, 0.0012, 0.0019,\n                      0.0013, 0.0014, 0.0013, 0.0027, 0.0010, 0.0013, 0.0015, 0.0016, 0.0022,\n                      0.0036, 0.0019, 0.0018, 0.0013, 0.0022, 0.0014, 0.0012, 0.0012, 0.0014,\n                      0.0020, 0.0016, 0.0016, 0.0012, 0.0021, 0.0020, 0.0014, 0.0013, 0.0013,\n                      0.0015, 0.0012, 0.0012, 0.0013, 0.0011, 0.0020, 0.0014, 0.0015, 0.0013,\n                      0.0014, 0.0019, 0.0012, 0.0013, 0.0014, 0.0015, 0.0015, 0.0012, 0.0020,\n                      0.0012, 0.0013, 0.0012, 0.0014, 0.0011, 0.0013, 0.0016, 0.0022, 0.0018,\n                      0.0017, 0.0014, 0.0015]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                     dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n                min_val=tensor([-0.1528, -0.1668, -0.2469, -0.1075, -0.1714, -0.1387, -0.1707, -0.1434,\n                        -0.1496, -0.1642, -0.1114, -0.4008, -0.1180, -0.1278, -0.1547, -0.1831,\n                        -0.1139, -0.1637, -0.2080, -0.2001, -0.1728, -0.1123, -0.1213, -0.1609,\n                        -0.1949, -0.1795, -0.3167, -0.1055, -0.1423, -0.1664, -0.1412, -0.1760,\n                        -0.1998, -0.2659, -0.2045, -0.2028, -0.1386, -0.1041, -0.1576, -0.1855,\n                        -0.1972, -0.1727, -0.1385, -0.3602, -0.3191, -0.1561, -0.1676, -0.1253,\n                        -0.1253, -0.1777, -0.2782, -0.1681, -0.1149, -0.1423, -0.1361, -0.1722,\n                        -0.1845, -0.1371, -0.1098, -0.1103, -0.2155, -0.1401, -0.1196, -0.1653,\n                        -0.1997, -0.1188, -0.0793, -0.1692, -0.1674, -0.2508, -0.1622, -0.1722,\n                        -0.1717, -0.1704, -0.1207, -0.1410, -0.1093, -0.1990, -0.1168, -0.1659,\n                        -0.1350, -0.1504, -0.1814, -0.2043, -0.1311, -0.1743, -0.2098, -0.1213,\n                        -0.1935, -0.1209, -0.1937, -0.1434, -0.1356, -0.2085, -0.1559, -0.1549,\n                        -0.1408, -0.1731, -0.2054, -0.1078, -0.1146, -0.1618, -0.1494, -0.1720,\n                        -0.1249, -0.1190, -0.1616, -0.1231, -0.1006, -0.1674, -0.1321, -0.1663,\n                        -0.1862, -0.1587, -0.1006, -0.1527, -0.2348, -0.1985, -0.1223, -0.0980,\n                        -0.2078, -0.1137, -0.1341, -0.1360, -0.1621, -0.1403, -0.2083, -0.2437,\n                        -0.1488, -0.1863, -0.1359, -0.1118, -0.1450, -0.1314, -0.1699, -0.1557,\n                        -0.1750, -0.0977, -0.1731, -0.1177, -0.1564, -0.1309, -0.2193, -0.1159,\n                        -0.1376, -0.1500, -0.1246, -0.1199, -0.1201, -0.1299, -0.1331, -0.1462,\n                        -0.1160, -0.1433, -0.2781, -0.1537, -0.1226, -0.1903, -0.0951, -0.1012,\n                        -0.1583, -0.1492, -0.1993, -0.1359, -0.1106, -0.0772, -0.1562, -0.1549,\n                        -0.1082, -0.1612, -0.2138, -0.3282, -0.1846, -0.1368, -0.1416, -0.1538,\n                        -0.2370, -0.1158, -0.1309, -0.2186, -0.1546, -0.1046, -0.2461, -0.2060,\n                        -0.1547, -0.1292, -0.1418, -0.1690, -0.1772, -0.1290, -0.1622, -0.1522,\n                        -0.1614, -0.1317, -0.2019, -0.1623, -0.1349, -0.2919, -0.1478, -0.1491,\n                        -0.1208, -0.4533, -0.0998, -0.2002, -0.1333, -0.1175, -0.1451, -0.1265,\n                        -0.2243, -0.1723, -0.1248, -0.1220, -0.1558, -0.1336, -0.1447, -0.1576,\n                        -0.1526, -0.1655, -0.1425, -0.1230, -0.1841, -0.1714, -0.1424, -0.1766,\n                        -0.1594, -0.1564, -0.2022, -0.2160, -0.3105, -0.1179, -0.1425, -0.1571,\n                        -0.1963, -0.1361, -0.1421, -0.1496, -0.1210, -0.1321, -0.1624, -0.1159,\n                        -0.1738, -0.1211, -0.1703, -0.2560, -0.1487, -0.1491, -0.1671, -0.1857,\n                        -0.1000, -0.1409, -0.1543, -0.1866, -0.1694, -0.1353, -0.1410, -0.1050,\n                        -0.1871, -0.1935, -0.1477, -0.1238, -0.2407, -0.1204, -0.1763, -0.1434,\n                        -0.1533, -0.1683, -0.1873, -0.1308, -0.1406, -0.1367, -0.1298, -0.1466,\n                        -0.1898, -0.3057, -0.1608, -0.1635, -0.1423, -0.1076, -0.1625, -0.1387,\n                        -0.1439, -0.1240, -0.1671, -0.1712, -0.1904, -0.1209, -0.1186, -0.1092,\n                        -0.2450, -0.3277, -0.2473, -0.4791, -0.1903, -0.1625, -0.1257, -0.1686,\n                        -0.1113, -0.1544, -0.1074, -0.1255, -0.1240, -0.1690, -0.1256, -0.1423,\n                        -0.1193, -0.1531, -0.1050, -0.1260, -0.1605, -0.2233, -0.2501, -0.1376,\n                        -0.1454, -0.1317, -0.1626, -0.0986, -0.1179, -0.2461, -0.1081, -0.1343,\n                        -0.1469, -0.2267, -0.1193, -0.1388, -0.1701, -0.0943, -0.0955, -0.1663,\n                        -0.1369, -0.1238, -0.1262, -0.2327, -0.2500, -0.1654, -0.1684, -0.3282,\n                        -0.1235, -0.1275, -0.1744, -0.1312, -0.1502, -0.1595, -0.2229, -0.1502,\n                        -0.1489, -0.1621, -0.1731, -0.1340, -0.2143, -0.1360, -0.2005, -0.1549,\n                        -0.1656, -0.1146, -0.1180, -0.1130, -0.1351, -0.1772, -0.1366, -0.1925,\n                        -0.1823, -0.1559, -0.1449, -0.1778, -0.1361, -0.2355, -0.1823, -0.1891,\n                        -0.1537, -0.1557, -0.1598, -0.1576, -0.1432, -0.1269, -0.3305, -0.1302,\n                        -0.1387, -0.1877, -0.1595, -0.1728, -0.1308, -0.1925, -0.1309, -0.1266,\n                        -0.2719, -0.2074, -0.1486, -0.1440, -0.2442, -0.1617, -0.1845, -0.1084,\n                        -0.0914, -0.2646, -0.1136, -0.2187, -0.1297, -0.3129, -0.1377, -0.1472,\n                        -0.0856, -0.1250, -0.1496, -0.1641, -0.1981, -0.1793, -0.2809, -0.1297,\n                        -0.3697, -0.1517, -0.1967, -0.1827, -0.2273, -0.1696, -0.2403, -0.2310,\n                        -0.1079, -0.1707, -0.1833, -0.1752, -0.1790, -0.1040, -0.1938, -0.2308,\n                        -0.1553, -0.2028, -0.1564, -0.1102, -0.2569, -0.2527, -0.3443, -0.1254,\n                        -0.1290, -0.1733, -0.1040, -0.1959, -0.1001, -0.0822, -0.1209, -0.2729,\n                        -0.1678, -0.1404, -0.1182, -0.1315, -0.1698, -0.1825, -0.1211, -0.1009,\n                        -0.1770, -0.1269, -0.1372, -0.1366, -0.1670, -0.1589, -0.1269, -0.2532,\n                        -0.1566, -0.2490, -0.1536, -0.1706, -0.1527, -0.1271, -0.1411, -0.1090,\n                        -0.1659, -0.2256, -0.1426, -0.1432, -0.1588, -0.1390, -0.1494, -0.1943,\n                        -0.2149, -0.2439, -0.1277, -0.3530, -0.1702, -0.1299, -0.1240, -0.1803,\n                        -0.1868, -0.1916, -0.1595, -0.1880, -0.1471, -0.1110, -0.1680, -0.1832,\n                        -0.1626, -0.1519, -0.1402, -0.1892, -0.1539, -0.1083, -0.1590, -0.1511,\n                        -0.3033, -0.2120, -0.1380, -0.2088, -0.1493, -0.1394, -0.1642, -0.1255,\n                        -0.1460, -0.1215, -0.1274, -0.1772, -0.1936, -0.1174, -0.1409, -0.1002,\n                        -0.1545, -0.1477, -0.1315, -0.2323, -0.1843, -0.2002, -0.1979, -0.2334,\n                        -0.1429, -0.1102, -0.1424, -0.2133, -0.2182, -0.3564, -0.0968, -0.1806,\n                        -0.1620, -0.1333, -0.2082, -0.2274, -0.1312, -0.1850, -0.1637, -0.1789,\n                        -0.2107, -0.1038, -0.1441, -0.1488, -0.1758, -0.1431, -0.1592, -0.1463,\n                        -0.1754, -0.1184, -0.2760, -0.1176, -0.1994, -0.1371, -0.1173, -0.1597,\n                        -0.1581, -0.1016, -0.1604, -0.1216, -0.1774, -0.1724, -0.1973, -0.0948,\n                        -0.1249, -0.1441, -0.1389, -0.1382, -0.1300, -0.1380, -0.1388, -0.1312,\n                        -0.1988, -0.0959, -0.1497, -0.1295, -0.1990, -0.1102, -0.2012, -0.1079,\n                        -0.1267, -0.1851, -0.1219, -0.1388, -0.1057, -0.1524, -0.3051, -0.1549,\n                        -0.1907, -0.1244, -0.1093, -0.0988, -0.1832, -0.2015, -0.2335, -0.1524,\n                        -0.4446, -0.1605, -0.1438, -0.1182, -0.2174, -0.1675, -0.1311, -0.1352,\n                        -0.1638, -0.1815, -0.1867, -0.1297, -0.1800, -0.1101, -0.1116, -0.1452,\n                        -0.1638, -0.2618, -0.1396, -0.1422, -0.1950, -0.1268, -0.1211, -0.1330,\n                        -0.1080, -0.1663, -0.1137, -0.1434, -0.1427, -0.1549, -0.1596, -0.1026,\n                        -0.1548, -0.1685, -0.1444, -0.1382, -0.2270, -0.3803, -0.2095, -0.1267,\n                        -0.1510, -0.1461, -0.1341, -0.1481, -0.1566, -0.1319, -0.1386, -0.3222,\n                        -0.0927, -0.2144, -0.2022, -0.3176, -0.1478, -0.1352, -0.1710, -0.1924,\n                        -0.1888, -0.1885, -0.2188, -0.2107, -0.1421, -0.1384, -0.1442, -0.1659,\n                        -0.1800, -0.0848, -0.2950, -0.2146, -0.1208, -0.2164, -0.2076, -0.1941,\n                        -0.1237, -0.1479, -0.1491, -0.1315, -0.1340, -0.2643, -0.1288, -0.1447,\n                        -0.1186, -0.1123, -0.1115, -0.1213, -0.1350, -0.0908, -0.2564, -0.1660,\n                        -0.1888, -0.2125, -0.1275, -0.1783, -0.1245, -0.2155, -0.1156, -0.2159,\n                        -0.1459, -0.1998, -0.1364, -0.2671, -0.1316, -0.1696, -0.1377, -0.1997,\n                        -0.1056, -0.1590, -0.2153, -0.1405, -0.0835, -0.3599, -0.1241, -0.2425,\n                        -0.2221, -0.1810, -0.1216, -0.1811, -0.1244, -0.1511, -0.1638, -0.1676,\n                        -0.1464, -0.1661, -0.1998, -0.1174, -0.1092, -0.1861, -0.1477, -0.2808,\n                        -0.4573, -0.1810, -0.1214, -0.1665, -0.1793, -0.1833, -0.1391, -0.1143,\n                        -0.1397, -0.1369, -0.1352, -0.2054, -0.1564, -0.1459, -0.2364, -0.1263,\n                        -0.1383, -0.1628, -0.1904, -0.1194, -0.1269, -0.1351, -0.0990, -0.1510,\n                        -0.1833, -0.1934, -0.1151, -0.1273, -0.2397, -0.1280, -0.0947, -0.1121,\n                        -0.1472, -0.1095, -0.1116, -0.2605, -0.1292, -0.1282, -0.1545, -0.1662,\n                        -0.1450, -0.1347, -0.2031, -0.2210, -0.1014, -0.1945, -0.1304, -0.1740]), max_val=tensor([0.1682, 0.1891, 0.1956, 0.1763, 0.1830, 0.2321, 0.2647, 0.1162, 0.1691,\n                        0.1218, 0.1830, 0.2553, 0.1865, 0.2096, 0.3982, 0.1578, 0.2493, 0.1415,\n                        0.1772, 0.1697, 0.1357, 0.1783, 0.2447, 0.1871, 0.2104, 0.1837, 0.1894,\n                        0.2227, 0.1217, 0.1556, 0.1949, 0.1702, 0.1458, 0.1860, 0.2310, 0.2665,\n                        0.2120, 0.1561, 0.1591, 0.1471, 0.1951, 0.1673, 0.1501, 0.3452, 0.1522,\n                        0.1883, 0.1415, 0.2049, 0.1720, 0.1232, 0.1767, 0.1590, 0.1750, 0.1558,\n                        0.1570, 0.1636, 0.1974, 0.1351, 0.1333, 0.2110, 0.2322, 0.1640, 0.2058,\n                        0.1744, 0.1500, 0.1698, 0.1458, 0.1561, 0.1724, 0.1746, 0.1792, 0.2805,\n                        0.1477, 0.1840, 0.2340, 0.1933, 0.1577, 0.2070, 0.2214, 0.1300, 0.2306,\n                        0.1712, 0.2383, 0.2142, 0.1681, 0.1912, 0.1971, 0.3298, 0.1514, 0.1583,\n                        0.1788, 0.1792, 0.1304, 0.1566, 0.1778, 0.1940, 0.1765, 0.1731, 0.2255,\n                        0.1813, 0.1663, 0.2248, 0.2463, 0.1445, 0.1689, 0.1732, 0.1485, 0.1508,\n                        0.1988, 0.1547, 0.1799, 0.1312, 0.3066, 0.1910, 0.2275, 0.1245, 0.3041,\n                        0.1412, 0.1487, 0.2153, 0.1458, 0.1650, 0.1785, 0.2859, 0.1949, 0.1771,\n                        0.1719, 0.3548, 0.1828, 0.1923, 0.2796, 0.2105, 0.1836, 0.1507, 0.3437,\n                        0.1610, 0.1737, 0.2121, 0.2910, 0.2123, 0.1589, 0.1799, 0.1434, 0.1878,\n                        0.1498, 0.2327, 0.2316, 0.1262, 0.2318, 0.1440, 0.1418, 0.2530, 0.2067,\n                        0.1959, 0.1543, 0.1785, 0.1822, 0.1530, 0.2044, 0.2791, 0.1727, 0.1387,\n                        0.1928, 0.2401, 0.1216, 0.1493, 0.1740, 0.2017, 0.1713, 0.2036, 0.1425,\n                        0.2251, 0.1679, 0.3289, 0.1497, 0.1676, 0.1656, 0.1870, 0.1318, 0.1542,\n                        0.1822, 0.1697, 0.2965, 0.1653, 0.1494, 0.1459, 0.1686, 0.3904, 0.1719,\n                        0.1930, 0.1519, 0.1380, 0.2357, 0.2356, 0.2019, 0.1443, 0.2650, 0.2081,\n                        0.2825, 0.1677, 0.2178, 0.1669, 0.1863, 0.1792, 0.1928, 0.1646, 0.1830,\n                        0.2266, 0.1341, 0.1241, 0.1420, 0.1641, 0.1511, 0.1823, 0.1833, 0.1896,\n                        0.1449, 0.1469, 0.2129, 0.1604, 0.2186, 0.1581, 0.2456, 0.1417, 0.2893,\n                        0.1430, 0.1857, 0.1983, 0.2655, 0.3222, 0.1906, 0.1981, 0.1796, 0.2110,\n                        0.3368, 0.1658, 0.1479, 0.1732, 0.1460, 0.2239, 0.1604, 0.1745, 0.1674,\n                        0.1099, 0.1677, 0.1219, 0.2055, 0.1535, 0.1674, 0.2086, 0.2291, 0.1533,\n                        0.1473, 0.2408, 0.1263, 0.1352, 0.1983, 0.2134, 0.1973, 0.1748, 0.1687,\n                        0.1779, 0.1751, 0.1794, 0.1392, 0.1815, 0.1278, 0.2113, 0.2098, 0.1657,\n                        0.1555, 0.2460, 0.1915, 0.1619, 0.1630, 0.1335, 0.1786, 0.1215, 0.2429,\n                        0.1752, 0.1541, 0.2051, 0.2344, 0.1603, 0.2614, 0.1494, 0.1497, 0.1224,\n                        0.1882, 0.1664, 0.4163, 0.1985, 0.2226, 0.1922, 0.2103, 0.5725, 0.1423,\n                        0.2234, 0.1353, 0.2082, 0.1847, 0.1494, 0.1461, 0.2181, 0.1453, 0.1852,\n                        0.2246, 0.1507, 0.2357, 0.1604, 0.2646, 0.2705, 0.1372, 0.1972, 0.1383,\n                        0.1726, 0.1950, 0.1951, 0.1756, 0.1930, 0.2144, 0.1451, 0.2480, 0.1381,\n                        0.1975, 0.1175, 0.2194, 0.1309, 0.2710, 0.3331, 0.1624, 0.1577, 0.2324,\n                        0.1513, 0.1828, 0.1443, 0.2711, 0.1615, 0.1647, 0.1767, 0.1634, 0.1794,\n                        0.1451, 0.1579, 0.1701, 0.1128, 0.1721, 0.2582, 0.4514, 0.2585, 0.1304,\n                        0.1185, 0.1854, 0.1939, 0.3778, 0.1851, 0.1596, 0.2157, 0.2012, 0.1816,\n                        0.1792, 0.1603, 0.1838, 0.1908, 0.1818, 0.1934, 0.1954, 0.2074, 0.1329,\n                        0.1680, 0.2063, 0.2163, 0.1331, 0.1306, 0.2670, 0.1959, 0.1884, 0.2028,\n                        0.1620, 0.2882, 0.2324, 0.1298, 0.2048, 0.1529, 0.1963, 0.2087, 0.1859,\n                        0.1893, 0.1114, 0.1399, 0.2397, 0.1387, 0.2292, 0.1291, 0.1834, 0.2184,\n                        0.1843, 0.2692, 0.2237, 0.2168, 0.1196, 0.2731, 0.1585, 0.1542, 0.1374,\n                        0.2325, 0.2032, 0.1417, 0.2179, 0.2438, 0.1603, 0.1532, 0.2405, 0.3639,\n                        0.1665, 0.1475, 0.2193, 0.2170, 0.1669, 0.1686, 0.2140, 0.1272, 0.1525,\n                        0.3761, 0.2375, 0.1466, 0.1369, 0.2029, 0.1708, 0.2591, 0.1620, 0.1213,\n                        0.1379, 0.1632, 0.1858, 0.1280, 0.1655, 0.1819, 0.1410, 0.1680, 0.1563,\n                        0.1826, 0.1166, 0.1689, 0.1327, 0.1945, 0.1686, 0.1776, 0.2289, 0.1797,\n                        0.1683, 0.1336, 0.2167, 0.2144, 0.1439, 0.3200, 0.2118, 0.2545, 0.1935,\n                        0.2478, 0.1386, 0.1808, 0.1594, 0.1741, 0.1633, 0.1658, 0.2230, 0.1535,\n                        0.2761, 0.2691, 0.2758, 0.1747, 0.1791, 0.2699, 0.1985, 0.4221, 0.1697,\n                        0.1136, 0.2171, 0.1360, 0.4038, 0.2075, 0.1871, 0.1882, 0.2753, 0.1892,\n                        0.2676, 0.1906, 0.3222, 0.2046, 0.1481, 0.2105, 0.1891, 0.1792, 0.2565,\n                        0.1767, 0.1639, 0.1892, 0.3448, 0.2431, 0.2444, 0.1629, 0.3036, 0.1385,\n                        0.1625, 0.1872, 0.1502, 0.1620, 0.2300, 0.1704, 0.1851, 0.1539, 0.3197,\n                        0.1871, 0.1726, 0.2379, 0.2737, 0.1806, 0.1534, 0.1492, 0.2517, 0.1344,\n                        0.1529, 0.1805, 0.3429, 0.2126, 0.2041, 0.1724, 0.2008, 0.1633, 0.1747,\n                        0.2502, 0.1649, 0.2923, 0.1429, 0.1432, 0.2055, 0.1596, 0.4016, 0.1395,\n                        0.1773, 0.1800, 0.1614, 0.2171, 0.2726, 0.1900, 0.2019, 0.2394, 0.2109,\n                        0.1416, 0.1552, 0.1972, 0.2111, 0.1352, 0.2103, 0.1367, 0.1753, 0.1373,\n                        0.1650, 0.2146, 0.1684, 0.1505, 0.1483, 0.1450, 0.2391, 0.1449, 0.1999,\n                        0.1316, 0.1602, 0.1493, 0.2302, 0.1733, 0.2592, 0.2650, 0.1867, 0.1537,\n                        0.1603, 0.1548, 0.1289, 0.2532, 0.1307, 0.1788, 0.1398, 0.1980, 0.2499,\n                        0.1601, 0.3257, 0.2095, 0.1164, 0.1333, 0.1867, 0.1727, 0.1960, 0.2019,\n                        0.1432, 0.1511, 0.2088, 0.1448, 0.1422, 0.1913, 0.2042, 0.1856, 0.2081,\n                        0.1673, 0.3832, 0.1225, 0.1263, 0.1903, 0.2363, 0.2319, 0.1232, 0.3657,\n                        0.3283, 0.1245, 0.1718, 0.1767, 0.1228, 0.1544, 0.1972, 0.1138, 0.1166,\n                        0.1690, 0.1640, 0.1537, 0.1797, 0.1696, 0.1947, 0.2259, 0.1999, 0.1462,\n                        0.2038, 0.2427, 0.1710, 0.2625, 0.1881, 0.2431, 0.1754, 0.1515, 0.1524,\n                        0.1418, 0.1426, 0.1602, 0.1450, 0.1491, 0.2252, 0.1255, 0.2407, 0.2022,\n                        0.1967, 0.1767, 0.1944, 0.2570, 0.3084, 0.1472, 0.1093, 0.1761, 0.1101,\n                        0.1684, 0.1543, 0.1721, 0.1524, 0.1080, 0.2236, 0.1777, 0.2049, 0.1980,\n                        0.2144, 0.1284, 0.2997, 0.0957, 0.1235, 0.1527, 0.1668, 0.1498, 0.2195,\n                        0.1857, 0.1919, 0.1773, 0.2275, 0.1524, 0.1694, 0.3328, 0.1852, 0.1917,\n                        0.1468, 0.1649, 0.1506, 0.2363, 0.2659, 0.1941, 0.1711, 0.1749, 0.1956,\n                        0.1647, 0.3450, 0.1432, 0.1461, 0.1805, 0.1914, 0.1452, 0.1754, 0.2489,\n                        0.1441, 0.1514, 0.3216, 0.1781, 0.2974, 0.1980, 0.1333, 0.1327, 0.2425,\n                        0.1559, 0.1787, 0.1543, 0.3421, 0.1278, 0.1697, 0.1729, 0.2080, 0.1925,\n                        0.2525, 0.2360, 0.2234, 0.1577, 0.2780, 0.1698, 0.1511, 0.1574, 0.1839,\n                        0.2515, 0.1982, 0.1491, 0.1368, 0.2716, 0.2566, 0.1808, 0.1592, 0.1649,\n                        0.1717, 0.1587, 0.1479, 0.1653, 0.1409, 0.2488, 0.1489, 0.1656, 0.1614,\n                        0.1722, 0.1699, 0.1556, 0.1697, 0.1816, 0.1882, 0.1962, 0.1558, 0.1578,\n                        0.1532, 0.1711, 0.1421, 0.1736, 0.1439, 0.1591, 0.2043, 0.2756, 0.2337,\n                        0.2194, 0.1770, 0.1929])\n              )\n            )\n          )\n          (4): GELU(approximate='none')\n          (5): Linear(\n            in_features=768, out_features=192, bias=True\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0018, 0.0022, 0.0016, 0.0021, 0.0015, 0.0018, 0.0025, 0.0017, 0.0017,\n                      0.0018, 0.0013, 0.0015, 0.0029, 0.0017, 0.0027, 0.0016, 0.0014, 0.0014,\n                      0.0017, 0.0031, 0.0021, 0.0020, 0.0039, 0.0017, 0.0013, 0.0016, 0.0020,\n                      0.0015, 0.0019, 0.0024, 0.0013, 0.0013, 0.0025, 0.0018, 0.0015, 0.0016,\n                      0.0019, 0.0015, 0.0039, 0.0018, 0.0015, 0.0018, 0.0021, 0.0020, 0.0023,\n                      0.0025, 0.0018, 0.0019, 0.0017, 0.0018, 0.0022, 0.0018, 0.0021, 0.0016,\n                      0.0022, 0.0023, 0.0020, 0.0013, 0.0014, 0.0014, 0.0019, 0.0012, 0.0019,\n                      0.0015, 0.0014, 0.0014, 0.0014, 0.0017, 0.0014, 0.0017, 0.0018, 0.0017,\n                      0.0017, 0.0020, 0.0019, 0.0015, 0.0021, 0.0016, 0.0024, 0.0013, 0.0016,\n                      0.0015, 0.0015, 0.0014, 0.0024, 0.0016, 0.0014, 0.0019, 0.0027, 0.0052,\n                      0.0018, 0.0018, 0.0022, 0.0019, 0.0032, 0.0014, 0.0014, 0.0018, 0.0016,\n                      0.0025, 0.0015, 0.0016, 0.0016, 0.0016, 0.0017, 0.0024, 0.0019, 0.0017,\n                      0.0017, 0.0016, 0.0018, 0.0016, 0.0021, 0.0018, 0.0013, 0.0019, 0.0022,\n                      0.0019, 0.0018, 0.0015, 0.0015, 0.0017, 0.0024, 0.0024, 0.0017, 0.0014,\n                      0.0015, 0.0022, 0.0020, 0.0017, 0.0022, 0.0027, 0.0013, 0.0016, 0.0030,\n                      0.0029, 0.0022, 0.0030, 0.0023, 0.0014, 0.0015, 0.0015, 0.0031, 0.0015,\n                      0.0018, 0.0023, 0.0018, 0.0017, 0.0025, 0.0016, 0.0017, 0.0023, 0.0016,\n                      0.0013, 0.0021, 0.0028, 0.0015, 0.0018, 0.0016, 0.0015, 0.0021, 0.0017,\n                      0.0017, 0.0018, 0.0019, 0.0019, 0.0016, 0.0018, 0.0032, 0.0017, 0.0014,\n                      0.0033, 0.0013, 0.0018, 0.0017, 0.0014, 0.0024, 0.0015, 0.0020, 0.0015,\n                      0.0020, 0.0015, 0.0015, 0.0015, 0.0024, 0.0013, 0.0027, 0.0018, 0.0018,\n                      0.0017, 0.0023, 0.0017]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                     dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n                min_val=tensor([-0.2304, -0.2786, -0.1727, -0.2676, -0.1766, -0.2331, -0.3201, -0.2149,\n                        -0.1815, -0.1702, -0.1626, -0.1865, -0.1777, -0.2175, -0.2319, -0.1986,\n                        -0.1511, -0.1764, -0.2205, -0.2082, -0.2671, -0.2050, -0.4252, -0.2208,\n                        -0.1694, -0.2036, -0.2230, -0.1973, -0.1860, -0.2553, -0.1545, -0.1665,\n                        -0.2022, -0.2255, -0.1941, -0.1766, -0.2439, -0.1899, -0.4957, -0.2269,\n                        -0.1867, -0.2066, -0.2319, -0.1874, -0.2446, -0.2100, -0.2353, -0.2411,\n                        -0.2145, -0.2330, -0.2833, -0.2319, -0.2674, -0.1892, -0.2825, -0.2922,\n                        -0.2575, -0.1517, -0.1833, -0.1636, -0.1991, -0.1566, -0.2444, -0.1898,\n                        -0.1475, -0.1844, -0.1683, -0.2179, -0.1828, -0.2239, -0.2245, -0.1840,\n                        -0.1367, -0.2544, -0.2387, -0.1818, -0.2171, -0.2065, -0.3015, -0.1678,\n                        -0.2030, -0.1969, -0.1947, -0.1662, -0.3092, -0.1897, -0.1746, -0.1739,\n                        -0.2171, -0.2491, -0.2056, -0.2337, -0.2204, -0.2424, -0.2763, -0.1818,\n                        -0.1494, -0.2308, -0.2065, -0.3228, -0.1893, -0.1471, -0.1991, -0.1721,\n                        -0.1767, -0.2616, -0.2429, -0.1781, -0.1594, -0.2073, -0.2278, -0.1949,\n                        -0.1768, -0.2032, -0.1312, -0.2468, -0.2193, -0.2404, -0.2334, -0.1730,\n                        -0.1700, -0.2144, -0.2197, -0.1846, -0.2154, -0.1721, -0.1817, -0.2841,\n                        -0.2515, -0.1693, -0.2803, -0.2432, -0.1473, -0.2067, -0.3043, -0.3659,\n                        -0.2134, -0.3841, -0.3002, -0.1403, -0.1904, -0.1965, -0.2205, -0.1895,\n                        -0.2342, -0.1861, -0.2316, -0.2216, -0.3197, -0.1845, -0.1906, -0.2989,\n                        -0.1670, -0.1694, -0.2326, -0.2843, -0.1977, -0.2293, -0.1696, -0.1903,\n                        -0.2062, -0.2092, -0.2155, -0.1917, -0.2425, -0.2406, -0.1980, -0.1836,\n                        -0.3930, -0.2082, -0.1629, -0.2595, -0.1636, -0.2349, -0.2221, -0.1329,\n                        -0.3121, -0.1655, -0.2553, -0.1827, -0.1912, -0.1895, -0.1767, -0.1649,\n                        -0.2901, -0.1472, -0.3458, -0.2332, -0.2259, -0.2165, -0.2903, -0.2164]), max_val=tensor([0.2093, 0.2560, 0.2066, 0.2557, 0.1922, 0.2103, 0.3177, 0.1918, 0.2108,\n                        0.2224, 0.1660, 0.1667, 0.3697, 0.1658, 0.3467, 0.1752, 0.1813, 0.1570,\n                        0.1948, 0.3936, 0.2401, 0.2540, 0.4915, 0.2057, 0.1621, 0.1595, 0.2481,\n                        0.1741, 0.2376, 0.3071, 0.1623, 0.1705, 0.3149, 0.2001, 0.1799, 0.2074,\n                        0.2246, 0.1680, 0.4696, 0.2007, 0.1850, 0.2305, 0.2677, 0.2574, 0.2950,\n                        0.3113, 0.2061, 0.1917, 0.1661, 0.1643, 0.1558, 0.1575, 0.1738, 0.2047,\n                        0.2790, 0.2371, 0.1870, 0.1679, 0.1784, 0.1734, 0.2461, 0.1577, 0.2059,\n                        0.1864, 0.1722, 0.1693, 0.1722, 0.1758, 0.1733, 0.2029, 0.2039, 0.2140,\n                        0.2155, 0.2194, 0.2286, 0.1856, 0.2664, 0.1948, 0.1880, 0.1709, 0.1879,\n                        0.1658, 0.1915, 0.1723, 0.2731, 0.1978, 0.1810, 0.2457, 0.3458, 0.6559,\n                        0.2287, 0.1900, 0.2846, 0.2306, 0.4024, 0.1699, 0.1751, 0.1756, 0.1707,\n                        0.2629, 0.1941, 0.2001, 0.1503, 0.2029, 0.2216, 0.3089, 0.2348, 0.2189,\n                        0.2126, 0.2023, 0.1784, 0.2087, 0.2637, 0.2262, 0.1701, 0.2224, 0.2745,\n                        0.2093, 0.2305, 0.1963, 0.1873, 0.1977, 0.3096, 0.3056, 0.1877, 0.1727,\n                        0.1906, 0.2706, 0.2591, 0.2161, 0.2374, 0.3415, 0.1664, 0.1533, 0.3774,\n                        0.2545, 0.2818, 0.2918, 0.2364, 0.1818, 0.1633, 0.1751, 0.3939, 0.1581,\n                        0.1886, 0.2909, 0.1867, 0.1987, 0.2225, 0.2004, 0.2181, 0.2111, 0.2087,\n                        0.1692, 0.2684, 0.3548, 0.1846, 0.1972, 0.2077, 0.1914, 0.2616, 0.2120,\n                        0.1466, 0.2283, 0.2185, 0.1682, 0.2018, 0.2331, 0.4051, 0.2147, 0.1735,\n                        0.4167, 0.1684, 0.2229, 0.1860, 0.1746, 0.2426, 0.1911, 0.2524, 0.1882,\n                        0.2511, 0.1835, 0.1842, 0.1958, 0.3055, 0.1652, 0.2101, 0.1597, 0.2184,\n                        0.1699, 0.2415, 0.1547])\n              )\n            )\n          )\n        )\n      )\n      (2): Module(\n        (block): Module(\n          (0): Conv2d(\n            192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0017, 0.0025, 0.0011, 0.0023, 0.0027, 0.0023, 0.0023, 0.0025, 0.0019,\n                      0.0028, 0.0026, 0.0030, 0.0008, 0.0026, 0.0013, 0.0010, 0.0018, 0.0023,\n                      0.0021, 0.0012, 0.0008, 0.0019, 0.0018, 0.0021, 0.0022, 0.0029, 0.0017,\n                      0.0025, 0.0013, 0.0024, 0.0026, 0.0026, 0.0014, 0.0023, 0.0026, 0.0015,\n                      0.0025, 0.0028, 0.0016, 0.0029, 0.0020, 0.0006, 0.0028, 0.0024, 0.0023,\n                      0.0004, 0.0016, 0.0026, 0.0028, 0.0008, 0.0025, 0.0016, 0.0014, 0.0003,\n                      0.0018, 0.0015, 0.0023, 0.0029, 0.0026, 0.0028, 0.0018, 0.0028, 0.0019,\n                      0.0026, 0.0026, 0.0024, 0.0007, 0.0027, 0.0025, 0.0007, 0.0025, 0.0026,\n                      0.0026, 0.0024, 0.0022, 0.0026, 0.0024, 0.0025, 0.0022, 0.0025, 0.0026,\n                      0.0026, 0.0006, 0.0017, 0.0024, 0.0029, 0.0007, 0.0008, 0.0005, 0.0011,\n                      0.0016, 0.0028, 0.0023, 0.0025, 0.0013, 0.0027, 0.0019, 0.0025, 0.0013,\n                      0.0023, 0.0010, 0.0027, 0.0025, 0.0017, 0.0029, 0.0007, 0.0017, 0.0021,\n                      0.0025, 0.0008, 0.0027, 0.0010, 0.0018, 0.0016, 0.0026, 0.0008, 0.0010,\n                      0.0024, 0.0019, 0.0004, 0.0027, 0.0005, 0.0022, 0.0027, 0.0004, 0.0009,\n                      0.0008, 0.0019, 0.0019, 0.0019, 0.0020, 0.0024, 0.0028, 0.0027, 0.0027,\n                      0.0028, 0.0007, 0.0016, 0.0021, 0.0029, 0.0025, 0.0020, 0.0018, 0.0025,\n                      0.0021, 0.0025, 0.0007, 0.0022, 0.0019, 0.0023, 0.0022, 0.0023, 0.0027,\n                      0.0017, 0.0026, 0.0025, 0.0007, 0.0023, 0.0017, 0.0028, 0.0028, 0.0023,\n                      0.0026, 0.0028, 0.0004, 0.0025, 0.0006, 0.0015, 0.0018, 0.0026, 0.0027,\n                      0.0016, 0.0028, 0.0023, 0.0024, 0.0017, 0.0016, 0.0024, 0.0025, 0.0023,\n                      0.0026, 0.0014, 0.0019, 0.0028, 0.0023, 0.0012, 0.0025, 0.0029, 0.0028,\n                      0.0021, 0.0008, 0.0027]), zero_point=tensor([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0, -128,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0, -128,    0,    0,    0,  127,    0,    0,\n                         0,  127,    0,    0,    0, -128,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0, -128,    0,    0, -128,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0, -128,    0,\n                         0,    0, -128,    0,  127,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,  127,\n                         0,  127,    0,    0, -128,    0, -128,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,  127,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,  127,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0, -128,    0,  127,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n                     dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n                min_val=tensor([-0.2088, -0.1090, -0.0032, -0.0914, -0.0245, -0.1363, -0.0759, -0.0588,\n                        -0.2423, -0.3537, -0.3289, -0.0253, -0.0962, -0.0306, -0.0109,  0.0009,\n                        -0.2308, -0.0086, -0.2734, -0.1474, -0.0080, -0.1868, -0.2304, -0.0065,\n                        -0.2834, -0.3661, -0.2126, -0.3172, -0.1692, -0.3128, -0.0365, -0.3383,\n                        -0.1764, -0.1368, -0.3336, -0.0013, -0.3258, -0.3616, -0.2092, -0.0317,\n                        -0.0047,  0.0029, -0.0279, -0.3085, -0.1058, -0.0979, -0.2103, -0.0672,\n                        -0.3543, -0.2007, -0.0989, -0.0072, -0.0112,  0.0071, -0.2331, -0.1110,\n                        -0.2953, -0.0280, -0.3370, -0.0192, -0.2262, -0.0138, -0.0057, -0.0471,\n                        -0.0303, -0.3095,  0.0006, -0.3463, -0.3160,  0.0029, -0.3197, -0.0726,\n                        -0.3344, -0.1013, -0.2808, -0.3265, -0.0286, -0.0116, -0.2865, -0.3204,\n                        -0.3311, -0.3387,  0.0069, -0.0051, -0.1379, -0.3764,  0.0050, -0.1088,\n                        -0.1241, -0.0227, -0.2092, -0.3523, -0.0551, -0.0056, -0.0230, -0.0669,\n                        -0.0023, -0.3231, -0.1618, -0.2919, -0.1279, -0.0260, -0.3148, -0.0039,\n                        -0.3658, -0.0850, -0.2182, -0.0173, -0.3242, -0.0274, -0.3515, -0.1311,\n                        -0.1853, -0.1747, -0.3374, -0.0114, -0.0032, -0.3095, -0.2163, -0.1076,\n                        -0.0252, -0.1249, -0.2871, -0.3422,  0.0019, -0.1188,  0.0021, -0.2274,\n                        -0.2411, -0.0072, -0.2598, -0.0495, -0.3569, -0.3443, -0.3464, -0.3564,\n                        -0.1848, -0.2084, -0.0830, -0.3722, -0.3141, -0.2543, -0.2278, -0.0438,\n                        -0.2675, -0.3166, -0.1870, -0.0143, -0.1984, -0.0049, -0.0552, -0.1575,\n                        -0.0258, -0.0022, -0.3357, -0.0993, -0.0906, -0.1023, -0.2216, -0.0304,\n                        -0.0214, -0.2914, -0.0228, -0.0292,  0.0023, -0.0420, -0.1577, -0.1029,\n                        -0.1834, -0.3378, -0.3437, -0.0110, -0.3618, -0.0897, -0.0161, -0.2180,\n                        -0.1914, -0.3044, -0.0827, -0.0254, -0.3386, -0.0030, -0.2433, -0.3599,\n                        -0.0368, -0.1504, -0.3214, -0.0583, -0.3632, -0.0866, -0.1041, -0.0536]), max_val=tensor([ 2.0965e-01,  3.1803e-01,  1.4324e-01,  2.8696e-01,  3.4546e-01,\n                         2.8791e-01,  2.9216e-01,  3.2361e-01,  1.5632e-02,  1.2278e-02,\n                         4.4870e-02,  3.7500e-01,  1.5767e-03,  3.3027e-01,  1.6689e-01,\n                         2.4539e-01,  3.9222e-03,  2.8657e-01,  8.4403e-03,  5.9205e-02,\n                         1.0436e-01,  2.3555e-01,  2.0943e-01,  2.7053e-01,  1.9395e-01,\n                         2.6434e-02,  1.2476e-02,  3.1984e-02,  3.5574e-03,  9.7574e-02,\n                         3.2562e-01,  8.7956e-02,  2.3270e-03,  2.9796e-01,  1.7045e-02,\n                         1.8899e-01,  3.3215e-02,  2.0486e-02,  2.0934e-01,  3.7292e-01,\n                         2.4882e-01,  1.4719e-01,  3.5728e-01,  9.8351e-02,  2.9542e-01,\n                        -2.3852e-02,  3.3508e-03,  3.3288e-01,  4.3768e-02, -2.5808e-04,\n                         3.1402e-01,  2.0712e-01,  1.7723e-01,  7.3309e-02,  2.0568e-01,\n                         1.9420e-01,  6.6557e-03,  3.6487e-01,  9.2905e-02,  3.4957e-01,\n                         2.2737e-01,  3.5288e-01,  2.3538e-01,  3.3477e-01,  3.2540e-01,\n                         1.7927e-02,  1.7907e-01,  1.7664e-02,  1.1187e-01,  1.7519e-01,\n                         3.0270e-02,  3.2970e-01,  6.4085e-02,  3.0650e-01,  1.9939e-01,\n                         3.3099e-02,  3.0786e-01,  3.2257e-01,  6.9765e-03,  5.2968e-02,\n                         2.0938e-02,  4.0463e-02,  1.4745e-01,  2.1584e-01,  2.9916e-01,\n                         2.3434e-02,  1.6750e-01,  2.7121e-03, -3.7672e-03,  1.3746e-01,\n                         2.6746e-03,  6.2360e-02,  2.9005e-01,  3.1809e-01,  1.6351e-01,\n                         3.4129e-01,  2.3888e-01,  5.2457e-02,  1.9830e-04,  9.0017e-02,\n                         7.1303e-03,  3.4043e-01,  8.8792e-02,  2.1991e-01,  2.1816e-02,\n                         6.8997e-02,  1.2653e-01,  2.6051e-01,  4.3142e-02,  9.8634e-02,\n                         1.3786e-02,  1.0659e-01,  2.2764e-01,  2.0312e-01,  2.2962e-02,\n                         9.9564e-02,  1.3071e-01,  5.5598e-02,  2.4102e-01, -8.8091e-04,\n                         3.3835e-01, -1.7206e-03,  9.6465e-02,  5.9903e-02,  1.1203e-01,\n                         2.1074e-03,  2.0617e-01,  2.4740e-01,  7.4961e-02,  2.4441e-01,\n                         1.3329e-01,  3.0330e-01,  2.3114e-02,  3.1521e-02,  5.7667e-02,\n                         4.3778e-02, -5.2347e-03,  1.6925e-02,  2.7145e-01,  2.7002e-02,\n                         2.3818e-02,  8.4581e-03,  4.9955e-02,  3.1322e-01,  1.1022e-03,\n                         2.6029e-02, -2.4245e-03,  2.8197e-01,  2.3919e-01,  2.9311e-01,\n                         2.8051e-01,  2.8805e-01,  3.4826e-01,  2.1142e-01,  3.6649e-02,\n                         3.2384e-01,  5.4907e-04,  2.8632e-01,  3.5266e-03,  3.6184e-01,\n                         3.6020e-01,  1.2564e-02,  3.3607e-01,  3.5775e-01,  1.0542e-01,\n                         3.1840e-01, -4.7609e-03,  1.9060e-01,  2.2977e-01,  9.2791e-02,\n                         2.8032e-02,  2.0830e-01,  3.0692e-02,  2.9718e-01,  3.0824e-01,\n                         9.7746e-04,  1.9717e-01,  2.8611e-02,  3.1357e-01,  2.9269e-01,\n                         4.6866e-02,  1.7844e-01,  6.4807e-03,  4.1638e-02,  2.9272e-01,\n                         4.1931e-04,  8.0409e-02,  3.6443e-01,  2.8497e-02,  2.7117e-01,\n                         9.9468e-02,  3.3992e-01])\n              )\n            )\n          )\n          (2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(\n            in_features=192, out_features=768, bias=True\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0012, 0.0013, 0.0021, 0.0016, 0.0013, 0.0012, 0.0011, 0.0013, 0.0015,\n                      0.0016, 0.0015, 0.0014, 0.0016, 0.0020, 0.0021, 0.0024, 0.0016, 0.0023,\n                      0.0018, 0.0012, 0.0013, 0.0010, 0.0012, 0.0017, 0.0012, 0.0015, 0.0013,\n                      0.0016, 0.0015, 0.0019, 0.0023, 0.0014, 0.0010, 0.0016, 0.0013, 0.0018,\n                      0.0015, 0.0014, 0.0013, 0.0017, 0.0017, 0.0013, 0.0015, 0.0016, 0.0012,\n                      0.0020, 0.0010, 0.0017, 0.0011, 0.0013, 0.0010, 0.0011, 0.0012, 0.0015,\n                      0.0021, 0.0014, 0.0014, 0.0012, 0.0016, 0.0016, 0.0013, 0.0021, 0.0023,\n                      0.0013, 0.0012, 0.0015, 0.0013, 0.0017, 0.0013, 0.0015, 0.0023, 0.0019,\n                      0.0014, 0.0014, 0.0018, 0.0015, 0.0017, 0.0015, 0.0011, 0.0015, 0.0015,\n                      0.0012, 0.0013, 0.0009, 0.0017, 0.0020, 0.0017, 0.0016, 0.0024, 0.0019,\n                      0.0012, 0.0016, 0.0013, 0.0013, 0.0021, 0.0020, 0.0012, 0.0018, 0.0017,\n                      0.0012, 0.0017, 0.0015, 0.0023, 0.0015, 0.0012, 0.0013, 0.0015, 0.0030,\n                      0.0011, 0.0012, 0.0012, 0.0017, 0.0016, 0.0021, 0.0013, 0.0017, 0.0013,\n                      0.0017, 0.0020, 0.0022, 0.0019, 0.0023, 0.0013, 0.0019, 0.0016, 0.0019,\n                      0.0014, 0.0016, 0.0011, 0.0017, 0.0012, 0.0013, 0.0021, 0.0017, 0.0020,\n                      0.0013, 0.0014, 0.0013, 0.0012, 0.0015, 0.0012, 0.0014, 0.0014, 0.0014,\n                      0.0023, 0.0024, 0.0028, 0.0014, 0.0013, 0.0022, 0.0010, 0.0016, 0.0015,\n                      0.0016, 0.0013, 0.0011, 0.0013, 0.0021, 0.0018, 0.0016, 0.0013, 0.0013,\n                      0.0019, 0.0011, 0.0012, 0.0017, 0.0011, 0.0027, 0.0012, 0.0012, 0.0009,\n                      0.0015, 0.0018, 0.0016, 0.0020, 0.0012, 0.0016, 0.0012, 0.0025, 0.0013,\n                      0.0014, 0.0019, 0.0015, 0.0017, 0.0012, 0.0028, 0.0017, 0.0021, 0.0013,\n                      0.0012, 0.0011, 0.0016, 0.0015, 0.0015, 0.0025, 0.0015, 0.0011, 0.0015,\n                      0.0015, 0.0012, 0.0016, 0.0019, 0.0011, 0.0018, 0.0015, 0.0011, 0.0012,\n                      0.0019, 0.0015, 0.0011, 0.0017, 0.0014, 0.0015, 0.0016, 0.0019, 0.0015,\n                      0.0016, 0.0017, 0.0014, 0.0013, 0.0012, 0.0014, 0.0015, 0.0013, 0.0016,\n                      0.0012, 0.0015, 0.0017, 0.0017, 0.0010, 0.0016, 0.0014, 0.0020, 0.0016,\n                      0.0012, 0.0017, 0.0015, 0.0016, 0.0015, 0.0020, 0.0013, 0.0015, 0.0020,\n                      0.0015, 0.0014, 0.0020, 0.0014, 0.0011, 0.0017, 0.0013, 0.0013, 0.0015,\n                      0.0017, 0.0010, 0.0025, 0.0013, 0.0016, 0.0012, 0.0011, 0.0023, 0.0023,\n                      0.0011, 0.0018, 0.0016, 0.0012, 0.0015, 0.0027, 0.0015, 0.0018, 0.0015,\n                      0.0022, 0.0019, 0.0015, 0.0017, 0.0014, 0.0018, 0.0022, 0.0013, 0.0015,\n                      0.0015, 0.0017, 0.0015, 0.0015, 0.0016, 0.0016, 0.0015, 0.0016, 0.0013,\n                      0.0013, 0.0015, 0.0014, 0.0021, 0.0012, 0.0009, 0.0016, 0.0011, 0.0013,\n                      0.0020, 0.0016, 0.0019, 0.0033, 0.0015, 0.0011, 0.0013, 0.0017, 0.0012,\n                      0.0017, 0.0014, 0.0018, 0.0015, 0.0013, 0.0016, 0.0019, 0.0015, 0.0011,\n                      0.0016, 0.0011, 0.0015, 0.0013, 0.0012, 0.0014, 0.0015, 0.0016, 0.0015,\n                      0.0015, 0.0013, 0.0015, 0.0012, 0.0012, 0.0013, 0.0018, 0.0014, 0.0019,\n                      0.0011, 0.0023, 0.0021, 0.0018, 0.0012, 0.0011, 0.0013, 0.0015, 0.0014,\n                      0.0030, 0.0019, 0.0012, 0.0010, 0.0017, 0.0020, 0.0011, 0.0011, 0.0013,\n                      0.0013, 0.0015, 0.0010, 0.0014, 0.0017, 0.0013, 0.0012, 0.0013, 0.0013,\n                      0.0012, 0.0017, 0.0018, 0.0014, 0.0028, 0.0020, 0.0016, 0.0020, 0.0018,\n                      0.0019, 0.0017, 0.0019, 0.0015, 0.0012, 0.0018, 0.0012, 0.0019, 0.0018,\n                      0.0021, 0.0016, 0.0013, 0.0012, 0.0014, 0.0013, 0.0015, 0.0012, 0.0013,\n                      0.0018, 0.0017, 0.0010, 0.0019, 0.0013, 0.0011, 0.0015, 0.0022, 0.0021,\n                      0.0016, 0.0020, 0.0018, 0.0013, 0.0010, 0.0015, 0.0012, 0.0014, 0.0014,\n                      0.0012, 0.0012, 0.0012, 0.0018, 0.0021, 0.0012, 0.0019, 0.0014, 0.0031,\n                      0.0016, 0.0014, 0.0018, 0.0015, 0.0015, 0.0014, 0.0016, 0.0010, 0.0022,\n                      0.0012, 0.0011, 0.0011, 0.0019, 0.0010, 0.0012, 0.0019, 0.0013, 0.0013,\n                      0.0010, 0.0015, 0.0013, 0.0023, 0.0013, 0.0018, 0.0019, 0.0020, 0.0016,\n                      0.0020, 0.0013, 0.0020, 0.0017, 0.0021, 0.0014, 0.0010, 0.0011, 0.0014,\n                      0.0012, 0.0013, 0.0013, 0.0020, 0.0015, 0.0023, 0.0015, 0.0015, 0.0014,\n                      0.0016, 0.0018, 0.0018, 0.0013, 0.0012, 0.0016, 0.0021, 0.0018, 0.0012,\n                      0.0012, 0.0014, 0.0012, 0.0011, 0.0009, 0.0017, 0.0014, 0.0018, 0.0015,\n                      0.0019, 0.0011, 0.0013, 0.0012, 0.0013, 0.0015, 0.0012, 0.0014, 0.0016,\n                      0.0014, 0.0019, 0.0016, 0.0013, 0.0011, 0.0021, 0.0016, 0.0015, 0.0018,\n                      0.0017, 0.0017, 0.0016, 0.0015, 0.0026, 0.0018, 0.0017, 0.0016, 0.0014,\n                      0.0016, 0.0017, 0.0011, 0.0015, 0.0017, 0.0022, 0.0014, 0.0013, 0.0014,\n                      0.0021, 0.0015, 0.0017, 0.0010, 0.0013, 0.0017, 0.0015, 0.0016, 0.0011,\n                      0.0012, 0.0012, 0.0016, 0.0027, 0.0017, 0.0016, 0.0014, 0.0022, 0.0023,\n                      0.0014, 0.0015, 0.0023, 0.0012, 0.0017, 0.0025, 0.0012, 0.0028, 0.0010,\n                      0.0014, 0.0021, 0.0012, 0.0021, 0.0015, 0.0017, 0.0018, 0.0015, 0.0017,\n                      0.0019, 0.0018, 0.0016, 0.0012, 0.0020, 0.0015, 0.0011, 0.0026, 0.0011,\n                      0.0017, 0.0015, 0.0014, 0.0012, 0.0017, 0.0013, 0.0014, 0.0016, 0.0014,\n                      0.0022, 0.0016, 0.0017, 0.0012, 0.0013, 0.0020, 0.0015, 0.0013, 0.0013,\n                      0.0014, 0.0018, 0.0010, 0.0015, 0.0019, 0.0014, 0.0014, 0.0013, 0.0016,\n                      0.0015, 0.0012, 0.0018, 0.0015, 0.0018, 0.0018, 0.0016, 0.0014, 0.0010,\n                      0.0014, 0.0016, 0.0021, 0.0015, 0.0014, 0.0012, 0.0011, 0.0016, 0.0021,\n                      0.0015, 0.0020, 0.0023, 0.0022, 0.0013, 0.0013, 0.0013, 0.0013, 0.0013,\n                      0.0016, 0.0026, 0.0019, 0.0015, 0.0019, 0.0013, 0.0012, 0.0011, 0.0013,\n                      0.0020, 0.0010, 0.0014, 0.0016, 0.0019, 0.0015, 0.0015, 0.0013, 0.0017,\n                      0.0012, 0.0017, 0.0014, 0.0019, 0.0018, 0.0012, 0.0012, 0.0012, 0.0019,\n                      0.0011, 0.0011, 0.0020, 0.0017, 0.0021, 0.0017, 0.0013, 0.0011, 0.0011,\n                      0.0015, 0.0017, 0.0012, 0.0015, 0.0017, 0.0014, 0.0017, 0.0014, 0.0014,\n                      0.0025, 0.0015, 0.0024, 0.0015, 0.0015, 0.0017, 0.0014, 0.0018, 0.0011,\n                      0.0018, 0.0016, 0.0012, 0.0013, 0.0016, 0.0021, 0.0012, 0.0015, 0.0013,\n                      0.0011, 0.0010, 0.0022, 0.0013, 0.0019, 0.0027, 0.0018, 0.0017, 0.0013,\n                      0.0024, 0.0018, 0.0015, 0.0014, 0.0015, 0.0012, 0.0022, 0.0015, 0.0017,\n                      0.0017, 0.0014, 0.0014, 0.0013, 0.0015, 0.0015, 0.0018, 0.0017, 0.0015,\n                      0.0013, 0.0021, 0.0013, 0.0010, 0.0015, 0.0013, 0.0015, 0.0019, 0.0019,\n                      0.0022, 0.0013, 0.0013, 0.0015, 0.0012, 0.0017, 0.0015, 0.0015, 0.0013,\n                      0.0014, 0.0022, 0.0012, 0.0013, 0.0017, 0.0015, 0.0025, 0.0017, 0.0015,\n                      0.0011, 0.0027, 0.0016, 0.0017, 0.0012, 0.0011, 0.0011, 0.0020, 0.0014,\n                      0.0015, 0.0018, 0.0011, 0.0016, 0.0023, 0.0016, 0.0015, 0.0013, 0.0012,\n                      0.0013, 0.0012, 0.0018, 0.0014, 0.0018, 0.0018, 0.0012, 0.0014, 0.0020,\n                      0.0018, 0.0023, 0.0014, 0.0014, 0.0014, 0.0017, 0.0013, 0.0020, 0.0015,\n                      0.0017, 0.0016, 0.0029]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                     dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n                min_val=tensor([-0.1306, -0.1397, -0.1276, -0.2034, -0.1415, -0.1170, -0.1244, -0.1493,\n                        -0.1886, -0.1194, -0.1503, -0.1854, -0.1715, -0.1309, -0.1969, -0.1263,\n                        -0.1589, -0.1141, -0.1638, -0.1323, -0.1208, -0.1311, -0.1414, -0.1805,\n                        -0.1347, -0.1450, -0.1677, -0.2096, -0.1125, -0.1625, -0.1996, -0.1130,\n                        -0.1295, -0.1473, -0.1242, -0.1767, -0.1009, -0.1664, -0.1409, -0.1505,\n                        -0.1366, -0.1511, -0.1635, -0.1722, -0.1431, -0.2597, -0.1299, -0.1251,\n                        -0.1338, -0.1348, -0.1207, -0.1463, -0.1478, -0.1919, -0.1683, -0.1043,\n                        -0.1793, -0.1103, -0.2057, -0.1428, -0.1727, -0.1605, -0.1303, -0.1273,\n                        -0.1309, -0.1929, -0.1422, -0.2153, -0.1618, -0.1828, -0.1517, -0.2019,\n                        -0.1516, -0.1749, -0.1306, -0.1580, -0.1206, -0.1478, -0.1005, -0.1274,\n                        -0.1914, -0.1148, -0.1634, -0.1199, -0.2236, -0.2328, -0.2234, -0.1708,\n                        -0.3035, -0.0938, -0.1072, -0.1359, -0.1082, -0.1694, -0.2625, -0.2599,\n                        -0.1299, -0.1412, -0.1210, -0.1080, -0.1138, -0.0945, -0.2912, -0.1555,\n                        -0.1398, -0.1480, -0.1615, -0.3805, -0.1036, -0.1567, -0.1227, -0.1395,\n                        -0.2100, -0.1721, -0.1560, -0.2126, -0.1537, -0.2155, -0.2189, -0.2845,\n                        -0.2397, -0.2914, -0.1510, -0.1098, -0.1891, -0.1679, -0.1661, -0.1800,\n                        -0.1111, -0.2012, -0.1118, -0.1096, -0.2047, -0.2156, -0.1643, -0.1687,\n                        -0.1380, -0.1490, -0.1567, -0.1328, -0.1124, -0.1127, -0.1227, -0.1479,\n                        -0.1358, -0.3022, -0.1574, -0.1827, -0.1437, -0.1763, -0.1260, -0.0955,\n                        -0.1369, -0.1374, -0.1728, -0.1289, -0.1195, -0.2708, -0.1515, -0.1706,\n                        -0.1724, -0.1644, -0.1794, -0.1096, -0.1447, -0.2125, -0.1462, -0.1529,\n                        -0.1237, -0.1547, -0.1155, -0.1078, -0.2217, -0.1386, -0.2059, -0.1259,\n                        -0.1466, -0.1291, -0.2695, -0.1321, -0.1425, -0.2394, -0.1858, -0.1583,\n                        -0.1064, -0.3565, -0.2119, -0.1895, -0.1370, -0.1407, -0.1418, -0.2028,\n                        -0.1895, -0.1608, -0.3264, -0.1658, -0.1446, -0.1343, -0.1026, -0.1567,\n                        -0.1863, -0.2066, -0.1255, -0.2335, -0.1619, -0.1205, -0.1541, -0.2455,\n                        -0.1500, -0.1350, -0.1488, -0.1326, -0.1942, -0.2064, -0.2426, -0.1663,\n                        -0.1192, -0.1002, -0.1443, -0.1637, -0.1586, -0.1548, -0.1884, -0.1089,\n                        -0.2058, -0.1378, -0.1325, -0.1787, -0.1865, -0.1168, -0.2111, -0.1354,\n                        -0.1663, -0.1116, -0.1491, -0.2221, -0.1911, -0.1536, -0.1932, -0.2508,\n                        -0.1116, -0.1892, -0.1513, -0.1503, -0.1814, -0.2563, -0.1843, -0.1152,\n                        -0.1553, -0.1206, -0.1562, -0.1137, -0.1882, -0.1288, -0.2063, -0.1637,\n                        -0.1461, -0.1061, -0.1456, -0.1386, -0.1654, -0.1372, -0.1243, -0.1544,\n                        -0.1491, -0.1904, -0.1228, -0.1667, -0.1426, -0.1895, -0.2775, -0.2475,\n                        -0.1920, -0.1267, -0.1624, -0.1530, -0.1236, -0.1272, -0.1693, -0.1695,\n                        -0.1289, -0.1954, -0.1578, -0.2017, -0.2091, -0.1772, -0.2005, -0.1466,\n                        -0.1698, -0.1095, -0.1479, -0.1917, -0.1518, -0.1168, -0.1322, -0.1224,\n                        -0.1229, -0.1455, -0.1995, -0.2404, -0.2663, -0.1942, -0.1412, -0.1657,\n                        -0.1577, -0.1529, -0.2125, -0.1262, -0.2264, -0.1381, -0.1217, -0.1329,\n                        -0.2396, -0.1932, -0.1289, -0.1957, -0.1334, -0.1255, -0.1244, -0.1393,\n                        -0.1782, -0.1491, -0.1731, -0.1243, -0.1966, -0.1284, -0.1980, -0.1369,\n                        -0.1490, -0.1380, -0.1342, -0.1493, -0.2418, -0.1126, -0.1879, -0.1624,\n                        -0.1440, -0.1394, -0.1323, -0.1415, -0.1904, -0.1721, -0.1533, -0.1135,\n                        -0.1087, -0.1292, -0.1742, -0.1900, -0.1163, -0.1354, -0.1590, -0.1312,\n                        -0.1565, -0.1211, -0.1730, -0.2150, -0.1498, -0.1570, -0.1636, -0.1210,\n                        -0.1593, -0.1817, -0.1503, -0.1223, -0.1392, -0.1759, -0.1730, -0.2607,\n                        -0.1462, -0.2474, -0.1252, -0.2434, -0.1499, -0.1385, -0.2362, -0.1562,\n                        -0.2477, -0.2265, -0.2277, -0.1968, -0.1654, -0.1325, -0.1650, -0.1464,\n                        -0.1729, -0.1306, -0.1689, -0.1859, -0.2188, -0.1265, -0.2389, -0.1631,\n                        -0.1438, -0.1564, -0.2242, -0.1852, -0.2005, -0.1363, -0.1199, -0.1105,\n                        -0.1275, -0.1780, -0.1425, -0.1746, -0.1408, -0.1377, -0.1529, -0.1233,\n                        -0.1136, -0.1003, -0.1348, -0.1772, -0.1555, -0.3909, -0.1903, -0.1748,\n                        -0.2189, -0.1040, -0.1967, -0.1488, -0.1931, -0.1158, -0.1883, -0.1544,\n                        -0.1300, -0.1382, -0.1759, -0.0956, -0.1218, -0.1536, -0.1479, -0.1707,\n                        -0.1323, -0.1728, -0.1374, -0.1926, -0.1217, -0.1537, -0.2439, -0.2566,\n                        -0.2063, -0.2611, -0.1327, -0.1275, -0.1862, -0.1030, -0.1825, -0.1081,\n                        -0.1252, -0.1267, -0.1558, -0.1421, -0.1183, -0.1249, -0.1083, -0.1528,\n                        -0.1905, -0.1876, -0.1747, -0.2090, -0.2244, -0.2368, -0.1631, -0.1462,\n                        -0.2041, -0.1373, -0.2047, -0.1178, -0.1600, -0.1518, -0.1501, -0.1420,\n                        -0.1194, -0.1045, -0.1666, -0.2288, -0.1950, -0.1973, -0.1330, -0.1569,\n                        -0.1536, -0.1369, -0.1351, -0.0934, -0.1339, -0.1476, -0.1270, -0.1036,\n                        -0.1621, -0.1674, -0.1300, -0.2063, -0.2066, -0.1402, -0.1746, -0.1271,\n                        -0.1538, -0.1796, -0.1315, -0.1443, -0.1541, -0.1739, -0.1291, -0.1583,\n                        -0.1490, -0.0882, -0.1427, -0.1952, -0.1240, -0.2380, -0.1847, -0.1061,\n                        -0.1542, -0.2719, -0.1426, -0.1780, -0.1299, -0.1672, -0.1755, -0.1389,\n                        -0.1633, -0.0933, -0.1273, -0.1288, -0.1410, -0.2321, -0.1366, -0.1587,\n                        -0.1358, -0.2774, -0.2021, -0.1760, -0.1861, -0.1117, -0.1101, -0.1485,\n                        -0.1473, -0.1353, -0.3541, -0.1332, -0.1845, -0.2746, -0.1537, -0.1572,\n                        -0.1876, -0.2143, -0.2010, -0.1262, -0.1839, -0.2391, -0.1591, -0.1428,\n                        -0.1475, -0.1208, -0.1528, -0.1362, -0.1669, -0.1430, -0.1797, -0.1666,\n                        -0.1580, -0.1511, -0.2188, -0.1612, -0.1771, -0.2010, -0.1241, -0.1878,\n                        -0.1999, -0.2135, -0.1588, -0.1230, -0.1601, -0.1762, -0.1451, -0.1682,\n                        -0.1366, -0.1242, -0.0866, -0.1883, -0.2280, -0.1603, -0.1566, -0.1663,\n                        -0.2090, -0.1083, -0.1547, -0.1377, -0.1508, -0.1065, -0.2346, -0.1522,\n                        -0.1333, -0.1112, -0.1185, -0.1396, -0.2745, -0.1966, -0.1159, -0.1569,\n                        -0.1339, -0.2107, -0.2720, -0.1956, -0.2008, -0.2958, -0.1488, -0.1349,\n                        -0.1261, -0.1426, -0.1698, -0.0991, -0.2105, -0.3357, -0.2069, -0.1652,\n                        -0.2040, -0.1167, -0.1416, -0.1364, -0.1303, -0.1148, -0.1158, -0.1393,\n                        -0.1038, -0.1892, -0.1245, -0.1397, -0.1657, -0.2135, -0.1177, -0.1539,\n                        -0.1579, -0.2371, -0.2345, -0.1368, -0.1462, -0.1369, -0.2436, -0.1363,\n                        -0.1428, -0.2501, -0.1313, -0.2457, -0.1087, -0.1433, -0.1411, -0.1320,\n                        -0.1164, -0.2153, -0.1344, -0.1632, -0.1570, -0.1109, -0.1837, -0.1844,\n                        -0.1820, -0.3232, -0.1478, -0.2380, -0.1394, -0.1389, -0.1540, -0.1439,\n                        -0.2246, -0.1351, -0.2230, -0.2110, -0.1451, -0.1198, -0.1245, -0.2637,\n                        -0.1311, -0.1410, -0.1055, -0.0965, -0.1065, -0.2759, -0.1238, -0.1494,\n                        -0.1256, -0.1633, -0.2108, -0.1640, -0.1900, -0.2357, -0.1424, -0.1232,\n                        -0.1203, -0.1054, -0.1297, -0.1843, -0.1822, -0.1566, -0.1136, -0.1290,\n                        -0.1579, -0.1967, -0.1246, -0.1704, -0.2129, -0.1909, -0.1689, -0.2721,\n                        -0.1612, -0.1006, -0.1244, -0.1043, -0.1328, -0.2191, -0.1448, -0.1682,\n                        -0.1545, -0.1548, -0.1922, -0.1506, -0.2190, -0.1667, -0.1237, -0.1486,\n                        -0.1752, -0.2849, -0.1490, -0.1698, -0.2178, -0.1378, -0.1176, -0.1498,\n                        -0.1935, -0.1284, -0.1712, -0.1156, -0.1397, -0.1393, -0.1410, -0.1375,\n                        -0.1674, -0.1824, -0.1744, -0.1558, -0.1436, -0.2018, -0.1859, -0.1417,\n                        -0.1571, -0.1464, -0.1538, -0.1181, -0.1091, -0.1219, -0.1578, -0.2312,\n                        -0.1744, -0.1586, -0.1724, -0.1674, -0.2352, -0.2815, -0.1127, -0.1854,\n                        -0.1068, -0.2185, -0.1671, -0.1905, -0.1857, -0.2240, -0.1989, -0.1296]), max_val=tensor([0.1488, 0.1631, 0.2624, 0.1677, 0.1652, 0.1544, 0.1351, 0.1615, 0.1620,\n                        0.1997, 0.1906, 0.1589, 0.1994, 0.2495, 0.2724, 0.3093, 0.2029, 0.2917,\n                        0.2328, 0.1487, 0.1600, 0.1210, 0.1514, 0.2112, 0.1553, 0.1874, 0.1161,\n                        0.1707, 0.1847, 0.2369, 0.2864, 0.1723, 0.1321, 0.1998, 0.1699, 0.2253,\n                        0.1852, 0.1745, 0.1644, 0.2130, 0.2145, 0.1602, 0.1846, 0.2049, 0.1547,\n                        0.2518, 0.1266, 0.2099, 0.1435, 0.1626, 0.1245, 0.1394, 0.1412, 0.1878,\n                        0.2627, 0.1773, 0.1596, 0.1551, 0.1719, 0.2061, 0.1480, 0.2605, 0.2863,\n                        0.1680, 0.1492, 0.1837, 0.1679, 0.1540, 0.1380, 0.1843, 0.2938, 0.2465,\n                        0.1755, 0.1278, 0.2299, 0.1947, 0.2200, 0.1939, 0.1364, 0.1928, 0.1690,\n                        0.1585, 0.1693, 0.0938, 0.1584, 0.2529, 0.2153, 0.2049, 0.2208, 0.2407,\n                        0.1508, 0.2008, 0.1679, 0.1594, 0.1786, 0.2215, 0.1473, 0.2275, 0.2186,\n                        0.1508, 0.2190, 0.1876, 0.1369, 0.1962, 0.1491, 0.1672, 0.1869, 0.2091,\n                        0.1376, 0.1497, 0.1527, 0.2154, 0.1536, 0.2621, 0.1622, 0.2208, 0.1596,\n                        0.2201, 0.2486, 0.1338, 0.2410, 0.2654, 0.1589, 0.2443, 0.2029, 0.2383,\n                        0.1801, 0.2005, 0.1451, 0.2106, 0.1542, 0.1642, 0.2636, 0.1271, 0.2572,\n                        0.1653, 0.1781, 0.1594, 0.1413, 0.1952, 0.1483, 0.1716, 0.1766, 0.1816,\n                        0.2980, 0.1633, 0.3591, 0.1788, 0.1611, 0.2755, 0.1177, 0.2018, 0.1887,\n                        0.2035, 0.1666, 0.1410, 0.1708, 0.1674, 0.2346, 0.2091, 0.1496, 0.1418,\n                        0.2432, 0.1374, 0.1583, 0.1570, 0.1443, 0.3433, 0.1545, 0.1221, 0.1164,\n                        0.1961, 0.2277, 0.2032, 0.2496, 0.1462, 0.1981, 0.1585, 0.3128, 0.1692,\n                        0.1764, 0.2381, 0.1642, 0.2160, 0.1499, 0.1815, 0.1679, 0.2688, 0.1590,\n                        0.1585, 0.1450, 0.1471, 0.1208, 0.1889, 0.1274, 0.1898, 0.1403, 0.1892,\n                        0.1936, 0.1414, 0.2092, 0.2466, 0.1351, 0.2087, 0.1925, 0.1382, 0.1558,\n                        0.1684, 0.1918, 0.1249, 0.2163, 0.1785, 0.1785, 0.1343, 0.1992, 0.1917,\n                        0.2089, 0.2133, 0.1841, 0.1650, 0.1468, 0.1730, 0.1550, 0.1638, 0.1849,\n                        0.1495, 0.1904, 0.2198, 0.2113, 0.1238, 0.1816, 0.1787, 0.2603, 0.1978,\n                        0.1214, 0.1304, 0.1190, 0.2086, 0.1774, 0.2149, 0.1624, 0.1596, 0.2557,\n                        0.1880, 0.1472, 0.1185, 0.1723, 0.1358, 0.2221, 0.1601, 0.1645, 0.1903,\n                        0.2163, 0.1145, 0.3122, 0.1360, 0.1995, 0.1475, 0.1298, 0.2947, 0.2980,\n                        0.1405, 0.2269, 0.2083, 0.1549, 0.1390, 0.3483, 0.1875, 0.2251, 0.1504,\n                        0.1375, 0.2158, 0.1597, 0.2109, 0.1734, 0.2252, 0.2771, 0.1635, 0.1908,\n                        0.1880, 0.2172, 0.1400, 0.1865, 0.1500, 0.2027, 0.1966, 0.1246, 0.1676,\n                        0.1405, 0.1866, 0.1777, 0.2730, 0.1390, 0.1022, 0.2077, 0.1349, 0.1602,\n                        0.2516, 0.1518, 0.2159, 0.4158, 0.1292, 0.1330, 0.1683, 0.2119, 0.1266,\n                        0.1760, 0.1721, 0.2108, 0.1913, 0.1618, 0.1983, 0.1568, 0.1330, 0.1384,\n                        0.1970, 0.1415, 0.1941, 0.1667, 0.1513, 0.1617, 0.1930, 0.1982, 0.1854,\n                        0.1736, 0.1676, 0.1866, 0.1581, 0.1585, 0.1685, 0.2341, 0.1836, 0.1388,\n                        0.1407, 0.2956, 0.2632, 0.2275, 0.1473, 0.1352, 0.1607, 0.1339, 0.1803,\n                        0.3808, 0.2364, 0.1558, 0.1036, 0.2124, 0.2488, 0.1423, 0.1449, 0.1634,\n                        0.1664, 0.1932, 0.1256, 0.1440, 0.1658, 0.1665, 0.1526, 0.1593, 0.1595,\n                        0.1388, 0.2107, 0.2281, 0.1815, 0.3610, 0.2492, 0.2049, 0.1877, 0.2330,\n                        0.2271, 0.2136, 0.2109, 0.1855, 0.1541, 0.1464, 0.1494, 0.1979, 0.1384,\n                        0.2729, 0.2053, 0.1512, 0.1568, 0.1804, 0.1705, 0.1884, 0.1552, 0.1667,\n                        0.2334, 0.1815, 0.1314, 0.1811, 0.1372, 0.1412, 0.1958, 0.2770, 0.2646,\n                        0.2035, 0.2551, 0.2226, 0.1675, 0.1196, 0.1907, 0.1585, 0.1686, 0.1810,\n                        0.1523, 0.1447, 0.1520, 0.2325, 0.2638, 0.1488, 0.2398, 0.1744, 0.1816,\n                        0.2057, 0.1766, 0.2265, 0.1963, 0.1672, 0.1737, 0.2031, 0.1286, 0.2828,\n                        0.1321, 0.1432, 0.1302, 0.2441, 0.1245, 0.1546, 0.2393, 0.1669, 0.1572,\n                        0.1201, 0.1865, 0.1616, 0.2880, 0.1612, 0.2290, 0.1601, 0.1249, 0.1685,\n                        0.1348, 0.1705, 0.2559, 0.2123, 0.2729, 0.1424, 0.1300, 0.1453, 0.1733,\n                        0.1412, 0.1635, 0.1681, 0.2507, 0.1865, 0.2983, 0.1479, 0.1637, 0.1773,\n                        0.1587, 0.1355, 0.1080, 0.1691, 0.1580, 0.2035, 0.2670, 0.2301, 0.1466,\n                        0.1363, 0.1768, 0.1466, 0.1436, 0.1158, 0.2191, 0.1767, 0.1636, 0.1612,\n                        0.2396, 0.1367, 0.1611, 0.1429, 0.1613, 0.1905, 0.1553, 0.1777, 0.1985,\n                        0.1806, 0.2420, 0.2072, 0.1455, 0.1386, 0.2688, 0.1856, 0.1937, 0.2237,\n                        0.2199, 0.2160, 0.2019, 0.1901, 0.3335, 0.2348, 0.2154, 0.2053, 0.1741,\n                        0.2007, 0.2148, 0.1346, 0.1767, 0.2178, 0.2857, 0.1783, 0.1692, 0.1827,\n                        0.1597, 0.1920, 0.2218, 0.1300, 0.1364, 0.2114, 0.1856, 0.1983, 0.1396,\n                        0.1569, 0.1465, 0.2066, 0.3378, 0.2139, 0.1979, 0.1733, 0.2775, 0.2960,\n                        0.1527, 0.1422, 0.2880, 0.1524, 0.2220, 0.3178, 0.1529, 0.1731, 0.1231,\n                        0.1231, 0.2074, 0.1549, 0.2617, 0.1754, 0.1602, 0.2332, 0.1881, 0.2113,\n                        0.2423, 0.2323, 0.2094, 0.1244, 0.2566, 0.1874, 0.1284, 0.3342, 0.1166,\n                        0.2140, 0.1890, 0.1785, 0.1458, 0.2089, 0.1450, 0.1643, 0.1524, 0.1749,\n                        0.2781, 0.2079, 0.2090, 0.1407, 0.1694, 0.2571, 0.1849, 0.1680, 0.1471,\n                        0.1739, 0.2267, 0.1300, 0.1845, 0.2475, 0.1834, 0.1756, 0.1353, 0.2048,\n                        0.1859, 0.1514, 0.2326, 0.1850, 0.2342, 0.1794, 0.2000, 0.1764, 0.1332,\n                        0.1718, 0.2075, 0.1486, 0.1394, 0.1788, 0.1584, 0.1388, 0.1352, 0.2252,\n                        0.1920, 0.2529, 0.1381, 0.2788, 0.1674, 0.1611, 0.1596, 0.1440, 0.1639,\n                        0.2027, 0.1733, 0.2451, 0.1863, 0.2466, 0.1694, 0.1484, 0.1242, 0.1599,\n                        0.2565, 0.1273, 0.1719, 0.2095, 0.2358, 0.1915, 0.1947, 0.1453, 0.2043,\n                        0.1509, 0.2101, 0.1762, 0.1818, 0.1739, 0.1511, 0.1517, 0.1567, 0.1601,\n                        0.1407, 0.1394, 0.1394, 0.2134, 0.2661, 0.2129, 0.1635, 0.1430, 0.1419,\n                        0.1910, 0.1672, 0.1499, 0.1949, 0.2212, 0.1761, 0.2179, 0.1646, 0.1349,\n                        0.1108, 0.1867, 0.2998, 0.1844, 0.1866, 0.2219, 0.1799, 0.1750, 0.1434,\n                        0.2347, 0.1788, 0.1549, 0.1617, 0.1991, 0.2094, 0.1472, 0.1844, 0.1652,\n                        0.1385, 0.1318, 0.2722, 0.1698, 0.2475, 0.3462, 0.2309, 0.2121, 0.1674,\n                        0.3068, 0.1511, 0.1940, 0.1775, 0.1948, 0.1531, 0.2799, 0.1899, 0.2153,\n                        0.2108, 0.1775, 0.1721, 0.1700, 0.1748, 0.1881, 0.2346, 0.1987, 0.1631,\n                        0.1670, 0.2041, 0.1241, 0.1215, 0.1924, 0.1691, 0.1914, 0.2390, 0.2374,\n                        0.2820, 0.1690, 0.1631, 0.1435, 0.1337, 0.1677, 0.1946, 0.1948, 0.1620,\n                        0.1640, 0.1942, 0.1348, 0.1572, 0.2036, 0.1937, 0.3126, 0.2174, 0.1696,\n                        0.1373, 0.3467, 0.1973, 0.2143, 0.1504, 0.1425, 0.1138, 0.2598, 0.1761,\n                        0.1906, 0.2319, 0.1403, 0.2055, 0.2930, 0.2079, 0.1942, 0.1652, 0.1338,\n                        0.1672, 0.1476, 0.2317, 0.1814, 0.1641, 0.2278, 0.1101, 0.1802, 0.2531,\n                        0.1427, 0.2903, 0.1826, 0.1364, 0.1785, 0.1611, 0.1432, 0.2556, 0.1809,\n                        0.2061, 0.1415, 0.3622])\n              )\n            )\n          )\n          (4): GELU(approximate='none')\n          (5): Linear(\n            in_features=768, out_features=192, bias=True\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0018, 0.0027, 0.0017, 0.0024, 0.0018, 0.0014, 0.0022, 0.0019, 0.0024,\n                      0.0017, 0.0018, 0.0014, 0.0032, 0.0021, 0.0015, 0.0014, 0.0016, 0.0016,\n                      0.0023, 0.0009, 0.0022, 0.0021, 0.0030, 0.0016, 0.0016, 0.0019, 0.0021,\n                      0.0015, 0.0015, 0.0017, 0.0015, 0.0018, 0.0016, 0.0019, 0.0015, 0.0021,\n                      0.0017, 0.0017, 0.0030, 0.0022, 0.0022, 0.0016, 0.0020, 0.0013, 0.0016,\n                      0.0022, 0.0020, 0.0016, 0.0018, 0.0019, 0.0019, 0.0014, 0.0015, 0.0019,\n                      0.0014, 0.0032, 0.0008, 0.0019, 0.0023, 0.0016, 0.0022, 0.0019, 0.0016,\n                      0.0018, 0.0024, 0.0018, 0.0012, 0.0016, 0.0019, 0.0019, 0.0032, 0.0019,\n                      0.0022, 0.0016, 0.0019, 0.0018, 0.0020, 0.0014, 0.0020, 0.0012, 0.0020,\n                      0.0017, 0.0020, 0.0016, 0.0014, 0.0018, 0.0017, 0.0025, 0.0020, 0.0034,\n                      0.0018, 0.0019, 0.0021, 0.0016, 0.0032, 0.0017, 0.0018, 0.0037, 0.0017,\n                      0.0016, 0.0018, 0.0032, 0.0018, 0.0015, 0.0018, 0.0025, 0.0019, 0.0046,\n                      0.0017, 0.0018, 0.0015, 0.0021, 0.0018, 0.0017, 0.0015, 0.0017, 0.0016,\n                      0.0014, 0.0018, 0.0015, 0.0016, 0.0018, 0.0024, 0.0021, 0.0017, 0.0016,\n                      0.0019, 0.0040, 0.0019, 0.0016, 0.0025, 0.0028, 0.0018, 0.0015, 0.0035,\n                      0.0017, 0.0021, 0.0022, 0.0032, 0.0017, 0.0017, 0.0022, 0.0013, 0.0019,\n                      0.0018, 0.0036, 0.0018, 0.0021, 0.0021, 0.0014, 0.0016, 0.0022, 0.0017,\n                      0.0020, 0.0016, 0.0016, 0.0015, 0.0033, 0.0015, 0.0014, 0.0022, 0.0014,\n                      0.0018, 0.0014, 0.0016, 0.0014, 0.0017, 0.0016, 0.0018, 0.0015, 0.0016,\n                      0.0019, 0.0016, 0.0039, 0.0017, 0.0016, 0.0014, 0.0015, 0.0020, 0.0016,\n                      0.0017, 0.0016, 0.0015, 0.0020, 0.0042, 0.0017, 0.0015, 0.0016, 0.0015,\n                      0.0017, 0.0034, 0.0013]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                     dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n                min_val=tensor([-0.2257, -0.3456, -0.1952, -0.3025, -0.2246, -0.1847, -0.2799, -0.1731,\n                        -0.3079, -0.2209, -0.1940, -0.1837, -0.4142, -0.2691, -0.1880, -0.1811,\n                        -0.1726, -0.1809, -0.2762, -0.1136, -0.1993, -0.2150, -0.3849, -0.2100,\n                        -0.2048, -0.2470, -0.2731, -0.1742, -0.1772, -0.1897, -0.1734, -0.1735,\n                        -0.1936, -0.2000, -0.1964, -0.2092, -0.2208, -0.2139, -0.1882, -0.2172,\n                        -0.2397, -0.1829, -0.2561, -0.1671, -0.2049, -0.1961, -0.2093, -0.1777,\n                        -0.2284, -0.2076, -0.1553, -0.1850, -0.1923, -0.2479, -0.1601, -0.4154,\n                        -0.0996, -0.1539, -0.2925, -0.2068, -0.2660, -0.1728, -0.2009, -0.2317,\n                        -0.1840, -0.1907, -0.1564, -0.1712, -0.2211, -0.1907, -0.2715, -0.2380,\n                        -0.2835, -0.1841, -0.1654, -0.2322, -0.2542, -0.1705, -0.1888, -0.1579,\n                        -0.2572, -0.2167, -0.2505, -0.1597, -0.1849, -0.2317, -0.2165, -0.3139,\n                        -0.2427, -0.3782, -0.2249, -0.2409, -0.2746, -0.2013, -0.4064, -0.2186,\n                        -0.1797, -0.4788, -0.2232, -0.1978, -0.2246, -0.4081, -0.2281, -0.1976,\n                        -0.2065, -0.3060, -0.2035, -0.2319, -0.2138, -0.2295, -0.1914, -0.2658,\n                        -0.2325, -0.2178, -0.1505, -0.2203, -0.1834, -0.1729, -0.2345, -0.1968,\n                        -0.2087, -0.2335, -0.2978, -0.2640, -0.2208, -0.2024, -0.2381, -0.1952,\n                        -0.1855, -0.1812, -0.3141, -0.3617, -0.1901, -0.1615, -0.2247, -0.1929,\n                        -0.2260, -0.2330, -0.2652, -0.2160, -0.1585, -0.1736, -0.1701, -0.1897,\n                        -0.2184, -0.4558, -0.1415, -0.1617, -0.2204, -0.1687, -0.1610, -0.2831,\n                        -0.1807, -0.2612, -0.2098, -0.1772, -0.1908, -0.4228, -0.1959, -0.1709,\n                        -0.2199, -0.1763, -0.2259, -0.1785, -0.2111, -0.1730, -0.2166, -0.2031,\n                        -0.2262, -0.1646, -0.2084, -0.2412, -0.1918, -0.5024, -0.2135, -0.2043,\n                        -0.1778, -0.1886, -0.2551, -0.2024, -0.1935, -0.2008, -0.1869, -0.1776,\n                        -0.5315, -0.1882, -0.1972, -0.1883, -0.1904, -0.2147, -0.2696, -0.1559]), max_val=tensor([0.1752, 0.2530, 0.2115, 0.2783, 0.1876, 0.1654, 0.2113, 0.2371, 0.1957,\n                        0.1792, 0.2258, 0.1828, 0.2382, 0.1824, 0.1659, 0.1723, 0.2049, 0.1998,\n                        0.2885, 0.0696, 0.2786, 0.2663, 0.2124, 0.1759, 0.1925, 0.2407, 0.1880,\n                        0.1868, 0.1853, 0.2199, 0.1890, 0.2292, 0.2019, 0.2384, 0.1890, 0.2614,\n                        0.1823, 0.2127, 0.3852, 0.2809, 0.2815, 0.2001, 0.1997, 0.1577, 0.1994,\n                        0.2751, 0.2501, 0.2062, 0.2258, 0.2362, 0.2465, 0.1841, 0.1705, 0.2279,\n                        0.1748, 0.3823, 0.0658, 0.2354, 0.2354, 0.1527, 0.2792, 0.2381, 0.1940,\n                        0.1731, 0.2999, 0.2242, 0.1536, 0.2028, 0.2363, 0.2468, 0.4036, 0.1971,\n                        0.2374, 0.2045, 0.2450, 0.1777, 0.2144, 0.1818, 0.2603, 0.1529, 0.1380,\n                        0.1629, 0.2524, 0.2016, 0.1752, 0.2258, 0.2112, 0.1792, 0.2602, 0.4332,\n                        0.1705, 0.1663, 0.2247, 0.1877, 0.3096, 0.1982, 0.2248, 0.4413, 0.2147,\n                        0.2093, 0.2086, 0.4003, 0.2153, 0.1902, 0.2228, 0.3167, 0.2375, 0.5858,\n                        0.1727, 0.1656, 0.1809, 0.2613, 0.2071, 0.1962, 0.1908, 0.1799, 0.1995,\n                        0.1836, 0.2080, 0.1909, 0.1536, 0.1647, 0.3083, 0.2164, 0.1920, 0.1708,\n                        0.2141, 0.5050, 0.2417, 0.2000, 0.2884, 0.2400, 0.2256, 0.1927, 0.4415,\n                        0.2167, 0.2704, 0.2811, 0.4077, 0.1990, 0.2114, 0.2784, 0.1317, 0.2444,\n                        0.2329, 0.2545, 0.2260, 0.2608, 0.2684, 0.1792, 0.2010, 0.2411, 0.2182,\n                        0.2045, 0.1880, 0.1979, 0.1661, 0.2756, 0.1822, 0.1788, 0.2746, 0.1758,\n                        0.1874, 0.1829, 0.1927, 0.1783, 0.1883, 0.1973, 0.1751, 0.1842, 0.2003,\n                        0.1607, 0.1980, 0.2715, 0.1833, 0.1536, 0.1429, 0.1915, 0.2314, 0.2048,\n                        0.2205, 0.1937, 0.1950, 0.2494, 0.4304, 0.2186, 0.1835, 0.1999, 0.1690,\n                        0.1942, 0.4348, 0.1600])\n              )\n            )\n          )\n        )\n      )\n    )\n    (4): Module(\n      (0): Module()\n      (1): Conv2d(\n        192, 384, kernel_size=(2, 2), stride=(2, 2)\n        (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0013, 0.0023, 0.0012, 0.0017, 0.0011, 0.0011, 0.0020, 0.0012, 0.0014,\n                  0.0018, 0.0017, 0.0019, 0.0011, 0.0009, 0.0015, 0.0013, 0.0013, 0.0013,\n                  0.0023, 0.0013, 0.0011, 0.0013, 0.0011, 0.0012, 0.0019, 0.0013, 0.0018,\n                  0.0016, 0.0011, 0.0014, 0.0018, 0.0014, 0.0014, 0.0010, 0.0010, 0.0019,\n                  0.0016, 0.0015, 0.0010, 0.0011, 0.0014, 0.0015, 0.0018, 0.0011, 0.0016,\n                  0.0012, 0.0016, 0.0012, 0.0015, 0.0011, 0.0014, 0.0018, 0.0014, 0.0008,\n                  0.0013, 0.0020, 0.0009, 0.0013, 0.0015, 0.0014, 0.0015, 0.0007, 0.0013,\n                  0.0011, 0.0016, 0.0011, 0.0016, 0.0017, 0.0014, 0.0017, 0.0014, 0.0015,\n                  0.0010, 0.0017, 0.0011, 0.0012, 0.0016, 0.0013, 0.0010, 0.0008, 0.0012,\n                  0.0015, 0.0011, 0.0019, 0.0008, 0.0024, 0.0013, 0.0010, 0.0017, 0.0020,\n                  0.0013, 0.0012, 0.0012, 0.0014, 0.0010, 0.0013, 0.0018, 0.0009, 0.0013,\n                  0.0007, 0.0012, 0.0015, 0.0014, 0.0015, 0.0019, 0.0014, 0.0019, 0.0010,\n                  0.0016, 0.0014, 0.0010, 0.0010, 0.0014, 0.0016, 0.0011, 0.0008, 0.0011,\n                  0.0012, 0.0018, 0.0011, 0.0014, 0.0014, 0.0015, 0.0011, 0.0017, 0.0010,\n                  0.0028, 0.0010, 0.0013, 0.0028, 0.0016, 0.0014, 0.0011, 0.0019, 0.0012,\n                  0.0014, 0.0015, 0.0018, 0.0015, 0.0018, 0.0011, 0.0012, 0.0033, 0.0016,\n                  0.0020, 0.0014, 0.0009, 0.0012, 0.0012, 0.0014, 0.0007, 0.0011, 0.0015,\n                  0.0009, 0.0010, 0.0013, 0.0013, 0.0011, 0.0012, 0.0013, 0.0015, 0.0014,\n                  0.0013, 0.0017, 0.0012, 0.0011, 0.0013, 0.0014, 0.0010, 0.0016, 0.0014,\n                  0.0012, 0.0011, 0.0009, 0.0010, 0.0016, 0.0018, 0.0025, 0.0008, 0.0019,\n                  0.0020, 0.0010, 0.0008, 0.0012, 0.0017, 0.0012, 0.0016, 0.0010, 0.0015,\n                  0.0012, 0.0015, 0.0017, 0.0013, 0.0021, 0.0014, 0.0015, 0.0014, 0.0022,\n                  0.0009, 0.0012, 0.0016, 0.0016, 0.0017, 0.0016, 0.0016, 0.0018, 0.0016,\n                  0.0013, 0.0014, 0.0013, 0.0014, 0.0011, 0.0014, 0.0014, 0.0018, 0.0013,\n                  0.0038, 0.0017, 0.0011, 0.0017, 0.0018, 0.0024, 0.0016, 0.0018, 0.0011,\n                  0.0018, 0.0010, 0.0015, 0.0013, 0.0007, 0.0015, 0.0013, 0.0012, 0.0013,\n                  0.0010, 0.0014, 0.0014, 0.0012, 0.0007, 0.0017, 0.0029, 0.0012, 0.0021,\n                  0.0011, 0.0010, 0.0012, 0.0013, 0.0017, 0.0015, 0.0013, 0.0021, 0.0018,\n                  0.0019, 0.0022, 0.0011, 0.0014, 0.0019, 0.0011, 0.0023, 0.0010, 0.0011,\n                  0.0009, 0.0015, 0.0011, 0.0006, 0.0017, 0.0013, 0.0012, 0.0022, 0.0012,\n                  0.0014, 0.0018, 0.0012, 0.0013, 0.0014, 0.0011, 0.0014, 0.0012, 0.0013,\n                  0.0011, 0.0019, 0.0016, 0.0012, 0.0018, 0.0015, 0.0013, 0.0021, 0.0009,\n                  0.0012, 0.0012, 0.0017, 0.0010, 0.0015, 0.0034, 0.0011, 0.0015, 0.0013,\n                  0.0020, 0.0012, 0.0012, 0.0012, 0.0014, 0.0021, 0.0014, 0.0015, 0.0017,\n                  0.0014, 0.0014, 0.0015, 0.0015, 0.0018, 0.0011, 0.0010, 0.0011, 0.0013,\n                  0.0018, 0.0012, 0.0017, 0.0019, 0.0018, 0.0026, 0.0012, 0.0013, 0.0018,\n                  0.0012, 0.0013, 0.0022, 0.0011, 0.0018, 0.0020, 0.0014, 0.0014, 0.0013,\n                  0.0010, 0.0016, 0.0014, 0.0014, 0.0018, 0.0024, 0.0012, 0.0020, 0.0013,\n                  0.0022, 0.0017, 0.0007, 0.0013, 0.0015, 0.0017, 0.0013, 0.0014, 0.0016,\n                  0.0012, 0.0010, 0.0006, 0.0011, 0.0024, 0.0012, 0.0010, 0.0012, 0.0011,\n                  0.0024, 0.0013, 0.0011, 0.0013, 0.0020, 0.0013, 0.0006, 0.0011, 0.0010,\n                  0.0015, 0.0014, 0.0012, 0.0010, 0.0014, 0.0010, 0.0020, 0.0010, 0.0017,\n                  0.0016, 0.0011, 0.0018, 0.0018, 0.0018, 0.0009]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                 dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n          (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n            min_val=tensor([-0.1310, -0.2988, -0.1206, -0.2136, -0.1413, -0.1133, -0.2593, -0.1585,\n                    -0.1798, -0.1295, -0.2120, -0.1163, -0.1079, -0.1007, -0.1962, -0.1718,\n                    -0.1613, -0.1301, -0.1754, -0.1710, -0.1357, -0.1652, -0.1307, -0.1401,\n                    -0.2398, -0.1251, -0.1485, -0.2017, -0.1332, -0.1764, -0.2257, -0.1471,\n                    -0.1144, -0.1297, -0.1190, -0.2372, -0.2090, -0.1875, -0.1061, -0.1399,\n                    -0.1808, -0.1732, -0.2311, -0.1365, -0.1369, -0.1496, -0.2072, -0.1391,\n                    -0.1426, -0.1443, -0.1828, -0.1725, -0.1767, -0.1010, -0.1574, -0.0912,\n                    -0.1178, -0.1394, -0.1879, -0.1106, -0.1890, -0.0845, -0.1679, -0.1329,\n                    -0.1447, -0.1409, -0.2005, -0.2183, -0.1740, -0.1274, -0.1746, -0.1493,\n                    -0.1226, -0.2170, -0.1041, -0.1473, -0.2062, -0.0948, -0.1339, -0.1059,\n                    -0.1414, -0.1943, -0.1125, -0.0776, -0.1073, -0.3054, -0.1602, -0.1165,\n                    -0.1899, -0.2565, -0.1321, -0.1365, -0.1508, -0.1829, -0.1201, -0.1159,\n                    -0.1719, -0.1174, -0.1295, -0.0936, -0.1487, -0.1879, -0.1768, -0.1968,\n                    -0.2482, -0.1354, -0.2417, -0.1242, -0.2029, -0.1831, -0.1123, -0.1315,\n                    -0.1439, -0.1892, -0.1389, -0.0998, -0.1187, -0.1296, -0.1868, -0.1390,\n                    -0.1779, -0.1558, -0.1517, -0.1382, -0.1405, -0.1295, -0.1439, -0.1247,\n                    -0.1687, -0.3624, -0.2013, -0.1742, -0.1213, -0.2431, -0.1573, -0.1778,\n                    -0.1905, -0.2267, -0.1707, -0.2270, -0.1248, -0.1596, -0.1636, -0.1260,\n                    -0.1884, -0.1538, -0.1060, -0.1266, -0.1230, -0.1344, -0.0887, -0.1458,\n                    -0.1889, -0.1051, -0.1256, -0.1691, -0.1070, -0.1459, -0.1421, -0.1563,\n                    -0.1920, -0.1734, -0.1532, -0.2204, -0.1417, -0.1376, -0.1692, -0.1764,\n                    -0.1226, -0.1673, -0.1807, -0.1535, -0.1308, -0.0789, -0.1295, -0.1886,\n                    -0.2251, -0.1325, -0.0811, -0.1081, -0.2570, -0.1259, -0.1029, -0.1477,\n                    -0.2173, -0.1482, -0.2061, -0.1075, -0.1918, -0.1531, -0.1600, -0.1573,\n                    -0.1638, -0.1893, -0.1524, -0.1965, -0.1650, -0.2760, -0.1181, -0.1381,\n                    -0.2035, -0.1544, -0.2141, -0.1403, -0.1930, -0.1080, -0.2025, -0.1548,\n                    -0.1823, -0.1631, -0.1805, -0.1267, -0.1606, -0.1737, -0.1701, -0.1689,\n                    -0.4850, -0.1979, -0.1472, -0.2120, -0.2332, -0.3091, -0.1610, -0.2009,\n                    -0.1279, -0.1342, -0.1337, -0.1551, -0.1727, -0.0843, -0.1442, -0.1692,\n                    -0.1516, -0.1622, -0.1262, -0.1768, -0.1790, -0.1516, -0.0817, -0.2123,\n                    -0.2232, -0.1278, -0.2649, -0.1226, -0.1108, -0.1514, -0.1255, -0.2194,\n                    -0.1595, -0.1707, -0.1762, -0.2272, -0.1655, -0.2771, -0.1074, -0.1470,\n                    -0.1324, -0.1410, -0.2945, -0.0982, -0.1451, -0.1013, -0.1857, -0.1333,\n                    -0.0826, -0.2164, -0.1674, -0.1286, -0.1084, -0.1587, -0.1852, -0.2283,\n                    -0.1365, -0.1716, -0.1567, -0.1264, -0.1727, -0.1548, -0.1381, -0.1461,\n                    -0.2386, -0.2007, -0.1506, -0.2291, -0.1892, -0.1662, -0.1971, -0.1205,\n                    -0.1540, -0.1565, -0.2146, -0.0800, -0.1672, -0.1187, -0.1442, -0.1904,\n                    -0.1711, -0.2401, -0.1585, -0.1466, -0.1527, -0.1405, -0.1797, -0.1748,\n                    -0.1950, -0.2124, -0.1762, -0.1803, -0.1922, -0.1902, -0.2344, -0.1063,\n                    -0.1229, -0.1221, -0.1392, -0.2261, -0.1567, -0.1829, -0.1378, -0.1550,\n                    -0.1524, -0.1351, -0.1431, -0.1250, -0.1515, -0.1637, -0.1533, -0.1405,\n                    -0.2338, -0.2027, -0.1356, -0.1819, -0.1654, -0.1227, -0.1677, -0.1816,\n                    -0.1216, -0.2260, -0.1510, -0.1163, -0.2513, -0.1614, -0.2753, -0.2197,\n                    -0.0927, -0.1384, -0.1978, -0.1878, -0.1023, -0.1355, -0.1823, -0.1416,\n                    -0.1227, -0.0830, -0.1465, -0.1454, -0.1532, -0.1254, -0.0929, -0.1357,\n                    -0.3044, -0.1723, -0.1411, -0.1377, -0.2536, -0.1680, -0.0760, -0.1397,\n                    -0.1139, -0.1069, -0.1836, -0.1199, -0.1191, -0.1533, -0.1088, -0.1560,\n                    -0.1286, -0.2227, -0.1078, -0.0923, -0.2329, -0.1977, -0.2338, -0.1099]), max_val=tensor([0.1705, 0.1242, 0.1569, 0.1628, 0.1444, 0.1436, 0.1342, 0.1429, 0.1513,\n                    0.2310, 0.1534, 0.2401, 0.1398, 0.1174, 0.1379, 0.1377, 0.1467, 0.1614,\n                    0.2984, 0.1307, 0.1160, 0.1508, 0.1378, 0.1488, 0.1929, 0.1672, 0.2274,\n                    0.1853, 0.1432, 0.1392, 0.1367, 0.1770, 0.1765, 0.0927, 0.1258, 0.1195,\n                    0.1579, 0.1901, 0.1304, 0.1313, 0.1513, 0.1895, 0.1616, 0.1239, 0.2012,\n                    0.1063, 0.1528, 0.1530, 0.1888, 0.1232, 0.1514, 0.2321, 0.1808, 0.0859,\n                    0.1642, 0.2509, 0.1030, 0.1699, 0.1857, 0.1794, 0.1693, 0.0844, 0.1674,\n                    0.1439, 0.2054, 0.1438, 0.1789, 0.1695, 0.1303, 0.2215, 0.1146, 0.1918,\n                    0.1093, 0.1935, 0.1407, 0.1311, 0.1989, 0.1689, 0.1314, 0.1069, 0.1541,\n                    0.1698, 0.1343, 0.2411, 0.0961, 0.2357, 0.1571, 0.1216, 0.2173, 0.1069,\n                    0.1661, 0.1490, 0.1547, 0.1646, 0.1257, 0.1703, 0.2228, 0.1013, 0.1678,\n                    0.0908, 0.1331, 0.1537, 0.1770, 0.1367, 0.1550, 0.1779, 0.1216, 0.0984,\n                    0.1203, 0.1518, 0.1270, 0.1186, 0.1807, 0.2021, 0.1272, 0.0892, 0.1341,\n                    0.1549, 0.2266, 0.1330, 0.1383, 0.1786, 0.1881, 0.1331, 0.2107, 0.1228,\n                    0.3552, 0.1018, 0.1632, 0.1333, 0.1781, 0.1440, 0.1405, 0.1026, 0.1347,\n                    0.1717, 0.1596, 0.1344, 0.1880, 0.1969, 0.1436, 0.1345, 0.4163, 0.2018,\n                    0.2572, 0.1757, 0.1117, 0.1577, 0.1566, 0.1723, 0.0778, 0.1458, 0.1334,\n                    0.1084, 0.1219, 0.1566, 0.1684, 0.1402, 0.1577, 0.1645, 0.1203, 0.1773,\n                    0.1642, 0.1320, 0.1490, 0.1123, 0.1542, 0.1584, 0.1088, 0.2022, 0.1143,\n                    0.0935, 0.1369, 0.1135, 0.1242, 0.2083, 0.1353, 0.3235, 0.1076, 0.2391,\n                    0.1769, 0.1204, 0.0867, 0.1200, 0.1971, 0.1279, 0.1922, 0.1213, 0.1885,\n                    0.1238, 0.1879, 0.2220, 0.1513, 0.2715, 0.1775, 0.1326, 0.1770, 0.1405,\n                    0.1196, 0.1558, 0.1743, 0.2080, 0.1404, 0.2095, 0.1991, 0.2265, 0.1585,\n                    0.1661, 0.1740, 0.1156, 0.1338, 0.1412, 0.1807, 0.1337, 0.2287, 0.1356,\n                    0.2087, 0.2114, 0.1155, 0.1703, 0.1477, 0.1751, 0.2011, 0.2325, 0.1345,\n                    0.2239, 0.1240, 0.1945, 0.1661, 0.0669, 0.1901, 0.1706, 0.1487, 0.1669,\n                    0.1192, 0.1580, 0.1698, 0.1523, 0.0850, 0.1678, 0.3743, 0.1519, 0.1895,\n                    0.1460, 0.1301, 0.1229, 0.1612, 0.1951, 0.1842, 0.1515, 0.2656, 0.1742,\n                    0.2376, 0.1306, 0.1433, 0.1747, 0.2422, 0.1236, 0.1679, 0.1301, 0.1219,\n                    0.1132, 0.1955, 0.1351, 0.0584, 0.1889, 0.1018, 0.1569, 0.2853, 0.1220,\n                    0.1676, 0.2080, 0.1475, 0.1350, 0.1744, 0.1369, 0.1728, 0.1273, 0.1590,\n                    0.1137, 0.1496, 0.1826, 0.1205, 0.2004, 0.1784, 0.1706, 0.2685, 0.1141,\n                    0.1108, 0.1475, 0.1571, 0.1329, 0.1854, 0.4291, 0.1233, 0.1576, 0.1355,\n                    0.2493, 0.1510, 0.1498, 0.1493, 0.1738, 0.2675, 0.1481, 0.1676, 0.1610,\n                    0.1696, 0.1312, 0.1582, 0.1618, 0.1044, 0.1335, 0.1205, 0.1407, 0.1666,\n                    0.1267, 0.1198, 0.2198, 0.2364, 0.2230, 0.3298, 0.1489, 0.1648, 0.2245,\n                    0.1338, 0.1678, 0.2842, 0.1124, 0.1206, 0.2584, 0.1819, 0.1287, 0.1244,\n                    0.0995, 0.1976, 0.1445, 0.1765, 0.1452, 0.3012, 0.1539, 0.2275, 0.1455,\n                    0.1808, 0.1307, 0.0794, 0.1622, 0.1629, 0.2205, 0.1638, 0.1760, 0.2052,\n                    0.1567, 0.1237, 0.0565, 0.1402, 0.3016, 0.1296, 0.1190, 0.1532, 0.1100,\n                    0.1901, 0.1579, 0.1000, 0.1650, 0.1296, 0.1286, 0.0816, 0.1273, 0.1331,\n                    0.1921, 0.1698, 0.1497, 0.1250, 0.1760, 0.1271, 0.2561, 0.1291, 0.2078,\n                    0.2025, 0.1356, 0.1198, 0.2328, 0.2025, 0.0910])\n          )\n        )\n      )\n    )\n    (5): Module(\n      (0): Module(\n        (block): Module(\n          (0): Conv2d(\n            384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.1955e-03, 1.9056e-03, 1.9990e-03, 1.4537e-03, 2.7619e-03, 2.4801e-03,\n                      9.9075e-04, 2.8098e-03, 2.4230e-03, 1.8673e-03, 8.4643e-04, 2.6903e-03,\n                      3.1350e-03, 2.6008e-03, 2.9463e-03, 2.2619e-03, 2.9372e-03, 3.0053e-03,\n                      2.4498e-03, 2.2193e-03, 3.3253e-03, 1.6853e-03, 3.1484e-03, 2.7498e-03,\n                      1.6370e-03, 4.5384e-04, 1.7707e-03, 1.7924e-03, 1.4217e-03, 2.4480e-03,\n                      1.8447e-03, 1.7480e-03, 2.2579e-03, 2.8447e-03, 2.8032e-03, 2.4022e-03,\n                      2.3186e-03, 2.7456e-03, 3.3361e-03, 2.8767e-03, 2.1501e-03, 1.4905e-03,\n                      2.7384e-03, 2.6535e-03, 2.5646e-03, 3.0971e-03, 2.5985e-03, 3.0428e-03,\n                      2.4123e-03, 2.1060e-03, 2.7142e-03, 2.7381e-03, 1.7591e-03, 3.5968e-03,\n                      2.5199e-03, 1.2293e-03, 3.2195e-03, 2.5089e-03, 1.4154e-03, 3.1234e-03,\n                      1.9802e-03, 2.3241e-03, 1.6443e-03, 1.5168e-03, 2.6483e-03, 1.2217e-03,\n                      1.9217e-03, 2.9376e-03, 3.0787e-03, 4.4185e-04, 2.9269e-03, 2.3704e-03,\n                      1.9351e-03, 1.3279e-03, 1.5498e-03, 3.1069e-03, 2.4953e-03, 8.2531e-04,\n                      2.6922e-03, 2.9220e-03, 2.7551e-03, 1.7003e-03, 1.6412e-03, 1.6015e-03,\n                      3.2453e-03, 2.9128e-03, 2.3217e-03, 3.2846e-03, 2.1182e-03, 2.8811e-03,\n                      1.7235e-03, 2.9635e-03, 1.7251e-03, 2.2676e-03, 3.1404e-03, 2.9512e-03,\n                      2.6500e-03, 2.8524e-03, 3.3479e-03, 2.0173e-03, 2.3907e-03, 2.7575e-03,\n                      3.2499e-03, 1.6566e-03, 1.6579e-03, 2.4920e-03, 5.2448e-04, 3.3428e-03,\n                      2.5030e-03, 3.5006e-03, 2.5115e-03, 3.1205e-03, 2.7863e-03, 2.3030e-03,\n                      1.6249e-03, 3.3068e-03, 2.7198e-03, 2.8281e-03, 8.8277e-04, 3.1729e-03,\n                      2.7148e-03, 2.4524e-03, 2.6865e-03, 2.3416e-03, 2.1789e-03, 2.1021e-03,\n                      1.4248e-03, 3.3095e-03, 2.7249e-03, 9.7244e-04, 2.6314e-03, 2.7448e-03,\n                      3.4302e-03, 2.6019e-03, 1.7982e-03, 3.1067e-03, 2.5188e-03, 1.3717e-03,\n                      2.5516e-03, 1.7917e-03, 2.7357e-03, 2.7114e-03, 2.3838e-03, 1.1049e-03,\n                      2.1284e-03, 2.4647e-03, 2.2790e-03, 2.6688e-03, 2.9316e-03, 2.7862e-03,\n                      7.6885e-04, 3.0021e-03, 2.9991e-03, 3.2088e-03, 3.0335e-03, 2.5645e-03,\n                      1.1913e-03, 3.0010e-03, 2.5194e-03, 1.7664e-03, 2.8347e-03, 2.5203e-03,\n                      2.0885e-03, 2.3604e-03, 3.2501e-03, 1.9975e-03, 2.9178e-03, 2.5370e-03,\n                      3.2556e-03, 2.9187e-03, 3.0352e-03, 1.8436e-03, 7.5919e-04, 1.6210e-03,\n                      2.8340e-03, 1.5767e-03, 1.9880e-03, 2.0685e-03, 3.1166e-03, 2.1237e-03,\n                      1.6398e-03, 2.8592e-03, 3.4581e-03, 1.9695e-03, 2.2384e-03, 2.3050e-03,\n                      1.5871e-03, 3.4517e-03, 2.2974e-03, 2.2861e-03, 1.1503e-03, 3.1235e-03,\n                      2.5771e-03, 1.6682e-03, 2.9483e-03, 2.6807e-03, 1.9459e-03, 1.4513e-03,\n                      2.7542e-03, 2.6581e-03, 1.6717e-03, 2.6961e-03, 2.5335e-03, 2.6888e-03,\n                      2.4756e-03, 2.8935e-03, 2.5597e-03, 2.1660e-03, 2.3086e-03, 1.6956e-03,\n                      1.8276e-03, 8.0138e-04, 1.9381e-03, 1.8030e-03, 1.9920e-03, 2.7976e-03,\n                      1.7775e-03, 2.3779e-03, 3.2628e-03, 3.1007e-03, 1.7824e-03, 1.7042e-03,\n                      2.9095e-03, 1.0971e-03, 2.5339e-03, 2.6881e-03, 2.6807e-03, 1.4848e-03,\n                      3.0415e-03, 2.2239e-04, 3.0143e-03, 2.1392e-03, 3.0491e-03, 2.8342e-03,\n                      2.8057e-03, 6.9226e-04, 2.1194e-03, 2.6987e-03, 1.5940e-03, 1.9047e-03,\n                      2.3200e-03, 3.2218e-03, 2.2803e-03, 2.2706e-03, 3.0741e-03, 2.4900e-03,\n                      2.6904e-03, 2.2278e-03, 1.8469e-03, 2.2822e-03, 2.0061e-03, 1.9857e-03,\n                      9.5260e-04, 1.5164e-03, 1.1954e-03, 2.8572e-03, 2.5963e-03, 2.9804e-03,\n                      2.6713e-03, 3.2496e-03, 3.0248e-03, 3.1799e-03, 2.9411e-03, 2.3771e-03,\n                      8.3329e-04, 2.3125e-03, 2.8755e-03, 6.1395e-04, 2.5373e-03, 2.2154e-03,\n                      2.2266e-03, 2.0238e-03, 2.6488e-03, 1.7607e-03, 4.6084e-04, 2.4759e-03,\n                      2.0013e-03, 1.7353e-03, 2.2190e-03, 3.0275e-03, 1.1326e-03, 2.7038e-03,\n                      2.9095e-03, 2.3957e-03, 1.4564e-03, 3.2498e-03, 2.0634e-03, 8.3675e-04,\n                      2.7408e-03, 2.6235e-03, 2.3017e-03, 3.1050e-03, 2.6223e-03, 1.6631e-03,\n                      2.8531e-03, 1.0888e-03, 2.5055e-03, 1.4363e-03, 6.6161e-04, 4.8650e-04,\n                      2.7167e-03, 2.3622e-03, 3.2559e-03, 2.8659e-03, 2.0258e-03, 2.3622e-03,\n                      1.7851e-03, 2.2326e-03, 2.3563e-03, 2.6447e-03, 3.0869e-03, 7.5273e-04,\n                      3.1775e-03, 3.1254e-03, 1.2258e-03, 2.9159e-04, 2.4457e-03, 1.3690e-03,\n                      1.5863e-03, 1.8815e-03, 2.6218e-03, 1.6548e-03, 2.5047e-03, 2.5148e-03,\n                      3.0213e-03, 2.9776e-03, 2.3470e-03, 2.0983e-03, 2.9106e-03, 2.6853e-03,\n                      2.8138e-03, 2.6124e-03, 3.4467e-03, 3.2428e-03, 4.3399e-03, 2.2316e-03,\n                      2.3017e-03, 1.8488e-03, 1.5033e-03, 2.9494e-03, 1.6289e-03, 2.8905e-03,\n                      1.5631e-03, 2.2413e-03, 1.5886e-03, 2.4248e-03, 3.0005e-03, 1.6580e-03,\n                      2.0987e-03, 2.6514e-03, 1.7350e-03, 1.8460e-03, 3.1309e-03, 1.0462e-03,\n                      2.6600e-03, 1.4274e-03, 2.3773e-03, 3.1013e-03, 1.4067e-03, 3.1894e-03,\n                      1.4717e-03, 2.4727e-03, 3.0855e-03, 2.9783e-03, 3.0166e-03, 2.8833e-03,\n                      9.0854e-05, 1.7256e-03, 2.4302e-03, 2.6639e-03, 1.8492e-03, 2.4373e-03,\n                      3.2527e-03, 2.3544e-03, 3.1812e-03, 2.2930e-03, 2.0171e-03, 2.7964e-03,\n                      2.3618e-03, 2.9111e-03, 1.7227e-03, 9.6254e-04, 3.0271e-03, 1.7765e-03]), zero_point=tensor([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0, -128,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,  127,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n                     dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n                min_val=tensor([-0.0037, -0.0665, -0.2559, -0.1668, -0.1114, -0.0651, -0.1268, -0.3596,\n                        -0.3101, -0.2390, -0.1083, -0.1488, -0.0812, -0.3329, -0.3771, -0.0283,\n                        -0.1363, -0.0790, -0.1871, -0.1818, -0.4256, -0.2157, -0.4030, -0.1371,\n                        -0.0485, -0.0055, -0.2080, -0.2022, -0.1820, -0.2895, -0.2361, -0.1593,\n                        -0.2890, -0.3641, -0.0251, -0.3075, -0.1269, -0.1261, -0.4270, -0.1575,\n                        -0.2752, -0.1908, -0.1652, -0.3397, -0.3283, -0.3964, -0.0170, -0.0698,\n                        -0.3088, -0.2110, -0.1968, -0.0293, -0.1363, -0.1420, -0.0239, -0.1466,\n                        -0.0500, -0.3211, -0.1812, -0.1088, -0.2535, -0.2168, -0.2105, -0.1942,\n                        -0.2292, -0.1564, -0.2460, -0.0621, -0.0804, -0.0258, -0.0635, -0.0345,\n                        -0.2409, -0.1700, -0.1984, -0.1313, -0.3194, -0.1056, -0.3446, -0.0432,\n                        -0.1747, -0.2176, -0.2101, -0.2050, -0.4154, -0.1491, -0.2972, -0.1155,\n                        -0.0380, -0.3688, -0.2206, -0.1108, -0.2012, -0.2903, -0.0906, -0.0303,\n                        -0.1008, -0.3651, -0.0838, -0.2582, -0.3060, -0.3530, -0.4160, -0.2070,\n                        -0.1931, -0.3190, -0.0671, -0.4279, -0.3204, -0.0455, -0.3215, -0.1336,\n                        -0.0664, -0.2948, -0.1342, -0.4233, -0.3481, -0.3620, -0.0023, -0.4061,\n                        -0.3475, -0.0399, -0.0144, -0.2997, -0.1193, -0.2691, -0.1824, -0.0800,\n                        -0.3488, -0.0045, -0.3368, -0.1513, -0.1276, -0.3330, -0.2007, -0.3977,\n                        -0.3224, -0.0899, -0.0295, -0.2293, -0.3502, -0.3471, -0.0724, -0.1414,\n                        -0.0482, -0.3155, -0.2917, -0.3416, -0.1447, -0.1615, -0.0984, -0.3843,\n                        -0.1210, -0.4107, -0.3883, -0.0200, -0.1525, -0.1170, -0.3225, -0.0328,\n                        -0.3628, -0.0254, -0.0398, -0.3021, -0.1143, -0.2557, -0.1268, -0.3247,\n                        -0.4167, -0.3736, -0.0204, -0.2360, -0.0972, -0.0847, -0.1114, -0.2018,\n                        -0.2181, -0.2648, -0.3989, -0.2681, -0.2099, -0.1877, -0.0866, -0.2521,\n                        -0.2865, -0.2950, -0.0692, -0.1023, -0.2941, -0.1040, -0.0060, -0.0827,\n                        -0.3299, -0.2135, -0.3774, -0.3431, -0.0102, -0.1695, -0.3525, -0.1795,\n                        -0.1459, -0.1221, -0.3243, -0.3442, -0.3169, -0.3704, -0.3276, -0.0222,\n                        -0.2955, -0.1362, -0.1744, -0.1026, -0.2481, -0.2308, -0.2550, -0.3581,\n                        -0.1036, -0.0159, -0.0760, -0.3969, -0.2281, -0.1945, -0.1250, -0.1404,\n                        -0.3243, -0.0892, -0.3431, -0.1849, -0.1391, -0.0285, -0.3858, -0.0365,\n                        -0.1487, -0.0710, -0.1589, -0.0840, -0.1577, -0.1196, -0.2040, -0.1248,\n                        -0.2970, -0.4124, -0.2919, -0.2906, -0.1238, -0.1118, -0.3444, -0.1632,\n                        -0.2364, -0.2921, -0.2568, -0.0083, -0.1219, -0.0146, -0.1530, -0.3657,\n                        -0.0280, -0.3815, -0.3419, -0.0998, -0.3872, -0.1124, -0.0133, -0.0357,\n                        -0.0770, -0.2282, -0.0962,  0.0011, -0.0662, -0.2836, -0.0353, -0.0248,\n                        -0.3391, -0.2254, -0.0590, -0.0688, -0.1792, -0.0340, -0.2840, -0.1257,\n                        -0.1335, -0.3461, -0.1135, -0.1731, -0.1864, -0.0828, -0.0411, -0.1071,\n                        -0.0248, -0.0115, -0.2946, -0.1471, -0.3357, -0.2129, -0.1251, -0.1394,\n                        -0.1792, -0.1838, -0.0847, -0.0378, -0.3477, -0.3024, -0.4168, -0.3668,\n                        -0.0899, -0.0303, -0.2285, -0.1098, -0.0211, -0.0584, -0.3951, -0.0019,\n                        -0.4067, -0.0343, -0.1569, -0.0744, -0.3130, -0.1752, -0.2030, -0.1644,\n                        -0.0153, -0.1408, -0.3206, -0.2385, -0.0813, -0.3811, -0.3004, -0.2686,\n                        -0.3726, -0.3437, -0.0252, -0.3344, -0.0801, -0.4151, -0.5555, -0.0891,\n                        -0.1975, -0.0698, -0.1868, -0.1407, -0.2085, -0.3700, -0.2001, -0.2266,\n                        -0.2033, -0.3104, -0.3841, -0.1516, -0.2686, -0.3394, -0.2221, -0.2363,\n                        -0.4008, -0.0665, -0.0176, -0.1827, -0.3043, -0.3970, -0.0090, -0.0523,\n                        -0.1884, -0.3165, -0.1394, -0.3812, -0.0764, -0.3691, -0.0116, -0.2025,\n                        -0.3111, -0.2175, -0.0579, -0.3120, -0.4163, -0.3014, -0.4072, -0.2935,\n                        -0.0951, -0.3579, -0.1965, -0.3726, -0.2043, -0.0283, -0.1004, -0.2180]), max_val=tensor([ 0.1518,  0.2420,  0.1659,  0.1846,  0.3508,  0.3150,  0.0129,  0.1404,\n                         0.1136,  0.1780,  0.0051,  0.3417,  0.3981,  0.0582,  0.1380,  0.2873,\n                         0.3730,  0.3817,  0.3111,  0.2818,  0.1515,  0.1383,  0.0945,  0.3492,\n                         0.2079,  0.0576,  0.2249,  0.2276,  0.0029,  0.3109,  0.1586,  0.2220,\n                         0.2324,  0.0618,  0.3560,  0.2139,  0.2945,  0.3487,  0.1663,  0.3653,\n                         0.2684,  0.1809,  0.3478,  0.2013,  0.1547,  0.0804,  0.3300,  0.3864,\n                         0.0225,  0.2675,  0.3447,  0.3477,  0.2234,  0.4568,  0.3200,  0.1561,\n                         0.4089,  0.0384,  0.0240,  0.3967,  0.1780,  0.2952,  0.1818,  0.0038,\n                         0.3363,  0.1062,  0.0696,  0.3731,  0.3910,  0.0561,  0.3717,  0.3010,\n                         0.2458,  0.1263,  0.0138,  0.3946,  0.1814,  0.0054,  0.1713,  0.3711,\n                         0.3499,  0.2064,  0.0378,  0.1176,  0.1181,  0.3699,  0.2177,  0.4171,\n                         0.2690,  0.0817,  0.1652,  0.3764,  0.2191,  0.1827,  0.3988,  0.3748,\n                         0.3366,  0.0108,  0.4252,  0.2529,  0.0217,  0.1674,  0.0520,  0.2104,\n                         0.2106,  0.0267,  0.0577,  0.1133,  0.2107,  0.4446,  0.0442,  0.3963,\n                         0.3539,  0.0272,  0.2064,  0.1263,  0.1130,  0.1587,  0.1121,  0.0766,\n                         0.1209,  0.3115,  0.3412,  0.1599,  0.2767,  0.2317,  0.1771,  0.4203,\n                         0.0195,  0.1235,  0.0282,  0.3486,  0.4356,  0.1952,  0.2284,  0.0452,\n                         0.0320,  0.1742,  0.3241,  0.2031,  0.1021,  0.0896,  0.3027,  0.0298,\n                         0.2703,  0.0662,  0.2814,  0.0893,  0.3723,  0.3539,  0.0101,  0.0488,\n                         0.3809,  0.1424,  0.2018,  0.3257,  0.1426,  0.3811,  0.0794,  0.2243,\n                         0.1868,  0.3201,  0.2652,  0.1223,  0.4128,  0.1616,  0.3706,  0.1159,\n                         0.1096,  0.1186,  0.3855,  0.0682,  0.0064,  0.2059,  0.3599,  0.1875,\n                         0.2525,  0.0396,  0.1765,  0.2697,  0.1737,  0.3631,  0.4392,  0.1046,\n                         0.2009,  0.2731,  0.2016,  0.4384,  0.2151,  0.2903,  0.1461,  0.3967,\n                         0.0190,  0.1819,  0.0317,  0.0284,  0.2471,  0.1843,  0.1578,  0.3376,\n                         0.2123,  0.3424,  0.0259,  0.0817,  0.0472,  0.1079,  0.0251,  0.2751,\n                         0.1906,  0.2153,  0.2321,  0.0721,  0.2044,  0.2028,  0.1422,  0.1749,\n                         0.2257,  0.3020,  0.4144,  0.0672,  0.1780,  0.2164,  0.3695,  0.0754,\n                         0.2314,  0.3414,  0.1222,  0.1886,  0.3863,  0.0147,  0.1106,  0.2717,\n                         0.3872,  0.3599,  0.3563,  0.0879,  0.2692,  0.3427,  0.1101,  0.2419,\n                         0.0203,  0.0746,  0.0127,  0.2716,  0.3904,  0.3162,  0.0948,  0.2829,\n                         0.1996,  0.1441,  0.0313,  0.2522,  0.0345,  0.1926,  0.1360,  0.0533,\n                         0.3297,  0.1153,  0.2035,  0.4127,  0.0864,  0.4038,  0.3735,  0.3019,\n                         0.1058,  0.2937,  0.3652,  0.1566,  0.3222,  0.2440,  0.2828,  0.2570,\n                         0.1296,  0.2217,  0.0156,  0.3144,  0.2542,  0.2204,  0.1468,  0.3845,\n                         0.1438,  0.1526,  0.3695,  0.3043,  0.0139,  0.4127,  0.2621,  0.0128,\n                         0.3481,  0.3332,  0.0918,  0.3943,  0.0121,  0.0163,  0.3623,  0.0563,\n                         0.3182,  0.1006,  0.0086,  0.0618,  0.0966,  0.1130,  0.0966,  0.1467,\n                         0.2573,  0.3000,  0.1857,  0.2835,  0.2993,  0.3359,  0.1427,  0.0956,\n                         0.1430,  0.3969,  0.1521, -0.0028,  0.2133,  0.1418,  0.1946,  0.2390,\n                         0.3330,  0.2102,  0.1998,  0.3194,  0.3837,  0.0294,  0.0818,  0.1993,\n                         0.0359,  0.0839,  0.3574,  0.1475,  0.4377,  0.0756,  0.0404,  0.2834,\n                         0.2923,  0.2348,  0.1909,  0.3746,  0.0878,  0.1360,  0.1576,  0.2846,\n                         0.1167,  0.1910,  0.1182,  0.2106,  0.0102,  0.0395,  0.1767,  0.2233,\n                         0.1361,  0.1329,  0.3378,  0.1784,  0.2123,  0.0430,  0.1787,  0.4051,\n                         0.1525,  0.0264,  0.3919,  0.1216,  0.3831,  0.0192,  0.0105,  0.2192,\n                         0.1720,  0.3383,  0.2349,  0.2091,  0.0547,  0.2210,  0.1331,  0.1547,\n                         0.2562,  0.1334,  0.3000,  0.1342,  0.2188,  0.1222,  0.3844,  0.2256])\n              )\n            )\n          )\n          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(\n            in_features=384, out_features=1536, bias=True\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0021, 0.0029, 0.0015,  ..., 0.0014, 0.0022, 0.0024]), zero_point=tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([-0.2008, -0.1223, -0.1966,  ..., -0.1675, -0.1488, -0.2219]), max_val=tensor([0.2723, 0.3710, 0.1736,  ..., 0.1742, 0.2826, 0.2998]))\n            )\n          )\n          (4): GELU(approximate='none')\n          (5): Linear(\n            in_features=1536, out_features=384, bias=True\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0015, 0.0024, 0.0017, 0.0021, 0.0014, 0.0019, 0.0013, 0.0016, 0.0021,\n                      0.0021, 0.0014, 0.0015, 0.0019, 0.0033, 0.0018, 0.0015, 0.0015, 0.0015,\n                      0.0020, 0.0015, 0.0026, 0.0019, 0.0019, 0.0017, 0.0015, 0.0018, 0.0021,\n                      0.0022, 0.0024, 0.0019, 0.0020, 0.0016, 0.0015, 0.0016, 0.0022, 0.0017,\n                      0.0015, 0.0023, 0.0022, 0.0015, 0.0019, 0.0035, 0.0029, 0.0015, 0.0019,\n                      0.0029, 0.0023, 0.0016, 0.0017, 0.0015, 0.0026, 0.0040, 0.0018, 0.0029,\n                      0.0015, 0.0025, 0.0036, 0.0014, 0.0024, 0.0019, 0.0021, 0.0021, 0.0016,\n                      0.0015, 0.0019, 0.0032, 0.0016, 0.0024, 0.0022, 0.0021, 0.0017, 0.0015,\n                      0.0033, 0.0019, 0.0015, 0.0026, 0.0022, 0.0020, 0.0023, 0.0025, 0.0015,\n                      0.0015, 0.0015, 0.0034, 0.0024, 0.0022, 0.0026, 0.0015, 0.0017, 0.0016,\n                      0.0017, 0.0014, 0.0015, 0.0015, 0.0015, 0.0018, 0.0019, 0.0018, 0.0022,\n                      0.0016, 0.0020, 0.0018, 0.0022, 0.0016, 0.0027, 0.0015, 0.0015, 0.0025,\n                      0.0018, 0.0032, 0.0014, 0.0015, 0.0022, 0.0018, 0.0017, 0.0020, 0.0018,\n                      0.0026, 0.0016, 0.0017, 0.0015, 0.0017, 0.0017, 0.0035, 0.0028, 0.0028,\n                      0.0019, 0.0018, 0.0015, 0.0019, 0.0019, 0.0017, 0.0021, 0.0025, 0.0015,\n                      0.0020, 0.0018, 0.0015, 0.0020, 0.0022, 0.0017, 0.0022, 0.0035, 0.0015,\n                      0.0019, 0.0014, 0.0020, 0.0015, 0.0015, 0.0016, 0.0021, 0.0019, 0.0020,\n                      0.0015, 0.0023, 0.0020, 0.0046, 0.0021, 0.0022, 0.0016, 0.0017, 0.0016,\n                      0.0014, 0.0022, 0.0016, 0.0014, 0.0018, 0.0020, 0.0027, 0.0015, 0.0014,\n                      0.0016, 0.0017, 0.0015, 0.0017, 0.0019, 0.0019, 0.0017, 0.0029, 0.0022,\n                      0.0020, 0.0016, 0.0029, 0.0016, 0.0017, 0.0015, 0.0017, 0.0028, 0.0024,\n                      0.0032, 0.0023, 0.0019, 0.0018, 0.0015, 0.0019, 0.0023, 0.0015, 0.0019,\n                      0.0015, 0.0016, 0.0026, 0.0020, 0.0013, 0.0016, 0.0013, 0.0017, 0.0014,\n                      0.0014, 0.0016, 0.0017, 0.0013, 0.0026, 0.0018, 0.0015, 0.0015, 0.0017,\n                      0.0038, 0.0019, 0.0024, 0.0017, 0.0021, 0.0039, 0.0020, 0.0016, 0.0016,\n                      0.0019, 0.0017, 0.0016, 0.0018, 0.0024, 0.0018, 0.0014, 0.0015, 0.0015,\n                      0.0014, 0.0014, 0.0016, 0.0018, 0.0023, 0.0014, 0.0026, 0.0036, 0.0018,\n                      0.0014, 0.0015, 0.0030, 0.0018, 0.0016, 0.0019, 0.0015, 0.0014, 0.0021,\n                      0.0022, 0.0022, 0.0044, 0.0019, 0.0014, 0.0020, 0.0025, 0.0018, 0.0015,\n                      0.0014, 0.0024, 0.0020, 0.0029, 0.0021, 0.0017, 0.0014, 0.0023, 0.0018,\n                      0.0015, 0.0016, 0.0017, 0.0018, 0.0023, 0.0019, 0.0020, 0.0014, 0.0016,\n                      0.0013, 0.0016, 0.0024, 0.0015, 0.0017, 0.0014, 0.0019, 0.0016, 0.0017,\n                      0.0018, 0.0014, 0.0019, 0.0023, 0.0012, 0.0036, 0.0023, 0.0017, 0.0019,\n                      0.0023, 0.0019, 0.0019, 0.0016, 0.0020, 0.0021, 0.0027, 0.0014, 0.0017,\n                      0.0015, 0.0014, 0.0014, 0.0019, 0.0025, 0.0014, 0.0017, 0.0014, 0.0014,\n                      0.0017, 0.0015, 0.0019, 0.0025, 0.0021, 0.0034, 0.0015, 0.0015, 0.0017,\n                      0.0014, 0.0019, 0.0023, 0.0016, 0.0018, 0.0019, 0.0014, 0.0014, 0.0030,\n                      0.0019, 0.0022, 0.0017, 0.0016, 0.0016, 0.0021, 0.0018, 0.0019, 0.0016,\n                      0.0017, 0.0021, 0.0026, 0.0016, 0.0018, 0.0015, 0.0021, 0.0018, 0.0014,\n                      0.0022, 0.0015, 0.0034, 0.0017, 0.0028, 0.0020, 0.0022, 0.0030, 0.0027,\n                      0.0024, 0.0017, 0.0016, 0.0015, 0.0016, 0.0013, 0.0048, 0.0013, 0.0016,\n                      0.0020, 0.0016, 0.0018, 0.0018, 0.0019, 0.0016, 0.0026, 0.0015, 0.0018,\n                      0.0021, 0.0029, 0.0018, 0.0015, 0.0020, 0.0016]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                     dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n                min_val=tensor([-0.1875, -0.3060, -0.1529, -0.2559, -0.1843, -0.2379, -0.1569, -0.1987,\n                        -0.2624, -0.1738, -0.1782, -0.1885, -0.2453, -0.3179, -0.2318, -0.1887,\n                        -0.1735, -0.1933, -0.1764, -0.1655, -0.3382, -0.1726, -0.2328, -0.2215,\n                        -0.1859, -0.2238, -0.1801, -0.2759, -0.2323, -0.2193, -0.2620, -0.2028,\n                        -0.1901, -0.2035, -0.2688, -0.2010, -0.1933, -0.2516, -0.2755, -0.1927,\n                        -0.2484, -0.4326, -0.3734, -0.1789, -0.2382, -0.3737, -0.2982, -0.2027,\n                        -0.1869, -0.1634, -0.1864, -0.5089, -0.2276, -0.3259, -0.1887, -0.3237,\n                        -0.4638, -0.1735, -0.3067, -0.1807, -0.2694, -0.2727, -0.2099, -0.1952,\n                        -0.2453, -0.4157, -0.1565, -0.2396, -0.1660, -0.1619, -0.1565, -0.1561,\n                        -0.4271, -0.2164, -0.1949, -0.3328, -0.2769, -0.2441, -0.2031, -0.3150,\n                        -0.1885, -0.1966, -0.1958, -0.4404, -0.1981, -0.2776, -0.3320, -0.1864,\n                        -0.1569, -0.1835, -0.2168, -0.1836, -0.1921, -0.1914, -0.1874, -0.2335,\n                        -0.2441, -0.2004, -0.2387, -0.2068, -0.1834, -0.1781, -0.1491, -0.2089,\n                        -0.3405, -0.1972, -0.1864, -0.3208, -0.1909, -0.2840, -0.1826, -0.1595,\n                        -0.2762, -0.2306, -0.2148, -0.2547, -0.2283, -0.3316, -0.2111, -0.1808,\n                        -0.1772, -0.1837, -0.2123, -0.4432, -0.2303, -0.3609, -0.1982, -0.1994,\n                        -0.1848, -0.2436, -0.2337, -0.2131, -0.2539, -0.3194, -0.1977, -0.2567,\n                        -0.2330, -0.1929, -0.2230, -0.1900, -0.2121, -0.2854, -0.4423, -0.1907,\n                        -0.2416, -0.1736, -0.1776, -0.1894, -0.1711, -0.1699, -0.2660, -0.2369,\n                        -0.2586, -0.1750, -0.2882, -0.2056, -0.5848, -0.2708, -0.2865, -0.2012,\n                        -0.1455, -0.2011, -0.1795, -0.2797, -0.2016, -0.1779, -0.1923, -0.2034,\n                        -0.3472, -0.1672, -0.1502, -0.1953, -0.2193, -0.1878, -0.1581, -0.2400,\n                        -0.2330, -0.1749, -0.3650, -0.2271, -0.2585, -0.1986, -0.3146, -0.2080,\n                        -0.2154, -0.1642, -0.1999, -0.3013, -0.2370, -0.3270, -0.2885, -0.2462,\n                        -0.2283, -0.1757, -0.2422, -0.2545, -0.1941, -0.2417, -0.1902, -0.1843,\n                        -0.3355, -0.1987, -0.1565, -0.1805, -0.1692, -0.2135, -0.1794, -0.1683,\n                        -0.2007, -0.2144, -0.1712, -0.3341, -0.2275, -0.1915, -0.1981, -0.2218,\n                        -0.3740, -0.2376, -0.3017, -0.2096, -0.2628, -0.3194, -0.1914, -0.1698,\n                        -0.2051, -0.2382, -0.2119, -0.1778, -0.1532, -0.3114, -0.2316, -0.1830,\n                        -0.1913, -0.1917, -0.1819, -0.1827, -0.2043, -0.2326, -0.2913, -0.1807,\n                        -0.3365, -0.2344, -0.2298, -0.1701, -0.1888, -0.3724, -0.2196, -0.2078,\n                        -0.1954, -0.1704, -0.1763, -0.1979, -0.1810, -0.2022, -0.2304, -0.2424,\n                        -0.1780, -0.1721, -0.2377, -0.2356, -0.1439, -0.1789, -0.2623, -0.1537,\n                        -0.3755, -0.2718, -0.2182, -0.1769, -0.3001, -0.2137, -0.1949, -0.1990,\n                        -0.2192, -0.1738, -0.2143, -0.2121, -0.2597, -0.1553, -0.2071, -0.1723,\n                        -0.1773, -0.3012, -0.1940, -0.2207, -0.1844, -0.2419, -0.2090, -0.1859,\n                        -0.2244, -0.1716, -0.2377, -0.2924, -0.1597, -0.4668, -0.2884, -0.1730,\n                        -0.1739, -0.2992, -0.2483, -0.1769, -0.2047, -0.1925, -0.2607, -0.2222,\n                        -0.1818, -0.1847, -0.1917, -0.1755, -0.1816, -0.1786, -0.3201, -0.1854,\n                        -0.1805, -0.1833, -0.1830, -0.2216, -0.1872, -0.1617, -0.3145, -0.2078,\n                        -0.4363, -0.1384, -0.1790, -0.1872, -0.1734, -0.2409, -0.2745, -0.2016,\n                        -0.2274, -0.2058, -0.1603, -0.1557, -0.3829, -0.1903, -0.2780, -0.1748,\n                        -0.2020, -0.1694, -0.2514, -0.2305, -0.2488, -0.2073, -0.2177, -0.1796,\n                        -0.3332, -0.1892, -0.2345, -0.1958, -0.1842, -0.2029, -0.1583, -0.2757,\n                        -0.1562, -0.4309, -0.1860, -0.3565, -0.2561, -0.2779, -0.3834, -0.3450,\n                        -0.3115, -0.1937, -0.2056, -0.1893, -0.1996, -0.1683, -0.4852, -0.1728,\n                        -0.2036, -0.2616, -0.2037, -0.1750, -0.1740, -0.2444, -0.2079, -0.2471,\n                        -0.1881, -0.2265, -0.2655, -0.1823, -0.1986, -0.1671, -0.2145, -0.1575]), max_val=tensor([0.1838, 0.3095, 0.2192, 0.2663, 0.1794, 0.1950, 0.1591, 0.2000, 0.2621,\n                        0.2686, 0.1631, 0.1599, 0.2056, 0.4206, 0.1635, 0.1526, 0.1857, 0.1909,\n                        0.2598, 0.1875, 0.1776, 0.2391, 0.2357, 0.2029, 0.1618, 0.2282, 0.2726,\n                        0.2587, 0.3051, 0.2472, 0.2529, 0.1671, 0.1770, 0.1612, 0.2785, 0.2099,\n                        0.1719, 0.2931, 0.2605, 0.1601, 0.1895, 0.4412, 0.2226, 0.1883, 0.2474,\n                        0.2872, 0.2658, 0.1521, 0.2142, 0.1947, 0.3360, 0.3242, 0.2312, 0.3667,\n                        0.1934, 0.2795, 0.2845, 0.1838, 0.2325, 0.2442, 0.2560, 0.1679, 0.1878,\n                        0.1681, 0.2214, 0.2260, 0.2030, 0.3107, 0.2814, 0.2629, 0.2202, 0.1883,\n                        0.3530, 0.2354, 0.1872, 0.2040, 0.1793, 0.2519, 0.2911, 0.3117, 0.1904,\n                        0.1738, 0.1685, 0.3113, 0.3108, 0.2456, 0.2291, 0.1848, 0.2204, 0.1986,\n                        0.2001, 0.1742, 0.1852, 0.1813, 0.1926, 0.1784, 0.2405, 0.2270, 0.2776,\n                        0.1997, 0.2528, 0.2317, 0.2839, 0.1768, 0.2237, 0.1415, 0.1911, 0.3122,\n                        0.2311, 0.4046, 0.1715, 0.1886, 0.2522, 0.1758, 0.2147, 0.2431, 0.2074,\n                        0.2078, 0.1665, 0.2207, 0.1968, 0.2214, 0.2121, 0.3159, 0.3567, 0.3111,\n                        0.2447, 0.2239, 0.1961, 0.2361, 0.2361, 0.1596, 0.2707, 0.3089, 0.1865,\n                        0.2500, 0.1807, 0.1950, 0.2602, 0.2783, 0.2106, 0.1415, 0.2379, 0.1772,\n                        0.2057, 0.1484, 0.2497, 0.1950, 0.1854, 0.1992, 0.2104, 0.2039, 0.2385,\n                        0.1875, 0.1732, 0.2578, 0.5099, 0.2062, 0.1575, 0.1789, 0.2121, 0.1735,\n                        0.1583, 0.2292, 0.2087, 0.1823, 0.2302, 0.2544, 0.1844, 0.1882, 0.1785,\n                        0.1991, 0.2094, 0.1820, 0.2180, 0.2229, 0.2444, 0.2106, 0.2427, 0.2777,\n                        0.2422, 0.1676, 0.3716, 0.2071, 0.1971, 0.1964, 0.2129, 0.3508, 0.3076,\n                        0.4061, 0.2552, 0.2331, 0.1845, 0.1968, 0.2313, 0.2873, 0.1586, 0.2069,\n                        0.1852, 0.2061, 0.2152, 0.2526, 0.1615, 0.2015, 0.1604, 0.1662, 0.1646,\n                        0.1789, 0.1712, 0.2007, 0.1628, 0.2072, 0.1723, 0.1774, 0.1955, 0.2100,\n                        0.4804, 0.2122, 0.2279, 0.2163, 0.2057, 0.4903, 0.2566, 0.2034, 0.1862,\n                        0.1932, 0.1730, 0.1976, 0.2288, 0.1950, 0.2016, 0.1509, 0.1780, 0.1590,\n                        0.1795, 0.1686, 0.1568, 0.2287, 0.1980, 0.1422, 0.2592, 0.4588, 0.1808,\n                        0.1759, 0.1370, 0.3803, 0.2286, 0.2071, 0.2462, 0.1962, 0.1673, 0.2604,\n                        0.2769, 0.2800, 0.5571, 0.2033, 0.1526, 0.2477, 0.3143, 0.2254, 0.1842,\n                        0.1579, 0.3065, 0.2590, 0.3129, 0.2105, 0.2183, 0.1659, 0.2678, 0.2256,\n                        0.1766, 0.2052, 0.1626, 0.2227, 0.2867, 0.2435, 0.1548, 0.1824, 0.1579,\n                        0.1527, 0.2089, 0.2586, 0.1762, 0.2092, 0.1785, 0.2374, 0.1821, 0.2105,\n                        0.2330, 0.1794, 0.2041, 0.2270, 0.1580, 0.2935, 0.2136, 0.2145, 0.2365,\n                        0.2014, 0.1637, 0.2440, 0.1966, 0.2524, 0.2634, 0.3436, 0.1824, 0.2098,\n                        0.1680, 0.1759, 0.1713, 0.2399, 0.3040, 0.1484, 0.2166, 0.1722, 0.1756,\n                        0.2074, 0.1742, 0.2375, 0.2259, 0.2694, 0.3697, 0.1860, 0.1846, 0.2184,\n                        0.1621, 0.2239, 0.2912, 0.1684, 0.1937, 0.2476, 0.1798, 0.1791, 0.2921,\n                        0.2364, 0.1895, 0.2163, 0.2001, 0.1996, 0.2616, 0.1884, 0.1563, 0.1988,\n                        0.1798, 0.2679, 0.2012, 0.2023, 0.2080, 0.1907, 0.2682, 0.2236, 0.1723,\n                        0.2857, 0.1941, 0.3308, 0.2139, 0.1785, 0.2288, 0.1660, 0.3127, 0.2745,\n                        0.2074, 0.2097, 0.2083, 0.1562, 0.1974, 0.1686, 0.6086, 0.1477, 0.1923,\n                        0.2235, 0.1909, 0.2255, 0.2324, 0.2274, 0.1740, 0.3243, 0.1883, 0.2025,\n                        0.2505, 0.3736, 0.2270, 0.1853, 0.2591, 0.2042])\n              )\n            )\n          )\n        )\n      )\n      (1): Module(\n        (block): Module(\n          (0): Conv2d(\n            384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0005, 0.0008, 0.0021, 0.0024, 0.0020, 0.0025, 0.0010, 0.0020, 0.0027,\n                      0.0026, 0.0007, 0.0026, 0.0028, 0.0004, 0.0024, 0.0026, 0.0027, 0.0027,\n                      0.0030, 0.0025, 0.0029, 0.0026, 0.0030, 0.0027, 0.0019, 0.0012, 0.0012,\n                      0.0005, 0.0024, 0.0029, 0.0016, 0.0023, 0.0027, 0.0027, 0.0005, 0.0030,\n                      0.0025, 0.0028, 0.0018, 0.0027, 0.0030, 0.0022, 0.0029, 0.0023, 0.0029,\n                      0.0032, 0.0026, 0.0030, 0.0025, 0.0027, 0.0031, 0.0013, 0.0017, 0.0005,\n                      0.0026, 0.0017, 0.0018, 0.0017, 0.0019, 0.0028, 0.0026, 0.0002, 0.0013,\n                      0.0029, 0.0031, 0.0012, 0.0019, 0.0027, 0.0030, 0.0012, 0.0030, 0.0020,\n                      0.0022, 0.0020, 0.0021, 0.0031, 0.0025, 0.0027, 0.0028, 0.0005, 0.0028,\n                      0.0028, 0.0011, 0.0015, 0.0009, 0.0019, 0.0025, 0.0017, 0.0018, 0.0022,\n                      0.0020, 0.0028, 0.0026, 0.0027, 0.0023, 0.0028, 0.0025, 0.0022, 0.0005,\n                      0.0027, 0.0007, 0.0028, 0.0030, 0.0026, 0.0019, 0.0024, 0.0019, 0.0013,\n                      0.0006, 0.0036, 0.0019, 0.0023, 0.0022, 0.0023, 0.0024, 0.0008, 0.0025,\n                      0.0028, 0.0006, 0.0032, 0.0007, 0.0026, 0.0027, 0.0021, 0.0013, 0.0024,\n                      0.0013, 0.0021, 0.0025, 0.0009, 0.0027, 0.0029, 0.0024, 0.0028, 0.0021,\n                      0.0031, 0.0016, 0.0018, 0.0004, 0.0024, 0.0029, 0.0030, 0.0023, 0.0020,\n                      0.0022, 0.0027, 0.0031, 0.0020, 0.0006, 0.0020, 0.0032, 0.0005, 0.0024,\n                      0.0005, 0.0025, 0.0023, 0.0007, 0.0030, 0.0025, 0.0012, 0.0030, 0.0024,\n                      0.0018, 0.0027, 0.0030, 0.0023, 0.0027, 0.0028, 0.0004, 0.0030, 0.0025,\n                      0.0009, 0.0008, 0.0007, 0.0026, 0.0023, 0.0027, 0.0022, 0.0016, 0.0029,\n                      0.0024, 0.0026, 0.0013, 0.0014, 0.0030, 0.0026, 0.0017, 0.0005, 0.0025,\n                      0.0014, 0.0028, 0.0031, 0.0025, 0.0022, 0.0028, 0.0023, 0.0021, 0.0020,\n                      0.0024, 0.0026, 0.0007, 0.0026, 0.0008, 0.0025, 0.0024, 0.0025, 0.0025,\n                      0.0016, 0.0027, 0.0021, 0.0005, 0.0030, 0.0025, 0.0024, 0.0023, 0.0025,\n                      0.0020, 0.0006, 0.0005, 0.0030, 0.0023, 0.0023, 0.0028, 0.0008, 0.0029,\n                      0.0020, 0.0026, 0.0017, 0.0029, 0.0025, 0.0026, 0.0017, 0.0019, 0.0028,\n                      0.0021, 0.0020, 0.0025, 0.0029, 0.0013, 0.0022, 0.0024, 0.0017, 0.0020,\n                      0.0025, 0.0024, 0.0011, 0.0028, 0.0019, 0.0017, 0.0025, 0.0019, 0.0024,\n                      0.0008, 0.0022, 0.0010, 0.0023, 0.0025, 0.0031, 0.0029, 0.0007, 0.0030,\n                      0.0033, 0.0027, 0.0028, 0.0010, 0.0025, 0.0027, 0.0013, 0.0028, 0.0027,\n                      0.0014, 0.0019, 0.0028, 0.0018, 0.0011, 0.0023, 0.0025, 0.0016, 0.0010,\n                      0.0029, 0.0009, 0.0018, 0.0022, 0.0016, 0.0018, 0.0028, 0.0019, 0.0005,\n                      0.0016, 0.0025, 0.0027, 0.0026, 0.0026, 0.0021, 0.0027, 0.0012, 0.0030,\n                      0.0016, 0.0018, 0.0012, 0.0007, 0.0006, 0.0026, 0.0022, 0.0022, 0.0023,\n                      0.0016, 0.0006, 0.0023, 0.0025, 0.0024, 0.0006, 0.0023, 0.0028, 0.0009,\n                      0.0010, 0.0025, 0.0016, 0.0019, 0.0023, 0.0026, 0.0027, 0.0025, 0.0022,\n                      0.0030, 0.0030, 0.0007, 0.0027, 0.0028, 0.0022, 0.0019, 0.0027, 0.0005,\n                      0.0005, 0.0026, 0.0013, 0.0020, 0.0022, 0.0026, 0.0021, 0.0020, 0.0030,\n                      0.0022, 0.0027, 0.0010, 0.0024, 0.0030, 0.0024, 0.0007, 0.0018, 0.0025,\n                      0.0024, 0.0028, 0.0008, 0.0022, 0.0021, 0.0029, 0.0006, 0.0005, 0.0005,\n                      0.0021, 0.0023, 0.0028, 0.0028, 0.0030, 0.0028, 0.0005, 0.0026, 0.0027,\n                      0.0027, 0.0021, 0.0029, 0.0021, 0.0026, 0.0009, 0.0020, 0.0020, 0.0026,\n                      0.0026, 0.0021, 0.0016, 0.0010, 0.0029, 0.0024]), zero_point=tensor([-128,    0,    0,    0,    0,    0,    0,    0,    0,    0, -128,    0,\n                         0,  127,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0, -128,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,  127,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,  127,    0,    0,    0,    0,\n                      -128,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0, -128,    0, -128,    0,    0,    0,    0,    0,    0,    0,\n                      -128,    0,    0,    0,    0,    0,    0,  127,    0,    0, -128,    0,\n                      -128,    0,    0,    0,    0,    0,    0,    0,    0,  127,    0,    0,\n                         0,    0,    0,    0,    0,    0,  127,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0, -128,    0,    0, -128,    0, -128,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                       127,    0,    0,    0, -128,  127,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0, -128,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0, -128,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0, -128,  127,    0,    0,    0,    0,  127,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                       127,    0,    0,    0,    0,    0,    0, -128,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0, -128,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                       127, -128,    0,    0,    0,    0,    0, -128,    0,    0,    0,  127,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,  127, -128,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                      -128,    0,    0,    0,    0,    0,    0,    0,    0,  127,  127,  127,\n                         0,    0,    0,    0,    0,    0,  127,    0,    0,    0,    0,    0,\n                         0,    0,  127,    0,    0,    0,    0,    0,    0,  127,    0,    0],\n                     dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n                min_val=tensor([ 1.7642e-03, -1.0780e-01, -2.6981e-01, -3.0340e-01, -2.5632e-01,\n                        -3.1775e-01, -1.3239e-01, -2.5096e-01, -3.3934e-01, -1.6231e-02,\n                         2.5086e-04, -3.2751e-01, -3.5772e-01, -1.0171e-01, -3.0246e-01,\n                        -3.2888e-01, -3.5045e-01, -3.4588e-01, -3.7820e-01, -1.4590e-02,\n                        -2.2272e-01, -3.3374e-01, -3.8938e-01, -1.4905e-01, -2.3789e-01,\n                        -1.5283e-01, -7.1161e-02, -1.6462e-03, -4.5892e-04, -6.3715e-02,\n                        -1.7408e-01, -9.8146e-03, -1.1915e-01, -1.8508e-01,  1.4387e-02,\n                        -3.8119e-01, -3.2589e-01, -1.3875e-01, -2.2783e-01, -3.4816e-01,\n                        -1.1172e-01, -2.7673e-01, -3.6913e-01, -2.9107e-01, -5.5805e-02,\n                        -4.1254e-01, -8.3527e-03, -3.8375e-01, -3.2093e-01, -3.3641e-02,\n                        -3.9801e-01, -1.6556e-01, -2.1156e-01, -1.3077e-01, -1.7157e-02,\n                        -2.2020e-01, -2.3366e-01, -2.1191e-01, -2.4524e-01, -1.0972e-02,\n                        -2.2955e-02, -2.6971e-02, -4.0574e-03, -1.7002e-02, -6.8039e-02,\n                        -1.1210e-01, -2.5057e-03, -3.3979e-01, -5.0113e-02, -1.8199e-03,\n                        -3.8370e-01, -2.9965e-03, -1.6124e-01, -2.5779e-01, -8.9355e-03,\n                        -9.6273e-02, -3.2621e-01, -3.4175e-01, -3.5774e-01, -1.1828e-01,\n                        -3.5598e-01, -3.6022e-01, -1.4477e-01, -5.3427e-02,  8.2698e-03,\n                        -1.3110e-01, -3.1376e-01, -1.2360e-03, -3.7928e-03, -2.7649e-01,\n                        -2.5164e-01, -1.5502e-01, -3.3817e-01, -3.5046e-01, -2.9283e-01,\n                        -3.5518e-01, -1.1564e-01, -2.6243e-01,  6.9844e-03, -3.4867e-01,\n                         3.8793e-03, -1.8433e-01, -6.3637e-02, -3.3429e-01, -2.1491e-01,\n                        -3.0348e-01, -2.4596e-01, -1.1789e-01,  4.9266e-03, -4.5665e-01,\n                        -2.4427e-01, -2.9013e-01, -1.5361e-01, -7.8592e-03, -3.1048e-01,\n                        -2.0384e-01, -3.1585e-01, -3.6212e-01,  1.1132e-04, -4.0968e-01,\n                         5.6897e-03, -1.5116e-02, -3.4113e-01, -2.7396e-01, -1.4293e-03,\n                        -1.6908e-01, -3.1852e-03, -8.6249e-03, -3.2482e-01, -2.2031e-01,\n                        -3.4423e-01, -3.6853e-01, -3.1103e-01, -3.5790e-01, -2.6813e-01,\n                        -4.0264e-01, -2.0138e-01, -2.2560e-01, -9.2159e-02, -3.0422e-01,\n                        -3.7172e-01, -4.5173e-02, -6.0047e-02, -5.3737e-03, -6.3315e-03,\n                        -2.5592e-02, -9.9941e-02, -2.5834e-01,  4.3665e-03, -2.2813e-01,\n                        -1.4966e-01,  2.1784e-04, -3.0556e-01,  1.1641e-03, -3.2561e-01,\n                        -2.9098e-01, -9.2394e-02, -2.6118e-02, -3.1835e-01, -1.4992e-01,\n                        -8.2603e-02, -3.0918e-01, -2.2603e-01, -1.1872e-01, -3.8478e-01,\n                        -1.1997e-02, -3.4867e-01, -1.0037e-01, -1.1010e-01, -2.4639e-02,\n                        -1.2879e-02, -1.1520e-01,  2.5395e-03, -1.7836e-01, -3.3081e-01,\n                        -2.8868e-02, -1.3634e-01, -5.7226e-03, -2.0787e-01, -7.9197e-02,\n                        -3.1210e-01, -3.2795e-01, -8.6547e-02, -1.7817e-01, -3.8195e-01,\n                        -3.3410e-01, -2.3164e-03,  7.1860e-03, -8.8761e-02, -1.1460e-01,\n                        -8.9036e-03, -3.3237e-02, -1.0165e-02, -2.8693e-01, -3.6270e-01,\n                        -2.9310e-01, -2.6407e-01, -2.5801e-01, -1.9561e-01, -3.3031e-01,\n                        -8.8955e-02, -3.3506e-01,  3.9756e-03, -1.8730e-01, -3.0132e-01,\n                        -1.4355e-01, -3.1513e-01, -1.9935e-01, -3.5503e-02, -2.7260e-01,\n                        -5.7670e-02, -1.0912e-01, -4.2533e-02, -3.0664e-01, -2.9604e-01,\n                        -1.4183e-01, -2.5955e-01,  3.3486e-03, -1.3236e-01, -7.5785e-02,\n                        -1.0449e-02, -2.9992e-01, -1.6012e-01, -2.0558e-01, -3.7291e-01,\n                        -2.5530e-01, -4.7745e-02, -3.7433e-02, -3.6625e-01, -3.2523e-01,\n                        -3.3393e-01, -9.9509e-03, -2.1852e-01, -3.6340e-01, -2.4242e-01,\n                        -2.6067e-01, -1.1483e-02, -3.9508e-02, -1.1003e-01, -2.7558e-01,\n                        -4.3397e-03, -2.2001e-01, -9.4164e-03, -3.2325e-01, -2.8420e-01,\n                        -1.2221e-01, -3.5858e-01, -1.1520e-01, -2.1312e-01, -3.1481e-01,\n                        -5.3566e-03, -1.4262e-01, -2.1185e-01, -2.8712e-01, -8.3464e-02,\n                        -1.3021e-01, -1.1803e-02, -3.9306e-01, -3.7638e-01,  6.0441e-03,\n                        -3.8214e-01, -5.2992e-02, -1.0122e-02, -2.3084e-02, -8.9093e-02,\n                        -3.2472e-01, -5.5015e-02, -1.6957e-01, -3.6284e-01, -1.3087e-01,\n                        -1.7497e-01, -2.4432e-01, -1.5972e-01, -1.0842e-01, -4.1552e-03,\n                        -2.9239e-01, -1.6049e-02, -2.0403e-01, -1.0038e-02, -1.2224e-02,\n                        -4.2372e-03, -2.3152e-01, -2.7882e-01, -1.9263e-01, -2.3660e-01,\n                        -1.4261e-01, -2.4729e-01,  1.0296e-02, -6.0020e-03, -3.2008e-01,\n                        -1.0356e-02, -2.3130e-01, -3.3253e-01, -4.7307e-02, -3.4663e-01,\n                        -1.0547e-04, -5.0852e-02, -3.8702e-03, -9.8510e-02, -1.5385e-01,\n                        -1.8139e-01,  1.0596e-02, -3.3874e-01, -2.7955e-01, -9.6325e-03,\n                        -5.4446e-03, -1.1510e-02,  2.5042e-05, -2.9777e-01, -3.1848e-01,\n                        -2.2228e-01, -1.4436e-01, -1.5581e-02, -3.5541e-01, -5.4117e-02,\n                        -1.3432e-01, -3.1437e-01, -1.5665e-01, -2.3695e-01, -2.9091e-01,\n                        -3.2763e-01, -3.4529e-01, -1.8624e-01, -1.4706e-01, -3.8667e-01,\n                        -6.5121e-02, -9.0201e-02, -3.9276e-02, -3.5581e-01, -2.7556e-01,\n                        -2.7215e-03, -4.1271e-02, -1.1714e-01,  1.2631e-02, -1.1277e-02,\n                        -1.6515e-01, -9.0301e-02, -2.8301e-01, -2.7669e-02, -2.7280e-01,\n                        -2.6134e-01, -9.5565e-02, -2.8095e-01, -3.4224e-01, -1.3284e-01,\n                        -3.0491e-01, -9.9662e-02, -3.0272e-01,  1.2795e-03, -2.2773e-01,\n                        -3.1745e-01, -3.0087e-01, -3.5787e-01, -1.0240e-01, -2.7546e-01,\n                        -2.4645e-02, -3.7681e-01, -1.6012e-01, -1.3050e-01, -1.1908e-01,\n                        -2.6315e-01, -2.9550e-01, -3.6294e-01, -1.2271e-02, -3.8915e-01,\n                        -1.3248e-02, -1.3329e-01, -3.3786e-01, -3.4306e-01, -8.6873e-02,\n                        -5.8312e-03, -3.6888e-01, -2.6649e-01, -1.1379e-01, -2.1835e-01,\n                        -2.5691e-01, -1.5797e-01, -3.3189e-01, -7.6002e-02, -2.7487e-01,\n                        -2.0813e-01, -2.5723e-01, -1.2420e-01, -3.1174e-01]), max_val=tensor([ 1.2960e-01,  1.0635e-01,  2.7788e-02,  1.4461e-01,  2.0895e-01,\n                         2.1973e-02,  3.0817e-04,  2.5822e-01,  9.2835e-02,  3.2828e-01,\n                         1.6612e-01,  1.6769e-01,  1.4419e-01, -3.0266e-02,  2.6770e-01,\n                         2.1219e-02,  1.2830e-01,  1.4726e-02,  1.1232e-01,  3.2237e-01,\n                         3.6196e-01,  2.9726e-02,  2.7107e-02,  3.3720e-01,  4.6696e-03,\n                         3.4465e-03,  1.4612e-01,  6.3098e-02,  3.0149e-01,  3.6989e-01,\n                         2.0616e-01,  2.9842e-01,  3.4866e-01,  3.4564e-01,  1.3731e-01,\n                         2.1534e-02,  7.3689e-02,  3.5468e-01,  1.2829e-01,  5.3131e-02,\n                         3.7557e-01,  1.5080e-01,  8.3456e-02,  2.3518e-01,  3.6781e-01,\n                         3.0488e-02,  3.2600e-01,  2.3411e-02,  1.0514e-02,  3.4516e-01,\n                         6.9574e-02,  1.5292e-01,  6.3008e-03, -2.6418e-02,  3.2386e-01,\n                         2.1343e-02,  1.3933e-01,  1.5236e-03,  2.0129e-02,  3.6003e-01,\n                         3.2550e-01,  2.2208e-02,  1.6603e-01,  3.7226e-01,  3.9804e-01,\n                         1.4890e-01,  2.4579e-01,  1.7957e-01,  3.8453e-01,  1.5218e-01,\n                         2.2335e-02,  2.6017e-01,  2.7793e-01,  3.4040e-03,  2.6332e-01,\n                         3.9472e-01,  5.0512e-02,  7.8399e-02,  9.1831e-02, -2.6668e-03,\n                         1.8931e-02,  5.4386e-02,  1.4050e-01,  1.9383e-01,  2.2235e-01,\n                         2.4077e-01,  1.1085e-01,  2.2152e-01,  2.2982e-01,  2.3950e-01,\n                         1.4450e-01,  3.5892e-01,  3.6000e-02,  3.0027e-02,  7.5252e-02,\n                         2.0268e-02,  3.1591e-01,  2.7895e-01,  1.2605e-01,  1.1054e-01,\n                         1.6865e-01,  3.5070e-01,  3.8543e-01,  1.2113e-02,  2.3691e-01,\n                         5.3892e-03,  6.7004e-03,  1.6705e-01,  1.4302e-01,  3.2711e-02,\n                         1.4743e-02,  1.1435e-01,  2.8307e-01,  2.8838e-01,  1.4117e-01,\n                        -1.5231e-02,  2.1361e-01,  1.2065e-01,  1.4161e-01,  3.8242e-02,\n                         1.8308e-01,  3.3159e-01,  1.0634e-02,  8.2702e-02,  1.6593e-01,\n                         3.0121e-01,  1.6182e-01,  2.6553e-01,  5.8153e-03, -1.0289e-03,\n                         1.1533e-02,  1.5262e-01,  2.0239e-01,  9.7576e-02,  1.6611e-01,\n                         5.3162e-02,  3.9916e-03,  1.2945e-01, -9.6156e-03,  3.6656e-02,\n                         7.8610e-02,  3.8004e-01,  2.9620e-01,  2.5349e-01,  2.8255e-01,\n                         3.3853e-01,  3.9413e-01,  5.5019e-03,  1.6380e-01,  2.4862e-01,\n                         4.0794e-01,  1.3466e-01,  2.7051e-01,  1.1977e-01,  2.8352e-01,\n                         2.4748e-01,  9.4851e-02,  3.8256e-01,  1.4216e-02,  7.2580e-03,\n                         3.8267e-01,  5.9962e-03,  1.2332e-02,  3.4591e-01,  7.5089e-02,\n                         2.9214e-01,  1.6244e-01,  3.5026e-01, -4.7450e-03,  3.7799e-01,\n                         3.1707e-01,  2.1264e-03,  1.9402e-01, -5.5109e-03,  1.6626e-02,\n                         2.8826e-01,  3.4148e-01,  2.7388e-01,  1.0150e-02,  3.7246e-01,\n                         1.5960e-02,  1.9239e-01,  1.6171e-01,  1.5336e-02,  8.0272e-02,\n                         1.8166e-01,  2.2182e-01,  1.1690e-01,  3.1187e-01,  1.7763e-01,\n                         3.6131e-01,  3.9986e-01,  3.1443e-01,  2.4381e-02,  1.4185e-02,\n                         6.8011e-02,  4.3399e-03,  4.5865e-02,  2.9945e-01,  1.7334e-01,\n                         3.6915e-02,  3.9837e-02,  2.0719e-01,  3.1587e-01,  1.3607e-02,\n                         3.1685e-01,  7.7070e-03,  8.0226e-03,  3.4499e-01,  9.7526e-02,\n                         4.5541e-02,  3.8114e-01,  3.1126e-01,  3.3947e-02,  1.3159e-02,\n                         3.1294e-01,  3.8410e-02,  1.5424e-01, -4.0725e-03,  3.7904e-01,\n                         2.9725e-01,  2.8194e-02,  3.5699e-01, -5.5523e-03,  1.0788e-01,\n                         2.5522e-01,  3.3614e-01,  2.1117e-01,  8.0357e-02,  2.8815e-01,\n                         1.5787e-01,  2.1078e-01,  2.3716e-01,  1.8211e-02,  2.6404e-01,\n                         7.6155e-03,  3.2014e-01,  3.6718e-01,  1.6014e-01,  1.4089e-01,\n                         2.9957e-01,  2.2030e-01,  2.5774e-01,  1.6907e-01,  3.0815e-01,\n                         1.4534e-01,  2.0595e-02,  2.4383e-01,  1.6781e-01,  1.0596e-01,\n                         2.4069e-01,  3.0134e-01, -4.1343e-04,  6.8464e-02,  1.2210e-01,\n                         2.8869e-01,  3.2234e-01,  1.8801e-01,  8.9001e-02,  1.7604e-01,\n                         6.9350e-02,  4.1897e-01,  3.4771e-01,  3.5374e-01,  1.2547e-01,\n                         1.6269e-01,  3.4548e-01,  4.6796e-03,  4.9743e-02,  3.4627e-01,\n                         9.8478e-03,  3.9081e-03,  3.5470e-01,  2.2604e-01,  1.4302e-01,\n                         9.8332e-02,  3.2368e-01,  3.7876e-02,  1.2786e-01,  3.6231e-01,\n                         1.1644e-01,  1.9014e-01,  1.9860e-01,  2.0440e-01,  1.0152e-03,\n                         3.6164e-01,  3.1142e-03,  1.1848e-01,  2.0152e-01,  1.8240e-02,\n                         3.4633e-01,  3.3325e-01,  1.9098e-02,  2.6845e-01,  1.6137e-01,\n                         1.5785e-01,  3.8037e-01,  2.0702e-01,  2.3369e-01,  2.5355e-04,\n                        -2.0440e-03,  1.5087e-01,  8.8748e-02,  1.0604e-01,  2.8446e-01,\n                         2.9740e-01,  2.0083e-01,  1.4794e-01,  5.7798e-03,  2.0832e-02,\n                         3.0042e-01, -3.8167e-03,  2.9748e-01,  1.7581e-02,  1.1865e-01,\n                         5.6204e-03,  1.5531e-01,  2.0311e-01,  1.6387e-02,  2.2874e-02,\n                         2.0808e-02,  1.5535e-02,  3.1672e-01,  2.8038e-01,  1.3851e-02,\n                         3.8443e-01,  6.9264e-02,  3.4259e-01,  2.0544e-02,  1.7732e-01,\n                         2.4110e-01,  3.4886e-01, -1.4603e-02,  1.3506e-01,  3.3137e-01,\n                         2.4062e-02,  2.5845e-01,  4.1245e-03,  3.3053e-01,  1.6964e-01,\n                         2.8761e-03,  3.7948e-01,  1.5979e-02,  9.0845e-02,  1.0923e-01,\n                         1.5502e-01,  3.8029e-01,  1.4444e-02,  1.6915e-01,  2.1327e-02,\n                         1.3285e-02,  7.3710e-02,  9.6061e-02,  8.7019e-02,  4.7173e-03,\n                         2.6553e-01,  9.6873e-02, -1.7664e-03, -3.9514e-03, -1.3463e-02,\n                         1.0265e-02,  5.8310e-03,  2.0328e-01,  3.4995e-01,  4.1474e-02,\n                         3.5751e-01, -3.5641e-03,  2.7215e-02,  1.0376e-01,  3.3952e-01,\n                         2.7007e-01,  1.5459e-01,  1.1232e-02,  3.2417e-01, -7.8129e-04,\n                         1.2440e-01,  2.5632e-01,  8.1900e-02,  3.3066e-01,  6.9170e-02,\n                         5.8540e-02, -8.1623e-04,  3.6497e-01,  3.6391e-02])\n              )\n            )\n          )\n          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(\n            in_features=384, out_features=1536, bias=True\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0016, 0.0015, 0.0020,  ..., 0.0014, 0.0014, 0.0012]), zero_point=tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([-0.2046, -0.1950, -0.2606,  ..., -0.1823, -0.1727, -0.1598]), max_val=tensor([0.1807, 0.1336, 0.2352,  ..., 0.1823, 0.1817, 0.1412]))\n            )\n          )\n          (4): GELU(approximate='none')\n          (5): Linear(\n            in_features=1536, out_features=384, bias=True\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0016, 0.0025, 0.0016, 0.0022, 0.0016, 0.0014, 0.0020, 0.0014, 0.0021,\n                      0.0021, 0.0014, 0.0019, 0.0019, 0.0026, 0.0017, 0.0014, 0.0015, 0.0018,\n                      0.0019, 0.0017, 0.0021, 0.0020, 0.0034, 0.0016, 0.0019, 0.0019, 0.0023,\n                      0.0031, 0.0024, 0.0025, 0.0016, 0.0018, 0.0020, 0.0016, 0.0021, 0.0014,\n                      0.0016, 0.0020, 0.0021, 0.0016, 0.0016, 0.0039, 0.0019, 0.0018, 0.0019,\n                      0.0033, 0.0021, 0.0020, 0.0015, 0.0018, 0.0016, 0.0048, 0.0016, 0.0034,\n                      0.0017, 0.0025, 0.0034, 0.0016, 0.0018, 0.0018, 0.0018, 0.0021, 0.0020,\n                      0.0017, 0.0020, 0.0029, 0.0018, 0.0023, 0.0031, 0.0025, 0.0016, 0.0016,\n                      0.0029, 0.0020, 0.0016, 0.0018, 0.0022, 0.0023, 0.0015, 0.0025, 0.0018,\n                      0.0016, 0.0023, 0.0024, 0.0024, 0.0023, 0.0022, 0.0015, 0.0016, 0.0015,\n                      0.0015, 0.0018, 0.0017, 0.0017, 0.0015, 0.0022, 0.0015, 0.0019, 0.0019,\n                      0.0021, 0.0020, 0.0015, 0.0026, 0.0017, 0.0031, 0.0014, 0.0013, 0.0035,\n                      0.0016, 0.0034, 0.0015, 0.0016, 0.0028, 0.0018, 0.0018, 0.0027, 0.0018,\n                      0.0017, 0.0016, 0.0018, 0.0018, 0.0017, 0.0019, 0.0034, 0.0024, 0.0031,\n                      0.0025, 0.0018, 0.0017, 0.0021, 0.0025, 0.0016, 0.0028, 0.0028, 0.0016,\n                      0.0036, 0.0016, 0.0019, 0.0030, 0.0023, 0.0018, 0.0016, 0.0023, 0.0020,\n                      0.0021, 0.0019, 0.0016, 0.0016, 0.0017, 0.0027, 0.0022, 0.0015, 0.0014,\n                      0.0018, 0.0026, 0.0019, 0.0040, 0.0015, 0.0020, 0.0015, 0.0018, 0.0015,\n                      0.0014, 0.0021, 0.0024, 0.0018, 0.0022, 0.0016, 0.0016, 0.0017, 0.0017,\n                      0.0019, 0.0019, 0.0024, 0.0016, 0.0031, 0.0015, 0.0021, 0.0026, 0.0022,\n                      0.0026, 0.0018, 0.0039, 0.0020, 0.0015, 0.0024, 0.0027, 0.0023, 0.0021,\n                      0.0023, 0.0019, 0.0021, 0.0016, 0.0017, 0.0020, 0.0020, 0.0018, 0.0037,\n                      0.0014, 0.0014, 0.0022, 0.0017, 0.0016, 0.0013, 0.0016, 0.0018, 0.0014,\n                      0.0014, 0.0019, 0.0024, 0.0021, 0.0023, 0.0020, 0.0015, 0.0025, 0.0021,\n                      0.0026, 0.0054, 0.0018, 0.0025, 0.0035, 0.0036, 0.0018, 0.0020, 0.0030,\n                      0.0026, 0.0020, 0.0017, 0.0014, 0.0039, 0.0019, 0.0015, 0.0020, 0.0019,\n                      0.0014, 0.0018, 0.0015, 0.0024, 0.0017, 0.0021, 0.0019, 0.0017, 0.0018,\n                      0.0016, 0.0015, 0.0037, 0.0015, 0.0021, 0.0027, 0.0014, 0.0021, 0.0020,\n                      0.0023, 0.0017, 0.0024, 0.0018, 0.0015, 0.0014, 0.0017, 0.0018, 0.0016,\n                      0.0022, 0.0021, 0.0018, 0.0039, 0.0017, 0.0021, 0.0016, 0.0028, 0.0017,\n                      0.0020, 0.0016, 0.0015, 0.0019, 0.0020, 0.0018, 0.0015, 0.0014, 0.0021,\n                      0.0014, 0.0016, 0.0020, 0.0016, 0.0022, 0.0015, 0.0018, 0.0020, 0.0015,\n                      0.0021, 0.0016, 0.0028, 0.0017, 0.0016, 0.0023, 0.0015, 0.0019, 0.0021,\n                      0.0020, 0.0019, 0.0021, 0.0017, 0.0016, 0.0028, 0.0025, 0.0018, 0.0022,\n                      0.0014, 0.0016, 0.0015, 0.0025, 0.0019, 0.0014, 0.0020, 0.0021, 0.0023,\n                      0.0019, 0.0015, 0.0029, 0.0027, 0.0021, 0.0029, 0.0015, 0.0016, 0.0039,\n                      0.0024, 0.0025, 0.0020, 0.0023, 0.0016, 0.0019, 0.0018, 0.0016, 0.0022,\n                      0.0024, 0.0024, 0.0020, 0.0018, 0.0024, 0.0034, 0.0018, 0.0018, 0.0022,\n                      0.0018, 0.0018, 0.0026, 0.0015, 0.0023, 0.0028, 0.0022, 0.0015, 0.0021,\n                      0.0021, 0.0016, 0.0040, 0.0014, 0.0023, 0.0018, 0.0023, 0.0018, 0.0025,\n                      0.0028, 0.0019, 0.0019, 0.0014, 0.0019, 0.0014, 0.0031, 0.0016, 0.0021,\n                      0.0014, 0.0018, 0.0015, 0.0026, 0.0021, 0.0021, 0.0021, 0.0017, 0.0018,\n                      0.0017, 0.0029, 0.0029, 0.0017, 0.0017, 0.0019]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                     dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n                min_val=tensor([-0.1988, -0.3252, -0.1723, -0.2761, -0.1736, -0.1645, -0.2565, -0.1847,\n                        -0.2635, -0.2432, -0.1778, -0.2160, -0.1821, -0.3310, -0.2095, -0.1816,\n                        -0.1933, -0.2254, -0.2280, -0.2176, -0.2471, -0.2600, -0.2842, -0.2058,\n                        -0.1932, -0.2396, -0.2947, -0.4017, -0.3121, -0.2245, -0.2036, -0.2354,\n                        -0.2536, -0.1852, -0.2179, -0.1776, -0.1846, -0.2123, -0.2545, -0.2094,\n                        -0.1629, -0.4932, -0.2376, -0.2362, -0.2466, -0.4236, -0.2689, -0.2352,\n                        -0.1499, -0.2236, -0.2096, -0.6084, -0.1672, -0.4356, -0.2001, -0.3193,\n                        -0.4288, -0.2053, -0.2059, -0.2319, -0.2215, -0.2125, -0.2597, -0.1915,\n                        -0.2586, -0.2328, -0.2275, -0.1739, -0.3931, -0.1874, -0.2096, -0.2087,\n                        -0.3709, -0.2234, -0.1650, -0.2271, -0.1965, -0.2654, -0.1860, -0.2510,\n                        -0.2294, -0.2051, -0.2923, -0.3106, -0.3027, -0.2712, -0.2859, -0.1883,\n                        -0.2088, -0.1503, -0.1777, -0.2249, -0.2162, -0.2184, -0.1858, -0.2855,\n                        -0.1620, -0.2440, -0.2449, -0.1958, -0.1760, -0.1950, -0.3298, -0.2126,\n                        -0.4022, -0.1766, -0.1656, -0.4421, -0.2092, -0.4312, -0.1666, -0.1686,\n                        -0.3026, -0.2282, -0.1778, -0.2753, -0.2240, -0.2156, -0.2106, -0.2131,\n                        -0.2288, -0.1699, -0.2281, -0.3591, -0.3074, -0.3499, -0.3215, -0.2325,\n                        -0.2123, -0.2162, -0.2883, -0.1698, -0.3610, -0.3610, -0.2024, -0.4616,\n                        -0.1819, -0.2383, -0.3038, -0.1752, -0.2041, -0.2104, -0.2264, -0.2530,\n                        -0.1781, -0.2045, -0.1929, -0.2084, -0.2123, -0.3468, -0.2857, -0.1849,\n                        -0.1789, -0.1706, -0.2683, -0.2240, -0.2593, -0.1872, -0.2516, -0.1923,\n                        -0.1793, -0.1934, -0.1849, -0.2586, -0.3014, -0.1724, -0.2828, -0.1952,\n                        -0.2099, -0.2144, -0.1797, -0.1551, -0.1751, -0.3012, -0.1628, -0.2109,\n                        -0.1720, -0.2147, -0.3272, -0.2504, -0.3307, -0.2344, -0.4939, -0.2177,\n                        -0.1750, -0.3015, -0.3423, -0.2481, -0.2317, -0.2538, -0.2198, -0.2717,\n                        -0.1878, -0.1967, -0.2001, -0.2461, -0.1769, -0.4756, -0.1651, -0.1732,\n                        -0.2763, -0.1900, -0.1795, -0.1671, -0.1866, -0.1857, -0.1587, -0.1751,\n                        -0.1872, -0.2861, -0.2661, -0.2693, -0.2598, -0.1961, -0.3146, -0.2367,\n                        -0.3366, -0.5313, -0.1951, -0.2192, -0.2077, -0.4568, -0.1933, -0.2616,\n                        -0.1979, -0.2804, -0.2173, -0.2073, -0.1631, -0.4989, -0.2481, -0.1590,\n                        -0.2527, -0.1930, -0.1405, -0.2348, -0.1917, -0.3134, -0.1806, -0.2386,\n                        -0.2152, -0.1847, -0.2347, -0.2035, -0.1625, -0.4770, -0.1691, -0.2434,\n                        -0.2774, -0.1648, -0.2652, -0.2260, -0.1627, -0.1771, -0.2519, -0.2276,\n                        -0.1810, -0.1799, -0.1633, -0.1979, -0.1860, -0.1778, -0.2629, -0.2195,\n                        -0.4952, -0.2128, -0.1860, -0.2037, -0.3529, -0.2190, -0.2500, -0.1685,\n                        -0.1720, -0.2394, -0.2533, -0.2237, -0.1917, -0.1459, -0.1851, -0.1665,\n                        -0.2021, -0.2176, -0.1987, -0.2803, -0.1781, -0.1761, -0.2542, -0.1687,\n                        -0.2267, -0.1987, -0.2239, -0.1844, -0.2044, -0.2894, -0.1864, -0.2045,\n                        -0.1888, -0.2362, -0.1870, -0.2630, -0.2238, -0.2033, -0.2972, -0.1743,\n                        -0.2244, -0.1845, -0.1694, -0.1871, -0.1870, -0.2930, -0.2387, -0.1787,\n                        -0.2017, -0.2305, -0.2314, -0.2394, -0.1513, -0.2015, -0.3490, -0.2264,\n                        -0.3378, -0.1936, -0.1746, -0.5043, -0.1809, -0.2413, -0.2492, -0.2090,\n                        -0.1543, -0.2129, -0.2256, -0.1659, -0.2457, -0.3008, -0.2013, -0.2375,\n                        -0.2356, -0.2517, -0.3017, -0.2351, -0.2312, -0.2836, -0.2290, -0.2348,\n                        -0.2491, -0.1916, -0.2451, -0.3005, -0.2869, -0.1979, -0.2636, -0.2263,\n                        -0.2073, -0.2546, -0.1491, -0.2872, -0.1625, -0.1869, -0.2063, -0.2359,\n                        -0.3168, -0.2426, -0.2406, -0.1756, -0.2467, -0.1756, -0.3411, -0.2053,\n                        -0.1765, -0.1660, -0.2261, -0.1921, -0.2532, -0.2699, -0.2634, -0.2631,\n                        -0.1740, -0.1985, -0.1977, -0.3657, -0.3722, -0.2232, -0.2124, -0.2230]), max_val=tensor([0.1805, 0.3058, 0.1987, 0.2004, 0.1982, 0.1757, 0.1820, 0.1746, 0.2330,\n                        0.2688, 0.1745, 0.2409, 0.2472, 0.2768, 0.2201, 0.1698, 0.1578, 0.1630,\n                        0.2388, 0.1723, 0.2697, 0.1850, 0.4303, 0.1813, 0.2350, 0.1991, 0.2792,\n                        0.2917, 0.2754, 0.3220, 0.1774, 0.1916, 0.2268, 0.2029, 0.2615, 0.1806,\n                        0.2070, 0.2547, 0.2636, 0.1960, 0.2015, 0.2260, 0.1995, 0.2074, 0.1843,\n                        0.3545, 0.2193, 0.2480, 0.1862, 0.2240, 0.1884, 0.3242, 0.2000, 0.2722,\n                        0.2182, 0.2543, 0.4297, 0.2013, 0.2233, 0.1742, 0.2303, 0.2610, 0.1889,\n                        0.2103, 0.1829, 0.3639, 0.1855, 0.2895, 0.2262, 0.3125, 0.1757, 0.1896,\n                        0.2286, 0.2578, 0.2012, 0.1928, 0.2839, 0.2888, 0.1902, 0.3150, 0.1819,\n                        0.2088, 0.2696, 0.3097, 0.1930, 0.2942, 0.2251, 0.1763, 0.1737, 0.1931,\n                        0.1966, 0.1785, 0.2218, 0.1794, 0.1893, 0.2437, 0.1963, 0.1983, 0.1848,\n                        0.2704, 0.2526, 0.1653, 0.2814, 0.2168, 0.3110, 0.1506, 0.1590, 0.3545,\n                        0.1916, 0.3013, 0.1919, 0.2075, 0.3516, 0.1739, 0.2318, 0.3444, 0.1977,\n                        0.1775, 0.1852, 0.2332, 0.1536, 0.2115, 0.2393, 0.4297, 0.3089, 0.3958,\n                        0.2031, 0.1860, 0.1775, 0.2669, 0.3160, 0.2080, 0.3097, 0.2087, 0.1817,\n                        0.3447, 0.2000, 0.2367, 0.3798, 0.2900, 0.2345, 0.2023, 0.2921, 0.2061,\n                        0.2685, 0.2375, 0.2050, 0.1852, 0.1997, 0.1992, 0.1838, 0.1938, 0.1782,\n                        0.2302, 0.3271, 0.2355, 0.5110, 0.1733, 0.2407, 0.1934, 0.2318, 0.1447,\n                        0.1810, 0.2726, 0.2570, 0.2342, 0.1957, 0.2025, 0.2002, 0.1940, 0.2197,\n                        0.2457, 0.2426, 0.2379, 0.2018, 0.3988, 0.1863, 0.2685, 0.2111, 0.2788,\n                        0.3125, 0.1825, 0.3284, 0.2564, 0.1895, 0.1918, 0.2380, 0.2973, 0.2672,\n                        0.2960, 0.2419, 0.2650, 0.2079, 0.2169, 0.2580, 0.2528, 0.2314, 0.3618,\n                        0.1744, 0.1624, 0.2405, 0.2127, 0.2014, 0.1710, 0.2021, 0.2286, 0.1726,\n                        0.1407, 0.2413, 0.3062, 0.2214, 0.2889, 0.1794, 0.1800, 0.2141, 0.2677,\n                        0.2255, 0.6823, 0.2259, 0.3130, 0.4412, 0.3432, 0.2284, 0.1865, 0.3768,\n                        0.3289, 0.2494, 0.2181, 0.1734, 0.3790, 0.2043, 0.1850, 0.2020, 0.2359,\n                        0.1775, 0.2314, 0.1754, 0.2175, 0.2157, 0.2686, 0.2354, 0.2096, 0.2275,\n                        0.1782, 0.1869, 0.2591, 0.1882, 0.2678, 0.3406, 0.1758, 0.2359, 0.2477,\n                        0.2886, 0.2143, 0.3062, 0.1567, 0.1947, 0.1694, 0.2209, 0.2266, 0.2033,\n                        0.2794, 0.2101, 0.2290, 0.2817, 0.1922, 0.2685, 0.1891, 0.3144, 0.1940,\n                        0.2055, 0.2068, 0.1909, 0.1698, 0.2255, 0.2308, 0.1961, 0.1754, 0.2621,\n                        0.1733, 0.1869, 0.2495, 0.1967, 0.2851, 0.1947, 0.2292, 0.2399, 0.1844,\n                        0.2656, 0.1829, 0.3584, 0.2139, 0.1593, 0.2556, 0.1550, 0.2467, 0.2626,\n                        0.2533, 0.2458, 0.1975, 0.2203, 0.1933, 0.3533, 0.3169, 0.2186, 0.2775,\n                        0.1729, 0.2055, 0.1783, 0.3160, 0.1752, 0.1645, 0.2601, 0.2606, 0.2913,\n                        0.1743, 0.1893, 0.3654, 0.2768, 0.2728, 0.3665, 0.1711, 0.1987, 0.4933,\n                        0.3041, 0.3127, 0.2485, 0.2979, 0.2031, 0.2458, 0.1524, 0.1983, 0.2827,\n                        0.2548, 0.3052, 0.2529, 0.2044, 0.2989, 0.4269, 0.2325, 0.2166, 0.1782,\n                        0.1934, 0.2243, 0.3291, 0.1754, 0.2925, 0.3555, 0.2437, 0.1935, 0.2710,\n                        0.2616, 0.1830, 0.5022, 0.1758, 0.2935, 0.2279, 0.2934, 0.2229, 0.3119,\n                        0.3546, 0.1775, 0.2224, 0.1694, 0.2118, 0.1713, 0.3890, 0.1995, 0.2675,\n                        0.1802, 0.2321, 0.1738, 0.3297, 0.1789, 0.1732, 0.2322, 0.2205, 0.2345,\n                        0.2148, 0.2229, 0.3348, 0.1750, 0.1993, 0.2353])\n              )\n            )\n          )\n        )\n      )\n      (2): Module(\n        (block): Module(\n          (0): Conv2d(\n            384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0005, 0.0016, 0.0025, 0.0026, 0.0028, 0.0030, 0.0008, 0.0029, 0.0019,\n                      0.0015, 0.0017, 0.0022, 0.0025, 0.0004, 0.0013, 0.0018, 0.0028, 0.0024,\n                      0.0017, 0.0025, 0.0006, 0.0028, 0.0031, 0.0028, 0.0022, 0.0015, 0.0014,\n                      0.0015, 0.0024, 0.0029, 0.0027, 0.0026, 0.0019, 0.0004, 0.0004, 0.0031,\n                      0.0025, 0.0021, 0.0005, 0.0023, 0.0014, 0.0028, 0.0021, 0.0028, 0.0017,\n                      0.0010, 0.0018, 0.0028, 0.0025, 0.0019, 0.0021, 0.0031, 0.0018, 0.0008,\n                      0.0027, 0.0016, 0.0011, 0.0010, 0.0024, 0.0019, 0.0014, 0.0008, 0.0014,\n                      0.0026, 0.0015, 0.0016, 0.0023, 0.0022, 0.0023, 0.0006, 0.0016, 0.0012,\n                      0.0020, 0.0024, 0.0017, 0.0019, 0.0023, 0.0016, 0.0029, 0.0010, 0.0028,\n                      0.0015, 0.0015, 0.0020, 0.0005, 0.0014, 0.0026, 0.0015, 0.0021, 0.0027,\n                      0.0021, 0.0027, 0.0014, 0.0024, 0.0013, 0.0029, 0.0027, 0.0018, 0.0005,\n                      0.0016, 0.0008, 0.0031, 0.0016, 0.0028, 0.0012, 0.0025, 0.0023, 0.0019,\n                      0.0004, 0.0026, 0.0020, 0.0006, 0.0015, 0.0026, 0.0022, 0.0011, 0.0027,\n                      0.0028, 0.0017, 0.0020, 0.0015, 0.0014, 0.0023, 0.0005, 0.0031, 0.0017,\n                      0.0014, 0.0015, 0.0025, 0.0014, 0.0016, 0.0027, 0.0019, 0.0019, 0.0026,\n                      0.0029, 0.0022, 0.0015, 0.0016, 0.0022, 0.0031, 0.0006, 0.0026, 0.0009,\n                      0.0024, 0.0023, 0.0031, 0.0016, 0.0004, 0.0026, 0.0012, 0.0017, 0.0020,\n                      0.0016, 0.0009, 0.0024, 0.0010, 0.0013, 0.0029, 0.0024, 0.0028, 0.0022,\n                      0.0028, 0.0018, 0.0026, 0.0018, 0.0027, 0.0028, 0.0004, 0.0024, 0.0011,\n                      0.0018, 0.0023, 0.0013, 0.0030, 0.0015, 0.0010, 0.0026, 0.0004, 0.0024,\n                      0.0014, 0.0028, 0.0028, 0.0024, 0.0024, 0.0026, 0.0010, 0.0005, 0.0024,\n                      0.0023, 0.0017, 0.0014, 0.0025, 0.0013, 0.0029, 0.0017, 0.0010, 0.0020,\n                      0.0028, 0.0026, 0.0028, 0.0024, 0.0017, 0.0028, 0.0026, 0.0031, 0.0023,\n                      0.0024, 0.0026, 0.0029, 0.0009, 0.0027, 0.0017, 0.0016, 0.0014, 0.0025,\n                      0.0024, 0.0016, 0.0005, 0.0025, 0.0028, 0.0013, 0.0024, 0.0011, 0.0018,\n                      0.0018, 0.0027, 0.0016, 0.0020, 0.0016, 0.0014, 0.0022, 0.0019, 0.0029,\n                      0.0027, 0.0012, 0.0025, 0.0024, 0.0017, 0.0028, 0.0016, 0.0016, 0.0022,\n                      0.0028, 0.0022, 0.0015, 0.0029, 0.0026, 0.0025, 0.0013, 0.0023, 0.0016,\n                      0.0021, 0.0017, 0.0016, 0.0022, 0.0026, 0.0019, 0.0010, 0.0004, 0.0025,\n                      0.0012, 0.0019, 0.0026, 0.0009, 0.0020, 0.0024, 0.0006, 0.0027, 0.0029,\n                      0.0016, 0.0011, 0.0026, 0.0017, 0.0011, 0.0018, 0.0027, 0.0029, 0.0011,\n                      0.0019, 0.0014, 0.0021, 0.0021, 0.0005, 0.0019, 0.0021, 0.0024, 0.0005,\n                      0.0007, 0.0021, 0.0018, 0.0025, 0.0026, 0.0014, 0.0028, 0.0011, 0.0014,\n                      0.0025, 0.0008, 0.0019, 0.0020, 0.0009, 0.0026, 0.0029, 0.0023, 0.0023,\n                      0.0024, 0.0017, 0.0021, 0.0027, 0.0021, 0.0012, 0.0016, 0.0032, 0.0022,\n                      0.0008, 0.0021, 0.0013, 0.0016, 0.0026, 0.0019, 0.0026, 0.0030, 0.0028,\n                      0.0030, 0.0024, 0.0013, 0.0017, 0.0030, 0.0028, 0.0008, 0.0028, 0.0004,\n                      0.0004, 0.0021, 0.0021, 0.0025, 0.0022, 0.0017, 0.0018, 0.0024, 0.0032,\n                      0.0019, 0.0019, 0.0011, 0.0027, 0.0023, 0.0021, 0.0005, 0.0021, 0.0028,\n                      0.0009, 0.0013, 0.0010, 0.0021, 0.0016, 0.0019, 0.0004, 0.0019, 0.0005,\n                      0.0021, 0.0026, 0.0021, 0.0020, 0.0021, 0.0029, 0.0004, 0.0015, 0.0027,\n                      0.0026, 0.0023, 0.0019, 0.0023, 0.0021, 0.0010, 0.0024, 0.0010, 0.0027,\n                      0.0018, 0.0020, 0.0017, 0.0011, 0.0026, 0.0017]), zero_point=tensor([-128,    0,    0,    0,    0,    0, -128,    0,    0,    0,    0,    0,\n                         0,  127,    0,    0,    0,    0,    0,    0,  127,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,  127,  127,    0,\n                         0,    0, -128,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,  127,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,  127,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                       127,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,  127,    0, -128,    0,    0,    0,    0,    0,    0,    0,\n                       127,    0,    0, -128,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,  127,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,  127,    0,  127,\n                         0,    0,    0,    0, -128,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                       127,    0,  127,    0,    0,    0,    0,    0,  127,    0,  127,    0,\n                         0,    0,    0,    0,    0,    0, -128,  127,    0,    0,    0,    0,\n                         0,    0,    0,    0,  127,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0, -128,    0,    0,    0,    0, -128,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0, -128,    0,    0,    0,    0,\n                         0,    0,    0, -128,    0,    0,    0, -128,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0, -128,    0,    0,    0,  127,\n                       127,    0,    0,    0,    0,    0,    0, -128,    0,    0, -128,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,  127,    0,  127, -128,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                      -128,    0,    0,    0,    0,    0,    0,    0,    0,  127,    0, -128,\n                         0,    0,    0,    0,    0,    0,  127,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0, -128,    0,    0],\n                     dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n                min_val=tensor([ 1.0128e-02, -1.7083e-01, -5.1749e-03, -3.3209e-02, -2.8741e-02,\n                        -3.7965e-01,  1.1189e-04, -7.1092e-02, -1.8211e-01, -1.8866e-01,\n                        -3.6189e-03, -2.7968e-01, -3.2172e-01, -1.0752e-01, -1.6807e-01,\n                        -2.3028e-01, -3.5982e-01, -3.0163e-01, -2.0798e-01, -8.5227e-03,\n                        -1.4344e-01, -3.6456e-01, -3.9204e-01, -3.5842e-01, -2.5025e-03,\n                        -1.7359e-01, -1.7467e-01, -1.7773e-01, -2.4528e-03, -2.5460e-02,\n                        -3.4701e-01, -3.3329e-01, -2.4870e-01, -1.1182e-01, -1.1096e-01,\n                        -1.9301e-02, -7.6678e-02, -2.7517e-01,  1.0864e-02, -2.9082e-01,\n                        -1.5875e-01, -3.6392e-01, -2.1905e-01, -3.5367e-01, -2.1392e-01,\n                        -1.2098e-02, -2.2896e-01, -3.6090e-01, -7.1715e-03, -2.4149e-01,\n                        -2.6600e-01, -3.9414e-01, -2.3173e-01, -1.0791e-01, -3.4914e-01,\n                        -2.0339e-01, -1.0964e-01, -2.5375e-01, -8.1200e-03, -2.4379e-01,\n                        -1.7562e-01, -7.8893e-02, -1.7427e-01, -1.1877e-02, -1.7828e-01,\n                        -2.0007e-01, -4.7034e-03, -2.8115e-01, -2.9217e-01, -7.9359e-02,\n                        -2.0869e-01, -2.9754e-01, -9.4504e-02, -3.0203e-01, -3.0840e-03,\n                        -2.3692e-01, -2.9718e-01, -1.9948e-01, -6.7600e-02, -4.1409e-04,\n                        -3.5613e-01, -1.8356e-01, -1.8725e-01, -1.5056e-01, -1.3508e-01,\n                        -1.7939e-01, -9.6485e-02, -1.9324e-01, -3.2706e-03, -9.0626e-03,\n                        -4.2742e-03, -3.6499e-02, -1.7675e-01, -6.0806e-02, -1.5559e-03,\n                        -3.6820e-01, -8.3134e-02, -2.3614e-01, -1.1683e-01, -2.1075e-01,\n                         3.8303e-04, -7.1207e-02, -2.0471e-01, -3.6357e-01, -7.3340e-03,\n                        -3.2343e-01, -2.9120e-01, -2.4934e-01, -9.9003e-02, -3.2649e-01,\n                        -2.5597e-01,  5.9212e-04, -1.7383e-01, -3.3612e-01, -2.7558e-01,\n                        -3.0865e-02, -9.5834e-02, -9.1144e-02, -1.8647e-03, -2.5754e-01,\n                        -2.3098e-03, -1.8365e-01, -2.6158e-03, -1.2057e-01, -3.9062e-01,\n                        -2.1242e-01, -1.8353e-01, -1.8592e-01, -3.8394e-03, -1.7540e-01,\n                        -2.0779e-01, -3.4361e-01, -2.3727e-01, -2.4586e-01, -1.0460e-02,\n                        -4.6191e-02, -2.7861e-01, -3.2669e-03, -2.0466e-01, -2.8477e-01,\n                        -3.9243e-01, -1.6079e-01, -3.3167e-01, -2.3453e-01, -8.2058e-04,\n                        -1.2600e-02, -2.8613e-02, -2.0610e-01,  1.4722e-02, -1.1788e-01,\n                        -1.5178e-01, -2.1190e-01, -2.5338e-01, -2.1050e-01, -1.1601e-01,\n                        -1.1204e-01, -1.0599e-01, -1.6044e-01, -3.6733e-01, -1.0282e-02,\n                        -1.1793e-02, -4.9420e-03, -3.3144e-03, -2.2130e-01, -1.3418e-01,\n                        -2.2813e-01, -8.0456e-02, -3.5905e-01, -9.2790e-02, -3.0465e-01,\n                        -2.7124e-01, -2.2602e-01, -2.9125e-01, -1.2502e-02, -3.8104e-01,\n                        -1.8344e-01, -2.6764e-01, -3.3327e-03, -1.0751e-01, -6.6366e-02,\n                        -1.6315e-01, -1.0733e-01, -3.5349e-01, -3.1139e-01, -3.0405e-01,\n                        -3.3851e-01,  4.4786e-04, -1.3237e-01, -2.4239e-01, -2.9603e-01,\n                        -2.0484e-01, -1.6490e-01, -9.9763e-03, -1.6317e-01, -3.7139e-01,\n                        -2.1207e-01, -2.5067e-01, -2.6176e-01, -7.6812e-02, -3.2745e-01,\n                        -4.2473e-02, -5.8893e-03, -3.6183e-03, -7.8501e-02, -1.1123e-02,\n                        -1.6460e-02, -2.9414e-01, -3.1876e-03, -3.2957e-01, -3.7028e-01,\n                        -1.1854e-01, -6.4615e-02, -2.1176e-01, -1.7422e-01, -1.7900e-01,\n                        -1.5712e-01, -3.0376e-01, -2.0463e-01,  8.0713e-03, -3.1761e-01,\n                        -3.5507e-01, -1.6527e-01, -4.4031e-02,  2.3623e-03, -2.3169e-01,\n                        -2.2758e-01, -3.4429e-01, -1.9908e-01, -2.5281e-01, -1.9919e-01,\n                        -1.6252e-01, -2.7939e-01, -1.0344e-01, -1.1875e-02, -3.4451e-01,\n                        -1.3863e-01, -3.1446e-01, -1.2642e-01, -2.0641e-01, -1.4299e-02,\n                        -1.9711e-01, -1.3602e-01, -2.7938e-01, -3.6247e-01, -2.8715e-01,\n                        -1.8816e-01, -3.7081e-01, -3.3544e-01, -3.2304e-01, -1.7182e-01,\n                        -5.7197e-03, -1.3548e-03, -2.7142e-01, -3.3373e-03, -8.4335e-03,\n                        -2.8571e-01, -1.5771e-02, -3.9191e-02, -1.7978e-02,  1.0007e-02,\n                        -3.8565e-02, -3.8418e-03, -2.3932e-01, -3.3446e-01, -1.2051e-01,\n                        -2.6154e-01, -3.0646e-01,  3.1514e-03, -3.4321e-01, -1.6674e-02,\n                        -2.0701e-01,  1.7748e-03, -3.3810e-01, -1.9744e-01, -1.4384e-01,\n                        -2.0799e-01, -9.2234e-03, -1.1868e-01, -1.4421e-01, -2.3932e-01,\n                        -1.2907e-01, -1.4384e-01, -8.1087e-03,  3.8149e-03, -1.4511e-04,\n                        -1.1622e-01, -3.0733e-01, -1.1806e-01, -1.8033e-01, -1.6863e-03,\n                        -2.3326e-01, -3.2532e-01, -3.3089e-01, -1.7186e-01, -3.6231e-01,\n                         6.2916e-04, -8.3620e-03, -3.2468e-01,  1.1840e-03, -2.4139e-01,\n                        -2.5215e-01, -1.3602e-03, -3.3287e-01, -1.2121e-01, -3.0064e-01,\n                        -1.5222e-03, -3.0578e-01, -1.5677e-02, -2.7184e-01, -3.4998e-01,\n                        -2.6533e-01, -1.4290e-01, -1.1861e-02, -1.5131e-02, -7.5736e-03,\n                        -1.0342e-01, -2.6714e-01, -7.4860e-02, -1.9331e-01, -3.3344e-01,\n                        -2.4147e-01, -3.2796e-01, -3.3410e-02, -9.6731e-02, -1.2938e-02,\n                        -1.8849e-01, -1.6364e-01, -2.1655e-01, -7.6040e-03, -5.3532e-02,\n                        -2.0420e-01, -3.6156e-01, -1.1425e-01,  1.5658e-02, -2.6699e-01,\n                        -2.7466e-01, -3.2603e-01, -2.8258e-01, -2.0175e-01, -2.3671e-01,\n                        -3.0659e-01, -7.4939e-02, -2.3786e-01, -2.4064e-01, -1.3827e-01,\n                        -5.6916e-02, -2.9162e-01, -8.2880e-03,  3.8369e-03, -2.7367e-01,\n                        -1.8065e-02, -7.3186e-03, -1.6417e-01, -8.6128e-02, -2.5575e-03,\n                        -2.0848e-01, -2.4915e-01, -1.0526e-01, -2.4618e-01,  6.8525e-03,\n                        -2.6846e-01, -3.3102e-01, -2.2607e-01, -2.5108e-01, -2.7145e-01,\n                        -1.4678e-02, -1.0439e-01, -1.9402e-01, -8.5844e-02, -9.5817e-02,\n                        -2.8953e-01, -1.2150e-02, -1.1421e-02, -2.6519e-01, -7.1741e-03,\n                        -4.5524e-02, -1.3303e-01, -6.4749e-02, -1.4762e-01, -2.5659e-01,\n                        -1.9861e-01,  4.3087e-04, -3.2854e-01, -2.2228e-01]), max_val=tensor([ 1.3373e-01,  2.0333e-01,  3.1263e-01,  3.3457e-01,  3.5147e-01,\n                         1.6854e-02,  1.9665e-01,  3.6452e-01,  2.3872e-01,  1.8738e-01,\n                         2.1558e-01,  2.0991e-01,  9.2843e-02, -9.0704e-03,  1.5668e-02,\n                         8.3737e-03,  3.3597e-02,  8.5220e-03,  2.1150e-01,  3.1443e-01,\n                        -2.2660e-02,  1.4774e-02,  2.8438e-02,  1.1421e-01,  2.7696e-01,\n                         1.8942e-01,  1.7745e-01,  1.8787e-01,  3.0417e-01,  3.7287e-01,\n                         3.3338e-02,  8.0479e-03,  6.8772e-03, -1.0052e-02, -1.3967e-02,\n                         3.8789e-01,  3.1567e-01,  9.6154e-02,  1.1513e-01,  4.7428e-03,\n                         1.8392e-01,  2.0937e-02,  2.6073e-01,  1.1961e-01,  1.5330e-01,\n                         1.3179e-01,  2.2657e-01,  1.2180e-02,  3.2011e-01,  1.9777e-01,\n                         4.4988e-02,  2.8122e-02,  2.7332e-03,  1.6212e-02,  1.3551e-02,\n                         1.9616e-01,  1.4314e-01, -2.3449e-03,  3.0973e-01,  3.9239e-04,\n                         1.8237e-01,  9.8719e-02,  1.6412e-01,  3.2586e-01,  1.9479e-01,\n                         1.1451e-01,  2.9726e-01,  2.3881e-01,  7.3265e-02,  3.3207e-03,\n                         1.4954e-01, -2.9030e-05,  2.4909e-01,  4.1993e-03,  2.1783e-01,\n                         1.4985e-02,  1.6830e-01,  1.9858e-01,  3.6900e-01,  1.2182e-01,\n                         4.6051e-02,  1.9342e-01,  1.8658e-01,  2.6012e-01, -9.5605e-03,\n                         1.3019e-01,  3.3256e-01,  4.1687e-02,  2.7138e-01,  3.4325e-01,\n                         2.7213e-01,  3.4502e-01,  1.7352e-01,  3.0167e-01,  1.6083e-01,\n                         2.5044e-02,  3.3866e-01,  9.5364e-02, -1.1618e-02,  2.0888e-01,\n                         2.0837e-01,  3.9285e-01,  1.5791e-01,  1.2505e-02,  1.5604e-01,\n                         6.1619e-03,  4.4190e-04,  2.4115e-01, -1.1950e-02,  5.3149e-02,\n                         6.1492e-03,  1.6286e-01,  1.9565e-01,  6.8068e-03,  2.3233e-01,\n                         1.4183e-01,  3.4076e-01,  3.5816e-01,  2.1147e-01,  1.3058e-02,\n                         1.9259e-01,  1.5258e-01,  2.9602e-01, -7.5600e-03,  1.9093e-02,\n                         1.9127e-01,  1.7891e-01,  5.8869e-03,  3.1228e-01,  1.6364e-01,\n                         1.9824e-01,  5.1391e-02,  1.8277e-01,  1.6084e-01,  3.2391e-01,\n                         3.6423e-01,  9.3883e-03,  1.8432e-01,  1.9290e-01,  9.7800e-02,\n                         2.8867e-02, -3.1649e-04,  1.4469e-01, -2.0986e-03,  3.0711e-01,\n                         2.9535e-01,  3.8988e-01,  3.0171e-03,  9.0287e-02,  3.3413e-01,\n                         6.1967e-02,  1.8110e-03,  2.1651e-01,  7.4688e-02,  8.5390e-03,\n                         3.1104e-01,  1.2376e-01,  1.1241e-02,  1.9995e-02,  3.0868e-01,\n                         3.6150e-01,  2.8427e-01,  3.5141e-01,  2.3296e-01,  3.2625e-01,\n                         1.4826e-01,  3.3708e-01,  9.2339e-02, -1.2245e-02,  5.4518e-03,\n                        -1.2991e-03,  4.0221e-02,  7.9790e-03,  1.6581e-01,  2.4849e-02,\n                         1.8742e-01, -7.9705e-04,  3.2413e-01, -1.6544e-02,  3.0757e-01,\n                         1.7712e-01,  3.5877e-01,  6.0675e-02,  8.1864e-02,  4.3202e-02,\n                         2.2578e-01,  2.5316e-01, -1.2393e-02,  3.0129e-01,  1.6596e-01,\n                         2.1175e-01,  1.7716e-01,  3.2149e-01,  1.6868e-01,  1.0774e-02,\n                         1.5345e-01, -7.9657e-04,  1.0055e-02,  3.5890e-01,  1.4573e-01,\n                         3.6085e-01,  3.0170e-01,  2.1669e-01,  3.5263e-01,  3.2965e-01,\n                         3.8809e-01,  5.4296e-03,  3.1033e-01,  1.0538e-02,  5.4535e-02,\n                         3.0967e-03,  3.4022e-01,  1.8002e-01,  1.9726e-01,  1.7472e-01,\n                         3.1780e-01,  3.4762e-02,  1.5875e-01,  1.2773e-01,  1.3784e-01,\n                         1.0640e-02,  1.5967e-01,  3.0077e-01,  2.7231e-01,  1.1830e-01,\n                         1.5067e-01,  3.1603e-02,  1.8346e-01,  8.9555e-03,  1.2308e-01,\n                         1.8309e-01,  6.4849e-03,  2.4439e-01,  3.6400e-01,  1.2153e-01,\n                         1.5467e-01,  5.4725e-03,  2.9865e-01,  2.1156e-01,  3.5599e-01,\n                         2.0532e-01,  2.0782e-01,  3.9739e-03,  9.6960e-02,  8.9927e-02,\n                         8.6566e-02,  2.1703e-02,  4.4309e-02,  1.2081e-01,  1.4231e-01,\n                         2.9746e-01,  2.0429e-01,  1.3654e-03,  2.2064e-01,  2.0748e-01,\n                         7.5688e-03,  3.3617e-01,  2.4679e-01,  1.2292e-01,  1.1087e-01,\n                         3.2128e-01,  1.5538e-01,  2.7780e-03,  1.4121e-02,  1.1968e-01,\n                         1.8415e-01,  3.3374e-02,  1.6372e-01,  1.4929e-02,  3.6504e-01,\n                         4.0452e-02,  2.7760e-01,  4.6495e-02,  2.1612e-01,  3.0001e-03,\n                         2.2252e-01,  3.4580e-01,  3.6207e-01,  1.0521e-01,  7.2353e-03,\n                         1.8053e-01,  2.6625e-01,  2.6958e-01,  1.3569e-01,  2.4581e-01,\n                         2.6193e-01,  2.3919e-03, -8.1476e-03, -1.7279e-03,  2.6307e-01,\n                         1.9964e-01,  9.1038e-02,  6.9912e-03,  1.7455e-01,  8.5323e-02,\n                         2.7826e-01,  1.8200e-01,  5.0076e-03,  1.9611e-01,  8.4813e-04,\n                         2.1721e-03,  1.1025e-01,  9.8064e-02,  3.6984e-01,  5.0990e-03,\n                         2.8980e-01,  7.6067e-03,  2.1968e-01,  2.2724e-03,  2.5660e-02,\n                         3.1605e-02,  1.5579e-01,  2.0644e-01,  4.0082e-01,  2.7815e-01,\n                         1.8708e-03,  1.0665e-01,  1.6973e-01,  2.0374e-01,  2.6531e-02,\n                         2.3224e-01,  7.0430e-03,  3.8111e-01,  3.4945e-01,  3.8523e-01,\n                         3.0883e-01,  1.4073e-01,  1.9397e-01,  3.8025e-01,  3.5294e-01,\n                        -2.5023e-03,  6.3748e-02, -1.6485e-02,  1.1265e-01,  7.6573e-03,\n                         1.3255e-01,  1.2733e-01,  7.5005e-03,  2.1293e-01,  1.1784e-01,\n                         1.2445e-03,  4.0420e-01,  2.0022e-03,  9.1667e-02,  9.1674e-02,\n                         3.4732e-01,  8.6724e-02,  2.6710e-01,  1.3270e-01,  4.6081e-03,\n                         3.5277e-01,  1.1026e-01,  1.6352e-01,  1.2961e-01,  2.6641e-01,\n                         1.8311e-03,  2.2100e-01, -1.7745e-03,  1.6596e-01,  1.2507e-01,\n                         4.6569e-03,  9.5549e-03,  2.6856e-01,  5.8332e-03,  2.3208e-01,\n                         3.6213e-01, -1.0066e-02,  1.9329e-01,  3.4733e-01,  3.3043e-01,\n                         4.3437e-03,  2.4303e-01,  2.9498e-01,  2.4520e-01,  1.2807e-01,\n                         3.0515e-01,  5.1561e-03,  3.4273e-01,  2.3458e-01,  2.4535e-01,\n                         2.2049e-01,  2.8205e-01,  8.0518e-02,  1.9751e-01])\n              )\n            )\n          )\n          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(\n            in_features=384, out_features=1536, bias=True\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0014, 0.0019, 0.0013,  ..., 0.0029, 0.0015, 0.0012]), zero_point=tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([-0.1855, -0.2466, -0.1711,  ..., -0.3719, -0.1612, -0.1571]), max_val=tensor([0.1655, 0.1569, 0.1631,  ..., 0.1356, 0.1847, 0.1570]))\n            )\n          )\n          (4): GELU(approximate='none')\n          (5): Linear(\n            in_features=1536, out_features=384, bias=True\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0015, 0.0025, 0.0017, 0.0018, 0.0014, 0.0016, 0.0018, 0.0021, 0.0018,\n                      0.0029, 0.0017, 0.0017, 0.0018, 0.0027, 0.0015, 0.0017, 0.0017, 0.0017,\n                      0.0016, 0.0015, 0.0025, 0.0019, 0.0020, 0.0022, 0.0020, 0.0024, 0.0023,\n                      0.0032, 0.0021, 0.0026, 0.0023, 0.0028, 0.0023, 0.0017, 0.0024, 0.0016,\n                      0.0017, 0.0021, 0.0020, 0.0020, 0.0025, 0.0030, 0.0018, 0.0023, 0.0017,\n                      0.0021, 0.0041, 0.0022, 0.0021, 0.0019, 0.0018, 0.0034, 0.0015, 0.0025,\n                      0.0016, 0.0027, 0.0032, 0.0014, 0.0023, 0.0022, 0.0019, 0.0018, 0.0018,\n                      0.0014, 0.0015, 0.0023, 0.0020, 0.0022, 0.0014, 0.0019, 0.0016, 0.0016,\n                      0.0023, 0.0025, 0.0019, 0.0016, 0.0020, 0.0034, 0.0015, 0.0018, 0.0018,\n                      0.0018, 0.0023, 0.0036, 0.0020, 0.0021, 0.0022, 0.0017, 0.0021, 0.0014,\n                      0.0019, 0.0019, 0.0022, 0.0024, 0.0017, 0.0019, 0.0033, 0.0018, 0.0014,\n                      0.0032, 0.0015, 0.0016, 0.0026, 0.0016, 0.0027, 0.0016, 0.0019, 0.0028,\n                      0.0016, 0.0046, 0.0016, 0.0017, 0.0039, 0.0028, 0.0020, 0.0019, 0.0016,\n                      0.0015, 0.0020, 0.0025, 0.0016, 0.0021, 0.0017, 0.0041, 0.0023, 0.0025,\n                      0.0026, 0.0018, 0.0015, 0.0041, 0.0035, 0.0015, 0.0034, 0.0022, 0.0020,\n                      0.0023, 0.0018, 0.0017, 0.0039, 0.0016, 0.0018, 0.0018, 0.0024, 0.0017,\n                      0.0023, 0.0018, 0.0019, 0.0015, 0.0016, 0.0024, 0.0020, 0.0023, 0.0014,\n                      0.0020, 0.0018, 0.0017, 0.0037, 0.0022, 0.0020, 0.0015, 0.0018, 0.0017,\n                      0.0015, 0.0046, 0.0025, 0.0018, 0.0015, 0.0015, 0.0015, 0.0015, 0.0020,\n                      0.0016, 0.0018, 0.0015, 0.0016, 0.0036, 0.0015, 0.0028, 0.0017, 0.0024,\n                      0.0026, 0.0022, 0.0060, 0.0026, 0.0016, 0.0028, 0.0025, 0.0018, 0.0026,\n                      0.0028, 0.0031, 0.0035, 0.0016, 0.0025, 0.0016, 0.0045, 0.0018, 0.0029,\n                      0.0015, 0.0020, 0.0021, 0.0019, 0.0020, 0.0021, 0.0017, 0.0018, 0.0015,\n                      0.0015, 0.0018, 0.0035, 0.0020, 0.0022, 0.0023, 0.0017, 0.0023, 0.0019,\n                      0.0033, 0.0041, 0.0016, 0.0023, 0.0024, 0.0030, 0.0018, 0.0018, 0.0018,\n                      0.0018, 0.0022, 0.0020, 0.0015, 0.0021, 0.0019, 0.0021, 0.0025, 0.0015,\n                      0.0016, 0.0016, 0.0022, 0.0024, 0.0023, 0.0025, 0.0027, 0.0018, 0.0017,\n                      0.0018, 0.0019, 0.0030, 0.0020, 0.0032, 0.0024, 0.0015, 0.0023, 0.0025,\n                      0.0027, 0.0026, 0.0034, 0.0021, 0.0015, 0.0020, 0.0016, 0.0022, 0.0023,\n                      0.0021, 0.0017, 0.0017, 0.0041, 0.0018, 0.0030, 0.0017, 0.0028, 0.0018,\n                      0.0016, 0.0018, 0.0022, 0.0031, 0.0021, 0.0020, 0.0017, 0.0015, 0.0023,\n                      0.0013, 0.0018, 0.0030, 0.0014, 0.0041, 0.0020, 0.0022, 0.0021, 0.0015,\n                      0.0017, 0.0016, 0.0022, 0.0028, 0.0018, 0.0034, 0.0017, 0.0029, 0.0013,\n                      0.0030, 0.0019, 0.0022, 0.0022, 0.0023, 0.0018, 0.0018, 0.0018, 0.0021,\n                      0.0017, 0.0022, 0.0019, 0.0028, 0.0020, 0.0022, 0.0017, 0.0023, 0.0033,\n                      0.0023, 0.0016, 0.0016, 0.0040, 0.0021, 0.0029, 0.0015, 0.0020, 0.0024,\n                      0.0014, 0.0021, 0.0029, 0.0016, 0.0014, 0.0017, 0.0018, 0.0017, 0.0021,\n                      0.0024, 0.0021, 0.0020, 0.0023, 0.0025, 0.0030, 0.0024, 0.0020, 0.0017,\n                      0.0020, 0.0023, 0.0025, 0.0016, 0.0018, 0.0021, 0.0023, 0.0018, 0.0018,\n                      0.0022, 0.0019, 0.0025, 0.0015, 0.0024, 0.0027, 0.0014, 0.0022, 0.0024,\n                      0.0024, 0.0021, 0.0034, 0.0014, 0.0027, 0.0016, 0.0027, 0.0016, 0.0021,\n                      0.0016, 0.0018, 0.0018, 0.0020, 0.0017, 0.0017, 0.0019, 0.0017, 0.0030,\n                      0.0016, 0.0019, 0.0021, 0.0020, 0.0017, 0.0017]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                     dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n                min_val=tensor([-0.1931, -0.3148, -0.1513, -0.2345, -0.1740, -0.1738, -0.1927, -0.1776,\n                        -0.1791, -0.2704, -0.1628, -0.2177, -0.2260, -0.3238, -0.1904, -0.2190,\n                        -0.1948, -0.2169, -0.1964, -0.1940, -0.3101, -0.2468, -0.2524, -0.2370,\n                        -0.2520, -0.2536, -0.2962, -0.4142, -0.2672, -0.3360, -0.2964, -0.2898,\n                        -0.2012, -0.2149, -0.3131, -0.1799, -0.2226, -0.2639, -0.1861, -0.2622,\n                        -0.3197, -0.3016, -0.1785, -0.2913, -0.2143, -0.2381, -0.4517, -0.2734,\n                        -0.1746, -0.2410, -0.1868, -0.4398, -0.1833, -0.2609, -0.1812, -0.3073,\n                        -0.4110, -0.1833, -0.2901, -0.1824, -0.2435, -0.2273, -0.2147, -0.1557,\n                        -0.1965, -0.2956, -0.2555, -0.2207, -0.1660, -0.2389, -0.1990, -0.1899,\n                        -0.2027, -0.3132, -0.1674, -0.1974, -0.2557, -0.4096, -0.1893, -0.2168,\n                        -0.2260, -0.2293, -0.2976, -0.4671, -0.2265, -0.2663, -0.2797, -0.2155,\n                        -0.2224, -0.1643, -0.2307, -0.2393, -0.1964, -0.3031, -0.1954, -0.2346,\n                        -0.2789, -0.1862, -0.1741, -0.4086, -0.1863, -0.2042, -0.3327, -0.2095,\n                        -0.3204, -0.2023, -0.1691, -0.3547, -0.1777, -0.3838, -0.1672, -0.1714,\n                        -0.4979, -0.2898, -0.2532, -0.2075, -0.1993, -0.1934, -0.1749, -0.2125,\n                        -0.1758, -0.1645, -0.2135, -0.3969, -0.3008, -0.2787, -0.3361, -0.2301,\n                        -0.1780, -0.4646, -0.4510, -0.1959, -0.4359, -0.2826, -0.2542, -0.2597,\n                        -0.2252, -0.2056, -0.3399, -0.1903, -0.2358, -0.2099, -0.3078, -0.2055,\n                        -0.2451, -0.2348, -0.2338, -0.1857, -0.1623, -0.3092, -0.2237, -0.2644,\n                        -0.1840, -0.2500, -0.2287, -0.2210, -0.4799, -0.2068, -0.2459, -0.1908,\n                        -0.1947, -0.2162, -0.1705, -0.5256, -0.2024, -0.2350, -0.1969, -0.1893,\n                        -0.1716, -0.1859, -0.1626, -0.1981, -0.2360, -0.1944, -0.2058, -0.4552,\n                        -0.1773, -0.1804, -0.1844, -0.2746, -0.3077, -0.2864, -0.6452, -0.3288,\n                        -0.1582, -0.2495, -0.3171, -0.2287, -0.3361, -0.2820, -0.3951, -0.4543,\n                        -0.2022, -0.3168, -0.2079, -0.4971, -0.2141, -0.3734, -0.1940, -0.2574,\n                        -0.2718, -0.2459, -0.2527, -0.2717, -0.1933, -0.2298, -0.1630, -0.1978,\n                        -0.2283, -0.2553, -0.1887, -0.2462, -0.2883, -0.2220, -0.2886, -0.2406,\n                        -0.3379, -0.5291, -0.2048, -0.2432, -0.3049, -0.3839, -0.2253, -0.2209,\n                        -0.1644, -0.2258, -0.2430, -0.2553, -0.1876, -0.1607, -0.2438, -0.2169,\n                        -0.2147, -0.1950, -0.1619, -0.2019, -0.2776, -0.2803, -0.2979, -0.3211,\n                        -0.3478, -0.2341, -0.1678, -0.1958, -0.2398, -0.3287, -0.2210, -0.4075,\n                        -0.1908, -0.1924, -0.2879, -0.3141, -0.1786, -0.3385, -0.2092, -0.2190,\n                        -0.1938, -0.2457, -0.2038, -0.1923, -0.1669, -0.2672, -0.2102, -0.2118,\n                        -0.5187, -0.2310, -0.3795, -0.2183, -0.3595, -0.2363, -0.2101, -0.2241,\n                        -0.1751, -0.3974, -0.2636, -0.2184, -0.1839, -0.1915, -0.2015, -0.1685,\n                        -0.2249, -0.3711, -0.1770, -0.3922, -0.2541, -0.2366, -0.2578, -0.1547,\n                        -0.1951, -0.1658, -0.2319, -0.3116, -0.2113, -0.3317, -0.2144, -0.3769,\n                        -0.1551, -0.3871, -0.2483, -0.2752, -0.1671, -0.2906, -0.2356, -0.1847,\n                        -0.2281, -0.2707, -0.2134, -0.1958, -0.2410, -0.3561, -0.2608, -0.1996,\n                        -0.1717, -0.2938, -0.4165, -0.2142, -0.1989, -0.2092, -0.5120, -0.2143,\n                        -0.3517, -0.1945, -0.1727, -0.1870, -0.1805, -0.2624, -0.3745, -0.1923,\n                        -0.1697, -0.2111, -0.2331, -0.2190, -0.2746, -0.3013, -0.2653, -0.2594,\n                        -0.2904, -0.3151, -0.3387, -0.3080, -0.2004, -0.1646, -0.2530, -0.2687,\n                        -0.3166, -0.1588, -0.2157, -0.2660, -0.2894, -0.2332, -0.2364, -0.2789,\n                        -0.2442, -0.3167, -0.1866, -0.2649, -0.3452, -0.1763, -0.2597, -0.2416,\n                        -0.3015, -0.2432, -0.2048, -0.1774, -0.3496, -0.1837, -0.3417, -0.2004,\n                        -0.2665, -0.2109, -0.2082, -0.2042, -0.2342, -0.2154, -0.1688, -0.2481,\n                        -0.1958, -0.3858, -0.2081, -0.2277, -0.2643, -0.2503, -0.1660, -0.2122]), max_val=tensor([0.1641, 0.3054, 0.2124, 0.2170, 0.1683, 0.2095, 0.2242, 0.2605, 0.2240,\n                        0.3673, 0.2153, 0.2058, 0.2145, 0.3492, 0.1809, 0.1947, 0.2117, 0.1702,\n                        0.2054, 0.1766, 0.3114, 0.2422, 0.2594, 0.2734, 0.1916, 0.3063, 0.2574,\n                        0.3329, 0.2536, 0.2285, 0.2177, 0.3534, 0.2963, 0.1877, 0.2051, 0.1989,\n                        0.2164, 0.2217, 0.2550, 0.1487, 0.2852, 0.3834, 0.2327, 0.2774, 0.1967,\n                        0.2649, 0.5252, 0.2762, 0.2636, 0.1723, 0.2254, 0.3308, 0.1863, 0.3156,\n                        0.1973, 0.3454, 0.3638, 0.1669, 0.2649, 0.2741, 0.2230, 0.2196, 0.2295,\n                        0.1803, 0.1948, 0.2725, 0.2116, 0.2855, 0.1829, 0.2065, 0.1926, 0.1992,\n                        0.2908, 0.3175, 0.2435, 0.1980, 0.2437, 0.4307, 0.1840, 0.2324, 0.2111,\n                        0.1558, 0.2340, 0.4624, 0.2585, 0.2071, 0.2208, 0.1792, 0.2626, 0.1793,\n                        0.2435, 0.1600, 0.2771, 0.2694, 0.2130, 0.2412, 0.4147, 0.2279, 0.1767,\n                        0.2400, 0.1847, 0.1960, 0.1927, 0.1737, 0.3400, 0.1822, 0.2433, 0.2581,\n                        0.2090, 0.5822, 0.2064, 0.2134, 0.4521, 0.3565, 0.2128, 0.2387, 0.1742,\n                        0.1817, 0.2534, 0.3116, 0.1974, 0.2684, 0.1764, 0.5237, 0.2435, 0.3136,\n                        0.2661, 0.2089, 0.1885, 0.5205, 0.3199, 0.1536, 0.3836, 0.2842, 0.1886,\n                        0.2961, 0.1829, 0.2216, 0.4894, 0.1983, 0.1752, 0.2283, 0.2643, 0.2210,\n                        0.2880, 0.1891, 0.2350, 0.1648, 0.1991, 0.3064, 0.2481, 0.2861, 0.1630,\n                        0.1851, 0.1940, 0.2155, 0.3028, 0.2828, 0.2484, 0.1842, 0.2295, 0.1566,\n                        0.1922, 0.5792, 0.3188, 0.1843, 0.1776, 0.1604, 0.1964, 0.1562, 0.2524,\n                        0.1996, 0.1772, 0.1835, 0.1829, 0.4333, 0.1860, 0.3539, 0.2166, 0.3078,\n                        0.3273, 0.2391, 0.7565, 0.2410, 0.2032, 0.3517, 0.2408, 0.2186, 0.1782,\n                        0.3504, 0.3858, 0.3904, 0.1726, 0.2090, 0.1929, 0.5735, 0.2308, 0.3310,\n                        0.1785, 0.2201, 0.2701, 0.1779, 0.1720, 0.2548, 0.2139, 0.2093, 0.1873,\n                        0.1722, 0.1757, 0.4424, 0.2477, 0.2793, 0.2154, 0.1795, 0.2585, 0.2232,\n                        0.4169, 0.5113, 0.1645, 0.2921, 0.2476, 0.3034, 0.2261, 0.2338, 0.2336,\n                        0.2035, 0.2743, 0.2429, 0.1699, 0.2727, 0.2445, 0.2619, 0.3201, 0.1558,\n                        0.2071, 0.1920, 0.2424, 0.3058, 0.2969, 0.2512, 0.3364, 0.2316, 0.2155,\n                        0.2257, 0.2337, 0.3792, 0.2545, 0.3679, 0.3059, 0.1413, 0.2951, 0.2698,\n                        0.3410, 0.3287, 0.4296, 0.2679, 0.1918, 0.2525, 0.1783, 0.2782, 0.2984,\n                        0.2419, 0.2103, 0.2035, 0.4336, 0.2311, 0.3279, 0.1742, 0.3602, 0.2196,\n                        0.1817, 0.2067, 0.2745, 0.2583, 0.2145, 0.2492, 0.2150, 0.1606, 0.2869,\n                        0.1654, 0.2047, 0.3754, 0.1506, 0.5240, 0.2351, 0.2754, 0.2679, 0.1855,\n                        0.2170, 0.2089, 0.2796, 0.3493, 0.2272, 0.4293, 0.1542, 0.1686, 0.1619,\n                        0.3528, 0.1912, 0.1895, 0.2731, 0.2659, 0.1983, 0.2247, 0.2156, 0.2128,\n                        0.2198, 0.2793, 0.2397, 0.3060, 0.2024, 0.2741, 0.2159, 0.2520, 0.2316,\n                        0.2899, 0.1934, 0.2038, 0.4867, 0.2633, 0.3646, 0.1888, 0.2485, 0.3017,\n                        0.1735, 0.2118, 0.2862, 0.1991, 0.1838, 0.2200, 0.1707, 0.1533, 0.1926,\n                        0.2315, 0.2507, 0.2157, 0.2069, 0.2133, 0.3819, 0.2163, 0.2556, 0.2179,\n                        0.1755, 0.2876, 0.3079, 0.2085, 0.2318, 0.2335, 0.2737, 0.2336, 0.2290,\n                        0.2183, 0.2029, 0.2407, 0.1643, 0.3061, 0.1845, 0.1651, 0.2788, 0.3077,\n                        0.2971, 0.2721, 0.4297, 0.1530, 0.2527, 0.2035, 0.2670, 0.1711, 0.1858,\n                        0.1890, 0.2286, 0.2254, 0.2594, 0.1988, 0.2206, 0.1986, 0.2121, 0.3451,\n                        0.2064, 0.2360, 0.1996, 0.2200, 0.2162, 0.2032])\n              )\n            )\n          )\n        )\n      )\n      (3): Module(\n        (block): Module(\n          (0): Conv2d(\n            384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.7040e-03, 1.6872e-03, 2.6542e-03, 2.3178e-03, 2.7051e-03, 2.5962e-03,\n                      2.0597e-03, 2.5098e-03, 2.2642e-03, 2.2753e-03, 1.9503e-03, 2.8474e-03,\n                      2.2246e-03, 2.7912e-04, 1.2677e-03, 2.4275e-03, 2.1431e-03, 2.6873e-03,\n                      2.2899e-03, 2.7833e-03, 2.5947e-03, 2.0163e-03, 2.3419e-03, 2.3624e-03,\n                      2.1970e-03, 2.1787e-03, 1.8672e-03, 1.9497e-03, 2.6721e-03, 3.3489e-03,\n                      2.5005e-03, 2.2395e-03, 2.5555e-03, 1.8729e-03, 1.9665e-04, 2.6273e-03,\n                      2.1143e-03, 2.7003e-03, 1.2059e-03, 2.6010e-03, 1.6806e-03, 1.9856e-03,\n                      1.9685e-03, 2.3130e-03, 1.9463e-03, 1.4972e-03, 2.4842e-03, 2.6491e-03,\n                      2.4374e-03, 1.3943e-03, 2.2528e-03, 2.7750e-03, 2.4428e-03, 7.4043e-04,\n                      2.0347e-03, 1.1096e-03, 4.2075e-04, 2.4406e-03, 2.3666e-03, 2.3344e-03,\n                      1.9662e-03, 2.7248e-03, 1.8412e-03, 2.6456e-03, 2.4793e-03, 1.1975e-03,\n                      2.0681e-03, 2.4859e-03, 2.5113e-03, 8.1203e-04, 2.2966e-03, 2.4622e-03,\n                      1.2911e-03, 2.0436e-03, 2.3979e-03, 1.8785e-03, 2.6593e-03, 1.9504e-03,\n                      2.2965e-03, 1.8355e-03, 2.2049e-03, 1.5619e-03, 2.0999e-03, 2.1812e-03,\n                      1.3807e-03, 2.2210e-03, 2.2779e-03, 1.1932e-03, 1.9784e-03, 2.5853e-03,\n                      2.1733e-03, 2.1466e-03, 2.5650e-03, 1.9084e-03, 2.0431e-03, 1.9612e-03,\n                      2.1410e-03, 1.8136e-03, 1.4913e-03, 1.8274e-03, 2.4347e-03, 2.3254e-03,\n                      2.2297e-03, 2.4919e-03, 8.4549e-04, 1.9762e-03, 2.4610e-03, 1.5260e-03,\n                      1.1055e-03, 2.2581e-03, 2.6095e-03, 1.6528e-03, 2.2704e-03, 2.1574e-03,\n                      2.7202e-03, 2.1612e-03, 2.6307e-03, 2.7245e-03, 2.0891e-03, 1.8953e-03,\n                      1.9765e-03, 1.9727e-03, 2.7595e-03, 1.8389e-03, 1.9094e-03, 2.3404e-03,\n                      1.8695e-03, 2.0533e-03, 2.5761e-03, 2.6618e-03, 2.6931e-03, 2.0663e-03,\n                      2.1907e-03, 1.7486e-03, 2.3100e-03, 2.6017e-03, 2.4988e-03, 2.4826e-03,\n                      2.2342e-03, 1.8989e-03, 2.3774e-03, 2.1427e-03, 2.6272e-03, 1.9034e-03,\n                      2.1069e-03, 2.5853e-03, 2.9455e-03, 1.6973e-03, 1.2149e-03, 2.4186e-03,\n                      8.1958e-04, 2.9094e-04, 2.4188e-03, 2.2853e-03, 2.0110e-03, 2.3503e-03,\n                      4.4061e-05, 2.3646e-03, 2.3327e-03, 2.6790e-03, 2.7266e-03, 2.1580e-03,\n                      2.3532e-03, 2.7949e-03, 2.0135e-03, 2.2761e-03, 2.6360e-03, 2.9707e-03,\n                      1.0106e-03, 2.6919e-03, 2.5861e-03, 2.2744e-03, 1.9120e-03, 1.6134e-03,\n                      2.4282e-03, 2.2013e-03, 2.7434e-03, 2.2920e-03, 6.8335e-04, 2.5145e-03,\n                      1.9015e-03, 2.2927e-03, 9.1686e-04, 1.4631e-03, 2.6172e-03, 2.5539e-03,\n                      2.1283e-03, 8.2727e-04, 2.7902e-03, 1.8837e-03, 2.7202e-03, 2.2076e-03,\n                      2.5230e-03, 1.8474e-03, 2.8073e-03, 2.2245e-03, 2.4982e-03, 2.2076e-03,\n                      2.2849e-03, 2.2333e-03, 2.0483e-03, 2.6254e-03, 2.4963e-03, 2.3935e-03,\n                      2.7494e-03, 2.2609e-03, 2.5440e-03, 2.4337e-03, 2.0719e-03, 2.1863e-03,\n                      8.3458e-04, 2.4108e-03, 1.3979e-03, 2.6025e-03, 2.2789e-03, 2.7095e-03,\n                      2.3052e-03, 2.5496e-03, 1.7825e-03, 2.4849e-03, 2.1597e-03, 2.0487e-03,\n                      1.4771e-03, 2.2124e-03, 2.6423e-03, 2.3856e-03, 2.6410e-03, 2.4661e-03,\n                      2.4264e-03, 1.6796e-04, 2.3272e-03, 2.6431e-03, 2.1496e-03, 2.2301e-03,\n                      2.3677e-03, 2.4091e-03, 2.2708e-03, 2.6998e-03, 1.7419e-03, 2.5577e-03,\n                      2.4717e-03, 2.6486e-03, 2.5098e-03, 2.7338e-03, 2.2513e-03, 2.4491e-03,\n                      2.8445e-03, 2.4604e-03, 2.8538e-03, 2.1482e-03, 2.1134e-03, 2.6024e-03,\n                      2.0974e-03, 2.4426e-03, 1.8471e-03, 2.6642e-03, 2.5613e-03, 2.0895e-03,\n                      2.1926e-03, 1.3989e-03, 1.8129e-03, 1.5835e-03, 2.0060e-03, 2.7458e-03,\n                      8.1131e-04, 2.4204e-03, 2.3046e-03, 1.4964e-03, 2.8175e-03, 2.1161e-03,\n                      2.5436e-03, 2.3213e-03, 1.4395e-03, 1.6823e-03, 1.0424e-03, 2.6348e-03,\n                      2.5680e-03, 2.8570e-03, 2.1058e-03, 2.5732e-03, 2.3184e-03, 2.5395e-03,\n                      2.5583e-03, 1.8515e-03, 2.2379e-03, 1.5271e-03, 2.2331e-03, 2.0813e-03,\n                      2.4248e-03, 2.5823e-03, 2.2351e-03, 1.8816e-03, 2.5312e-03, 1.9084e-03,\n                      2.3758e-03, 2.3252e-03, 2.4359e-03, 1.9275e-03, 2.4655e-03, 2.4112e-03,\n                      2.4319e-03, 1.4873e-03, 2.8059e-03, 2.9493e-03, 2.2088e-03, 2.3791e-03,\n                      2.3894e-03, 2.5662e-03, 2.1531e-03, 2.1847e-03, 2.2646e-03, 2.2761e-03,\n                      2.2017e-03, 2.3214e-03, 1.9132e-03, 2.0044e-03, 2.8000e-03, 1.2524e-03,\n                      1.9298e-03, 1.9445e-03, 2.4137e-03, 2.6935e-03, 2.4263e-03, 2.7065e-03,\n                      2.7745e-03, 2.1066e-03, 2.3694e-03, 1.9863e-03, 2.8674e-03, 2.5303e-03,\n                      1.8110e-03, 2.2460e-03, 8.6871e-04, 3.9773e-04, 2.1338e-03, 2.0851e-03,\n                      2.8297e-03, 2.2348e-03, 9.5818e-04, 2.8792e-03, 2.0148e-03, 2.5017e-03,\n                      1.5677e-03, 2.5333e-03, 1.2971e-03, 2.8355e-03, 2.1356e-03, 2.1116e-03,\n                      1.0920e-03, 2.4644e-03, 1.9846e-03, 8.2442e-04, 1.5449e-03, 8.8685e-04,\n                      2.5476e-03, 1.6997e-03, 2.2060e-03, 1.7987e-03, 2.1371e-03, 6.7053e-04,\n                      1.9044e-03, 2.5716e-03, 2.0471e-03, 2.5257e-03, 2.8057e-03, 2.7714e-03,\n                      5.0586e-04, 2.5700e-03, 2.1595e-03, 1.8268e-03, 2.3692e-03, 1.9974e-03,\n                      2.2993e-03, 2.6608e-03, 2.3881e-03, 2.2917e-03, 2.6100e-03, 2.0972e-03,\n                      2.3606e-03, 1.6069e-03, 1.2633e-03, 2.4057e-03, 2.6717e-03, 1.3378e-03]), zero_point=tensor([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,  127,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0, -128,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,  127,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,  127,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n                     dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n                min_val=tensor([-0.0202, -0.2160, -0.0190, -0.0848, -0.1646, -0.3323, -0.0762, -0.3213,\n                        -0.2898, -0.1891, -0.2496, -0.0734, -0.1675, -0.0712, -0.1623, -0.3107,\n                        -0.0873, -0.3440, -0.1698, -0.3563, -0.0183, -0.2581, -0.1351, -0.2482,\n                        -0.1665, -0.2789, -0.2390, -0.2323, -0.0131, -0.0201, -0.3201, -0.2867,\n                        -0.3271, -0.0646,  0.0027, -0.1493, -0.2706, -0.0680, -0.1543, -0.3329,\n                        -0.2151, -0.2378, -0.2520, -0.2961, -0.2491, -0.1916, -0.3180, -0.3391,\n                        -0.3120, -0.1674, -0.1085, -0.0157, -0.3127, -0.0948, -0.1926, -0.1420,\n                        -0.0539, -0.0608, -0.0072, -0.2988, -0.2517, -0.3488, -0.2357, -0.3386,\n                        -0.3173, -0.1533, -0.1848, -0.3182, -0.3214, -0.0654, -0.2577, -0.3152,\n                        -0.1653, -0.1746, -0.0192, -0.2404, -0.1076, -0.0114, -0.2165, -0.2349,\n                        -0.2078, -0.1999, -0.2116, -0.2792, -0.0195, -0.1376, -0.0719, -0.1527,\n                        -0.2532, -0.3309, -0.2068, -0.2748, -0.0349, -0.2443, -0.2615, -0.2510,\n                        -0.2265, -0.2321, -0.1909, -0.0560, -0.0196, -0.2447, -0.2304, -0.3190,\n                        -0.1082, -0.2529, -0.3150, -0.1715, -0.1415, -0.2890, -0.3340, -0.2116,\n                        -0.2906, -0.1615, -0.1043, -0.0618, -0.1573, -0.0900, -0.2674, -0.2426,\n                        -0.0882, -0.2252, -0.0224, -0.1726, -0.2387, -0.1465, -0.2104, -0.2628,\n                        -0.3297, -0.0073, -0.0078, -0.2645, -0.2804, -0.0220, -0.2957, -0.0559,\n                        -0.3198, -0.3178, -0.0126, -0.2431, -0.2195, -0.2743, -0.3363, -0.2436,\n                        -0.1780, -0.0186, -0.0580, -0.2173, -0.1555, -0.2078, -0.0286, -0.0372,\n                        -0.0881, -0.2925, -0.0415, -0.3008, -0.0048, -0.0455, -0.2012, -0.3429,\n                        -0.3490, -0.1718, -0.3012, -0.3577, -0.2461, -0.2913, -0.0693, -0.0431,\n                        -0.1294, -0.0197, -0.3310, -0.2911, -0.0098, -0.1640, -0.3108, -0.1715,\n                        -0.3512, -0.2934, -0.0055, -0.1146, -0.2399, -0.2705, -0.1174, -0.1873,\n                        -0.3350, -0.3269, -0.2724, -0.1059, -0.3571, -0.2411, -0.0065, -0.0133,\n                        -0.3229, -0.2365, -0.0669, -0.2847, -0.3198, -0.1916, -0.2720, -0.2594,\n                        -0.2297, -0.0166, -0.0431, -0.3064, -0.0364, -0.2894, -0.3256, -0.3115,\n                        -0.2652, -0.1571, -0.0878, -0.3086, -0.1545, -0.3331, -0.2917, -0.0784,\n                        -0.2951, -0.3263, -0.0172, -0.3181, -0.2136, -0.2130, -0.1842, -0.1419,\n                        -0.0949, -0.0858, -0.0300, -0.3157, -0.0473, -0.0215, -0.2979, -0.3383,\n                        -0.2751, -0.2030, -0.2622, -0.3084, -0.1589, -0.0510, -0.2230, -0.3274,\n                        -0.3164, -0.3390, -0.3213, -0.0509, -0.1773, -0.0872, -0.0294, -0.1017,\n                        -0.0652, -0.1646, -0.1688, -0.0192, -0.1680, -0.0269, -0.0105, -0.3410,\n                        -0.0239, -0.2675, -0.2807, -0.1364, -0.2320, -0.2027, -0.0196, -0.3515,\n                        -0.0771, -0.3098, -0.2182, -0.1915, -0.3606, -0.2709, -0.0769, -0.1430,\n                        -0.1262, -0.2153, -0.1334, -0.0081, -0.3287, -0.0993, -0.2695, -0.0240,\n                        -0.2968, -0.3251, -0.1578, -0.2370, -0.2865, -0.1811, -0.2858, -0.0707,\n                        -0.3104, -0.3305, -0.2861, -0.2408, -0.0187, -0.2443, -0.2195, -0.2976,\n                        -0.0133, -0.2101, -0.0227, -0.3086, -0.0210, -0.0611, -0.0448, -0.0902,\n                        -0.2827, -0.3045, -0.3058, -0.0150, -0.2756, -0.2796, -0.2899, -0.0088,\n                        -0.0532, -0.2971, -0.1271, -0.0108, -0.0668, -0.1603, -0.2470, -0.2327,\n                        -0.3090, -0.3448, -0.2270, -0.0658, -0.0224, -0.2696, -0.3033, -0.2533,\n                        -0.0144, -0.3239, -0.2318, -0.2505, -0.1112, -0.1014, -0.2731, -0.1547,\n                        -0.3622, -0.1522, -0.0984, -0.0609, -0.2010, -0.2156, -0.0405, -0.1211,\n                        -0.1437, -0.1019, -0.2734, -0.2012, -0.1398, -0.1060, -0.2540, -0.0944,\n                        -0.1977, -0.1135, -0.0912, -0.2176, -0.2824, -0.0847, -0.0155, -0.0078,\n                        -0.2343, -0.0450, -0.2620, -0.0234, -0.3591, -0.0203, -0.1290, -0.3290,\n                        -0.2764, -0.2338, -0.3033, -0.2557, -0.2943, -0.1330, -0.3057, -0.1249,\n                        -0.0201, -0.2684, -0.1059, -0.2057, -0.1617, -0.0982, -0.3420, -0.1712]), max_val=tensor([ 0.2164,  0.1424,  0.3371,  0.2944,  0.3436,  0.1050,  0.2616,  0.2254,\n                         0.0605,  0.2890,  0.0951,  0.3616,  0.2825, -0.0017,  0.1582,  0.0176,\n                         0.2722,  0.0208,  0.2908,  0.0310,  0.3295,  0.2103,  0.2974,  0.3000,\n                         0.2790,  0.0126,  0.1927,  0.2476,  0.3394,  0.4253,  0.1105,  0.1764,\n                         0.0217,  0.2379,  0.0501,  0.3337,  0.2108,  0.3429,  0.0046,  0.0614,\n                         0.0219,  0.2522,  0.1651,  0.2209,  0.1065,  0.0808,  0.0178,  0.1050,\n                         0.1192,  0.1771,  0.2861,  0.3524,  0.0413,  0.0031,  0.2584,  0.1215,\n                         0.0459,  0.3100,  0.3006,  0.0318,  0.2335,  0.1496,  0.2295,  0.0331,\n                         0.0853,  0.1150,  0.2627,  0.1542,  0.1366,  0.1031,  0.2917,  0.0934,\n                         0.1083,  0.2595,  0.3045,  0.0082,  0.3377,  0.2477,  0.2917,  0.0593,\n                         0.2800,  0.1733,  0.2667,  0.0126,  0.1753,  0.2821,  0.2893,  0.0094,\n                         0.1917,  0.1900,  0.2760,  0.0317,  0.3258,  0.1447,  0.0612,  0.0673,\n                         0.2719,  0.1317,  0.0951,  0.2321,  0.3092,  0.2953,  0.2832,  0.0818,\n                         0.1049,  0.2026,  0.0395,  0.1938,  0.0076,  0.0126,  0.0085,  0.1429,\n                         0.0162,  0.2740,  0.3455,  0.2745,  0.3341,  0.3460,  0.1333,  0.0141,\n                         0.2510,  0.2505,  0.3505,  0.2335,  0.2425,  0.2972,  0.2374,  0.0770,\n                         0.0334,  0.3381,  0.3420,  0.2325,  0.1301,  0.2221,  0.1315,  0.3304,\n                         0.0795,  0.0093,  0.2837,  0.2366,  0.3019,  0.0452,  0.0462,  0.2299,\n                         0.2676,  0.3283,  0.3741,  0.1618,  0.0102,  0.3072,  0.1041,  0.0270,\n                         0.3072,  0.0138,  0.2554,  0.1219,  0.0056,  0.3003,  0.2962,  0.0201,\n                         0.0184,  0.2741,  0.0258,  0.0425,  0.2557,  0.1471,  0.3348,  0.3773,\n                         0.1196,  0.3419,  0.0251,  0.0934,  0.2428,  0.2049,  0.1977,  0.2796,\n                         0.0436,  0.1500,  0.0868,  0.3193,  0.2415,  0.2912,  0.1090,  0.1633,\n                         0.1773,  0.1023,  0.1737,  0.0010,  0.1386,  0.1278,  0.3455,  0.2804,\n                         0.0758,  0.2332,  0.3565,  0.1081,  0.0789,  0.2804,  0.2902,  0.2836,\n                         0.2601,  0.3334,  0.3170,  0.2170,  0.3492,  0.1965,  0.0357,  0.0517,\n                         0.1724,  0.2777,  0.1060,  0.0609,  0.1775,  0.0166,  0.1489,  0.3441,\n                         0.0160,  0.0198,  0.2264,  0.0950,  0.2743,  0.2602,  0.1876,  0.2810,\n                         0.3356,  0.3030,  0.3354,  0.0357,  0.3082,  0.0210,  0.1400,  0.0200,\n                         0.1098,  0.2832,  0.3007,  0.1195,  0.2884,  0.3429,  0.1542,  0.0581,\n                         0.0176,  0.0189,  0.0671,  0.3472,  0.2859,  0.3110,  0.3613,  0.3125,\n                         0.3624,  0.2728,  0.2684,  0.3305,  0.2664,  0.3102,  0.2346,  0.0897,\n                         0.3253,  0.1535,  0.1905,  0.1777,  0.0103,  0.1293,  0.2548,  0.0186,\n                         0.1030,  0.1362,  0.2927,  0.0116,  0.0348,  0.1706,  0.3230,  0.2948,\n                         0.1828,  0.0140,  0.0792,  0.3346,  0.0245,  0.3628,  0.1035,  0.3268,\n                         0.0130,  0.0511,  0.3249,  0.2348,  0.0714,  0.1939,  0.1876,  0.2643,\n                         0.0255,  0.0278,  0.2181,  0.2223,  0.3215,  0.0030,  0.3017,  0.0185,\n                         0.3094,  0.2448,  0.3131,  0.0149,  0.3089,  0.1889,  0.3564,  0.3746,\n                         0.1781,  0.1066,  0.1136,  0.3259,  0.1714,  0.1827,  0.0814,  0.2891,\n                         0.2796,  0.0212,  0.2430,  0.2546,  0.3556,  0.1502,  0.1041,  0.2470,\n                         0.0110,  0.0497,  0.3081,  0.3437,  0.3524,  0.2666,  0.0804,  0.2523,\n                         0.3642,  0.0685,  0.2115,  0.2852,  0.0069, -0.0018,  0.0149,  0.2648,\n                         0.0850,  0.2838,  0.1217,  0.3657,  0.2559,  0.3177,  0.1991,  0.3217,\n                         0.1647,  0.3601,  0.1037,  0.2682,  0.0079,  0.3130,  0.2242,  0.1047,\n                         0.0394,  0.0658,  0.3235,  0.1777,  0.1575,  0.2284,  0.2714,  0.0852,\n                         0.2419,  0.3266,  0.1788,  0.3208,  0.0279,  0.3520, -0.0028,  0.0431,\n                         0.2178,  0.1903,  0.0383,  0.0117,  0.0123,  0.3379,  0.0281,  0.2910,\n                         0.3315,  0.1515,  0.2998,  0.1611,  0.1407,  0.3055,  0.0931,  0.1299])\n              )\n            )\n          )\n          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(\n            in_features=384, out_features=1536, bias=True\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0015, 0.0018, 0.0010,  ..., 0.0013, 0.0013, 0.0013]), zero_point=tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([-0.1836, -0.2324, -0.1190,  ..., -0.1616, -0.1318, -0.1716]), max_val=tensor([0.1942, 0.1669, 0.1239,  ..., 0.1384, 0.1624, 0.1518]))\n            )\n          )\n          (4): GELU(approximate='none')\n          (5): Linear(\n            in_features=1536, out_features=384, bias=True\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0015, 0.0027, 0.0017, 0.0019, 0.0021, 0.0019, 0.0019, 0.0018, 0.0016,\n                      0.0015, 0.0014, 0.0023, 0.0022, 0.0044, 0.0015, 0.0020, 0.0018, 0.0016,\n                      0.0019, 0.0016, 0.0025, 0.0014, 0.0024, 0.0017, 0.0014, 0.0020, 0.0023,\n                      0.0018, 0.0016, 0.0021, 0.0022, 0.0024, 0.0017, 0.0015, 0.0018, 0.0018,\n                      0.0014, 0.0017, 0.0017, 0.0016, 0.0020, 0.0016, 0.0018, 0.0029, 0.0023,\n                      0.0030, 0.0020, 0.0021, 0.0016, 0.0017, 0.0016, 0.0033, 0.0017, 0.0019,\n                      0.0014, 0.0029, 0.0045, 0.0019, 0.0017, 0.0020, 0.0020, 0.0022, 0.0021,\n                      0.0015, 0.0016, 0.0029, 0.0016, 0.0027, 0.0020, 0.0019, 0.0021, 0.0016,\n                      0.0025, 0.0020, 0.0015, 0.0024, 0.0020, 0.0031, 0.0014, 0.0017, 0.0016,\n                      0.0016, 0.0024, 0.0022, 0.0025, 0.0016, 0.0030, 0.0016, 0.0015, 0.0018,\n                      0.0013, 0.0021, 0.0018, 0.0022, 0.0016, 0.0023, 0.0025, 0.0022, 0.0018,\n                      0.0027, 0.0019, 0.0020, 0.0025, 0.0015, 0.0031, 0.0017, 0.0017, 0.0022,\n                      0.0015, 0.0059, 0.0018, 0.0018, 0.0025, 0.0015, 0.0016, 0.0015, 0.0019,\n                      0.0018, 0.0016, 0.0016, 0.0016, 0.0016, 0.0018, 0.0027, 0.0031, 0.0053,\n                      0.0025, 0.0016, 0.0016, 0.0022, 0.0021, 0.0017, 0.0026, 0.0036, 0.0018,\n                      0.0031, 0.0017, 0.0030, 0.0030, 0.0021, 0.0016, 0.0017, 0.0023, 0.0025,\n                      0.0018, 0.0022, 0.0025, 0.0017, 0.0015, 0.0030, 0.0024, 0.0015, 0.0018,\n                      0.0017, 0.0016, 0.0019, 0.0022, 0.0020, 0.0016, 0.0016, 0.0024, 0.0015,\n                      0.0016, 0.0022, 0.0016, 0.0019, 0.0018, 0.0015, 0.0016, 0.0017, 0.0016,\n                      0.0015, 0.0017, 0.0015, 0.0019, 0.0019, 0.0016, 0.0022, 0.0020, 0.0016,\n                      0.0025, 0.0021, 0.0031, 0.0017, 0.0017, 0.0019, 0.0020, 0.0015, 0.0017,\n                      0.0022, 0.0023, 0.0021, 0.0016, 0.0021, 0.0018, 0.0034, 0.0015, 0.0023,\n                      0.0017, 0.0016, 0.0019, 0.0015, 0.0015, 0.0025, 0.0014, 0.0016, 0.0013,\n                      0.0016, 0.0016, 0.0028, 0.0018, 0.0021, 0.0017, 0.0016, 0.0019, 0.0024,\n                      0.0020, 0.0018, 0.0015, 0.0022, 0.0019, 0.0020, 0.0020, 0.0017, 0.0024,\n                      0.0021, 0.0035, 0.0016, 0.0016, 0.0021, 0.0027, 0.0013, 0.0020, 0.0018,\n                      0.0016, 0.0015, 0.0016, 0.0023, 0.0041, 0.0015, 0.0020, 0.0021, 0.0015,\n                      0.0015, 0.0019, 0.0030, 0.0017, 0.0026, 0.0016, 0.0016, 0.0021, 0.0020,\n                      0.0023, 0.0015, 0.0025, 0.0018, 0.0017, 0.0016, 0.0017, 0.0020, 0.0015,\n                      0.0026, 0.0026, 0.0014, 0.0035, 0.0026, 0.0035, 0.0016, 0.0018, 0.0017,\n                      0.0017, 0.0018, 0.0016, 0.0026, 0.0021, 0.0021, 0.0014, 0.0015, 0.0027,\n                      0.0015, 0.0019, 0.0027, 0.0019, 0.0032, 0.0017, 0.0014, 0.0029, 0.0015,\n                      0.0014, 0.0018, 0.0022, 0.0023, 0.0015, 0.0025, 0.0015, 0.0014, 0.0017,\n                      0.0031, 0.0018, 0.0019, 0.0015, 0.0015, 0.0025, 0.0024, 0.0020, 0.0016,\n                      0.0018, 0.0018, 0.0013, 0.0017, 0.0015, 0.0017, 0.0022, 0.0020, 0.0020,\n                      0.0018, 0.0016, 0.0020, 0.0024, 0.0017, 0.0027, 0.0014, 0.0019, 0.0026,\n                      0.0015, 0.0015, 0.0018, 0.0021, 0.0016, 0.0018, 0.0019, 0.0019, 0.0017,\n                      0.0015, 0.0016, 0.0022, 0.0016, 0.0017, 0.0028, 0.0030, 0.0026, 0.0017,\n                      0.0016, 0.0017, 0.0023, 0.0018, 0.0029, 0.0018, 0.0021, 0.0014, 0.0014,\n                      0.0018, 0.0016, 0.0049, 0.0015, 0.0020, 0.0018, 0.0017, 0.0018, 0.0018,\n                      0.0023, 0.0016, 0.0016, 0.0016, 0.0035, 0.0014, 0.0030, 0.0020, 0.0020,\n                      0.0017, 0.0015, 0.0017, 0.0030, 0.0016, 0.0015, 0.0017, 0.0015, 0.0026,\n                      0.0015, 0.0018, 0.0016, 0.0015, 0.0016, 0.0016]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                     dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n                min_val=tensor([-0.1750, -0.3481, -0.2212, -0.2216, -0.2625, -0.1874, -0.2396, -0.1630,\n                        -0.2014, -0.1925, -0.1727, -0.2927, -0.2662, -0.5573, -0.1752, -0.1582,\n                        -0.1959, -0.1599, -0.1898, -0.2063, -0.3150, -0.1790, -0.3001, -0.2115,\n                        -0.1847, -0.2383, -0.2924, -0.2023, -0.2036, -0.2663, -0.2081, -0.1939,\n                        -0.1801, -0.1984, -0.1861, -0.2180, -0.1838, -0.1713, -0.1991, -0.1945,\n                        -0.2039, -0.1836, -0.2250, -0.1985, -0.2241, -0.3788, -0.2311, -0.2611,\n                        -0.1831, -0.1832, -0.2029, -0.2534, -0.2182, -0.2392, -0.1775, -0.3653,\n                        -0.5811, -0.1853, -0.2120, -0.1876, -0.2399, -0.2795, -0.2729, -0.1869,\n                        -0.2014, -0.3753, -0.1957, -0.2680, -0.2551, -0.2392, -0.2224, -0.1986,\n                        -0.3156, -0.2518, -0.1866, -0.1927, -0.2618, -0.3753, -0.1488, -0.2121,\n                        -0.2067, -0.2019, -0.1388, -0.2765, -0.2427, -0.2009, -0.3901, -0.2089,\n                        -0.1957, -0.2358, -0.1719, -0.1887, -0.1919, -0.2758, -0.1683, -0.1908,\n                        -0.3251, -0.2865, -0.1815, -0.2735, -0.2434, -0.1818, -0.2884, -0.1538,\n                        -0.4023, -0.1832, -0.1619, -0.2779, -0.1932, -0.4667, -0.1975, -0.2344,\n                        -0.3157, -0.1853, -0.1738, -0.1813, -0.2396, -0.2155, -0.2069, -0.1993,\n                        -0.2068, -0.1627, -0.2272, -0.3455, -0.2141, -0.5025, -0.2688, -0.2071,\n                        -0.1959, -0.2843, -0.2690, -0.1922, -0.2723, -0.4654, -0.1734, -0.2415,\n                        -0.1676, -0.3885, -0.3646, -0.2648, -0.1948, -0.2149, -0.3000, -0.1795,\n                        -0.2051, -0.1919, -0.2381, -0.2207, -0.1862, -0.2370, -0.2393, -0.1928,\n                        -0.1918, -0.2121, -0.1854, -0.2411, -0.2297, -0.1883, -0.1852, -0.2033,\n                        -0.3058, -0.1885, -0.2016, -0.2582, -0.1829, -0.2440, -0.2344, -0.1768,\n                        -0.1907, -0.1778, -0.1841, -0.1973, -0.2207, -0.1552, -0.2410, -0.2347,\n                        -0.1792, -0.2094, -0.2536, -0.2017, -0.3222, -0.2630, -0.3976, -0.1792,\n                        -0.1797, -0.2463, -0.2548, -0.1613, -0.1943, -0.2791, -0.2981, -0.2723,\n                        -0.1646, -0.2082, -0.2011, -0.3284, -0.1894, -0.2905, -0.2055, -0.1704,\n                        -0.2471, -0.1744, -0.1715, -0.1897, -0.1812, -0.1721, -0.1693, -0.2050,\n                        -0.1990, -0.2970, -0.2029, -0.2410, -0.2157, -0.2064, -0.2455, -0.3022,\n                        -0.2558, -0.2202, -0.1952, -0.2451, -0.1832, -0.2586, -0.2415, -0.2135,\n                        -0.2155, -0.2639, -0.4536, -0.2083, -0.2002, -0.2657, -0.2185, -0.1711,\n                        -0.2499, -0.1807, -0.1637, -0.1977, -0.1988, -0.2394, -0.5301, -0.1759,\n                        -0.2482, -0.2732, -0.1576, -0.1976, -0.2114, -0.2861, -0.2197, -0.3325,\n                        -0.1933, -0.1838, -0.2558, -0.2597, -0.2174, -0.1839, -0.1725, -0.2245,\n                        -0.2152, -0.2021, -0.2068, -0.2533, -0.1836, -0.3283, -0.2165, -0.1733,\n                        -0.4476, -0.1843, -0.4421, -0.1989, -0.2360, -0.2220, -0.2121, -0.2245,\n                        -0.1813, -0.3282, -0.1807, -0.2647, -0.1696, -0.1893, -0.2795, -0.1974,\n                        -0.2368, -0.2137, -0.2426, -0.4058, -0.1889, -0.1734, -0.3763, -0.1734,\n                        -0.1847, -0.1855, -0.2126, -0.2933, -0.1942, -0.3139, -0.1921, -0.1800,\n                        -0.2172, -0.4015, -0.2289, -0.1816, -0.1865, -0.1898, -0.3167, -0.3096,\n                        -0.2508, -0.1839, -0.2230, -0.2208, -0.1607, -0.2076, -0.1608, -0.2144,\n                        -0.2755, -0.2566, -0.2504, -0.2340, -0.2099, -0.2539, -0.3045, -0.2054,\n                        -0.3465, -0.1459, -0.1396, -0.3196, -0.1908, -0.1951, -0.2292, -0.1700,\n                        -0.1768, -0.2265, -0.2271, -0.2489, -0.2184, -0.1957, -0.1504, -0.1802,\n                        -0.1619, -0.1961, -0.3623, -0.3883, -0.2356, -0.2122, -0.1745, -0.2238,\n                        -0.2284, -0.2299, -0.3186, -0.2257, -0.2260, -0.1701, -0.1791, -0.2198,\n                        -0.1609, -0.6240, -0.1769, -0.2498, -0.2001, -0.1801, -0.2299, -0.2266,\n                        -0.2985, -0.1986, -0.2010, -0.2023, -0.4516, -0.1603, -0.2184, -0.1925,\n                        -0.2506, -0.2123, -0.1884, -0.2178, -0.3831, -0.2050, -0.1888, -0.2014,\n                        -0.1979, -0.2414, -0.1891, -0.2261, -0.2050, -0.1795, -0.1870, -0.1841]), max_val=tensor([0.1899, 0.3211, 0.1928, 0.2467, 0.2250, 0.2397, 0.2004, 0.2312, 0.1928,\n                        0.1797, 0.1785, 0.2104, 0.2731, 0.2427, 0.1866, 0.2561, 0.2223, 0.2094,\n                        0.2458, 0.1981, 0.2355, 0.1787, 0.3085, 0.1777, 0.1646, 0.2511, 0.2085,\n                        0.2312, 0.1692, 0.2346, 0.2765, 0.2992, 0.2180, 0.1914, 0.2340, 0.2297,\n                        0.1599, 0.2133, 0.2114, 0.2030, 0.2478, 0.2055, 0.2087, 0.3643, 0.2934,\n                        0.3514, 0.2541, 0.2637, 0.1971, 0.2203, 0.1982, 0.4138, 0.2089, 0.2086,\n                        0.1741, 0.2978, 0.1976, 0.2400, 0.2061, 0.2603, 0.2540, 0.2339, 0.2109,\n                        0.1724, 0.1759, 0.2961, 0.2041, 0.3374, 0.2032, 0.1840, 0.2604, 0.1842,\n                        0.2746, 0.2323, 0.1750, 0.3098, 0.1866, 0.3889, 0.1819, 0.2122, 0.1877,\n                        0.1759, 0.3046, 0.2732, 0.3132, 0.1799, 0.2256, 0.1919, 0.1916, 0.1965,\n                        0.1549, 0.2667, 0.2233, 0.2534, 0.2006, 0.2966, 0.2461, 0.1848, 0.2280,\n                        0.3428, 0.1735, 0.2544, 0.3219, 0.1896, 0.2318, 0.2130, 0.2197, 0.2436,\n                        0.1698, 0.7552, 0.2241, 0.1896, 0.2353, 0.1843, 0.2015, 0.1884, 0.1871,\n                        0.2287, 0.1606, 0.2061, 0.1648, 0.2027, 0.1847, 0.2682, 0.3951, 0.6733,\n                        0.3160, 0.2029, 0.2038, 0.2631, 0.2616, 0.2157, 0.3243, 0.3689, 0.2238,\n                        0.3947, 0.2153, 0.1637, 0.3755, 0.1525, 0.1996, 0.2025, 0.2911, 0.3193,\n                        0.2347, 0.2837, 0.3219, 0.2039, 0.1889, 0.3834, 0.3002, 0.1711, 0.2269,\n                        0.1583, 0.2042, 0.1740, 0.2769, 0.2577, 0.2043, 0.1434, 0.1807, 0.1738,\n                        0.1808, 0.2775, 0.2074, 0.2169, 0.1858, 0.1967, 0.1990, 0.2114, 0.2032,\n                        0.1540, 0.1927, 0.1938, 0.2137, 0.2439, 0.2060, 0.2763, 0.2309, 0.1732,\n                        0.2322, 0.2378, 0.2488, 0.2125, 0.2104, 0.2473, 0.2170, 0.1942, 0.2188,\n                        0.2008, 0.2933, 0.2632, 0.2039, 0.2611, 0.2284, 0.4294, 0.1789, 0.2379,\n                        0.2109, 0.2073, 0.1649, 0.1848, 0.1946, 0.3127, 0.1553, 0.1969, 0.1708,\n                        0.1667, 0.2022, 0.3494, 0.2226, 0.2624, 0.2048, 0.1944, 0.2274, 0.2399,\n                        0.2165, 0.2347, 0.1731, 0.2818, 0.2415, 0.1813, 0.2541, 0.1967, 0.3036,\n                        0.2364, 0.1954, 0.1733, 0.1840, 0.2275, 0.3464, 0.1694, 0.2360, 0.2278,\n                        0.2069, 0.1634, 0.1765, 0.2964, 0.2981, 0.1865, 0.2531, 0.2249, 0.1933,\n                        0.1906, 0.2381, 0.3798, 0.1953, 0.2190, 0.2051, 0.2092, 0.2720, 0.2017,\n                        0.2898, 0.1870, 0.3142, 0.2147, 0.1797, 0.1781, 0.2219, 0.1999, 0.1927,\n                        0.2698, 0.3281, 0.1685, 0.1787, 0.3250, 0.2886, 0.1762, 0.2214, 0.1815,\n                        0.1605, 0.1620, 0.2027, 0.3207, 0.2632, 0.2589, 0.1841, 0.1682, 0.3473,\n                        0.1608, 0.2263, 0.3436, 0.2368, 0.2491, 0.2099, 0.1744, 0.2082, 0.1858,\n                        0.1752, 0.2233, 0.2749, 0.2291, 0.1785, 0.2737, 0.1588, 0.1749, 0.1805,\n                        0.2304, 0.2256, 0.2421, 0.1707, 0.1953, 0.2107, 0.2084, 0.1978, 0.1979,\n                        0.2333, 0.2267, 0.1588, 0.2133, 0.1879, 0.1657, 0.2410, 0.1706, 0.2548,\n                        0.1634, 0.1860, 0.1894, 0.2422, 0.2218, 0.3308, 0.1762, 0.2418, 0.3242,\n                        0.1844, 0.1892, 0.2082, 0.2665, 0.2062, 0.1699, 0.2467, 0.1975, 0.1857,\n                        0.1824, 0.2075, 0.2801, 0.2062, 0.2188, 0.2262, 0.3331, 0.3313, 0.1788,\n                        0.1997, 0.2167, 0.2858, 0.1599, 0.3735, 0.2076, 0.2656, 0.1775, 0.1630,\n                        0.2335, 0.2034, 0.2848, 0.1869, 0.1939, 0.2228, 0.2144, 0.2160, 0.1950,\n                        0.2466, 0.1569, 0.1656, 0.1665, 0.3758, 0.1780, 0.3767, 0.2591, 0.2426,\n                        0.2042, 0.1643, 0.2153, 0.3076, 0.2023, 0.1875, 0.2155, 0.1727, 0.3272,\n                        0.1780, 0.1969, 0.1982, 0.1860, 0.2021, 0.1994])\n              )\n            )\n          )\n        )\n      )\n      (4): Module(\n        (block): Module(\n          (0): Conv2d(\n            384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0021, 0.0018, 0.0027, 0.0023, 0.0026, 0.0024, 0.0028, 0.0025, 0.0014,\n                      0.0028, 0.0026, 0.0021, 0.0023, 0.0007, 0.0013, 0.0025, 0.0027, 0.0025,\n                      0.0010, 0.0027, 0.0012, 0.0027, 0.0018, 0.0019, 0.0027, 0.0021, 0.0026,\n                      0.0026, 0.0017, 0.0029, 0.0015, 0.0028, 0.0026, 0.0014, 0.0003, 0.0016,\n                      0.0025, 0.0024, 0.0008, 0.0017, 0.0015, 0.0024, 0.0022, 0.0018, 0.0008,\n                      0.0012, 0.0018, 0.0024, 0.0027, 0.0021, 0.0020, 0.0018, 0.0028, 0.0004,\n                      0.0028, 0.0016, 0.0002, 0.0022, 0.0028, 0.0004, 0.0026, 0.0028, 0.0026,\n                      0.0026, 0.0016, 0.0010, 0.0027, 0.0024, 0.0021, 0.0022, 0.0028, 0.0027,\n                      0.0013, 0.0028, 0.0007, 0.0013, 0.0022, 0.0015, 0.0027, 0.0015, 0.0025,\n                      0.0027, 0.0024, 0.0020, 0.0007, 0.0015, 0.0019, 0.0020, 0.0028, 0.0015,\n                      0.0027, 0.0012, 0.0028, 0.0026, 0.0024, 0.0028, 0.0021, 0.0019, 0.0007,\n                      0.0014, 0.0022, 0.0021, 0.0017, 0.0027, 0.0021, 0.0027, 0.0026, 0.0014,\n                      0.0007, 0.0018, 0.0012, 0.0013, 0.0018, 0.0027, 0.0025, 0.0011, 0.0004,\n                      0.0016, 0.0027, 0.0024, 0.0026, 0.0028, 0.0026, 0.0017, 0.0015, 0.0015,\n                      0.0026, 0.0025, 0.0026, 0.0016, 0.0015, 0.0014, 0.0014, 0.0013, 0.0027,\n                      0.0025, 0.0026, 0.0023, 0.0015, 0.0025, 0.0023, 0.0014, 0.0022, 0.0024,\n                      0.0028, 0.0027, 0.0014, 0.0027, 0.0014, 0.0017, 0.0008, 0.0024, 0.0018,\n                      0.0027, 0.0007, 0.0022, 0.0002, 0.0025, 0.0027, 0.0026, 0.0021, 0.0027,\n                      0.0026, 0.0019, 0.0014, 0.0024, 0.0010, 0.0014, 0.0005, 0.0025, 0.0020,\n                      0.0014, 0.0019, 0.0022, 0.0015, 0.0026, 0.0015, 0.0028, 0.0004, 0.0014,\n                      0.0027, 0.0013, 0.0012, 0.0014, 0.0025, 0.0023, 0.0028, 0.0005, 0.0022,\n                      0.0015, 0.0018, 0.0015, 0.0027, 0.0028, 0.0026, 0.0012, 0.0028, 0.0025,\n                      0.0020, 0.0019, 0.0016, 0.0019, 0.0024, 0.0018, 0.0028, 0.0021, 0.0026,\n                      0.0027, 0.0014, 0.0022, 0.0025, 0.0026, 0.0025, 0.0027, 0.0027, 0.0015,\n                      0.0020, 0.0018, 0.0016, 0.0022, 0.0027, 0.0027, 0.0017, 0.0027, 0.0008,\n                      0.0014, 0.0015, 0.0028, 0.0018, 0.0018, 0.0016, 0.0026, 0.0016, 0.0026,\n                      0.0020, 0.0028, 0.0027, 0.0022, 0.0018, 0.0027, 0.0017, 0.0017, 0.0027,\n                      0.0012, 0.0004, 0.0010, 0.0020, 0.0025, 0.0025, 0.0024, 0.0027, 0.0019,\n                      0.0028, 0.0026, 0.0016, 0.0028, 0.0026, 0.0012, 0.0018, 0.0014, 0.0007,\n                      0.0015, 0.0003, 0.0027, 0.0008, 0.0024, 0.0009, 0.0014, 0.0019, 0.0024,\n                      0.0026, 0.0027, 0.0012, 0.0021, 0.0022, 0.0024, 0.0026, 0.0014, 0.0021,\n                      0.0016, 0.0027, 0.0015, 0.0005, 0.0021, 0.0026, 0.0014, 0.0027, 0.0007,\n                      0.0021, 0.0025, 0.0023, 0.0023, 0.0023, 0.0017, 0.0017, 0.0024, 0.0019,\n                      0.0028, 0.0027, 0.0026, 0.0021, 0.0019, 0.0021, 0.0021, 0.0028, 0.0027,\n                      0.0025, 0.0024, 0.0028, 0.0023, 0.0017, 0.0010, 0.0022, 0.0025, 0.0011,\n                      0.0023, 0.0015, 0.0020, 0.0018, 0.0026, 0.0018, 0.0025, 0.0016, 0.0016,\n                      0.0026, 0.0019, 0.0014, 0.0024, 0.0012, 0.0024, 0.0025, 0.0024, 0.0006,\n                      0.0004, 0.0022, 0.0025, 0.0021, 0.0027, 0.0028, 0.0019, 0.0027, 0.0021,\n                      0.0028, 0.0013, 0.0012, 0.0016, 0.0013, 0.0027, 0.0009, 0.0026, 0.0027,\n                      0.0009, 0.0017, 0.0010, 0.0026, 0.0028, 0.0013, 0.0024, 0.0018, 0.0006,\n                      0.0030, 0.0028, 0.0005, 0.0017, 0.0022, 0.0018, 0.0004, 0.0015, 0.0024,\n                      0.0021, 0.0028, 0.0004, 0.0027, 0.0019, 0.0024, 0.0022, 0.0029, 0.0022,\n                      0.0014, 0.0021, 0.0017, 0.0027, 0.0025, 0.0023]), zero_point=tensor([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0, -128,    0,\n                         0,    0, -128,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,  127,    0,    0, -128,    0,    0, -128,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,  127,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                      -128,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                      -128,    0,  127,    0,    0,    0,    0,    0, -128,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0, -128,    0,\n                         0,    0,    0,    0,    0,    0,    0, -128,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0, -128,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0, -128,    0, -128,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,  127,    0,    0,    0,    0, -128,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,  127,  127,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,  127,\n                         0,    0,  127,    0,    0,    0,  127,    0,    0,    0,    0, -128,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n                     dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n                min_val=tensor([-2.7500e-01, -1.4151e-01, -3.4357e-01, -2.9334e-01, -1.9929e-02,\n                        -1.2597e-02, -9.8762e-03, -1.0687e-01, -9.2328e-03, -3.5442e-01,\n                        -9.0819e-03, -1.3125e-01, -1.4232e-01, -8.8411e-02, -1.3096e-01,\n                        -3.2244e-01, -3.7580e-02, -3.2441e-01, -2.3714e-03, -3.4284e-01,\n                        -1.5869e-01, -4.1618e-02, -1.7872e-01, -2.4357e-01, -1.6449e-02,\n                        -6.4649e-03, -3.3581e-01, -3.3170e-01, -7.4170e-03, -3.7416e-01,\n                        -1.4779e-01, -3.5382e-01, -3.3680e-01, -1.8035e-01,  8.6538e-03,\n                        -1.5281e-02, -3.1970e-01, -9.0451e-02,  1.9365e-03, -1.3185e-03,\n                        -6.2776e-02, -1.3122e-01, -2.8473e-01, -2.3013e-01, -8.0892e-03,\n                        -9.1899e-02, -2.2222e-01, -3.0672e-01, -3.5184e-01, -2.7010e-01,\n                        -1.2527e-01, -2.8353e-02, -8.5167e-03, -9.8790e-02, -2.0883e-02,\n                        -1.3221e-01,  1.8412e-03, -7.1952e-03, -3.5288e-01,  2.4304e-03,\n                        -3.2744e-01, -3.6366e-01, -3.2763e-01, -3.3498e-01, -2.0628e-01,\n                        -1.0687e-01, -2.1251e-02, -1.8065e-01, -2.7104e-01, -5.9525e-03,\n                        -3.5705e-01, -1.5078e-02, -9.2777e-02, -3.5273e-01, -1.8250e-01,\n                        -6.5865e-03, -2.7984e-01, -1.9491e-01, -3.7898e-02, -5.7689e-03,\n                        -3.2268e-01, -2.1515e-02, -6.7409e-02, -2.5708e-01,  1.9826e-03,\n                        -1.8646e-01, -2.4122e-01, -2.1560e-03, -1.7180e-02, -1.8806e-01,\n                        -4.5111e-02, -5.0650e-03, -3.6050e-01, -6.9548e-02, -3.0413e-01,\n                        -2.0803e-02, -1.7189e-01, -1.3882e-01, -3.1905e-03, -1.7607e-01,\n                        -2.8718e-01, -2.6745e-01, -2.2395e-01, -5.0129e-02, -2.6310e-01,\n                        -3.4979e-01, -3.3029e-01, -1.8170e-01,  1.1489e-03, -2.9242e-02,\n                        -3.0843e-01, -1.6487e-01, -2.3313e-01, -2.3980e-02, -1.2607e-01,\n                        -6.7223e-03,  6.3062e-03, -1.9864e-01, -3.4261e-01, -1.7662e-02,\n                        -3.3504e-01, -1.3797e-02, -1.8709e-02, -2.1426e-01, -1.8980e-01,\n                        -1.4669e-01, -3.3710e-01, -1.7754e-02, -3.3495e-01, -2.0693e-01,\n                        -1.9427e-01, -1.6061e-01, -1.4017e-01, -1.6968e-01, -3.4153e-01,\n                        -8.0496e-02, -1.6232e-02, -2.9422e-01, -1.8589e-01, -5.7575e-02,\n                        -8.3963e-02, -5.3341e-03, -7.3361e-02, -3.1300e-01, -1.8249e-02,\n                        -1.5865e-02, -1.7985e-01, -1.7312e-02, -1.7362e-01, -5.1662e-02,\n                        -1.0791e-01, -3.1227e-02, -1.5270e-02, -3.5198e-01, -8.8837e-02,\n                        -2.8116e-01, -1.1293e-02, -3.1853e-01, -3.4002e-01, -1.3877e-02,\n                        -2.4145e-02, -1.3728e-02, -1.3532e-02, -2.4071e-01, -1.7986e-01,\n                        -8.7987e-03, -2.6892e-04, -7.8564e-04, -6.3432e-02, -3.2398e-01,\n                        -2.6193e-01, -1.3746e-02, -2.4877e-01, -1.0034e-01, -1.9149e-01,\n                        -9.2919e-03, -1.2776e-01, -3.6276e-01,  5.5609e-03, -1.8146e-01,\n                        -2.3471e-02, -1.5230e-01, -8.2132e-03, -1.0919e-01, -3.2133e-01,\n                        -8.8690e-02, -1.9355e-02,  2.3225e-03, -2.3988e-01, -1.9544e-01,\n                        -2.3404e-01, -1.9054e-01, -3.5137e-01, -3.9915e-02, -3.7517e-02,\n                        -1.5754e-01, -2.0502e-02, -3.2002e-01, -7.3236e-02, -2.4483e-01,\n                        -2.0421e-01, -2.3838e-01, -3.0502e-01, -2.3046e-01, -3.5279e-01,\n                        -2.6278e-01, -3.3189e-01, -1.6555e-02, -1.2857e-01, -4.9433e-02,\n                        -7.4894e-02, -8.8798e-02, -4.1638e-02, -3.4303e-01, -2.4224e-02,\n                        -1.9079e-01, -8.5954e-02, -2.2466e-01, -2.0035e-01, -1.3383e-01,\n                        -3.3984e-01, -3.5933e-02, -2.2331e-01, -3.3955e-01, -1.0473e-02,\n                        -9.8461e-02, -1.8804e-01, -2.1820e-02, -2.3626e-01, -9.4217e-02,\n                        -1.9963e-01, -3.3665e-01, -2.0843e-01, -3.2703e-01, -1.7098e-01,\n                        -3.6416e-01, -3.0300e-02, -8.2450e-02, -2.3506e-01, -1.3294e-02,\n                        -2.1615e-01, -2.1271e-01, -1.3199e-02, -3.1727e-03,  1.0057e-02,\n                        -1.3163e-01, -1.2903e-01, -1.1991e-01, -3.1455e-01, -3.0397e-01,\n                        -3.4656e-01, -8.9324e-02, -3.5438e-01, -3.3191e-01, -1.3280e-02,\n                        -1.7858e-02, -1.9140e-02, -1.4929e-01, -1.6406e-02, -1.8519e-01,\n                         1.9524e-04, -1.0761e-01,  7.0477e-03, -2.1498e-02, -7.7066e-02,\n                        -3.0321e-01, -8.9211e-02, -1.8183e-01, -2.4441e-01, -3.0373e-01,\n                        -3.3737e-01, -3.5001e-01, -1.5172e-01, -7.2285e-02, -2.8124e-01,\n                        -3.0160e-01, -3.3736e-01, -3.6478e-02, -1.4023e-01, -8.3337e-03,\n                        -3.5135e-01, -1.2535e-01, -1.2588e-01, -2.7412e-01, -3.3155e-01,\n                        -8.6896e-02, -3.1348e-02,  1.7931e-04, -2.6958e-01, -3.1366e-01,\n                        -2.9816e-01, -2.9552e-01, -1.3021e-02, -2.1138e-01, -2.1283e-01,\n                        -3.1307e-01, -2.4898e-01, -3.5255e-01, -3.5130e-01, -8.2960e-03,\n                        -2.6905e-01, -2.4043e-01, -1.2978e-01, -2.6793e-01, -3.5654e-01,\n                        -3.4747e-01, -7.3916e-03, -3.1180e-01, -2.1317e-02, -7.5152e-02,\n                        -8.3440e-03, -1.2799e-01, -2.8285e-01, -3.2373e-01, -1.3767e-01,\n                        -9.2862e-03, -7.7104e-04, -1.5158e-01, -2.2660e-01, -5.8071e-02,\n                        -2.0910e-01, -1.1705e-02, -1.4675e-01, -2.0779e-01, -3.3852e-01,\n                        -2.3906e-01, -1.7307e-01, -3.1277e-01, -1.5079e-01, -8.9150e-02,\n                        -6.7899e-02, -3.1094e-01, -1.5945e-01, -1.0003e-01, -2.0375e-02,\n                        -5.9739e-02, -7.2840e-02, -3.4587e-01, -3.5851e-01, -2.3957e-01,\n                        -3.4990e-01, -1.1082e-01, -2.3141e-02, -1.7098e-01, -1.0765e-01,\n                        -2.0994e-01, -1.0272e-01, -2.6059e-02, -1.8568e-02, -2.5824e-02,\n                        -4.0638e-02, -1.2097e-01, -2.1192e-01, -1.3177e-01, -1.2931e-02,\n                        -3.5750e-01, -1.7213e-01, -2.6365e-02, -4.7994e-03, -1.4601e-01,\n                        -3.8333e-01, -1.0996e-02, -1.2250e-01, -1.0396e-03, -7.3781e-02,\n                        -2.2546e-01, -9.6480e-02, -7.2653e-03, -3.0630e-01, -1.1024e-01,\n                        -3.5686e-01,  4.7451e-03, -3.3984e-01, -1.8583e-01, -2.2241e-02,\n                        -7.5292e-02, -1.2001e-02, -1.0443e-01, -1.7816e-01, -1.1905e-01,\n                        -2.2134e-01, -3.5058e-01, -3.2731e-02, -1.1807e-02]), max_val=tensor([ 7.5439e-03,  2.2466e-01,  1.1583e-02,  1.1151e-01,  3.2662e-01,\n                         3.0733e-01,  3.5723e-01,  3.1810e-01,  1.8065e-01,  1.8116e-02,\n                         3.3097e-01,  2.7268e-01,  2.9825e-01,  1.3470e-02,  1.6484e-01,\n                         1.0157e-02,  3.4090e-01,  1.6128e-02,  1.2532e-01,  8.1576e-03,\n                         1.1707e-02,  3.3672e-01,  2.3318e-01,  7.8219e-03,  3.4204e-01,\n                         2.6983e-01,  2.2581e-02,  4.4176e-02,  2.1815e-01,  1.7573e-02,\n                         1.8935e-01,  1.9801e-02,  1.9789e-02,  3.6518e-03,  8.8855e-02,\n                         2.0426e-01,  9.7046e-02,  3.0075e-01,  1.9851e-01,  2.1100e-01,\n                         1.9205e-01,  3.0411e-01,  8.9846e-02,  5.7891e-02,  9.9427e-02,\n                         1.5235e-01,  2.2611e-01,  5.8892e-02,  2.9379e-02,  1.5980e-01,\n                         2.5035e-01,  2.2297e-01,  3.5596e-01, -5.1127e-03,  3.5380e-01,\n                         2.0393e-01,  6.0176e-02,  2.7600e-01,  1.3523e-02,  1.0986e-01,\n                         5.1798e-02,  7.4867e-02,  4.2068e-02,  2.7129e-02,  1.2306e-02,\n                         1.2236e-01,  3.4893e-01,  3.0527e-01,  2.8617e-02,  2.7782e-01,\n                         4.6723e-02,  3.4371e-01,  1.7114e-01,  1.6920e-02, -2.1270e-03,\n                         1.6238e-01,  6.6534e-02,  1.8840e-01,  3.4381e-01,  1.9152e-01,\n                         1.8802e-02,  3.4874e-01,  3.0804e-01,  2.4277e-01,  1.6895e-01,\n                         1.6294e-01,  1.3785e-01,  2.5043e-01,  3.6132e-01,  3.1388e-02,\n                         3.4553e-01,  1.4730e-01,  1.8263e-02,  3.3043e-01,  1.7446e-02,\n                         3.6011e-01,  2.6431e-01,  2.4214e-01,  8.6320e-02,  8.0967e-02,\n                         1.4332e-02,  9.4099e-03,  7.6222e-02,  3.3989e-01,  1.6621e-01,\n                         2.0421e-02,  1.2653e-02,  1.7237e-01,  1.8482e-01,  2.2687e-01,\n                        -6.3635e-04,  1.4175e-01,  2.1987e-01,  3.4248e-01,  3.1256e-01,\n                         1.4047e-01,  1.0887e-01,  1.5530e-01,  9.9452e-03,  3.0581e-01,\n                         1.4120e-02,  3.5253e-01,  3.2393e-01,  2.1314e-01,  1.7492e-01,\n                         1.9553e-01,  2.1234e-02,  3.1491e-01,  1.1995e-02,  2.0831e-01,\n                         1.8520e-01,  1.7701e-01,  1.7428e-01,  1.1389e-01,  6.4714e-02,\n                         3.1816e-01,  3.3155e-01,  6.2855e-03,  1.7945e-01,  3.2020e-01,\n                         2.8844e-01,  1.7188e-01,  2.8134e-01,  7.1174e-02,  3.5350e-01,\n                         3.4440e-01,  1.6539e-01,  3.4162e-01,  5.8883e-03,  2.1949e-01,\n                         1.0154e-01,  3.0116e-01,  2.3020e-01,  2.0833e-02,  3.5278e-03,\n                         1.0862e-01,  2.6447e-02,  7.0695e-02,  6.2067e-02,  3.3001e-01,\n                         2.6690e-01,  3.3945e-01,  3.2977e-01,  2.3013e-01,  1.4154e-02,\n                         2.9976e-01,  1.3254e-01,  1.8087e-01,  5.1262e-04,  2.0164e-02,\n                         1.6481e-03,  1.7471e-01,  1.6148e-03,  2.7346e-01,  1.6396e-01,\n                         3.3013e-01,  1.8428e-01,  1.5258e-02,  9.6771e-02,  3.8679e-02,\n                         3.4325e-01,  1.6149e-01,  1.5087e-01,  1.7698e-01,  4.4445e-02,\n                         2.8675e-01,  3.4926e-01,  1.1924e-01,  2.8353e-01,  1.5336e-01,\n                         2.2504e-01,  1.8708e-01,  1.7667e-02,  3.5061e-01,  3.2406e-01,\n                         1.4886e-01,  3.5004e-01,  9.5648e-03,  2.6014e-01,  2.0183e-01,\n                         1.2207e-01,  5.3307e-02,  6.3964e-03,  1.3130e-01,  2.0678e-02,\n                         1.1658e-01,  1.2194e-02,  3.4460e-01,  1.8207e-01,  2.8240e-01,\n                         3.1460e-01,  3.3447e-01,  3.2098e-01,  1.6295e-02,  3.4226e-01,\n                         1.5081e-01,  2.4997e-01,  2.1789e-01,  2.0222e-04,  2.7833e-01,\n                         6.9156e-02,  3.4546e-01,  7.7125e-02,  1.6951e-02,  1.0195e-01,\n                         1.8241e-01,  1.5770e-01,  3.5347e-01,  2.9718e-03,  2.3494e-01,\n                         1.0921e-01,  1.4624e-02,  1.0857e-01,  4.9876e-02,  2.4960e-01,\n                         1.2172e-02,  3.4321e-01,  2.7908e-01,  1.5314e-01,  3.4721e-01,\n                         2.0847e-01,  1.0137e-01,  3.4160e-01,  1.5704e-01,  1.0485e-01,\n                         1.0377e-01,  2.5697e-01,  3.1128e-01,  5.9524e-02,  6.0224e-02,\n                         1.7186e-02,  2.3568e-01,  1.9331e-02,  9.4839e-03,  2.0678e-01,\n                         3.5115e-01,  3.2413e-01,  1.2059e-01,  2.2667e-01,  2.8671e-03,\n                         1.8055e-01,  1.8488e-01,  8.1829e-02,  3.3802e-01,  1.0786e-01,\n                         4.0852e-02,  1.1630e-01,  1.2706e-01,  7.7814e-02,  7.2881e-02,\n                         1.3475e-02,  1.7173e-02,  1.4128e-01,  2.6451e-01,  5.9732e-03,\n                         4.1535e-02,  1.5020e-02,  1.7445e-01,  2.6999e-01,  2.0835e-01,\n                         7.3763e-03,  1.9074e-01, -3.8463e-03,  1.0171e-01,  1.1316e-02,\n                         1.8065e-01,  3.4406e-01,  1.7653e-01,  1.0919e-02,  8.4823e-03,\n                         6.2666e-02,  7.6992e-02,  2.9624e-01,  2.0952e-01,  4.2519e-03,\n                         7.5653e-03,  8.2507e-03,  1.8984e-02,  1.0829e-02,  3.2494e-01,\n                         8.8355e-03,  7.5210e-03,  2.6451e-01,  1.5037e-01,  1.8979e-02,\n                         1.5639e-02,  3.2173e-01,  4.6766e-03,  3.5533e-01,  2.9308e-01,\n                         2.1619e-01,  5.9215e-03,  2.3489e-02,  8.1314e-03,  1.1464e-01,\n                         2.9226e-01,  1.8688e-01,  2.5920e-01,  2.2493e-01,  3.2627e-01,\n                         2.2285e-01,  3.1538e-01,  2.0158e-01,  1.2964e-01,  2.9996e-02,\n                         1.1700e-01,  1.6812e-01,  3.1005e-02,  1.0138e-02,  3.0695e-01,\n                         3.1790e-01,  1.7508e-02, -5.0400e-04, -3.4260e-03,  2.7966e-01,\n                         3.1629e-01,  2.6700e-01,  2.1859e-02,  2.1662e-02,  2.3647e-01,\n                         1.8313e-02,  2.6397e-01,  3.6074e-01,  4.5695e-02,  1.5851e-01,\n                         1.8125e-01,  1.6685e-01,  3.4556e-01,  1.1200e-01,  3.2644e-01,\n                         3.4884e-01,  1.1069e-01,  3.2160e-02,  1.0046e-01,  3.3004e-01,\n                         2.2866e-02,  1.2295e-01,  3.0485e-01,  2.2432e-01, -1.7461e-03,\n                         1.7417e-02,  3.5626e-01, -6.0816e-03,  2.1578e-01,  2.7565e-01,\n                         1.1409e-02, -1.1266e-02,  1.9622e-01,  9.1291e-02,  2.6927e-01,\n                         1.8069e-02,  1.1051e-01,  1.1148e-02,  2.3902e-01,  2.9871e-01,\n                         2.7920e-01,  3.7084e-01,  2.8117e-01,  6.7755e-03,  2.6106e-01,\n                         1.4999e-01,  1.1691e-02,  3.1983e-01,  2.9427e-01])\n              )\n            )\n          )\n          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(\n            in_features=384, out_features=1536, bias=True\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0013, 0.0013, 0.0009,  ..., 0.0013, 0.0014, 0.0013]), zero_point=tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([-0.1632, -0.1552, -0.1205,  ..., -0.1624, -0.1835, -0.1663]), max_val=tensor([0.1640, 0.1644, 0.1190,  ..., 0.1225, 0.1461, 0.1420]))\n            )\n          )\n          (4): GELU(approximate='none')\n          (5): Linear(\n            in_features=1536, out_features=384, bias=True\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0017, 0.0024, 0.0013, 0.0025, 0.0015, 0.0015, 0.0015, 0.0015, 0.0014,\n                      0.0025, 0.0016, 0.0018, 0.0020, 0.0022, 0.0020, 0.0017, 0.0020, 0.0015,\n                      0.0014, 0.0015, 0.0027, 0.0017, 0.0019, 0.0019, 0.0016, 0.0016, 0.0021,\n                      0.0020, 0.0017, 0.0028, 0.0022, 0.0022, 0.0018, 0.0020, 0.0020, 0.0018,\n                      0.0021, 0.0014, 0.0021, 0.0015, 0.0021, 0.0025, 0.0020, 0.0022, 0.0019,\n                      0.0026, 0.0011, 0.0022, 0.0017, 0.0015, 0.0017, 0.0033, 0.0018, 0.0019,\n                      0.0016, 0.0060, 0.0033, 0.0017, 0.0016, 0.0015, 0.0019, 0.0024, 0.0018,\n                      0.0014, 0.0017, 0.0031, 0.0019, 0.0022, 0.0020, 0.0019, 0.0021, 0.0016,\n                      0.0024, 0.0018, 0.0016, 0.0020, 0.0017, 0.0026, 0.0020, 0.0017, 0.0016,\n                      0.0016, 0.0018, 0.0010, 0.0019, 0.0020, 0.0021, 0.0016, 0.0016, 0.0014,\n                      0.0016, 0.0020, 0.0017, 0.0024, 0.0014, 0.0014, 0.0023, 0.0018, 0.0022,\n                      0.0038, 0.0020, 0.0022, 0.0036, 0.0015, 0.0024, 0.0019, 0.0017, 0.0021,\n                      0.0015, 0.0041, 0.0017, 0.0018, 0.0015, 0.0019, 0.0018, 0.0018, 0.0016,\n                      0.0018, 0.0016, 0.0016, 0.0015, 0.0015, 0.0015, 0.0016, 0.0031, 0.0041,\n                      0.0022, 0.0013, 0.0017, 0.0010, 0.0042, 0.0016, 0.0026, 0.0030, 0.0016,\n                      0.0035, 0.0014, 0.0017, 0.0009, 0.0018, 0.0025, 0.0016, 0.0020, 0.0019,\n                      0.0017, 0.0017, 0.0024, 0.0016, 0.0020, 0.0020, 0.0021, 0.0016, 0.0016,\n                      0.0022, 0.0015, 0.0024, 0.0029, 0.0022, 0.0018, 0.0014, 0.0017, 0.0014,\n                      0.0016, 0.0012, 0.0017, 0.0014, 0.0015, 0.0016, 0.0016, 0.0015, 0.0015,\n                      0.0019, 0.0020, 0.0016, 0.0020, 0.0021, 0.0019, 0.0014, 0.0018, 0.0019,\n                      0.0035, 0.0027, 0.0019, 0.0021, 0.0015, 0.0024, 0.0019, 0.0016, 0.0031,\n                      0.0021, 0.0009, 0.0061, 0.0016, 0.0019, 0.0018, 0.0009, 0.0017, 0.0052,\n                      0.0018, 0.0018, 0.0027, 0.0016, 0.0016, 0.0023, 0.0015, 0.0018, 0.0013,\n                      0.0015, 0.0015, 0.0023, 0.0020, 0.0021, 0.0017, 0.0017, 0.0022, 0.0022,\n                      0.0041, 0.0010, 0.0016, 0.0022, 0.0029, 0.0019, 0.0026, 0.0016, 0.0022,\n                      0.0029, 0.0031, 0.0018, 0.0017, 0.0017, 0.0026, 0.0018, 0.0020, 0.0016,\n                      0.0028, 0.0018, 0.0020, 0.0028, 0.0031, 0.0024, 0.0010, 0.0019, 0.0015,\n                      0.0017, 0.0016, 0.0023, 0.0016, 0.0035, 0.0037, 0.0019, 0.0017, 0.0016,\n                      0.0025, 0.0015, 0.0016, 0.0016, 0.0015, 0.0019, 0.0016, 0.0019, 0.0017,\n                      0.0022, 0.0025, 0.0016, 0.0037, 0.0025, 0.0033, 0.0015, 0.0019, 0.0018,\n                      0.0015, 0.0019, 0.0017, 0.0037, 0.0020, 0.0018, 0.0013, 0.0016, 0.0018,\n                      0.0014, 0.0018, 0.0023, 0.0015, 0.0054, 0.0015, 0.0016, 0.0017, 0.0016,\n                      0.0016, 0.0018, 0.0016, 0.0028, 0.0014, 0.0009, 0.0020, 0.0016, 0.0016,\n                      0.0019, 0.0016, 0.0016, 0.0016, 0.0016, 0.0020, 0.0021, 0.0016, 0.0016,\n                      0.0019, 0.0017, 0.0016, 0.0032, 0.0015, 0.0016, 0.0019, 0.0015, 0.0022,\n                      0.0015, 0.0015, 0.0020, 0.0012, 0.0018, 0.0013, 0.0014, 0.0017, 0.0021,\n                      0.0018, 0.0019, 0.0009, 0.0019, 0.0016, 0.0022, 0.0016, 0.0017, 0.0016,\n                      0.0023, 0.0013, 0.0024, 0.0016, 0.0018, 0.0055, 0.0032, 0.0018, 0.0014,\n                      0.0017, 0.0020, 0.0031, 0.0019, 0.0025, 0.0015, 0.0022, 0.0015, 0.0022,\n                      0.0023, 0.0016, 0.0024, 0.0015, 0.0021, 0.0023, 0.0017, 0.0019, 0.0016,\n                      0.0030, 0.0015, 0.0018, 0.0017, 0.0031, 0.0013, 0.0027, 0.0016, 0.0023,\n                      0.0021, 0.0014, 0.0017, 0.0018, 0.0018, 0.0016, 0.0026, 0.0019, 0.0033,\n                      0.0015, 0.0025, 0.0017, 0.0015, 0.0019, 0.0016]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                     dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n                min_val=tensor([-0.2181, -0.2739, -0.1617, -0.2657, -0.1869, -0.1841, -0.1925, -0.1874,\n                        -0.1789, -0.2301, -0.1928, -0.1972, -0.2413, -0.2860, -0.2054, -0.2139,\n                        -0.2604, -0.1644, -0.1773, -0.1632, -0.2953, -0.1653, -0.2219, -0.2039,\n                        -0.1810, -0.2067, -0.2730, -0.2196, -0.1784, -0.1958, -0.2477, -0.2836,\n                        -0.1680, -0.2586, -0.2580, -0.2320, -0.2180, -0.1789, -0.2736, -0.1969,\n                        -0.2724, -0.3169, -0.2619, -0.2828, -0.2379, -0.3386, -0.1131, -0.2773,\n                        -0.2208, -0.1977, -0.2235, -0.4233, -0.2265, -0.2387, -0.1685, -0.7703,\n                        -0.4243, -0.1514, -0.2014, -0.1948, -0.2377, -0.2137, -0.2284, -0.1763,\n                        -0.1729, -0.3609, -0.2468, -0.2777, -0.2526, -0.1908, -0.2294, -0.2050,\n                        -0.2391, -0.1966, -0.2016, -0.2568, -0.2147, -0.3320, -0.1858, -0.1575,\n                        -0.2012, -0.2030, -0.2282, -0.1317, -0.2491, -0.2609, -0.2057, -0.1658,\n                        -0.1741, -0.1774, -0.2075, -0.2596, -0.2172, -0.2841, -0.1733, -0.1840,\n                        -0.2954, -0.2241, -0.2783, -0.2959, -0.2046, -0.1754, -0.2843, -0.1967,\n                        -0.3022, -0.2402, -0.1853, -0.2726, -0.1704, -0.5287, -0.2147, -0.2241,\n                        -0.1785, -0.1937, -0.2033, -0.2288, -0.2086, -0.2310, -0.2110, -0.1801,\n                        -0.1890, -0.1675, -0.1839, -0.2019, -0.2175, -0.4231, -0.2025, -0.1638,\n                        -0.2180, -0.1185, -0.5164, -0.1647, -0.3142, -0.3316, -0.1679, -0.4481,\n                        -0.1844, -0.2062, -0.1151, -0.2277, -0.3242, -0.1967, -0.1707, -0.2010,\n                        -0.2201, -0.1994, -0.1889, -0.1704, -0.2537, -0.2533, -0.2047, -0.1725,\n                        -0.1919, -0.1606, -0.1828, -0.3023, -0.2911, -0.2399, -0.1783, -0.1643,\n                        -0.2128, -0.1553, -0.1991, -0.1503, -0.1760, -0.1846, -0.1883, -0.2111,\n                        -0.2041, -0.1779, -0.1816, -0.2476, -0.2046, -0.2075, -0.2588, -0.2517,\n                        -0.2141, -0.1839, -0.2339, -0.2390, -0.4505, -0.3364, -0.2377, -0.2161,\n                        -0.1755, -0.2879, -0.1727, -0.1867, -0.3948, -0.2091, -0.1087, -0.7811,\n                        -0.2069, -0.1729, -0.2080, -0.1195, -0.2189, -0.4256, -0.2350, -0.2286,\n                        -0.3473, -0.1774, -0.2040, -0.2411, -0.1557, -0.2360, -0.1680, -0.1978,\n                        -0.1982, -0.2947, -0.2514, -0.2666, -0.2120, -0.2133, -0.2028, -0.2699,\n                        -0.3851, -0.1281, -0.2109, -0.2870, -0.2803, -0.2238, -0.2920, -0.1997,\n                        -0.1519, -0.3143, -0.2487, -0.2231, -0.2134, -0.2128, -0.3265, -0.1959,\n                        -0.2546, -0.1692, -0.1832, -0.1795, -0.2499, -0.3622, -0.3995, -0.1813,\n                        -0.1238, -0.2163, -0.1979, -0.1796, -0.2096, -0.2952, -0.1735, -0.4105,\n                        -0.2058, -0.2430, -0.2141, -0.2011, -0.3224, -0.1927, -0.2104, -0.2057,\n                        -0.1801, -0.2395, -0.2022, -0.2475, -0.1869, -0.2440, -0.3230, -0.1986,\n                        -0.4712, -0.3164, -0.4168, -0.1807, -0.2327, -0.2255, -0.1897, -0.2381,\n                        -0.2187, -0.4743, -0.2572, -0.2112, -0.1633, -0.1733, -0.2261, -0.1776,\n                        -0.1769, -0.1931, -0.1889, -0.5102, -0.1881, -0.1857, -0.2158, -0.1985,\n                        -0.1849, -0.1868, -0.2078, -0.3530, -0.1855, -0.1120, -0.1984, -0.2052,\n                        -0.1852, -0.2084, -0.2050, -0.1897, -0.1689, -0.1777, -0.2388, -0.2073,\n                        -0.2063, -0.2033, -0.2493, -0.1716, -0.2040, -0.2958, -0.1870, -0.1990,\n                        -0.2375, -0.1801, -0.2765, -0.1889, -0.1894, -0.2197, -0.1472, -0.2011,\n                        -0.1675, -0.1663, -0.1876, -0.2696, -0.2291, -0.2322, -0.1010, -0.2368,\n                        -0.2016, -0.2877, -0.1853, -0.2161, -0.1917, -0.1846, -0.1696, -0.3024,\n                        -0.1841, -0.2074, -0.4716, -0.4054, -0.1799, -0.1783, -0.2225, -0.2219,\n                        -0.2945, -0.2241, -0.1945, -0.1886, -0.2436, -0.1873, -0.1940, -0.2923,\n                        -0.2083, -0.3026, -0.1539, -0.2634, -0.2881, -0.1663, -0.1975, -0.1815,\n                        -0.3599, -0.1893, -0.2058, -0.2144, -0.3864, -0.1692, -0.1783, -0.2085,\n                        -0.2954, -0.2470, -0.1738, -0.1842, -0.2102, -0.2114, -0.1994, -0.3377,\n                        -0.2477, -0.3384, -0.1707, -0.3167, -0.2155, -0.1621, -0.2407, -0.1987]), max_val=tensor([0.2098, 0.3077, 0.1659, 0.3207, 0.1909, 0.1899, 0.1856, 0.1816, 0.1726,\n                        0.3141, 0.2046, 0.2294, 0.2480, 0.2843, 0.2545, 0.1513, 0.2117, 0.1885,\n                        0.1803, 0.1967, 0.3397, 0.2150, 0.2403, 0.2367, 0.2029, 0.1774, 0.1828,\n                        0.2552, 0.2201, 0.3553, 0.2779, 0.2245, 0.2299, 0.1915, 0.1691, 0.1716,\n                        0.2663, 0.1599, 0.2126, 0.1885, 0.2217, 0.2602, 0.2162, 0.2254, 0.2433,\n                        0.3004, 0.1366, 0.2440, 0.1716, 0.1655, 0.1988, 0.2659, 0.1595, 0.2335,\n                        0.1995, 0.5732, 0.2807, 0.2103, 0.1811, 0.1933, 0.2052, 0.3079, 0.2005,\n                        0.1840, 0.2154, 0.3950, 0.1920, 0.2061, 0.2232, 0.2409, 0.2613, 0.1618,\n                        0.3036, 0.2239, 0.1823, 0.2066, 0.2074, 0.2893, 0.2498, 0.2204, 0.1969,\n                        0.1976, 0.1806, 0.1134, 0.1906, 0.2300, 0.2633, 0.2050, 0.1973, 0.1834,\n                        0.1948, 0.1923, 0.1778, 0.3080, 0.1829, 0.1739, 0.2605, 0.1820, 0.1653,\n                        0.4786, 0.2517, 0.2764, 0.4580, 0.1950, 0.2564, 0.1788, 0.2121, 0.2600,\n                        0.1846, 0.5166, 0.2199, 0.2098, 0.1861, 0.2383, 0.2294, 0.1789, 0.1831,\n                        0.2114, 0.1914, 0.2051, 0.1613, 0.1960, 0.1928, 0.1816, 0.3993, 0.5202,\n                        0.2778, 0.1681, 0.1467, 0.1267, 0.5361, 0.1992, 0.3350, 0.3830, 0.2032,\n                        0.2411, 0.1792, 0.2178, 0.1122, 0.1925, 0.2120, 0.2022, 0.2542, 0.2375,\n                        0.2106, 0.2185, 0.2985, 0.1980, 0.1993, 0.2464, 0.2611, 0.2011, 0.2037,\n                        0.2778, 0.1908, 0.3021, 0.3665, 0.2735, 0.2268, 0.1827, 0.2058, 0.1729,\n                        0.1933, 0.1458, 0.2200, 0.1731, 0.1840, 0.1969, 0.2065, 0.1953, 0.1859,\n                        0.1769, 0.2481, 0.1548, 0.1993, 0.2629, 0.2450, 0.1781, 0.1996, 0.1621,\n                        0.3471, 0.3367, 0.2027, 0.2665, 0.1883, 0.2986, 0.2398, 0.2027, 0.3686,\n                        0.2670, 0.1129, 0.7237, 0.1732, 0.2364, 0.2266, 0.1153, 0.1762, 0.6545,\n                        0.2151, 0.2286, 0.2837, 0.2016, 0.1669, 0.2921, 0.1922, 0.1973, 0.1703,\n                        0.1881, 0.1794, 0.2850, 0.1605, 0.2330, 0.1894, 0.1907, 0.2798, 0.2819,\n                        0.5201, 0.1304, 0.1985, 0.2593, 0.3701, 0.2429, 0.3324, 0.1910, 0.2837,\n                        0.3708, 0.3928, 0.2276, 0.2001, 0.1609, 0.2815, 0.2278, 0.2372, 0.1995,\n                        0.3519, 0.2239, 0.1709, 0.2645, 0.3810, 0.3049, 0.1296, 0.2387, 0.1927,\n                        0.2135, 0.1988, 0.2558, 0.2072, 0.4416, 0.4679, 0.1897, 0.2060, 0.2051,\n                        0.2052, 0.1871, 0.1964, 0.1969, 0.1869, 0.1877, 0.1934, 0.1940, 0.2158,\n                        0.2853, 0.2196, 0.1797, 0.2312, 0.2304, 0.3617, 0.1948, 0.2388, 0.2008,\n                        0.1820, 0.1822, 0.2186, 0.3199, 0.2437, 0.2300, 0.1594, 0.2020, 0.2344,\n                        0.1793, 0.2266, 0.2892, 0.1830, 0.6918, 0.1654, 0.2080, 0.2014, 0.1850,\n                        0.2047, 0.2316, 0.1827, 0.2659, 0.1646, 0.1149, 0.2541, 0.2003, 0.2008,\n                        0.2352, 0.1918, 0.2065, 0.2035, 0.1978, 0.2536, 0.2704, 0.1970, 0.1933,\n                        0.1733, 0.2166, 0.2059, 0.4091, 0.1737, 0.1967, 0.2134, 0.1897, 0.2315,\n                        0.1703, 0.1949, 0.2537, 0.1395, 0.2253, 0.1521, 0.1819, 0.2133, 0.2258,\n                        0.1804, 0.2448, 0.1101, 0.2404, 0.1861, 0.2844, 0.1984, 0.1892, 0.2026,\n                        0.2959, 0.1666, 0.1890, 0.1988, 0.2272, 0.7037, 0.2908, 0.2283, 0.1747,\n                        0.1735, 0.2598, 0.3936, 0.2386, 0.3197, 0.1724, 0.2823, 0.1819, 0.2751,\n                        0.2744, 0.1779, 0.2978, 0.1903, 0.2611, 0.1977, 0.2114, 0.2427, 0.2086,\n                        0.3787, 0.1881, 0.2223, 0.1912, 0.3909, 0.1632, 0.3382, 0.1598, 0.2181,\n                        0.2697, 0.1809, 0.2212, 0.2316, 0.2228, 0.1541, 0.2184, 0.1740, 0.4161,\n                        0.1843, 0.2526, 0.1836, 0.1860, 0.2377, 0.1905])\n              )\n            )\n          )\n        )\n      )\n      (5): Module(\n        (block): Module(\n          (0): Conv2d(\n            384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0019, 0.0021, 0.0021, 0.0022, 0.0014, 0.0017, 0.0022, 0.0007, 0.0014,\n                      0.0022, 0.0021, 0.0013, 0.0016, 0.0006, 0.0015, 0.0019, 0.0014, 0.0018,\n                      0.0015, 0.0019, 0.0013, 0.0020, 0.0025, 0.0008, 0.0021, 0.0020, 0.0023,\n                      0.0023, 0.0012, 0.0030, 0.0011, 0.0021, 0.0019, 0.0016, 0.0011, 0.0011,\n                      0.0022, 0.0014, 0.0010, 0.0016, 0.0014, 0.0022, 0.0020, 0.0012, 0.0011,\n                      0.0009, 0.0024, 0.0016, 0.0021, 0.0020, 0.0021, 0.0026, 0.0019, 0.0009,\n                      0.0021, 0.0021, 0.0005, 0.0019, 0.0022, 0.0015, 0.0023, 0.0016, 0.0022,\n                      0.0015, 0.0017, 0.0013, 0.0023, 0.0016, 0.0019, 0.0020, 0.0015, 0.0020,\n                      0.0015, 0.0022, 0.0018, 0.0012, 0.0015, 0.0019, 0.0020, 0.0015, 0.0018,\n                      0.0021, 0.0022, 0.0028, 0.0012, 0.0020, 0.0020, 0.0017, 0.0021, 0.0015,\n                      0.0024, 0.0016, 0.0020, 0.0021, 0.0018, 0.0019, 0.0015, 0.0012, 0.0006,\n                      0.0010, 0.0014, 0.0004, 0.0027, 0.0022, 0.0023, 0.0020, 0.0018, 0.0015,\n                      0.0018, 0.0045, 0.0020, 0.0010, 0.0026, 0.0021, 0.0014, 0.0014, 0.0006,\n                      0.0020, 0.0021, 0.0011, 0.0017, 0.0021, 0.0019, 0.0010, 0.0016, 0.0029,\n                      0.0022, 0.0017, 0.0020, 0.0025, 0.0023, 0.0019, 0.0017, 0.0009, 0.0022,\n                      0.0022, 0.0018, 0.0022, 0.0020, 0.0022, 0.0015, 0.0014, 0.0015, 0.0021,\n                      0.0022, 0.0014, 0.0011, 0.0019, 0.0013, 0.0009, 0.0013, 0.0019, 0.0016,\n                      0.0018, 0.0012, 0.0013, 0.0003, 0.0022, 0.0023, 0.0020, 0.0017, 0.0018,\n                      0.0017, 0.0026, 0.0017, 0.0016, 0.0016, 0.0014, 0.0012, 0.0018, 0.0017,\n                      0.0013, 0.0011, 0.0016, 0.0013, 0.0021, 0.0015, 0.0022, 0.0009, 0.0014,\n                      0.0023, 0.0011, 0.0011, 0.0010, 0.0016, 0.0018, 0.0023, 0.0008, 0.0010,\n                      0.0013, 0.0026, 0.0010, 0.0019, 0.0023, 0.0015, 0.0018, 0.0022, 0.0023,\n                      0.0011, 0.0018, 0.0008, 0.0016, 0.0019, 0.0014, 0.0018, 0.0014, 0.0019,\n                      0.0020, 0.0016, 0.0018, 0.0021, 0.0022, 0.0014, 0.0018, 0.0020, 0.0011,\n                      0.0014, 0.0027, 0.0011, 0.0023, 0.0023, 0.0023, 0.0016, 0.0021, 0.0020,\n                      0.0024, 0.0014, 0.0022, 0.0015, 0.0020, 0.0009, 0.0017, 0.0016, 0.0021,\n                      0.0007, 0.0022, 0.0022, 0.0020, 0.0010, 0.0020, 0.0025, 0.0015, 0.0021,\n                      0.0011, 0.0008, 0.0018, 0.0018, 0.0023, 0.0017, 0.0021, 0.0022, 0.0015,\n                      0.0022, 0.0018, 0.0014, 0.0018, 0.0019, 0.0010, 0.0017, 0.0015, 0.0013,\n                      0.0010, 0.0005, 0.0016, 0.0018, 0.0018, 0.0014, 0.0019, 0.0015, 0.0019,\n                      0.0020, 0.0022, 0.0014, 0.0016, 0.0019, 0.0019, 0.0018, 0.0011, 0.0012,\n                      0.0015, 0.0017, 0.0018, 0.0011, 0.0022, 0.0019, 0.0019, 0.0022, 0.0019,\n                      0.0015, 0.0019, 0.0022, 0.0019, 0.0017, 0.0027, 0.0013, 0.0019, 0.0012,\n                      0.0022, 0.0022, 0.0021, 0.0016, 0.0015, 0.0018, 0.0013, 0.0023, 0.0021,\n                      0.0019, 0.0019, 0.0021, 0.0022, 0.0015, 0.0015, 0.0013, 0.0015, 0.0021,\n                      0.0018, 0.0015, 0.0021, 0.0021, 0.0023, 0.0022, 0.0019, 0.0013, 0.0018,\n                      0.0019, 0.0027, 0.0020, 0.0017, 0.0009, 0.0011, 0.0020, 0.0015, 0.0014,\n                      0.0007, 0.0015, 0.0018, 0.0011, 0.0022, 0.0023, 0.0017, 0.0023, 0.0016,\n                      0.0022, 0.0012, 0.0016, 0.0011, 0.0014, 0.0023, 0.0015, 0.0020, 0.0023,\n                      0.0011, 0.0012, 0.0012, 0.0018, 0.0022, 0.0011, 0.0017, 0.0012, 0.0012,\n                      0.0024, 0.0020, 0.0012, 0.0016, 0.0027, 0.0015, 0.0003, 0.0020, 0.0021,\n                      0.0020, 0.0021, 0.0006, 0.0016, 0.0015, 0.0015, 0.0021, 0.0019, 0.0021,\n                      0.0014, 0.0012, 0.0018, 0.0021, 0.0016, 0.0018]), zero_point=tensor([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0, -128,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,  127,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0, -128,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n                     dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n                min_val=tensor([-1.4282e-01, -2.6802e-01, -1.1517e-01, -1.5087e-01, -1.0168e-01,\n                        -1.2461e-01, -2.7691e-01, -3.6167e-02, -9.5674e-02, -2.7989e-01,\n                        -2.7442e-01, -1.1491e-01, -2.0607e-01, -8.1594e-02, -1.8752e-01,\n                        -2.4708e-01, -1.2648e-01, -1.3156e-01, -1.1726e-01, -1.2042e-01,\n                        -1.6802e-01, -2.5149e-01, -1.2200e-01, -1.0283e-01, -2.6820e-01,\n                        -2.5224e-01, -1.6913e-01, -1.6701e-01, -1.4973e-01, -4.6885e-02,\n                        -1.2375e-01, -1.5130e-01, -1.2013e-01, -2.0782e-01, -1.3822e-01,\n                        -1.1322e-01, -1.5883e-01, -1.7308e-01, -1.0999e-01, -1.0139e-01,\n                        -1.7712e-01, -1.5463e-01, -2.6070e-01, -1.0268e-01, -1.3608e-01,\n                        -1.1167e-01, -1.4800e-02, -1.3203e-01, -1.4503e-01, -2.5483e-01,\n                        -1.0660e-01, -1.8975e-02, -2.4720e-01, -1.1335e-01, -1.3623e-01,\n                        -1.6477e-01, -6.1131e-02, -1.2945e-01, -2.8405e-01, -1.9384e-01,\n                        -2.9120e-01, -1.5861e-01, -2.7713e-01, -1.3829e-02, -1.1077e-01,\n                        -1.6111e-01, -2.9071e-01, -1.7601e-01, -2.4113e-01, -1.2708e-01,\n                        -1.6404e-01, -2.5908e-01, -7.4741e-02, -2.8405e-01, -1.3180e-01,\n                        -1.0589e-01, -1.9751e-01, -2.4076e-01, -1.3565e-01, -1.9188e-01,\n                        -1.4259e-01, -1.4759e-01, -1.4825e-01, -3.5294e-01, -1.5575e-01,\n                        -2.5547e-01, -5.9768e-02, -1.0245e-01, -1.5095e-01, -1.9736e-01,\n                        -3.0324e-01, -2.0191e-01, -1.3501e-01, -2.6918e-01, -1.0885e-01,\n                        -1.2394e-01, -1.9521e-01, -1.5731e-01, -7.6096e-02, -1.0473e-01,\n                        -1.1128e-01, -6.5864e-03, -3.4201e-01, -2.8100e-01, -2.8959e-01,\n                        -2.5819e-01, -1.2479e-01, -1.8595e-01, -2.3631e-01, -6.8671e-02,\n                        -9.8701e-03, -1.3225e-01, -7.8202e-03, -1.6125e-01, -1.2847e-01,\n                        -1.0402e-01, -7.2234e-02, -2.5826e-01, -1.4829e-01, -1.3671e-01,\n                        -2.1423e-01, -1.5617e-01, -1.3182e-01,  2.2176e-04, -1.7220e-01,\n                        -3.6783e-01, -1.7517e-01, -1.3223e-02, -1.4568e-01, -1.0955e-02,\n                        -8.3365e-03, -1.3826e-01, -1.9508e-01, -1.0615e-01, -2.7871e-01,\n                        -2.8747e-01, -1.2769e-01, -2.7754e-01, -2.0558e-03, -2.8638e-01,\n                        -1.9029e-01, -1.0735e-01, -1.9770e-01, -1.6336e-01, -1.5357e-01,\n                        -1.8348e-01, -1.3833e-01, -2.3913e-01, -1.0482e-01, -1.2153e-01,\n                        -1.3677e-01, -1.2081e-01, -1.1172e-01, -2.3176e-01, -9.5188e-02,\n                        -1.6365e-01, -3.8398e-02, -1.6330e-01, -1.5072e-01, -1.2933e-01,\n                        -3.2621e-02, -1.1808e-01, -1.1523e-01, -3.2666e-01, -2.1228e-01,\n                        -1.0585e-01, -1.2222e-01, -1.7752e-01, -1.0023e-01, -1.1949e-01,\n                        -2.1364e-01, -1.6764e-01, -7.3341e-03, -1.3349e-01, -1.0424e-01,\n                        -2.7018e-01, -1.2097e-01, -1.6310e-01, -9.5739e-02, -1.7716e-01,\n                        -1.7700e-01, -1.3502e-01, -1.4017e-01, -7.3379e-02, -2.0567e-01,\n                        -2.3024e-01, -1.6086e-01, -9.2296e-02, -1.0089e-01, -1.5847e-01,\n                        -7.3451e-03, -2.6765e-01, -2.4067e-01, -2.9535e-01, -1.2383e-01,\n                        -3.7740e-02, -1.4976e-01, -2.9424e-01, -1.4683e-01, -4.6462e-02,\n                        -1.0628e-01, -1.2444e-01, -2.4912e-01, -7.5347e-02, -1.2992e-01,\n                        -1.7482e-01, -2.3779e-01, -1.4216e-01, -2.0068e-01, -7.1486e-02,\n                        -2.7373e-01, -1.4765e-01, -1.1992e-01, -2.2561e-01, -1.4964e-01,\n                        -1.3881e-01, -1.7971e-01, -2.1577e-02, -1.4118e-01, -2.9877e-01,\n                        -3.0065e-01, -1.6509e-01, -2.1079e-01, -1.5050e-01, -2.2792e-02,\n                        -3.0933e-01, -2.5184e-02, -1.6168e-01, -1.9678e-01, -3.7387e-02,\n                        -1.1367e-01, -2.2117e-01, -2.0649e-01, -2.6811e-01, -9.2325e-02,\n                        -1.4714e-01, -1.5225e-01, -2.5298e-01, -1.1513e-01, -2.5845e-01,\n                        -3.1428e-01, -1.9730e-01, -1.4409e-01, -7.8122e-03, -5.6617e-03,\n                        -2.2285e-01, -2.2580e-01, -1.7558e-01, -2.2030e-01, -2.6644e-01,\n                        -1.6089e-01, -1.0731e-01, -1.6432e-01, -1.2274e-01, -1.0298e-01,\n                        -1.1097e-01, -1.2916e-01, -1.2677e-01, -1.1208e-01, -1.8594e-01,\n                        -1.0950e-01, -1.0412e-01, -7.7220e-03, -1.4397e-01, -1.9211e-01,\n                        -2.3241e-01, -1.4575e-01, -2.3971e-01, -1.8731e-01, -2.4812e-01,\n                        -1.3691e-01, -1.5561e-01, -1.8342e-01, -1.9885e-01, -1.7792e-01,\n                        -2.4411e-01, -2.3362e-01, -1.4481e-01, -8.4097e-02, -1.1078e-01,\n                        -2.2010e-01, -2.2462e-01, -1.0737e-01, -2.7643e-01, -2.4912e-01,\n                        -2.4305e-01, -2.8599e-01, -1.2117e-01, -1.2168e-01, -1.3114e-01,\n                        -1.6423e-01, -1.3114e-01, -1.2358e-01, -3.5144e-01, -1.8882e-02,\n                        -1.3067e-01, -1.5088e-01, -1.6170e-01, -2.8798e-01, -1.4540e-01,\n                        -1.1037e-01, -1.1385e-01, -2.5467e-02, -1.4490e-01, -2.9615e-01,\n                        -2.6647e-01, -2.4548e-01, -2.4522e-01, -1.4568e-01, -2.8084e-01,\n                        -1.9777e-01, -1.3177e-02, -1.6938e-01, -1.1745e-01, -2.7475e-01,\n                        -2.3560e-01, -1.9451e-01, -1.6744e-01, -2.7413e-01, -1.6622e-01,\n                        -1.5106e-02, -1.2563e-01, -1.7193e-01, -2.2743e-01, -1.1723e-01,\n                        -4.9312e-02, -2.5891e-01, -1.1660e-01, -1.0884e-02, -9.7817e-02,\n                        -2.5787e-01, -1.1999e-01, -1.8512e-01, -9.3264e-02, -1.1360e-01,\n                        -1.5463e-01, -1.3715e-01, -1.5970e-01, -2.8851e-01, -2.2215e-01,\n                        -1.7116e-01, -2.0192e-01, -1.4802e-01, -1.4915e-01, -2.1088e-01,\n                        -9.1032e-03, -1.7813e-01, -1.6305e-01, -2.1800e-02, -2.5088e-01,\n                        -1.5146e-01, -1.2342e-01, -1.4775e-01, -1.4983e-01, -2.3665e-01,\n                        -1.8765e-01, -1.1099e-01, -2.1428e-01, -1.0927e-01, -1.1723e-01,\n                        -1.8072e-01, -2.5573e-01, -2.7979e-02, -1.1951e-01, -6.5508e-02,\n                        -1.0584e-01,  2.8387e-03, -1.4594e-01, -1.6996e-01, -1.6183e-01,\n                        -2.6470e-01, -7.4253e-02, -1.6014e-02, -1.1512e-01, -1.0872e-01,\n                        -1.5313e-01, -2.2280e-02, -2.6439e-01, -1.8302e-01, -1.1156e-01,\n                        -2.3278e-01, -2.7341e-01, -2.0518e-01, -1.2238e-01]), max_val=tensor([ 0.2462,  0.1611,  0.2682,  0.2747,  0.1741,  0.2150,  0.1615,  0.0934,\n                         0.1827,  0.1496,  0.1546,  0.1595,  0.1279,  0.0122,  0.1016,  0.1297,\n                         0.1730,  0.2246,  0.1845,  0.2403,  0.0401,  0.1518,  0.3150,  0.0158,\n                         0.1487,  0.1535,  0.2882,  0.2978,  0.1011,  0.3842,  0.1399,  0.2703,\n                         0.2402,  0.1129,  0.0904,  0.1377,  0.2751,  0.0206,  0.1315,  0.2042,\n                         0.0997,  0.2825,  0.1675,  0.1518,  0.1109,  0.1135,  0.3026,  0.1982,\n                         0.2610,  0.1429,  0.2680,  0.3296,  0.1319,  0.1085,  0.2700,  0.2694,\n                         0.0074,  0.2385,  0.1488,  0.1054,  0.1658,  0.1971,  0.1548,  0.1921,\n                         0.2201,  0.1442,  0.1575,  0.2002,  0.2002,  0.2537,  0.1918,  0.1458,\n                         0.1943,  0.1624,  0.2312,  0.1502,  0.1259,  0.1471,  0.2535,  0.1086,\n                         0.2265,  0.2696,  0.2835,  0.0198,  0.0087,  0.1570,  0.2483,  0.2098,\n                         0.2612,  0.1094,  0.1616,  0.1183,  0.2494,  0.1468,  0.2237,  0.2392,\n                         0.1727,  0.1436,  0.0223,  0.1303,  0.1816,  0.0559,  0.1091,  0.1524,\n                         0.1716,  0.1296,  0.2270,  0.0712,  0.1421,  0.5709,  0.2603,  0.1043,\n                         0.3316,  0.2703,  0.1761,  0.1787,  0.0033,  0.0216,  0.2614,  0.0064,\n                         0.1271,  0.2621,  0.2381,  0.2658,  0.2092,  0.0895,  0.2786,  0.2157,\n                         0.2530,  0.3149,  0.2907,  0.2431,  0.2127,  0.1141,  0.1553,  0.1778,\n                         0.2348,  0.1472,  0.2506,  0.1561,  0.1104,  0.1795,  0.1034,  0.2726,\n                         0.2764,  0.1231,  0.1325,  0.1292,  0.1690,  0.0995,  0.1606,  0.2413,\n                         0.2054,  0.1373,  0.1559,  0.1069,  0.0211,  0.2751,  0.2876,  0.2535,\n                         0.2149,  0.2261,  0.2204,  0.0137,  0.1210,  0.2052,  0.2009,  0.1323,\n                         0.1524,  0.2273,  0.1152,  0.1182,  0.1446,  0.1975,  0.1651,  0.1385,\n                         0.1944,  0.2741,  0.1164,  0.1068,  0.2908,  0.1181,  0.0017,  0.1244,\n                         0.0938,  0.1365,  0.2950,  0.1065,  0.1326,  0.1670,  0.3349, -0.0070,\n                         0.1330,  0.1739,  0.1868,  0.2254,  0.2767,  0.1634,  0.0766,  0.2331,\n                         0.1038,  0.1984,  0.1278,  0.1752,  0.2260,  0.1077,  0.1262,  0.2502,\n                         0.1265,  0.2283,  0.1595,  0.2830,  0.1804,  0.1111,  0.2571,  0.1226,\n                         0.0936,  0.3467,  0.0170,  0.2266,  0.1678,  0.2928,  0.0983,  0.2713,\n                         0.2490,  0.2518,  0.1787,  0.2839,  0.1040,  0.2527,  0.1125,  0.1219,\n                         0.1224,  0.1483,  0.0132,  0.2784,  0.2773,  0.1487,  0.1215,  0.1421,\n                         0.0130,  0.1106,  0.2667,  0.1399,  0.1048,  0.2314,  0.1197,  0.2871,\n                         0.0520,  0.1591,  0.2811,  0.1922,  0.2827,  0.2239,  0.1819,  0.2304,\n                         0.2367,  0.1329,  0.2100,  0.1182,  0.1707,  0.1270,  0.0696,  0.2011,\n                         0.2319,  0.1245,  0.1802,  0.1409,  0.1343,  0.1427,  0.2480,  0.2763,\n                         0.1392,  0.0604,  0.2365,  0.1432,  0.1201,  0.0835,  0.1550,  0.1952,\n                         0.1308,  0.0400,  0.1418,  0.1679,  0.1406,  0.2051,  0.1670,  0.2398,\n                         0.1902,  0.2374,  0.2751,  0.2468,  0.2212,  0.0109,  0.1675,  0.2371,\n                         0.0112,  0.2785,  0.1522,  0.2677,  0.2002,  0.1966,  0.2313,  0.1604,\n                         0.1670,  0.1533,  0.1330,  0.1279,  0.2656,  0.1701,  0.1170,  0.1872,\n                         0.0971,  0.1912,  0.1627,  0.1287,  0.1096,  0.2675,  0.0261,  0.2912,\n                         0.2855,  0.2448,  0.1324,  0.0319,  0.2409,  0.3431,  0.1792,  0.2135,\n                         0.1143,  0.1433,  0.1402,  0.1842,  0.1020,  0.0141,  0.1934,  0.2313,\n                         0.0647,  0.2828,  0.1585,  0.0481,  0.2937,  0.1184,  0.2739,  0.1119,\n                         0.0992,  0.1431,  0.0520,  0.2889,  0.1853,  0.1448,  0.2861,  0.1396,\n                         0.0555,  0.1263,  0.1335,  0.2746,  0.1391,  0.1091,  0.1509,  0.1537,\n                         0.3087,  0.1380,  0.1475,  0.2079,  0.3484,  0.1947,  0.0668,  0.2553,\n                         0.2722,  0.2561,  0.1311,  0.0009,  0.2092,  0.1944,  0.1957,  0.2630,\n                         0.2414,  0.1563,  0.1167,  0.1492,  0.1438,  0.1574,  0.1188,  0.2309])\n              )\n            )\n          )\n          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(\n            in_features=384, out_features=1536, bias=True\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0015, 0.0021, 0.0013,  ..., 0.0015, 0.0020, 0.0014]), zero_point=tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([-0.1769, -0.2656, -0.1667,  ..., -0.1920, -0.2149, -0.1746]), max_val=tensor([0.1921, 0.1943, 0.1353,  ..., 0.1029, 0.2591, 0.1227]))\n            )\n          )\n          (4): GELU(approximate='none')\n          (5): Linear(\n            in_features=1536, out_features=384, bias=True\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0017, 0.0034, 0.0015, 0.0020, 0.0018, 0.0017, 0.0024, 0.0013, 0.0014,\n                      0.0018, 0.0016, 0.0018, 0.0025, 0.0017, 0.0015, 0.0015, 0.0018, 0.0016,\n                      0.0015, 0.0016, 0.0026, 0.0016, 0.0032, 0.0018, 0.0018, 0.0019, 0.0013,\n                      0.0016, 0.0016, 0.0047, 0.0023, 0.0016, 0.0014, 0.0015, 0.0022, 0.0015,\n                      0.0014, 0.0015, 0.0015, 0.0014, 0.0021, 0.0018, 0.0019, 0.0016, 0.0020,\n                      0.0027, 0.0034, 0.0024, 0.0015, 0.0013, 0.0015, 0.0030, 0.0020, 0.0015,\n                      0.0013, 0.0036, 0.0014, 0.0016, 0.0017, 0.0017, 0.0014, 0.0020, 0.0019,\n                      0.0015, 0.0014, 0.0027, 0.0020, 0.0020, 0.0013, 0.0015, 0.0017, 0.0017,\n                      0.0038, 0.0030, 0.0014, 0.0015, 0.0017, 0.0034, 0.0014, 0.0016, 0.0018,\n                      0.0017, 0.0016, 0.0021, 0.0020, 0.0017, 0.0019, 0.0016, 0.0016, 0.0015,\n                      0.0015, 0.0014, 0.0014, 0.0018, 0.0014, 0.0018, 0.0020, 0.0023, 0.0014,\n                      0.0027, 0.0018, 0.0015, 0.0026, 0.0019, 0.0028, 0.0016, 0.0017, 0.0025,\n                      0.0013, 0.0050, 0.0016, 0.0017, 0.0023, 0.0016, 0.0029, 0.0014, 0.0015,\n                      0.0023, 0.0018, 0.0018, 0.0018, 0.0016, 0.0013, 0.0018, 0.0020, 0.0042,\n                      0.0028, 0.0014, 0.0015, 0.0031, 0.0036, 0.0017, 0.0021, 0.0020, 0.0017,\n                      0.0026, 0.0015, 0.0020, 0.0024, 0.0014, 0.0020, 0.0017, 0.0015, 0.0017,\n                      0.0019, 0.0022, 0.0020, 0.0018, 0.0014, 0.0017, 0.0034, 0.0018, 0.0016,\n                      0.0015, 0.0018, 0.0021, 0.0047, 0.0016, 0.0017, 0.0019, 0.0017, 0.0015,\n                      0.0015, 0.0018, 0.0017, 0.0014, 0.0014, 0.0015, 0.0018, 0.0017, 0.0016,\n                      0.0015, 0.0016, 0.0015, 0.0016, 0.0017, 0.0016, 0.0019, 0.0018, 0.0016,\n                      0.0023, 0.0018, 0.0022, 0.0014, 0.0016, 0.0018, 0.0024, 0.0014, 0.0032,\n                      0.0022, 0.0017, 0.0029, 0.0022, 0.0016, 0.0020, 0.0033, 0.0015, 0.0024,\n                      0.0019, 0.0018, 0.0015, 0.0017, 0.0016, 0.0022, 0.0016, 0.0021, 0.0015,\n                      0.0016, 0.0014, 0.0026, 0.0015, 0.0023, 0.0018, 0.0015, 0.0018, 0.0027,\n                      0.0036, 0.0043, 0.0016, 0.0023, 0.0017, 0.0024, 0.0022, 0.0017, 0.0021,\n                      0.0022, 0.0024, 0.0014, 0.0014, 0.0018, 0.0033, 0.0019, 0.0021, 0.0016,\n                      0.0016, 0.0015, 0.0018, 0.0018, 0.0019, 0.0014, 0.0034, 0.0016, 0.0014,\n                      0.0016, 0.0018, 0.0018, 0.0014, 0.0017, 0.0018, 0.0017, 0.0027, 0.0019,\n                      0.0019, 0.0015, 0.0015, 0.0016, 0.0020, 0.0016, 0.0017, 0.0018, 0.0014,\n                      0.0015, 0.0018, 0.0019, 0.0026, 0.0018, 0.0028, 0.0016, 0.0020, 0.0019,\n                      0.0016, 0.0016, 0.0020, 0.0025, 0.0017, 0.0016, 0.0014, 0.0017, 0.0028,\n                      0.0015, 0.0015, 0.0018, 0.0014, 0.0019, 0.0024, 0.0018, 0.0017, 0.0013,\n                      0.0014, 0.0016, 0.0016, 0.0015, 0.0014, 0.0036, 0.0015, 0.0022, 0.0015,\n                      0.0021, 0.0021, 0.0015, 0.0015, 0.0016, 0.0021, 0.0018, 0.0019, 0.0017,\n                      0.0016, 0.0017, 0.0016, 0.0017, 0.0013, 0.0017, 0.0018, 0.0019, 0.0020,\n                      0.0016, 0.0014, 0.0023, 0.0049, 0.0017, 0.0016, 0.0014, 0.0015, 0.0016,\n                      0.0013, 0.0020, 0.0029, 0.0021, 0.0016, 0.0021, 0.0017, 0.0014, 0.0016,\n                      0.0013, 0.0014, 0.0021, 0.0014, 0.0022, 0.0022, 0.0022, 0.0028, 0.0015,\n                      0.0017, 0.0014, 0.0024, 0.0014, 0.0016, 0.0025, 0.0019, 0.0016, 0.0018,\n                      0.0017, 0.0016, 0.0039, 0.0015, 0.0019, 0.0016, 0.0015, 0.0018, 0.0018,\n                      0.0024, 0.0016, 0.0018, 0.0016, 0.0026, 0.0013, 0.0014, 0.0015, 0.0017,\n                      0.0021, 0.0015, 0.0014, 0.0021, 0.0019, 0.0014, 0.0031, 0.0015, 0.0017,\n                      0.0015, 0.0018, 0.0014, 0.0019, 0.0016, 0.0018]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                     dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n                min_val=tensor([-0.1771, -0.2910, -0.1979, -0.2519, -0.2316, -0.2038, -0.3020, -0.1695,\n                        -0.1785, -0.2316, -0.2112, -0.1903, -0.3156, -0.2027, -0.1631, -0.1848,\n                        -0.2213, -0.1558, -0.1702, -0.1703, -0.3339, -0.1821, -0.2228, -0.2000,\n                        -0.2270, -0.2305, -0.1641, -0.2075, -0.2098, -0.2869, -0.2689, -0.2091,\n                        -0.1759, -0.1633, -0.2000, -0.1888, -0.1836, -0.1978, -0.1676, -0.1763,\n                        -0.2134, -0.2328, -0.2324, -0.1728, -0.2591, -0.2759, -0.4161, -0.2581,\n                        -0.1714, -0.1617, -0.1921, -0.3809, -0.2497, -0.1670, -0.1576, -0.2599,\n                        -0.1670, -0.1647, -0.2129, -0.2159, -0.1800, -0.2588, -0.1920, -0.1888,\n                        -0.1630, -0.3419, -0.2506, -0.2404, -0.1677, -0.1828, -0.1778, -0.1829,\n                        -0.2704, -0.3869, -0.1845, -0.1961, -0.2126, -0.3853, -0.1719, -0.2019,\n                        -0.1964, -0.1593, -0.1523, -0.2687, -0.1484, -0.2205, -0.2440, -0.2041,\n                        -0.2047, -0.1909, -0.1974, -0.1808, -0.1754, -0.1711, -0.1819, -0.2296,\n                        -0.2574, -0.1836, -0.1832, -0.2949, -0.2337, -0.1691, -0.3386, -0.1577,\n                        -0.3600, -0.2089, -0.2183, -0.2487, -0.1653, -0.3030, -0.1883, -0.1719,\n                        -0.2898, -0.2063, -0.3373, -0.1841, -0.1972, -0.2029, -0.2327, -0.2361,\n                        -0.1674, -0.1541, -0.1665, -0.1941, -0.1764, -0.5330, -0.3543, -0.1850,\n                        -0.1692, -0.3191, -0.4649, -0.1911, -0.1609, -0.2460, -0.1621, -0.3360,\n                        -0.1937, -0.1480, -0.3118, -0.1790, -0.2591, -0.2136, -0.1799, -0.2174,\n                        -0.1894, -0.1861, -0.2544, -0.1676, -0.1789, -0.1672, -0.4334, -0.2213,\n                        -0.2065, -0.1837, -0.2351, -0.2695, -0.3764, -0.1987, -0.2125, -0.2382,\n                        -0.2213, -0.1972, -0.1944, -0.2167, -0.2006, -0.1770, -0.1628, -0.1974,\n                        -0.1752, -0.2222, -0.2029, -0.1782, -0.2072, -0.1857, -0.1961, -0.2064,\n                        -0.2051, -0.2406, -0.1734, -0.1787, -0.2915, -0.1795, -0.2064, -0.1791,\n                        -0.2010, -0.2345, -0.3085, -0.1789, -0.4152, -0.2473, -0.2189, -0.3653,\n                        -0.2811, -0.2076, -0.2283, -0.3722, -0.1834, -0.3127, -0.1905, -0.2241,\n                        -0.1827, -0.2225, -0.2109, -0.2271, -0.1865, -0.1690, -0.1967, -0.1855,\n                        -0.1759, -0.3346, -0.1863, -0.1827, -0.2286, -0.1870, -0.2248, -0.3480,\n                        -0.2778, -0.5457, -0.1564, -0.2923, -0.2121, -0.3074, -0.2771, -0.1879,\n                        -0.2737, -0.2824, -0.2328, -0.1562, -0.1526, -0.2342, -0.4221, -0.1904,\n                        -0.2744, -0.1783, -0.1895, -0.1928, -0.2263, -0.1784, -0.2476, -0.1812,\n                        -0.4176, -0.2072, -0.1757, -0.1936, -0.2312, -0.1780, -0.1756, -0.2182,\n                        -0.2353, -0.1741, -0.2911, -0.2022, -0.1911, -0.1844, -0.1954, -0.2035,\n                        -0.2603, -0.1513, -0.1963, -0.1862, -0.1579, -0.1961, -0.1795, -0.2409,\n                        -0.3377, -0.2289, -0.3050, -0.1979, -0.2085, -0.2485, -0.1560, -0.1825,\n                        -0.2448, -0.3224, -0.2160, -0.2111, -0.1646, -0.2156, -0.2759, -0.1561,\n                        -0.1883, -0.2250, -0.1746, -0.2185, -0.3016, -0.1874, -0.2006, -0.1699,\n                        -0.1752, -0.1968, -0.1743, -0.1969, -0.1856, -0.4565, -0.1924, -0.2807,\n                        -0.1907, -0.2637, -0.2715, -0.1873, -0.1829, -0.1714, -0.1757, -0.2106,\n                        -0.2426, -0.2216, -0.1692, -0.2176, -0.1796, -0.2160, -0.1567, -0.2156,\n                        -0.2362, -0.2426, -0.2558, -0.1683, -0.1830, -0.2222, -0.5716, -0.2118,\n                        -0.1863, -0.1741, -0.1917, -0.2059, -0.1722, -0.1708, -0.2938, -0.2108,\n                        -0.1905, -0.2673, -0.2235, -0.1584, -0.2021, -0.1701, -0.1724, -0.1876,\n                        -0.1821, -0.2421, -0.2793, -0.2877, -0.3623, -0.1908, -0.1782, -0.1823,\n                        -0.2423, -0.1796, -0.2059, -0.3164, -0.1454, -0.2057, -0.2276, -0.2188,\n                        -0.2049, -0.3150, -0.1868, -0.2232, -0.1991, -0.1971, -0.2106, -0.1832,\n                        -0.3041, -0.1687, -0.1840, -0.2026, -0.3353, -0.1609, -0.1603, -0.1960,\n                        -0.2211, -0.2739, -0.1947, -0.1821, -0.1658, -0.2414, -0.1849, -0.3606,\n                        -0.1759, -0.1747, -0.1957, -0.2229, -0.1794, -0.1850, -0.1739, -0.2266]), max_val=tensor([0.2192, 0.4284, 0.1656, 0.1755, 0.2182, 0.2156, 0.2377, 0.1580, 0.1608,\n                        0.1992, 0.1927, 0.2250, 0.3011, 0.2202, 0.1921, 0.1871, 0.2274, 0.2032,\n                        0.1867, 0.2040, 0.2076, 0.1983, 0.4011, 0.2321, 0.1786, 0.2405, 0.1525,\n                        0.1960, 0.1807, 0.5937, 0.2971, 0.1716, 0.1617, 0.1902, 0.2762, 0.1895,\n                        0.1562, 0.1867, 0.1871, 0.1835, 0.2631, 0.1748, 0.2398, 0.1985, 0.2001,\n                        0.3426, 0.4287, 0.2998, 0.1890, 0.1611, 0.1798, 0.3222, 0.1839, 0.1963,\n                        0.1686, 0.4632, 0.1821, 0.1998, 0.2033, 0.1907, 0.1827, 0.1606, 0.2393,\n                        0.1598, 0.1808, 0.2473, 0.2446, 0.2550, 0.1711, 0.1870, 0.2148, 0.2125,\n                        0.4788, 0.3448, 0.1505, 0.1754, 0.2008, 0.4273, 0.1721, 0.2004, 0.2233,\n                        0.2154, 0.2090, 0.2685, 0.2545, 0.1979, 0.1980, 0.1982, 0.1853, 0.1614,\n                        0.1958, 0.1810, 0.1721, 0.2339, 0.1732, 0.1818, 0.2372, 0.2892, 0.1456,\n                        0.3382, 0.1700, 0.1906, 0.2090, 0.2439, 0.3083, 0.1862, 0.1753, 0.3165,\n                        0.1714, 0.6349, 0.2024, 0.2180, 0.1793, 0.1967, 0.3707, 0.1669, 0.1761,\n                        0.2900, 0.2060, 0.1802, 0.2312, 0.1997, 0.1636, 0.2332, 0.2492, 0.4529,\n                        0.2434, 0.1820, 0.1953, 0.3880, 0.4306, 0.2173, 0.2649, 0.2557, 0.2143,\n                        0.2369, 0.1923, 0.2505, 0.2928, 0.1722, 0.2317, 0.1935, 0.1916, 0.2148,\n                        0.2429, 0.2791, 0.2094, 0.2260, 0.1837, 0.2160, 0.3102, 0.2244, 0.1893,\n                        0.1895, 0.1897, 0.2493, 0.5966, 0.1900, 0.2087, 0.1838, 0.1803, 0.1683,\n                        0.1678, 0.2341, 0.2121, 0.1784, 0.1740, 0.1627, 0.2345, 0.1697, 0.1686,\n                        0.1857, 0.1774, 0.1923, 0.2012, 0.2097, 0.1763, 0.2334, 0.2300, 0.1992,\n                        0.1965, 0.2233, 0.2795, 0.1840, 0.2003, 0.1896, 0.2863, 0.1753, 0.4078,\n                        0.2791, 0.2213, 0.3542, 0.1648, 0.1794, 0.2597, 0.4242, 0.1928, 0.2540,\n                        0.2406, 0.2194, 0.1867, 0.1849, 0.1972, 0.2840, 0.2030, 0.2608, 0.1705,\n                        0.2018, 0.1494, 0.2881, 0.1625, 0.2887, 0.1782, 0.1759, 0.2111, 0.2857,\n                        0.4628, 0.5207, 0.2010, 0.1789, 0.1961, 0.3108, 0.1981, 0.2183, 0.2222,\n                        0.1960, 0.3097, 0.1735, 0.1791, 0.1582, 0.3662, 0.2353, 0.2141, 0.2046,\n                        0.1981, 0.1704, 0.1858, 0.2239, 0.2214, 0.1608, 0.4342, 0.1875, 0.1455,\n                        0.2046, 0.1939, 0.2346, 0.1735, 0.1779, 0.2040, 0.2103, 0.3422, 0.2414,\n                        0.2389, 0.1965, 0.1805, 0.1952, 0.1776, 0.2042, 0.2204, 0.2321, 0.1749,\n                        0.1932, 0.2271, 0.2005, 0.3273, 0.2266, 0.3584, 0.2029, 0.2527, 0.1769,\n                        0.1979, 0.1991, 0.2565, 0.2628, 0.1887, 0.1992, 0.1812, 0.1774, 0.3591,\n                        0.1861, 0.1857, 0.1771, 0.1653, 0.2443, 0.1919, 0.2310, 0.2194, 0.1624,\n                        0.1817, 0.2008, 0.1999, 0.1541, 0.1784, 0.4004, 0.1875, 0.1987, 0.1813,\n                        0.2523, 0.2006, 0.1896, 0.1925, 0.2077, 0.2644, 0.2223, 0.2234, 0.1829,\n                        0.1989, 0.1807, 0.2039, 0.1842, 0.1607, 0.1673, 0.2129, 0.1606, 0.1967,\n                        0.2052, 0.1777, 0.2923, 0.6161, 0.1818, 0.2022, 0.1628, 0.1762, 0.1725,\n                        0.1630, 0.2559, 0.3745, 0.2606, 0.2015, 0.2285, 0.1565, 0.1796, 0.1681,\n                        0.1697, 0.1800, 0.2640, 0.1770, 0.2803, 0.2821, 0.2453, 0.2620, 0.1619,\n                        0.2109, 0.1784, 0.3033, 0.1794, 0.2030, 0.2461, 0.2392, 0.1822, 0.1428,\n                        0.1824, 0.1839, 0.4948, 0.1928, 0.2372, 0.1711, 0.1648, 0.2226, 0.2223,\n                        0.2255, 0.2012, 0.2314, 0.1810, 0.3068, 0.1571, 0.1813, 0.1436, 0.2079,\n                        0.2614, 0.1896, 0.1732, 0.2689, 0.1960, 0.1824, 0.3956, 0.1932, 0.2097,\n                        0.1922, 0.2284, 0.1808, 0.2428, 0.2009, 0.1742])\n              )\n            )\n          )\n        )\n      )\n      (6): Module(\n        (block): Module(\n          (0): Conv2d(\n            384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0024, 0.0027, 0.0020, 0.0025, 0.0020, 0.0021, 0.0026, 0.0013, 0.0016,\n                      0.0025, 0.0023, 0.0020, 0.0017, 0.0006, 0.0019, 0.0020, 0.0026, 0.0017,\n                      0.0007, 0.0024, 0.0015, 0.0022, 0.0023, 0.0018, 0.0024, 0.0019, 0.0027,\n                      0.0026, 0.0012, 0.0027, 0.0012, 0.0026, 0.0019, 0.0018, 0.0007, 0.0009,\n                      0.0026, 0.0017, 0.0010, 0.0020, 0.0018, 0.0026, 0.0024, 0.0019, 0.0006,\n                      0.0011, 0.0013, 0.0017, 0.0024, 0.0024, 0.0022, 0.0011, 0.0023, 0.0003,\n                      0.0025, 0.0026, 0.0003, 0.0020, 0.0024, 0.0014, 0.0026, 0.0014, 0.0026,\n                      0.0008, 0.0016, 0.0012, 0.0027, 0.0016, 0.0017, 0.0019, 0.0020, 0.0024,\n                      0.0016, 0.0025, 0.0016, 0.0007, 0.0019, 0.0011, 0.0025, 0.0016, 0.0021,\n                      0.0026, 0.0027, 0.0018, 0.0011, 0.0019, 0.0013, 0.0017, 0.0025, 0.0016,\n                      0.0028, 0.0018, 0.0025, 0.0026, 0.0019, 0.0023, 0.0018, 0.0012, 0.0011,\n                      0.0013, 0.0017, 0.0003, 0.0016, 0.0025, 0.0025, 0.0024, 0.0022, 0.0015,\n                      0.0024, 0.0011, 0.0018, 0.0003, 0.0003, 0.0027, 0.0016, 0.0003, 0.0012,\n                      0.0013, 0.0024, 0.0016, 0.0019, 0.0024, 0.0020, 0.0010, 0.0028, 0.0021,\n                      0.0025, 0.0017, 0.0023, 0.0011, 0.0007, 0.0016, 0.0012, 0.0013, 0.0026,\n                      0.0028, 0.0023, 0.0021, 0.0011, 0.0027, 0.0021, 0.0012, 0.0021, 0.0026,\n                      0.0025, 0.0014, 0.0015, 0.0023, 0.0007, 0.0018, 0.0014, 0.0022, 0.0013,\n                      0.0020, 0.0005, 0.0015, 0.0003, 0.0025, 0.0024, 0.0025, 0.0013, 0.0021,\n                      0.0019, 0.0020, 0.0021, 0.0014, 0.0010, 0.0009, 0.0006, 0.0022, 0.0018,\n                      0.0015, 0.0007, 0.0014, 0.0016, 0.0023, 0.0014, 0.0026, 0.0006, 0.0008,\n                      0.0027, 0.0018, 0.0012, 0.0013, 0.0014, 0.0021, 0.0027, 0.0003, 0.0013,\n                      0.0015, 0.0019, 0.0014, 0.0023, 0.0027, 0.0017, 0.0011, 0.0026, 0.0025,\n                      0.0016, 0.0021, 0.0015, 0.0016, 0.0020, 0.0020, 0.0019, 0.0015, 0.0021,\n                      0.0026, 0.0020, 0.0017, 0.0026, 0.0018, 0.0016, 0.0018, 0.0024, 0.0014,\n                      0.0018, 0.0010, 0.0012, 0.0012, 0.0028, 0.0027, 0.0018, 0.0023, 0.0012,\n                      0.0014, 0.0024, 0.0024, 0.0013, 0.0008, 0.0013, 0.0022, 0.0020, 0.0026,\n                      0.0004, 0.0026, 0.0026, 0.0021, 0.0012, 0.0025, 0.0011, 0.0011, 0.0023,\n                      0.0009, 0.0007, 0.0025, 0.0021, 0.0025, 0.0022, 0.0026, 0.0025, 0.0020,\n                      0.0025, 0.0022, 0.0017, 0.0020, 0.0020, 0.0009, 0.0018, 0.0012, 0.0014,\n                      0.0012, 0.0008, 0.0012, 0.0009, 0.0020, 0.0011, 0.0023, 0.0021, 0.0024,\n                      0.0022, 0.0026, 0.0012, 0.0020, 0.0020, 0.0021, 0.0021, 0.0010, 0.0017,\n                      0.0013, 0.0023, 0.0014, 0.0004, 0.0025, 0.0023, 0.0017, 0.0027, 0.0023,\n                      0.0016, 0.0017, 0.0026, 0.0019, 0.0020, 0.0010, 0.0015, 0.0021, 0.0014,\n                      0.0026, 0.0023, 0.0023, 0.0018, 0.0015, 0.0006, 0.0015, 0.0027, 0.0023,\n                      0.0020, 0.0020, 0.0025, 0.0025, 0.0020, 0.0004, 0.0016, 0.0007, 0.0022,\n                      0.0019, 0.0017, 0.0025, 0.0010, 0.0027, 0.0010, 0.0021, 0.0006, 0.0011,\n                      0.0022, 0.0015, 0.0015, 0.0015, 0.0011, 0.0015, 0.0026, 0.0013, 0.0014,\n                      0.0012, 0.0018, 0.0022, 0.0012, 0.0027, 0.0026, 0.0018, 0.0027, 0.0015,\n                      0.0025, 0.0014, 0.0015, 0.0014, 0.0019, 0.0025, 0.0006, 0.0019, 0.0026,\n                      0.0007, 0.0016, 0.0010, 0.0021, 0.0023, 0.0017, 0.0017, 0.0014, 0.0010,\n                      0.0027, 0.0022, 0.0010, 0.0017, 0.0013, 0.0013, 0.0005, 0.0025, 0.0024,\n                      0.0024, 0.0024, 0.0008, 0.0020, 0.0016, 0.0008, 0.0024, 0.0024, 0.0025,\n                      0.0019, 0.0019, 0.0024, 0.0026, 0.0023, 0.0023]), zero_point=tensor([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,  127,    0,    0,  127,    0,    0,    0,\n                         0,    0,    0, -128,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,  127,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,  127,  127,    0,    0,  127,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,  127,    0,    0,    0,    0,    0,  127,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,  127,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0, -128,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,  127,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0, -128,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,  127,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,  127,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n                     dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n                min_val=tensor([-0.0121, -0.0531, -0.2508, -0.3202, -0.0677, -0.0107, -0.3294, -0.0064,\n                        -0.0066, -0.0126, -0.2928, -0.0196, -0.2034, -0.0732, -0.0209, -0.2602,\n                        -0.3274, -0.0009, -0.0038, -0.3011, -0.0936, -0.2878, -0.2972, -0.2251,\n                        -0.0071, -0.0075, -0.3411, -0.3336, -0.0029, -0.3475, -0.0957, -0.0158,\n                        -0.0079, -0.2310, -0.0936, -0.1172, -0.3346, -0.2137, -0.0006, -0.2581,\n                        -0.2280, -0.0433, -0.3048, -0.2390, -0.0026, -0.0831, -0.1161, -0.2223,\n                        -0.0108, -0.3088, -0.2849, -0.1258, -0.2964, -0.0872, -0.0184, -0.3324,\n                        -0.0782, -0.2514, -0.3127, -0.1794, -0.3297, -0.1770, -0.0107,  0.0024,\n                        -0.2075, -0.1298, -0.3516, -0.2030, -0.2233, -0.0018, -0.0389, -0.0074,\n                        -0.2053, -0.3224, -0.2041, -0.0868, -0.1361, -0.1425, -0.0384, -0.1994,\n                        -0.2626, -0.0246, -0.0308, -0.2141, -0.0010, -0.2452, -0.1294, -0.2233,\n                        -0.0063, -0.2089, -0.3527, -0.0049, -0.3139, -0.0280, -0.0128, -0.2956,\n                        -0.1775, -0.1504, -0.1368, -0.0849, -0.2132, -0.0765, -0.2026, -0.3191,\n                        -0.3189, -0.3029, -0.2772, -0.1923, -0.0316, -0.1398, -0.2330, -0.0846,\n                        -0.0689, -0.0115, -0.1416, -0.0863, -0.1566, -0.0932, -0.3099, -0.0031,\n                        -0.2377, -0.3064, -0.0130, -0.1336, -0.0091, -0.1436, -0.3250, -0.0030,\n                        -0.2928, -0.1258, -0.0840, -0.0112, -0.1475, -0.1054, -0.0191, -0.0453,\n                        -0.0092, -0.2717, -0.1126, -0.0337, -0.1118, -0.0095, -0.0176, -0.0417,\n                        -0.0110, -0.1776, -0.0619, -0.0155, -0.1785, -0.1490, -0.1797, -0.0171,\n                        -0.1705, -0.2617, -0.1285, -0.1963, -0.0112, -0.0448, -0.3119, -0.0167,\n                        -0.0060, -0.2653, -0.2408, -0.1830, -0.2709, -0.1744, -0.0015, -0.1133,\n                        -0.0818, -0.0100, -0.2303, -0.1965, -0.1796, -0.0740, -0.0042, -0.2989,\n                        -0.1838, -0.0079, -0.0774, -0.1009, -0.3443, -0.0658, -0.0097, -0.1659,\n                        -0.1730, -0.0711, -0.3415,  0.0056, -0.0901, -0.1896, -0.2112, -0.1748,\n                        -0.0078, -0.0213, -0.0088, -0.1444, -0.3266, -0.3162, -0.1691, -0.2688,\n                        -0.1912, -0.2004, -0.2601, -0.2580, -0.2387, -0.1913, -0.2734, -0.0130,\n                        -0.2556, -0.2117, -0.0424, -0.0736, -0.1985, -0.1413, -0.3015, -0.0596,\n                        -0.2336, -0.1218, -0.0072, -0.0941, -0.0223, -0.0200, -0.2294, -0.2964,\n                        -0.0094, -0.1851, -0.3083, -0.0240, -0.0049, -0.0004, -0.0490, -0.0099,\n                        -0.0749, -0.3375, -0.0981, -0.0215, -0.0185, -0.0738, -0.1523, -0.0293,\n                        -0.1216, -0.0010, -0.2925, -0.1146, -0.0922, -0.3257, -0.0213, -0.3226,\n                        -0.0913, -0.3307, -0.3149, -0.2583, -0.3185, -0.2782, -0.0089, -0.2513,\n                        -0.2566, -0.1196, -0.0105, -0.1572, -0.0061, -0.1592, -0.1040, -0.0105,\n                        -0.1155, -0.1167, -0.1152, -0.2957, -0.0430, -0.3044, -0.0039, -0.0084,\n                        -0.1588, -0.2513, -0.2574, -0.0780, -0.0119, -0.0079, -0.2204, -0.0017,\n                        -0.2901, -0.1779,  0.0004, -0.3150, -0.2930, -0.0178, -0.3503, -0.0333,\n                        -0.2007, -0.2158, -0.3374, -0.2418, -0.2550, -0.1334, -0.1860, -0.0087,\n                        -0.0300, -0.0112, -0.2985, -0.2988, -0.2312, -0.0044, -0.0799, -0.1961,\n                        -0.0138, -0.2920, -0.2612, -0.2594, -0.0161, -0.0347, -0.0128, -0.0924,\n                        -0.1317, -0.0017, -0.2862, -0.0050, -0.2238, -0.3207, -0.1019, -0.3462,\n                        -0.1216, -0.2632, -0.0208, -0.0167, -0.0592, -0.1926, -0.1980, -0.1955,\n                        -0.0091, -0.1892, -0.3295, -0.1651, -0.0050, -0.0038, -0.2328, -0.2857,\n                        -0.1494, -0.0138, -0.0160, -0.2317, -0.3461, -0.0070, -0.3256, -0.1547,\n                        -0.1923, -0.1832, -0.2476, -0.0079, -0.0798, -0.0032, -0.0226, -0.0019,\n                        -0.0092, -0.1273, -0.2643, -0.2984, -0.2152, -0.0062, -0.1774, -0.0016,\n                        -0.0110, -0.0107, -0.0023, -0.2146, -0.0927, -0.0017, -0.1198, -0.3150,\n                        -0.3127, -0.0392, -0.0095, -0.0963, -0.0073, -0.1979, -0.0090, -0.0432,\n                        -0.3046, -0.0372, -0.2492, -0.2370, -0.3113, -0.3284, -0.0240, -0.0154]), max_val=tensor([ 3.0683e-01,  3.4119e-01,  8.7956e-03,  3.5561e-02,  2.5559e-01,\n                         2.6203e-01,  1.0176e-02,  1.6995e-01,  2.0316e-01,  3.2308e-01,\n                         7.4445e-03,  2.5127e-01,  2.1158e-01,  6.8182e-03,  2.3947e-01,\n                         4.4115e-03,  3.9534e-02,  2.1595e-01,  8.2959e-02,  1.8224e-02,\n                         1.8947e-01,  5.1803e-02,  9.3076e-02,  9.3190e-03,  3.0618e-01,\n                         2.3745e-01,  2.5933e-02,  2.0887e-02,  1.5854e-01,  6.0783e-02,\n                         1.5201e-01,  3.2795e-01,  2.3539e-01,  1.7063e-03,  5.0445e-03,\n                         6.7847e-02,  2.6898e-02,  6.7143e-03,  1.2623e-01,  1.0291e-02,\n                         1.2666e-01,  3.3123e-01,  4.5868e-02,  9.3658e-02,  7.7098e-02,\n                         1.3503e-01,  1.6657e-01,  1.8573e-01,  3.0140e-01,  4.0842e-02,\n                         4.6707e-02,  1.4195e-01,  6.2348e-03, -2.5233e-03,  3.1228e-01,\n                         5.0968e-02, -3.1535e-03,  1.2634e-02,  1.8435e-02,  2.2169e-03,\n                         2.4317e-02,  1.0481e-01,  3.3532e-01,  2.1571e-01,  4.9086e-03,\n                         1.5079e-01,  1.7677e-02,  1.8485e-01,  1.0729e-02,  2.4751e-01,\n                         2.5891e-01,  3.0151e-01,  8.4051e-02,  8.2180e-03,  4.6251e-03,\n                         8.0427e-04,  2.4054e-01,  1.2782e-01,  3.1159e-01,  2.4328e-03,\n                         2.3690e-02,  3.2716e-01,  3.3858e-01,  2.2356e-01,  1.3687e-01,\n                         1.2034e-01,  1.6433e-01,  6.1867e-03,  3.1309e-01,  7.2738e-03,\n                         3.0868e-02,  2.2905e-01,  2.0075e-02,  3.3538e-01,  2.3710e-01,\n                         1.3552e-02,  2.3249e-01,  1.5519e-01,  7.0254e-03,  1.6359e-01,\n                         4.8916e-03, -3.9105e-03,  1.6655e-01,  3.2720e-02,  4.1651e-02,\n                         1.4012e-02,  4.3475e-03,  1.3784e-01,  3.0398e-01,  8.4625e-02,\n                         1.1307e-03, -5.9507e-03, -1.1098e-03,  3.3989e-01,  2.0924e-01,\n                        -8.8140e-04,  2.5038e-03,  1.6119e-01,  8.9876e-03,  2.0630e-01,\n                         7.6830e-03,  1.6840e-02,  2.4882e-01,  1.4204e-02,  3.6084e-01,\n                         2.6668e-01,  1.1052e-02,  2.1127e-01,  1.1748e-02,  1.4591e-01,\n                         8.3694e-02,  1.9831e-01,  1.4174e-01,  1.6289e-01,  3.3329e-01,\n                         3.5819e-01,  2.8996e-01,  7.7572e-03,  1.3464e-01,  3.4464e-01,\n                         2.6096e-01,  1.5304e-01,  2.6975e-01,  3.2942e-01,  3.1579e-01,\n                         1.6354e-01,  1.9376e-01,  2.9129e-01, -4.2842e-05,  2.2604e-01,\n                         1.4421e-01,  2.8034e-01,  7.9903e-04,  1.0932e-02, -2.6496e-03,\n                         1.0221e-01,  3.7368e-02,  3.2084e-01,  2.7435e-02,  3.1690e-01,\n                         1.7130e-01,  8.7318e-03,  7.8368e-03,  2.5187e-01,  5.2873e-02,\n                         5.2600e-03,  1.2364e-01,  3.4727e-03,  3.4519e-03,  2.7552e-01,\n                         1.0647e-02,  1.7628e-03, -1.6126e-03,  1.8092e-01,  2.0793e-01,\n                         8.6340e-03,  4.7952e-03,  3.3524e-01,  2.3677e-03,  1.3227e-02,\n                         2.4523e-02,  2.2383e-01,  1.5289e-01,  9.5279e-03,  1.9366e-03,\n                         2.6407e-01,  8.8544e-03,  7.9859e-02,  1.6179e-01,  9.6244e-02,\n                         2.3835e-01,  1.3450e-01,  2.9477e-01,  3.4824e-01,  2.1684e-01,\n                         1.3622e-01,  1.2697e-02,  4.9788e-03,  2.0273e-01,  6.6304e-02,\n                         1.2649e-01,  3.5074e-03,  5.6624e-03,  1.7902e-01,  3.7856e-03,\n                         1.4248e-01,  1.1266e-02,  3.3197e-01,  1.2089e-02,  9.0730e-02,\n                         3.2994e-01,  2.2678e-01,  7.8921e-02,  2.2639e-01,  7.5569e-03,\n                         1.7537e-01,  1.5664e-01,  1.0059e-01,  1.5345e-01,  1.5146e-01,\n                         3.5502e-01,  3.4719e-01,  1.2129e-01,  3.6864e-03,  1.4779e-01,\n                         8.7507e-02,  4.8515e-02,  3.0150e-01,  1.6498e-01,  1.0115e-01,\n                         1.6943e-01,  2.7369e-01,  2.6002e-01,  2.9015e-02, -3.8809e-03,\n                         3.3427e-01,  3.2801e-01,  2.6188e-01,  8.0131e-02,  3.1206e-01,\n                         1.3878e-01,  1.3369e-01,  4.0801e-03,  3.3355e-03,  5.3315e-03,\n                         4.2011e-02,  2.6534e-01,  3.9473e-02,  2.8388e-01,  2.2184e-02,\n                         9.6549e-03,  1.1365e-01,  7.5564e-03,  6.3303e-03,  2.2197e-01,\n                         7.9478e-03,  6.3320e-03,  9.3152e-03,  2.2963e-01,  3.9565e-03,\n                         1.7731e-01,  9.3762e-02,  3.5169e-03,  1.4913e-01,  1.1935e-01,\n                         2.5429e-01,  1.4448e-01,  2.2738e-02,  2.6813e-01,  3.1401e-02,\n                         2.7586e-01,  3.2798e-01,  1.0146e-01,  1.0372e-01,  4.0126e-03,\n                         2.6588e-01,  2.7269e-01,  1.3241e-01,  1.0533e-01,  1.6726e-01,\n                         4.5310e-03,  6.7689e-02,  1.0868e-01,  4.0115e-02,  2.0814e-03,\n                         2.1358e-01,  1.9523e-02,  2.9492e-01,  9.0202e-03,  8.8036e-03,\n                         3.3987e-02,  9.0046e-02,  1.1075e-02,  1.1248e-01,  7.2081e-03,\n                         2.6663e-01,  1.7480e-01,  3.3223e-01,  1.0923e-02,  8.4599e-03,\n                         3.9834e-03,  1.9055e-01,  6.4795e-02,  6.5941e-02,  3.4825e-01,\n                         8.8924e-03,  1.4611e-02,  6.8778e-03,  3.1545e-01,  3.1280e-01,\n                         2.5091e-01, -8.4811e-03,  2.0124e-01,  8.8391e-02,  8.8555e-03,\n                         2.4254e-01,  8.9151e-03,  5.6549e-02,  1.2461e-01,  3.4432e-02,\n                         1.2700e-03,  9.2545e-03,  7.0731e-02,  1.3562e-01,  2.8119e-01,\n                         7.8303e-04,  1.6970e-01,  1.7177e-01,  1.4439e-01,  1.2310e-01,\n                         2.8937e-02,  2.3764e-03,  1.7779e-01,  1.5179e-01,  1.1030e-02,\n                         3.6498e-02,  4.9941e-03,  3.4279e-01,  3.3456e-01,  6.2458e-02,\n                         9.3978e-03,  1.8622e-01,  1.0526e-02,  1.7908e-01,  6.4245e-02,\n                         7.3099e-03,  5.2497e-02,  3.2322e-01,  7.3396e-02,  2.4091e-01,\n                         3.3591e-01,  8.6688e-02,  2.0890e-01,  1.0823e-01,  4.9898e-03,\n                         4.6406e-03,  1.1089e-02,  2.1432e-01,  7.1197e-02,  1.2994e-01,\n                         3.4352e-01,  2.7972e-01,  1.2470e-01,  6.5455e-03,  1.6187e-01,\n                         1.7068e-01, -4.5951e-04,  2.7363e-02,  3.8380e-02,  2.9963e-01,\n                         3.0762e-01,  4.6353e-03,  2.5548e-01,  2.0650e-01,  9.5454e-02,\n                         3.0478e-01,  9.9411e-03,  3.1276e-01,  1.2731e-02,  1.4814e-01,\n                         5.0956e-02,  1.7166e-02,  2.9018e-01,  2.9822e-01])\n              )\n            )\n          )\n          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(\n            in_features=384, out_features=1536, bias=True\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0021, 0.0012, 0.0014,  ..., 0.0015, 0.0017, 0.0015]), zero_point=tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([-0.1881, -0.1472, -0.1597,  ..., -0.1790, -0.2127, -0.1396]), max_val=tensor([0.2727, 0.1414, 0.1719,  ..., 0.1965, 0.1865, 0.1846]))\n            )\n          )\n          (4): GELU(approximate='none')\n          (5): Linear(\n            in_features=1536, out_features=384, bias=True\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0016, 0.0029, 0.0016, 0.0025, 0.0018, 0.0023, 0.0016, 0.0017, 0.0017,\n                      0.0017, 0.0014, 0.0016, 0.0032, 0.0018, 0.0017, 0.0017, 0.0017, 0.0013,\n                      0.0016, 0.0016, 0.0018, 0.0015, 0.0019, 0.0019, 0.0015, 0.0019, 0.0020,\n                      0.0021, 0.0020, 0.0029, 0.0039, 0.0017, 0.0020, 0.0016, 0.0015, 0.0018,\n                      0.0015, 0.0014, 0.0015, 0.0016, 0.0023, 0.0017, 0.0019, 0.0017, 0.0016,\n                      0.0041, 0.0015, 0.0018, 0.0014, 0.0016, 0.0021, 0.0056, 0.0014, 0.0016,\n                      0.0017, 0.0039, 0.0015, 0.0018, 0.0017, 0.0017, 0.0018, 0.0017, 0.0016,\n                      0.0013, 0.0019, 0.0036, 0.0014, 0.0020, 0.0016, 0.0015, 0.0019, 0.0017,\n                      0.0022, 0.0033, 0.0016, 0.0021, 0.0018, 0.0011, 0.0014, 0.0016, 0.0016,\n                      0.0015, 0.0016, 0.0025, 0.0015, 0.0022, 0.0026, 0.0014, 0.0025, 0.0015,\n                      0.0015, 0.0016, 0.0017, 0.0016, 0.0016, 0.0020, 0.0018, 0.0018, 0.0013,\n                      0.0032, 0.0015, 0.0021, 0.0027, 0.0015, 0.0023, 0.0018, 0.0018, 0.0019,\n                      0.0017, 0.0047, 0.0016, 0.0021, 0.0022, 0.0016, 0.0022, 0.0016, 0.0016,\n                      0.0023, 0.0021, 0.0016, 0.0015, 0.0015, 0.0018, 0.0017, 0.0026, 0.0038,\n                      0.0022, 0.0015, 0.0016, 0.0011, 0.0011, 0.0014, 0.0017, 0.0029, 0.0015,\n                      0.0023, 0.0017, 0.0020, 0.0013, 0.0014, 0.0017, 0.0018, 0.0016, 0.0014,\n                      0.0014, 0.0015, 0.0028, 0.0018, 0.0019, 0.0024, 0.0023, 0.0016, 0.0017,\n                      0.0019, 0.0013, 0.0021, 0.0095, 0.0022, 0.0015, 0.0020, 0.0018, 0.0015,\n                      0.0015, 0.0021, 0.0019, 0.0015, 0.0016, 0.0015, 0.0017, 0.0016, 0.0016,\n                      0.0020, 0.0013, 0.0017, 0.0015, 0.0039, 0.0015, 0.0019, 0.0023, 0.0016,\n                      0.0022, 0.0024, 0.0015, 0.0018, 0.0014, 0.0018, 0.0028, 0.0015, 0.0041,\n                      0.0037, 0.0022, 0.0015, 0.0015, 0.0017, 0.0021, 0.0012, 0.0017, 0.0026,\n                      0.0017, 0.0016, 0.0018, 0.0017, 0.0016, 0.0018, 0.0015, 0.0019, 0.0013,\n                      0.0013, 0.0016, 0.0041, 0.0016, 0.0020, 0.0017, 0.0017, 0.0015, 0.0031,\n                      0.0016, 0.0011, 0.0015, 0.0031, 0.0016, 0.0018, 0.0019, 0.0014, 0.0016,\n                      0.0025, 0.0019, 0.0016, 0.0017, 0.0017, 0.0015, 0.0014, 0.0019, 0.0015,\n                      0.0017, 0.0016, 0.0015, 0.0031, 0.0024, 0.0014, 0.0015, 0.0017, 0.0016,\n                      0.0019, 0.0018, 0.0022, 0.0018, 0.0015, 0.0021, 0.0014, 0.0034, 0.0021,\n                      0.0017, 0.0019, 0.0016, 0.0018, 0.0015, 0.0016, 0.0021, 0.0017, 0.0019,\n                      0.0023, 0.0016, 0.0014, 0.0030, 0.0022, 0.0016, 0.0014, 0.0015, 0.0016,\n                      0.0014, 0.0018, 0.0015, 0.0022, 0.0026, 0.0022, 0.0015, 0.0017, 0.0016,\n                      0.0015, 0.0027, 0.0015, 0.0019, 0.0017, 0.0015, 0.0016, 0.0014, 0.0019,\n                      0.0016, 0.0015, 0.0016, 0.0016, 0.0015, 0.0012, 0.0017, 0.0017, 0.0017,\n                      0.0018, 0.0020, 0.0015, 0.0015, 0.0018, 0.0021, 0.0021, 0.0016, 0.0015,\n                      0.0014, 0.0014, 0.0014, 0.0019, 0.0015, 0.0018, 0.0018, 0.0017, 0.0027,\n                      0.0015, 0.0014, 0.0018, 0.0025, 0.0022, 0.0018, 0.0012, 0.0016, 0.0022,\n                      0.0016, 0.0022, 0.0011, 0.0024, 0.0018, 0.0032, 0.0016, 0.0016, 0.0017,\n                      0.0019, 0.0017, 0.0014, 0.0014, 0.0014, 0.0027, 0.0031, 0.0016, 0.0017,\n                      0.0015, 0.0014, 0.0030, 0.0016, 0.0015, 0.0027, 0.0030, 0.0016, 0.0016,\n                      0.0019, 0.0015, 0.0054, 0.0015, 0.0026, 0.0018, 0.0014, 0.0018, 0.0017,\n                      0.0027, 0.0013, 0.0015, 0.0015, 0.0030, 0.0014, 0.0014, 0.0018, 0.0018,\n                      0.0017, 0.0013, 0.0016, 0.0014, 0.0018, 0.0014, 0.0016, 0.0023, 0.0018,\n                      0.0017, 0.0028, 0.0015, 0.0013, 0.0015, 0.0014]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                     dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n                min_val=tensor([-0.2018, -0.2770, -0.1936, -0.3145, -0.1784, -0.2934, -0.1775, -0.2195,\n                        -0.2068, -0.2194, -0.1673, -0.1883, -0.4105, -0.2272, -0.1711, -0.2144,\n                        -0.2226, -0.1693, -0.1998, -0.2048, -0.2280, -0.1680, -0.2216, -0.2405,\n                        -0.1908, -0.2383, -0.2264, -0.2662, -0.2562, -0.3663, -0.5012, -0.1778,\n                        -0.2551, -0.1952, -0.1880, -0.1978, -0.1917, -0.1774, -0.1977, -0.1983,\n                        -0.3003, -0.2183, -0.2455, -0.2104, -0.1888, -0.3903, -0.1929, -0.1849,\n                        -0.1639, -0.2106, -0.2672, -0.7188, -0.1651, -0.1910, -0.2139, -0.4937,\n                        -0.1929, -0.1695, -0.1757, -0.1759, -0.1881, -0.2088, -0.2021, -0.1551,\n                        -0.2386, -0.4656, -0.1751, -0.1941, -0.2043, -0.1842, -0.1732, -0.1783,\n                        -0.2034, -0.4288, -0.2100, -0.1618, -0.1927, -0.1368, -0.1807, -0.2027,\n                        -0.2059, -0.1927, -0.2032, -0.3176, -0.1546, -0.2441, -0.3308, -0.1492,\n                        -0.2990, -0.1893, -0.1872, -0.1959, -0.2138, -0.1693, -0.2104, -0.2597,\n                        -0.1789, -0.2366, -0.1614, -0.4018, -0.1876, -0.2669, -0.3405, -0.1694,\n                        -0.2387, -0.1603, -0.1623, -0.2380, -0.2192, -0.5240, -0.2110, -0.2640,\n                        -0.2774, -0.2001, -0.2803, -0.1768, -0.1980, -0.1836, -0.2675, -0.1522,\n                        -0.1883, -0.1890, -0.2356, -0.1844, -0.3301, -0.3850, -0.2846, -0.1929,\n                        -0.2095, -0.1431, -0.1068, -0.1815, -0.2124, -0.2654, -0.1802, -0.2920,\n                        -0.1778, -0.2610, -0.1604, -0.1748, -0.2193, -0.2297, -0.1807, -0.1701,\n                        -0.1712, -0.1612, -0.1823, -0.1790, -0.2395, -0.2134, -0.2839, -0.1571,\n                        -0.2201, -0.2374, -0.1564, -0.2531, -1.2138, -0.2795, -0.1975, -0.1757,\n                        -0.2069, -0.1864, -0.1717, -0.2299, -0.2399, -0.1679, -0.2104, -0.1857,\n                        -0.2151, -0.2022, -0.1988, -0.1743, -0.1563, -0.2204, -0.1966, -0.2055,\n                        -0.1962, -0.1676, -0.2924, -0.1783, -0.2872, -0.3103, -0.1982, -0.1682,\n                        -0.1838, -0.2302, -0.3573, -0.1947, -0.4599, -0.4741, -0.2804, -0.1892,\n                        -0.1915, -0.1955, -0.2655, -0.1532, -0.1652, -0.3080, -0.2181, -0.2089,\n                        -0.2321, -0.2147, -0.2085, -0.2173, -0.1761, -0.2395, -0.1705, -0.1686,\n                        -0.1499, -0.5209, -0.1740, -0.2243, -0.1636, -0.1599, -0.1873, -0.3917,\n                        -0.1780, -0.1470, -0.1957, -0.2351, -0.2018, -0.2262, -0.2400, -0.1600,\n                        -0.1808, -0.2426, -0.2485, -0.1908, -0.2137, -0.2116, -0.1857, -0.1726,\n                        -0.2427, -0.1839, -0.2179, -0.2107, -0.1814, -0.3983, -0.2278, -0.1691,\n                        -0.1386, -0.2188, -0.1895, -0.2376, -0.2105, -0.2816, -0.1864, -0.1887,\n                        -0.2651, -0.1735, -0.4371, -0.2745, -0.2144, -0.2285, -0.2050, -0.1975,\n                        -0.1960, -0.1914, -0.1965, -0.1886, -0.1750, -0.2882, -0.2074, -0.1617,\n                        -0.2823, -0.2809, -0.1922, -0.1849, -0.1573, -0.1785, -0.1676, -0.2282,\n                        -0.1681, -0.1861, -0.3329, -0.1847, -0.1882, -0.2120, -0.2026, -0.1764,\n                        -0.3458, -0.1953, -0.1897, -0.2205, -0.1759, -0.2030, -0.1773, -0.2463,\n                        -0.1834, -0.1731, -0.1783, -0.1968, -0.1900, -0.1132, -0.1930, -0.2157,\n                        -0.1816, -0.1628, -0.2385, -0.1983, -0.1758, -0.2343, -0.2115, -0.1973,\n                        -0.2075, -0.1902, -0.1679, -0.1696, -0.1574, -0.1879, -0.1909, -0.2180,\n                        -0.1693, -0.2042, -0.3407, -0.1794, -0.1756, -0.2290, -0.1720, -0.2791,\n                        -0.2030, -0.1544, -0.2000, -0.2181, -0.1926, -0.2761, -0.1318, -0.1789,\n                        -0.1538, -0.4112, -0.1948, -0.1988, -0.1641, -0.2475, -0.1897, -0.1839,\n                        -0.1778, -0.1793, -0.3504, -0.2712, -0.1875, -0.1898, -0.1884, -0.1791,\n                        -0.3803, -0.1672, -0.1900, -0.3404, -0.1901, -0.1850, -0.2016, -0.2365,\n                        -0.1826, -0.3702, -0.1858, -0.3343, -0.2263, -0.1688, -0.1832, -0.2231,\n                        -0.3423, -0.1719, -0.1927, -0.1964, -0.2955, -0.1838, -0.1820, -0.1822,\n                        -0.2242, -0.1763, -0.1544, -0.2018, -0.1740, -0.2331, -0.1565, -0.2100,\n                        -0.2989, -0.2173, -0.2128, -0.2349, -0.1527, -0.1679, -0.1732, -0.1821]), max_val=tensor([0.2059, 0.3649, 0.2039, 0.2174, 0.2325, 0.1891, 0.1972, 0.1729, 0.2159,\n                        0.2062, 0.1778, 0.2068, 0.3113, 0.2134, 0.2108, 0.1866, 0.2129, 0.1486,\n                        0.2002, 0.1742, 0.2196, 0.1864, 0.2362, 0.2440, 0.1667, 0.2231, 0.2513,\n                        0.1767, 0.1787, 0.2684, 0.4276, 0.2109, 0.1989, 0.2053, 0.1664, 0.2274,\n                        0.1775, 0.1789, 0.1798, 0.2044, 0.2175, 0.1949, 0.2288, 0.2109, 0.2011,\n                        0.5227, 0.1951, 0.2267, 0.1826, 0.1970, 0.2138, 0.4190, 0.1742, 0.2005,\n                        0.1940, 0.4862, 0.1707, 0.2260, 0.2202, 0.2196, 0.2231, 0.2138, 0.2028,\n                        0.1677, 0.2033, 0.3156, 0.1759, 0.2516, 0.1646, 0.1942, 0.2464, 0.2201,\n                        0.2787, 0.1578, 0.1743, 0.2702, 0.2226, 0.1352, 0.1832, 0.2001, 0.1694,\n                        0.1733, 0.2033, 0.2711, 0.1949, 0.2749, 0.2451, 0.1826, 0.3214, 0.1716,\n                        0.1913, 0.2060, 0.1604, 0.2030, 0.1664, 0.1742, 0.2244, 0.2231, 0.1648,\n                        0.4084, 0.1678, 0.2279, 0.2991, 0.1880, 0.2917, 0.2314, 0.2321, 0.1892,\n                        0.1594, 0.5927, 0.2082, 0.1957, 0.2280, 0.1792, 0.2227, 0.2079, 0.2028,\n                        0.2894, 0.1871, 0.2048, 0.1837, 0.1869, 0.1863, 0.2202, 0.2012, 0.4866,\n                        0.2075, 0.1960, 0.1929, 0.1256, 0.1367, 0.1695, 0.2100, 0.3650, 0.1844,\n                        0.1756, 0.2200, 0.2078, 0.1424, 0.1824, 0.2170, 0.1557, 0.2035, 0.1722,\n                        0.1799, 0.1929, 0.3551, 0.2325, 0.2002, 0.3012, 0.2861, 0.2029, 0.1478,\n                        0.2339, 0.1679, 0.2681, 0.6959, 0.2276, 0.1850, 0.2579, 0.2265, 0.1656,\n                        0.1899, 0.2712, 0.1992, 0.1912, 0.1930, 0.1926, 0.1756, 0.1722, 0.1821,\n                        0.2505, 0.1658, 0.2084, 0.1813, 0.4939, 0.1909, 0.2440, 0.1978, 0.2015,\n                        0.2583, 0.2567, 0.1865, 0.2320, 0.1658, 0.1881, 0.2100, 0.1801, 0.5180,\n                        0.3671, 0.2526, 0.1692, 0.1616, 0.2153, 0.2036, 0.1340, 0.2220, 0.3243,\n                        0.1893, 0.1585, 0.2288, 0.1864, 0.1907, 0.2329, 0.1923, 0.2451, 0.1600,\n                        0.1639, 0.1984, 0.4559, 0.2049, 0.2484, 0.2120, 0.2158, 0.1770, 0.2609,\n                        0.2079, 0.1374, 0.1618, 0.3894, 0.1783, 0.1750, 0.1680, 0.1839, 0.1987,\n                        0.3224, 0.1976, 0.2025, 0.2042, 0.1656, 0.1899, 0.1733, 0.2314, 0.1940,\n                        0.1701, 0.1715, 0.1886, 0.1967, 0.3092, 0.1811, 0.1879, 0.2072, 0.1978,\n                        0.2161, 0.2244, 0.2308, 0.2233, 0.1789, 0.1660, 0.1802, 0.1956, 0.1856,\n                        0.1708, 0.2411, 0.1781, 0.2289, 0.1828, 0.2082, 0.2622, 0.2106, 0.2370,\n                        0.2096, 0.1904, 0.1716, 0.3835, 0.2652, 0.1975, 0.1732, 0.1896, 0.2072,\n                        0.1738, 0.2020, 0.1939, 0.2808, 0.2599, 0.2781, 0.1847, 0.1832, 0.1896,\n                        0.1871, 0.2430, 0.1882, 0.2451, 0.2043, 0.1964, 0.1941, 0.1791, 0.1920,\n                        0.2016, 0.1876, 0.1976, 0.2061, 0.1489, 0.1525, 0.2112, 0.1885, 0.2151,\n                        0.2237, 0.2552, 0.1605, 0.1879, 0.1803, 0.2667, 0.2649, 0.1572, 0.1624,\n                        0.1821, 0.1746, 0.1834, 0.2376, 0.1783, 0.2239, 0.2237, 0.2165, 0.2871,\n                        0.1941, 0.1558, 0.1569, 0.3173, 0.1985, 0.2230, 0.1540, 0.1749, 0.2774,\n                        0.2080, 0.2665, 0.1387, 0.3095, 0.2238, 0.3178, 0.2053, 0.1854, 0.2177,\n                        0.2008, 0.2154, 0.1796, 0.1790, 0.1768, 0.2874, 0.3880, 0.2004, 0.2211,\n                        0.1777, 0.1658, 0.3659, 0.1997, 0.1858, 0.1776, 0.3798, 0.2050, 0.1952,\n                        0.2432, 0.1874, 0.6815, 0.1884, 0.2392, 0.2119, 0.1819, 0.2228, 0.1844,\n                        0.3156, 0.1671, 0.1924, 0.1714, 0.3853, 0.1796, 0.1742, 0.2345, 0.2054,\n                        0.2126, 0.1625, 0.1742, 0.1768, 0.2100, 0.1831, 0.1775, 0.1887, 0.2284,\n                        0.2121, 0.3591, 0.1930, 0.1604, 0.1886, 0.1619])\n              )\n            )\n          )\n        )\n      )\n      (7): Module(\n        (block): Module(\n          (0): Conv2d(\n            384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0021, 0.0025, 0.0018, 0.0025, 0.0014, 0.0017, 0.0024, 0.0011, 0.0015,\n                      0.0025, 0.0022, 0.0014, 0.0020, 0.0006, 0.0016, 0.0018, 0.0022, 0.0015,\n                      0.0007, 0.0019, 0.0013, 0.0019, 0.0024, 0.0013, 0.0023, 0.0022, 0.0025,\n                      0.0024, 0.0011, 0.0028, 0.0013, 0.0022, 0.0018, 0.0014, 0.0008, 0.0007,\n                      0.0024, 0.0013, 0.0011, 0.0013, 0.0016, 0.0024, 0.0023, 0.0016, 0.0004,\n                      0.0010, 0.0014, 0.0017, 0.0021, 0.0024, 0.0013, 0.0012, 0.0020, 0.0009,\n                      0.0020, 0.0026, 0.0007, 0.0018, 0.0023, 0.0012, 0.0024, 0.0008, 0.0025,\n                      0.0013, 0.0015, 0.0010, 0.0025, 0.0019, 0.0018, 0.0019, 0.0006, 0.0021,\n                      0.0012, 0.0023, 0.0017, 0.0008, 0.0016, 0.0011, 0.0010, 0.0015, 0.0013,\n                      0.0024, 0.0025, 0.0005, 0.0009, 0.0019, 0.0014, 0.0014, 0.0022, 0.0015,\n                      0.0023, 0.0019, 0.0015, 0.0024, 0.0017, 0.0021, 0.0017, 0.0013, 0.0013,\n                      0.0012, 0.0015, 0.0009, 0.0011, 0.0022, 0.0024, 0.0022, 0.0019, 0.0010,\n                      0.0018, 0.0006, 0.0011, 0.0008, 0.0008, 0.0023, 0.0012, 0.0007, 0.0014,\n                      0.0019, 0.0022, 0.0013, 0.0016, 0.0021, 0.0017, 0.0007, 0.0034, 0.0013,\n                      0.0022, 0.0013, 0.0018, 0.0012, 0.0005, 0.0016, 0.0006, 0.0012, 0.0023,\n                      0.0025, 0.0018, 0.0021, 0.0010, 0.0026, 0.0018, 0.0011, 0.0017, 0.0024,\n                      0.0024, 0.0022, 0.0014, 0.0019, 0.0012, 0.0013, 0.0015, 0.0018, 0.0014,\n                      0.0011, 0.0011, 0.0015, 0.0001, 0.0024, 0.0022, 0.0023, 0.0020, 0.0017,\n                      0.0015, 0.0003, 0.0018, 0.0017, 0.0016, 0.0012, 0.0012, 0.0020, 0.0015,\n                      0.0016, 0.0017, 0.0009, 0.0013, 0.0023, 0.0017, 0.0024, 0.0008, 0.0008,\n                      0.0027, 0.0016, 0.0010, 0.0010, 0.0011, 0.0016, 0.0024, 0.0011, 0.0011,\n                      0.0010, 0.0004, 0.0014, 0.0021, 0.0025, 0.0016, 0.0009, 0.0022, 0.0023,\n                      0.0015, 0.0017, 0.0013, 0.0014, 0.0018, 0.0018, 0.0014, 0.0015, 0.0018,\n                      0.0021, 0.0018, 0.0014, 0.0025, 0.0012, 0.0017, 0.0019, 0.0021, 0.0010,\n                      0.0015, 0.0008, 0.0009, 0.0009, 0.0025, 0.0025, 0.0017, 0.0021, 0.0010,\n                      0.0017, 0.0022, 0.0024, 0.0012, 0.0016, 0.0007, 0.0018, 0.0018, 0.0024,\n                      0.0009, 0.0024, 0.0024, 0.0019, 0.0008, 0.0019, 0.0010, 0.0015, 0.0020,\n                      0.0012, 0.0004, 0.0025, 0.0013, 0.0024, 0.0019, 0.0025, 0.0023, 0.0018,\n                      0.0022, 0.0019, 0.0005, 0.0019, 0.0014, 0.0011, 0.0014, 0.0013, 0.0012,\n                      0.0013, 0.0009, 0.0007, 0.0009, 0.0018, 0.0011, 0.0021, 0.0018, 0.0022,\n                      0.0018, 0.0023, 0.0013, 0.0017, 0.0021, 0.0015, 0.0020, 0.0013, 0.0017,\n                      0.0011, 0.0022, 0.0016, 0.0008, 0.0023, 0.0021, 0.0019, 0.0025, 0.0019,\n                      0.0017, 0.0015, 0.0025, 0.0011, 0.0015, 0.0010, 0.0012, 0.0019, 0.0015,\n                      0.0025, 0.0024, 0.0022, 0.0015, 0.0015, 0.0006, 0.0018, 0.0025, 0.0020,\n                      0.0019, 0.0017, 0.0021, 0.0025, 0.0011, 0.0010, 0.0012, 0.0007, 0.0023,\n                      0.0017, 0.0014, 0.0026, 0.0010, 0.0024, 0.0009, 0.0017, 0.0017, 0.0006,\n                      0.0016, 0.0012, 0.0013, 0.0016, 0.0012, 0.0016, 0.0023, 0.0012, 0.0011,\n                      0.0011, 0.0015, 0.0023, 0.0011, 0.0024, 0.0026, 0.0017, 0.0025, 0.0013,\n                      0.0023, 0.0013, 0.0011, 0.0014, 0.0022, 0.0025, 0.0006, 0.0017, 0.0022,\n                      0.0007, 0.0015, 0.0014, 0.0020, 0.0021, 0.0015, 0.0016, 0.0011, 0.0011,\n                      0.0027, 0.0021, 0.0012, 0.0016, 0.0009, 0.0012, 0.0008, 0.0021, 0.0024,\n                      0.0024, 0.0022, 0.0008, 0.0018, 0.0015, 0.0015, 0.0023, 0.0022, 0.0023,\n                      0.0016, 0.0018, 0.0023, 0.0022, 0.0019, 0.0015]), zero_point=tensor([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,  127,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,  127,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,  127,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0, -128,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,  127,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0, -128,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n                     dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n                min_val=tensor([-2.6625e-01, -3.2553e-01, -2.3225e-01, -3.1938e-01, -1.8045e-01,\n                        -2.1772e-01, -1.3014e-02, -6.2456e-03, -7.1342e-03, -1.8686e-02,\n                        -6.7605e-03, -3.8734e-03, -6.7280e-02, -7.1432e-02, -8.8493e-03,\n                        -1.9792e-02, -4.8306e-02, -5.3048e-03, -9.5182e-02, -2.4547e-01,\n                        -1.6384e-01, -1.7918e-02, -3.1353e-01, -9.0825e-03, -2.9288e-01,\n                        -2.7563e-01, -2.6447e-02, -3.0660e-01, -8.7483e-03, -3.5400e-01,\n                        -1.7115e-01, -1.1835e-02, -2.2547e-01, -1.8123e-01, -7.6055e-03,\n                        -9.5464e-02, -3.2523e-02, -7.7328e-03, -1.0096e-02, -5.5825e-03,\n                        -1.3851e-01, -3.1072e-01, -2.9427e-01, -2.0881e-02, -9.9283e-02,\n                        -7.7962e-02, -1.2079e-01, -2.1123e-01, -1.7379e-02, -3.7454e-02,\n                        -9.1418e-03, -1.2857e-01, -7.8406e-03, -6.5734e-03, -1.0736e-02,\n                        -6.0829e-02, -3.4394e-03, -2.3642e-01, -1.7082e-02, -4.3242e-03,\n                        -2.2792e-02, -9.6524e-02, -1.7713e-02, -1.1493e-02, -1.9077e-01,\n                        -1.1357e-01, -3.2363e-01, -8.9684e-02, -2.3909e-02, -2.3834e-01,\n                        -7.2776e-02, -1.1806e-02, -1.5077e-01, -2.9124e-01, -1.1980e-03,\n                        -8.5910e-03, -1.2432e-01, -1.1097e-01, -3.6299e-03, -8.1232e-03,\n                        -1.6680e-01, -3.2429e-02, -3.8658e-02, -1.2814e-01, -7.6258e-03,\n                        -2.3693e-01, -1.7901e-01, -6.9323e-03, -2.7576e-01, -8.3649e-03,\n                        -1.3773e-02, -2.5371e-02, -7.8317e-03, -3.0394e-01, -1.2175e-02,\n                        -2.7275e-01, -2.2111e-01, -1.6120e-01, -5.4120e-03, -1.5582e-01,\n                        -1.8623e-03, -1.2152e-01, -9.4021e-02, -3.1355e-02, -5.0361e-02,\n                        -2.8135e-01, -2.4642e-01, -1.2839e-01, -2.2810e-01, -5.5386e-02,\n                        -5.5204e-03, -1.0814e-02, -7.8373e-02, -2.9070e-01, -1.5855e-01,\n                        -8.9316e-02, -1.7476e-01, -2.4276e-01, -2.8093e-01, -1.6316e-01,\n                        -2.0788e-01, -2.6318e-01, -2.1564e-01, -1.6518e-03, -4.3217e-01,\n                        -8.4406e-02, -2.8703e-01, -1.6829e-01, -9.4535e-03, -1.5682e-01,\n                        -6.4175e-02, -2.0153e-01, -3.9483e-02, -1.5401e-01, -2.9234e-01,\n                        -3.2439e-01, -1.2517e-02, -2.6595e-01, -1.1411e-01, -3.2830e-01,\n                        -2.3645e-01, -3.6570e-04, -2.1247e-01, -4.2370e-02, -1.0463e-02,\n                        -5.7033e-02, -1.7287e-01, -1.5050e-02, -5.1779e-03, -1.6823e-01,\n                        -5.8614e-02, -2.3334e-01, -6.4097e-03, -1.2487e-02, -4.2050e-03,\n                        -1.0247e-01, -1.1456e-02, -4.5460e-02, -2.8405e-02, -2.8924e-01,\n                        -2.5797e-01, -2.1432e-01, -1.0832e-02, -7.6457e-02, -2.5186e-02,\n                        -8.0562e-03, -2.0211e-01, -6.7756e-03, -1.1834e-02, -2.5133e-01,\n                        -9.0916e-03, -2.0851e-01, -2.1514e-01, -1.1540e-01, -1.6499e-01,\n                        -2.9811e-01, -2.1579e-01, -1.0939e-02, -1.0616e-01, -1.0989e-02,\n                        -2.3677e-02, -2.8801e-03, -1.3177e-01, -1.2725e-01, -1.1590e-02,\n                        -2.0873e-01, -3.0284e-01, -1.4237e-01, -1.4499e-01, -1.2379e-01,\n                         5.3559e-03, -1.3139e-01, -2.0787e-02, -3.1752e-01, -2.3404e-02,\n                        -1.1021e-01, -1.1689e-02, -2.9865e-01, -1.9402e-01, -8.4810e-02,\n                        -1.6433e-02, -5.8599e-03, -1.3988e-02, -1.6813e-01, -1.0334e-02,\n                        -1.9012e-01, -2.3440e-01, -2.7144e-01, -1.6335e-02, -1.2922e-01,\n                        -2.9253e-02, -7.4593e-02, -2.1595e-01, -2.4714e-01, -1.3564e-02,\n                        -1.2904e-01, -1.9003e-01, -9.1164e-02, -2.0886e-03, -1.1253e-01,\n                        -3.5496e-02, -2.5104e-02, -1.0360e-01, -2.6917e-01, -1.3041e-01,\n                        -2.1672e-01, -2.8777e-01, -3.0763e-01, -1.5798e-01, -1.5695e-02,\n                        -2.4049e-03, -8.0959e-03, -2.2786e-01, -3.1144e-01, -4.6759e-03,\n                        -3.1132e-01, -3.1217e-01, -2.3819e-01, -1.4303e-02, -2.4920e-01,\n                        -1.1318e-01, -1.9386e-01, -1.3988e-02, -1.4905e-02, -9.9497e-02,\n                        -4.9092e-02, -1.6385e-01, -3.0607e-01, -3.0091e-02, -3.2177e-02,\n                        -1.1742e-02, -9.8668e-02, -1.1606e-02, -1.3234e-02,  7.4326e-03,\n                        -5.5507e-02, -9.3379e-03, -1.0978e-02, -5.7669e-03, -5.9848e-03,\n                        -1.5572e-01, -1.6543e-01, -1.1631e-01, -4.3988e-02, -7.2498e-02,\n                        -1.0987e-01, -8.7654e-02, -1.2319e-02, -2.3552e-01, -3.3589e-02,\n                        -1.5319e-02, -4.7845e-03, -1.6336e-01, -1.0837e-01, -3.3318e-03,\n                        -2.1853e-02, -2.5050e-01, -4.1593e-03, -2.2174e-01, -1.0429e-02,\n                        -2.8097e-01, -2.6508e-02, -1.6579e-03, -2.9830e-01, -2.6464e-01,\n                        -2.7587e-02, -7.6644e-03, -1.2375e-02, -1.1716e-02, -1.9371e-01,\n                        -3.2184e-01, -1.3705e-01, -9.7020e-03, -1.2270e-01, -1.4839e-01,\n                        -2.4519e-01, -8.9061e-02, -3.1373e-01, -3.0232e-01, -2.7849e-01,\n                        -7.3163e-03, -1.9162e-01, -7.2052e-02, -5.8146e-02, -3.1600e-01,\n                        -2.5832e-01, -1.3909e-02, -5.5295e-03, -8.2938e-03, -5.6171e-02,\n                        -1.3704e-01, -7.2305e-03, -2.4418e-02, -8.6085e-02, -1.0436e-02,\n                        -1.0596e-02, -3.6462e-03, -4.2680e-02, -9.3011e-02, -3.4345e-02,\n                        -3.6466e-03, -1.4144e-02, -2.1407e-01, -7.3597e-02, -2.1009e-01,\n                        -6.0057e-04, -1.7124e-01, -2.0426e-01, -1.4813e-01, -1.4285e-01,\n                        -2.9952e-01, -1.0074e-02, -1.4399e-02, -5.3426e-03, -1.9175e-01,\n                        -2.9653e-01, -1.3629e-01, -3.1275e-01, -3.3660e-01, -3.1700e-02,\n                        -3.1603e-01, -1.6711e-01, -7.4431e-03, -8.7496e-02, -9.2964e-02,\n                        -1.8469e-01, -2.7957e-01, -3.2417e-01, -8.0610e-02, -1.0794e-02,\n                        -2.7774e-01, -6.1261e-03, -8.8288e-03, -1.1585e-01, -2.5176e-01,\n                        -2.7406e-01, -7.6917e-03, -1.0604e-02, -1.3726e-01, -3.0059e-03,\n                        -1.5529e-02, -1.0065e-02, -6.8061e-03, -1.0575e-02, -1.1749e-01,\n                        -1.4996e-01, -1.0344e-01, -1.1387e-02, -3.0276e-01, -3.6256e-02,\n                        -2.8328e-01, -1.0604e-01, -1.6594e-02, -1.7825e-01, -1.9209e-01,\n                        -2.9448e-01, -2.1680e-02, -2.9209e-01, -8.4658e-03, -2.3553e-01,\n                        -5.1493e-02, -2.8429e-01, -2.5494e-02, -1.9498e-01]), max_val=tensor([ 0.0082,  0.0544,  0.0136,  0.0481,  0.0290,  0.0154,  0.3041,  0.1410,\n                         0.1864,  0.3140,  0.2774,  0.1809,  0.2524,  0.0526,  0.2006,  0.2287,\n                         0.2797,  0.1914,  0.0028,  0.0112,  0.1232,  0.2362,  0.0520,  0.1613,\n                         0.0164,  0.0077,  0.3225,  0.0175,  0.1366,  0.0703,  0.0983,  0.2820,\n                         0.0111,  0.0106,  0.0990,  0.0046,  0.3111,  0.1674,  0.1449,  0.1697,\n                         0.2024,  0.0387,  0.0430,  0.1972, -0.0007,  0.1237,  0.1786,  0.1675,\n                         0.2704,  0.3043,  0.1663,  0.1504,  0.2508,  0.1202,  0.2583,  0.3268,\n                         0.0862,  0.0120,  0.2942,  0.1531,  0.3094,  0.0963,  0.3139,  0.1700,\n                         0.0117,  0.1308,  0.0114,  0.2442,  0.2287,  0.0035,  0.0025,  0.2625,\n                         0.1124,  0.0046,  0.2176,  0.0990,  0.2028,  0.1397,  0.1310,  0.1878,\n                         0.0337,  0.3004,  0.3190, -0.0142,  0.1127,  0.0953,  0.0826,  0.1841,\n                         0.0095,  0.1902,  0.2950,  0.2394,  0.1953,  0.0302,  0.2158,  0.0281,\n                         0.1780,  0.1195,  0.1690,  0.0772,  0.1941,  0.0129,  0.1386,  0.2804,\n                         0.3070,  0.0188,  0.0093,  0.1093,  0.0136,  0.0748,  0.1379,  0.1032,\n                         0.1060,  0.0104,  0.1275,  0.0030,  0.0081,  0.0059,  0.0111,  0.0048,\n                         0.0124,  0.0231,  0.0133,  0.0873,  0.0018,  0.1605,  0.0173,  0.0051,\n                         0.2247,  0.1334,  0.0622,  0.0129,  0.0699,  0.1042,  0.0232,  0.0437,\n                         0.2305,  0.0074,  0.1233,  0.0290,  0.1290,  0.1416,  0.0107,  0.3076,\n                         0.3000,  0.2847,  0.0045,  0.2458,  0.1506,  0.1446,  0.1863,  0.0092,\n                         0.1744,  0.1441,  0.1334,  0.1946,  0.0175,  0.3049,  0.2756,  0.0227,\n                         0.0354,  0.0126,  0.1876, -0.0042,  0.2333,  0.2204,  0.0112,  0.1513,\n                         0.1529,  0.0176,  0.1918,  0.0095,  0.0054,  0.0008,  0.0108,  0.0097,\n                         0.0060,  0.3012,  0.0114,  0.1006,  0.3370,  0.1999,  0.0031,  0.0103,\n                         0.1453,  0.0538,  0.0063,  0.0025,  0.0846,  0.0924,  0.0997,  0.1732,\n                         0.2629,  0.0209,  0.2090,  0.0970,  0.2827,  0.0074,  0.1163,  0.2197,\n                         0.1690,  0.1730,  0.2320,  0.2271,  0.1761,  0.1409,  0.0106,  0.0172,\n                         0.2302,  0.1779,  0.3149,  0.1523,  0.0873,  0.0175,  0.2658,  0.1068,\n                         0.1556,  0.1048,  0.1128,  0.1205,  0.3161,  0.3178,  0.2168,  0.0051,\n                         0.0038,  0.0911,  0.0474,  0.0229,  0.0130,  0.2002,  0.0850,  0.2261,\n                         0.0631,  0.0421,  0.1117,  0.0159,  0.0179,  0.0424,  0.0983,  0.0095,\n                         0.1275,  0.0100,  0.2515,  0.1515, -0.0078,  0.3113,  0.0065,  0.0527,\n                         0.2435,  0.3203,  0.2921,  0.2223,  0.2853,  0.2451,  0.1247,  0.2369,\n                         0.1818,  0.1412,  0.1756,  0.1642,  0.0043,  0.0949,  0.0041,  0.0944,\n                         0.1180,  0.2225,  0.1340,  0.2724,  0.0356,  0.2745,  0.2334,  0.2870,\n                         0.1071,  0.2204,  0.2633,  0.1958,  0.0138,  0.1672,  0.1092,  0.1388,\n                         0.0068,  0.1984,  0.1032,  0.0458,  0.0118,  0.2459,  0.3173,  0.2377,\n                         0.2211,  0.0137,  0.0421,  0.0373,  0.1962,  0.1095,  0.0061,  0.0159,\n                         0.1926,  0.0123,  0.0151,  0.0049,  0.1873,  0.0053,  0.0444,  0.2239,\n                         0.0184,  0.0114,  0.2475,  0.2156,  0.2693,  0.3171,  0.0091,  0.1209,\n                         0.1550,  0.0042,  0.2868,  0.2153,  0.1754,  0.3359,  0.1215,  0.3096,\n                         0.1162,  0.2201,  0.0117,  0.0060,  0.0135,  0.1570,  0.1658,  0.1162,\n                         0.0082,  0.2009,  0.0426,  0.1578,  0.1433,  0.1438,  0.0063,  0.0565,\n                         0.0157,  0.0133,  0.0178,  0.2180,  0.0245,  0.0067,  0.2865,  0.1595,\n                         0.1394,  0.0023,  0.0368,  0.0057,  0.0278,  0.2220,  0.0170,  0.0936,\n                         0.1924,  0.1724,  0.0137,  0.0096,  0.1911,  0.2010,  0.0051,  0.1354,\n                         0.3372,  0.2686,  0.1518,  0.1980,  0.0579,  0.0049,  0.0052,  0.2653,\n                         0.0407,  0.2986,  0.0098,  0.0043,  0.2324,  0.1880,  0.0171,  0.0435,\n                         0.2771,  0.0386,  0.1990,  0.1316,  0.2959,  0.0109,  0.2457,  0.0103])\n              )\n            )\n          )\n          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(\n            in_features=384, out_features=1536, bias=True\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0013, 0.0013, 0.0012,  ..., 0.0014, 0.0035, 0.0015]), zero_point=tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([-0.1670, -0.1642, -0.1594,  ..., -0.1459, -0.1718, -0.1857]), max_val=tensor([0.1272, 0.1333, 0.1544,  ..., 0.1718, 0.4464, 0.1517]))\n            )\n          )\n          (4): GELU(approximate='none')\n          (5): Linear(\n            in_features=1536, out_features=384, bias=True\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0015, 0.0017, 0.0017, 0.0017, 0.0014, 0.0016, 0.0019, 0.0016, 0.0017,\n                      0.0015, 0.0015, 0.0019, 0.0027, 0.0023, 0.0015, 0.0014, 0.0018, 0.0014,\n                      0.0015, 0.0014, 0.0016, 0.0015, 0.0018, 0.0014, 0.0014, 0.0024, 0.0018,\n                      0.0028, 0.0015, 0.0022, 0.0025, 0.0019, 0.0019, 0.0017, 0.0015, 0.0016,\n                      0.0016, 0.0016, 0.0018, 0.0015, 0.0026, 0.0017, 0.0020, 0.0016, 0.0023,\n                      0.0019, 0.0020, 0.0016, 0.0017, 0.0021, 0.0017, 0.0037, 0.0017, 0.0018,\n                      0.0016, 0.0019, 0.0016, 0.0014, 0.0014, 0.0015, 0.0019, 0.0024, 0.0026,\n                      0.0015, 0.0017, 0.0026, 0.0016, 0.0015, 0.0017, 0.0022, 0.0020, 0.0016,\n                      0.0017, 0.0041, 0.0015, 0.0017, 0.0018, 0.0012, 0.0020, 0.0015, 0.0019,\n                      0.0020, 0.0018, 0.0029, 0.0020, 0.0023, 0.0018, 0.0018, 0.0018, 0.0015,\n                      0.0015, 0.0017, 0.0016, 0.0014, 0.0018, 0.0017, 0.0015, 0.0020, 0.0015,\n                      0.0019, 0.0015, 0.0018, 0.0029, 0.0015, 0.0017, 0.0014, 0.0014, 0.0016,\n                      0.0013, 0.0032, 0.0020, 0.0018, 0.0021, 0.0017, 0.0022, 0.0017, 0.0016,\n                      0.0021, 0.0019, 0.0018, 0.0013, 0.0017, 0.0014, 0.0018, 0.0022, 0.0033,\n                      0.0017, 0.0018, 0.0015, 0.0015, 0.0013, 0.0017, 0.0025, 0.0017, 0.0016,\n                      0.0022, 0.0016, 0.0019, 0.0012, 0.0015, 0.0022, 0.0015, 0.0015, 0.0015,\n                      0.0015, 0.0018, 0.0024, 0.0015, 0.0016, 0.0017, 0.0022, 0.0018, 0.0016,\n                      0.0020, 0.0015, 0.0016, 0.0097, 0.0021, 0.0017, 0.0018, 0.0016, 0.0017,\n                      0.0017, 0.0024, 0.0016, 0.0015, 0.0016, 0.0016, 0.0014, 0.0015, 0.0015,\n                      0.0017, 0.0016, 0.0018, 0.0019, 0.0037, 0.0014, 0.0017, 0.0014, 0.0019,\n                      0.0019, 0.0020, 0.0019, 0.0023, 0.0016, 0.0016, 0.0040, 0.0015, 0.0025,\n                      0.0030, 0.0029, 0.0018, 0.0015, 0.0015, 0.0016, 0.0015, 0.0013, 0.0021,\n                      0.0017, 0.0018, 0.0016, 0.0016, 0.0013, 0.0021, 0.0013, 0.0018, 0.0016,\n                      0.0016, 0.0016, 0.0026, 0.0014, 0.0035, 0.0014, 0.0017, 0.0018, 0.0024,\n                      0.0022, 0.0014, 0.0015, 0.0026, 0.0015, 0.0017, 0.0016, 0.0016, 0.0015,\n                      0.0015, 0.0026, 0.0016, 0.0016, 0.0014, 0.0018, 0.0016, 0.0017, 0.0015,\n                      0.0018, 0.0015, 0.0014, 0.0024, 0.0025, 0.0016, 0.0016, 0.0016, 0.0017,\n                      0.0020, 0.0018, 0.0021, 0.0015, 0.0019, 0.0023, 0.0014, 0.0032, 0.0016,\n                      0.0019, 0.0018, 0.0018, 0.0018, 0.0015, 0.0018, 0.0018, 0.0017, 0.0016,\n                      0.0015, 0.0014, 0.0016, 0.0027, 0.0018, 0.0019, 0.0019, 0.0016, 0.0017,\n                      0.0018, 0.0020, 0.0021, 0.0018, 0.0026, 0.0016, 0.0014, 0.0015, 0.0017,\n                      0.0016, 0.0015, 0.0016, 0.0017, 0.0016, 0.0013, 0.0017, 0.0016, 0.0017,\n                      0.0018, 0.0013, 0.0020, 0.0019, 0.0014, 0.0017, 0.0016, 0.0014, 0.0019,\n                      0.0021, 0.0014, 0.0016, 0.0016, 0.0019, 0.0021, 0.0020, 0.0014, 0.0016,\n                      0.0016, 0.0015, 0.0015, 0.0019, 0.0021, 0.0017, 0.0015, 0.0018, 0.0027,\n                      0.0015, 0.0014, 0.0014, 0.0019, 0.0015, 0.0018, 0.0017, 0.0017, 0.0024,\n                      0.0014, 0.0023, 0.0013, 0.0017, 0.0016, 0.0019, 0.0015, 0.0020, 0.0018,\n                      0.0015, 0.0016, 0.0016, 0.0016, 0.0016, 0.0028, 0.0018, 0.0016, 0.0020,\n                      0.0015, 0.0018, 0.0026, 0.0018, 0.0019, 0.0023, 0.0022, 0.0017, 0.0016,\n                      0.0022, 0.0014, 0.0028, 0.0015, 0.0030, 0.0019, 0.0014, 0.0023, 0.0014,\n                      0.0023, 0.0017, 0.0019, 0.0014, 0.0023, 0.0018, 0.0017, 0.0014, 0.0020,\n                      0.0016, 0.0015, 0.0016, 0.0018, 0.0017, 0.0014, 0.0020, 0.0020, 0.0018,\n                      0.0018, 0.0024, 0.0017, 0.0019, 0.0020, 0.0016]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                     dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n                min_val=tensor([-0.1954, -0.2142, -0.1681, -0.2170, -0.1802, -0.1988, -0.2464, -0.2062,\n                        -0.2147, -0.1920, -0.1940, -0.2408, -0.3431, -0.1901, -0.1657, -0.1624,\n                        -0.2054, -0.1852, -0.1782, -0.1730, -0.2048, -0.1961, -0.2048, -0.1747,\n                        -0.1797, -0.2298, -0.2097, -0.1775, -0.1946, -0.2730, -0.2904, -0.2450,\n                        -0.2464, -0.2117, -0.1937, -0.1989, -0.2088, -0.2092, -0.1790, -0.1798,\n                        -0.2584, -0.2155, -0.2281, -0.1894, -0.2312, -0.2461, -0.2501, -0.1699,\n                        -0.1664, -0.1878, -0.1864, -0.4780, -0.1837, -0.2169, -0.1626, -0.2115,\n                        -0.2084, -0.1776, -0.1831, -0.1975, -0.2411, -0.3048, -0.2101, -0.1849,\n                        -0.1811, -0.3390, -0.1685, -0.1843, -0.2116, -0.2877, -0.2228, -0.1591,\n                        -0.2146, -0.5217, -0.1948, -0.1651, -0.2251, -0.1563, -0.2601, -0.1956,\n                        -0.2379, -0.2564, -0.2264, -0.2988, -0.2519, -0.2522, -0.2111, -0.2250,\n                        -0.2011, -0.1919, -0.1879, -0.2192, -0.1855, -0.1740, -0.2280, -0.1903,\n                        -0.1894, -0.1762, -0.1947, -0.2363, -0.1682, -0.2356, -0.3206, -0.1718,\n                        -0.2212, -0.1748, -0.1619, -0.2012, -0.1620, -0.4070, -0.2123, -0.2170,\n                        -0.2677, -0.2059, -0.2877, -0.2181, -0.2006, -0.2383, -0.1992, -0.2314,\n                        -0.1592, -0.1799, -0.1740, -0.2362, -0.2641, -0.3974, -0.2171, -0.2354,\n                        -0.1742, -0.1556, -0.1337, -0.2014, -0.3182, -0.2220, -0.2008, -0.2761,\n                        -0.1783, -0.2271, -0.1553, -0.1925, -0.2873, -0.1742, -0.1532, -0.1888,\n                        -0.1961, -0.2364, -0.2147, -0.1969, -0.1803, -0.2172, -0.2422, -0.1841,\n                        -0.2106, -0.2321, -0.1831, -0.2034, -0.5572, -0.1923, -0.2182, -0.2272,\n                        -0.2029, -0.2039, -0.2017, -0.3104, -0.2064, -0.1912, -0.2053, -0.1708,\n                        -0.1623, -0.1614, -0.1960, -0.1788, -0.2050, -0.2262, -0.2447, -0.2788,\n                        -0.1791, -0.1774, -0.1818, -0.2440, -0.2443, -0.2520, -0.2406, -0.2943,\n                        -0.2096, -0.1759, -0.3775, -0.1783, -0.2120, -0.3842, -0.3661, -0.1419,\n                        -0.1944, -0.1730, -0.2035, -0.1946, -0.1698, -0.2656, -0.2154, -0.2122,\n                        -0.1952, -0.1992, -0.1679, -0.2664, -0.1636, -0.2297, -0.1990, -0.1797,\n                        -0.1973, -0.2621, -0.1789, -0.3819, -0.1807, -0.2192, -0.2317, -0.3030,\n                        -0.1763, -0.1321, -0.1930, -0.2899, -0.1969, -0.2104, -0.1905, -0.1912,\n                        -0.1953, -0.1979, -0.2046, -0.1762, -0.1633, -0.1788, -0.2280, -0.2002,\n                        -0.2149, -0.1891, -0.2306, -0.1814, -0.1832, -0.3025, -0.3258, -0.1937,\n                        -0.2089, -0.1597, -0.1752, -0.1976, -0.1719, -0.2749, -0.1620, -0.2450,\n                        -0.2719, -0.1751, -0.3066, -0.1971, -0.1697, -0.1726, -0.2336, -0.1545,\n                        -0.1927, -0.2248, -0.1924, -0.2131, -0.1995, -0.1948, -0.1794, -0.1991,\n                        -0.2308, -0.2305, -0.2463, -0.1938, -0.1793, -0.1950, -0.2014, -0.2017,\n                        -0.2537, -0.2265, -0.2099, -0.1997, -0.1621, -0.1960, -0.2161, -0.2035,\n                        -0.1943, -0.2110, -0.2218, -0.1865, -0.1692, -0.1812, -0.2107, -0.2115,\n                        -0.2302, -0.1623, -0.2512, -0.2433, -0.1752, -0.2122, -0.1963, -0.1799,\n                        -0.2479, -0.2189, -0.1668, -0.1823, -0.1935, -0.2071, -0.2080, -0.2442,\n                        -0.1843, -0.1950, -0.2085, -0.1969, -0.1967, -0.2375, -0.2649, -0.1704,\n                        -0.1776, -0.1910, -0.3508, -0.1900, -0.1843, -0.1652, -0.1058, -0.1876,\n                        -0.2245, -0.2223, -0.1855, -0.2471, -0.1818, -0.2698, -0.1629, -0.2199,\n                        -0.2063, -0.2465, -0.1870, -0.1810, -0.2365, -0.1912, -0.2001, -0.1875,\n                        -0.1837, -0.2009, -0.2269, -0.2299, -0.2021, -0.2521, -0.1871, -0.2308,\n                        -0.2716, -0.1831, -0.1945, -0.2908, -0.2611, -0.1838, -0.2080, -0.1724,\n                        -0.1800, -0.3646, -0.1949, -0.3903, -0.1819, -0.1790, -0.2967, -0.1788,\n                        -0.1699, -0.2179, -0.2370, -0.1758, -0.2993, -0.2343, -0.2236, -0.1780,\n                        -0.1685, -0.1959, -0.1965, -0.2057, -0.2245, -0.1705, -0.1828, -0.2506,\n                        -0.2500, -0.2242, -0.1952, -0.2261, -0.1819, -0.2444, -0.2618, -0.1904]), max_val=tensor([0.1888, 0.2201, 0.2110, 0.1983, 0.1826, 0.2078, 0.2247, 0.2050, 0.2214,\n                        0.1607, 0.1707, 0.1965, 0.2057, 0.2929, 0.1907, 0.1730, 0.2325, 0.1621,\n                        0.1955, 0.1572, 0.1843, 0.1702, 0.2308, 0.1764, 0.1832, 0.3057, 0.2283,\n                        0.3573, 0.1908, 0.2805, 0.3164, 0.1840, 0.1770, 0.2054, 0.1922, 0.2083,\n                        0.1944, 0.2028, 0.2313, 0.1883, 0.3350, 0.1711, 0.2529, 0.2035, 0.2866,\n                        0.1805, 0.1794, 0.2089, 0.2131, 0.2711, 0.2147, 0.2509, 0.2200, 0.2267,\n                        0.2003, 0.2399, 0.2027, 0.1721, 0.1719, 0.1884, 0.2111, 0.2755, 0.3295,\n                        0.1876, 0.2150, 0.1695, 0.1998, 0.1871, 0.1972, 0.2338, 0.2498, 0.1990,\n                        0.2085, 0.3850, 0.1880, 0.2110, 0.1967, 0.1581, 0.1875, 0.1947, 0.1786,\n                        0.1734, 0.2308, 0.3648, 0.1848, 0.2945, 0.2297, 0.1911, 0.2303, 0.1887,\n                        0.1801, 0.2149, 0.1978, 0.1712, 0.2196, 0.2126, 0.1862, 0.2554, 0.1736,\n                        0.2398, 0.1912, 0.2083, 0.3627, 0.1881, 0.1890, 0.1783, 0.1833, 0.2005,\n                        0.1714, 0.3768, 0.2553, 0.2264, 0.2282, 0.2189, 0.1788, 0.1569, 0.1788,\n                        0.2705, 0.2389, 0.1979, 0.1645, 0.2156, 0.1556, 0.2141, 0.2851, 0.4169,\n                        0.2162, 0.1789, 0.1889, 0.1862, 0.1615, 0.2203, 0.2888, 0.2160, 0.1981,\n                        0.1803, 0.2064, 0.2399, 0.1581, 0.1831, 0.2120, 0.1935, 0.1895, 0.1852,\n                        0.1730, 0.1702, 0.3028, 0.1648, 0.2087, 0.2082, 0.2832, 0.2273, 0.1878,\n                        0.2503, 0.1917, 0.1841, 1.2270, 0.2649, 0.1632, 0.2005, 0.1900, 0.2136,\n                        0.2142, 0.2232, 0.1894, 0.1881, 0.1774, 0.2008, 0.1759, 0.1910, 0.1896,\n                        0.2127, 0.2019, 0.1895, 0.1843, 0.4692, 0.1790, 0.2186, 0.1814, 0.1901,\n                        0.2199, 0.2482, 0.2009, 0.1922, 0.1664, 0.2083, 0.5086, 0.1893, 0.3231,\n                        0.2273, 0.2539, 0.2240, 0.1783, 0.1937, 0.1742, 0.1673, 0.1672, 0.2552,\n                        0.2034, 0.2228, 0.2049, 0.1799, 0.1682, 0.2191, 0.1566, 0.1962, 0.1576,\n                        0.2090, 0.2088, 0.3317, 0.1643, 0.4469, 0.1775, 0.2213, 0.1932, 0.1905,\n                        0.2744, 0.1794, 0.1920, 0.3309, 0.1953, 0.2168, 0.1986, 0.1989, 0.1888,\n                        0.1814, 0.3356, 0.2043, 0.2019, 0.1732, 0.2049, 0.2064, 0.1980, 0.1783,\n                        0.1705, 0.1881, 0.1817, 0.2822, 0.2425, 0.2009, 0.1443, 0.2018, 0.2219,\n                        0.2484, 0.2313, 0.2025, 0.1926, 0.1846, 0.2968, 0.1634, 0.4044, 0.1993,\n                        0.2434, 0.2240, 0.2122, 0.2296, 0.1710, 0.1747, 0.2285, 0.1895, 0.1757,\n                        0.1954, 0.1816, 0.2003, 0.3405, 0.1936, 0.1981, 0.2443, 0.1975, 0.2149,\n                        0.2332, 0.2602, 0.2694, 0.2258, 0.3248, 0.2054, 0.1760, 0.1688, 0.1920,\n                        0.1806, 0.1902, 0.1978, 0.1863, 0.2075, 0.1607, 0.2097, 0.1991, 0.1713,\n                        0.2238, 0.1693, 0.2248, 0.2332, 0.1740, 0.1409, 0.2018, 0.1600, 0.1698,\n                        0.2646, 0.1775, 0.2071, 0.2041, 0.2384, 0.2713, 0.2522, 0.1709, 0.2003,\n                        0.1684, 0.1935, 0.1765, 0.1927, 0.1772, 0.2118, 0.1847, 0.2304, 0.3033,\n                        0.1780, 0.1717, 0.1797, 0.2421, 0.1900, 0.1825, 0.1942, 0.2208, 0.3106,\n                        0.1636, 0.2931, 0.1230, 0.2118, 0.1815, 0.2244, 0.1767, 0.2567, 0.1960,\n                        0.1681, 0.2043, 0.2018, 0.2065, 0.2003, 0.3514, 0.2219, 0.2017, 0.1626,\n                        0.1937, 0.2011, 0.3251, 0.2242, 0.2385, 0.2132, 0.2746, 0.2109, 0.1884,\n                        0.2800, 0.1545, 0.3397, 0.1856, 0.2036, 0.2463, 0.1582, 0.2215, 0.1650,\n                        0.2960, 0.2040, 0.2446, 0.1701, 0.2086, 0.2120, 0.1953, 0.1789, 0.2525,\n                        0.2014, 0.1696, 0.2069, 0.1820, 0.2136, 0.1826, 0.2114, 0.2125, 0.2090,\n                        0.2309, 0.3090, 0.2123, 0.1637, 0.1996, 0.2056])\n              )\n            )\n          )\n        )\n      )\n      (8): Module(\n        (block): Module(\n          (0): Conv2d(\n            384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.9898e-03, 1.7744e-03, 1.6383e-03, 1.4800e-03, 1.4294e-03, 1.4592e-03,\n                      2.0284e-03, 1.4918e-03, 1.4855e-03, 1.8933e-03, 1.9580e-03, 1.8260e-03,\n                      1.9763e-03, 4.2783e-04, 1.5265e-03, 1.6684e-03, 1.2785e-03, 1.6023e-03,\n                      8.3498e-04, 1.7074e-03, 1.2436e-03, 1.7012e-03, 1.2903e-03, 1.3683e-03,\n                      1.9127e-03, 1.9872e-03, 2.0299e-03, 1.9420e-03, 1.2642e-03, 2.1180e-03,\n                      9.6830e-04, 1.9721e-03, 1.4325e-03, 1.7120e-03, 1.2602e-03, 1.7562e-03,\n                      1.7359e-03, 1.4376e-03, 1.3319e-03, 1.7450e-03, 1.3820e-03, 1.8072e-03,\n                      1.2582e-03, 1.5207e-03, 1.2515e-03, 9.7209e-04, 1.1020e-03, 1.5918e-03,\n                      1.9050e-03, 1.5888e-03, 1.1559e-03, 8.6470e-04, 1.8118e-03, 1.3590e-03,\n                      1.8125e-03, 1.5318e-03, 8.2024e-04, 1.7345e-03, 2.0208e-03, 1.5403e-03,\n                      1.8999e-03, 1.5285e-03, 1.9828e-03, 1.5995e-03, 1.3627e-03, 8.9900e-04,\n                      2.0427e-03, 1.0576e-03, 8.3952e-04, 1.9581e-03, 9.3641e-04, 1.8575e-03,\n                      9.9196e-04, 2.0134e-03, 1.6525e-03, 1.4387e-03, 9.7395e-04, 1.1705e-03,\n                      1.4115e-03, 1.5330e-03, 1.5104e-03, 1.5906e-03, 1.6843e-03, 2.0710e-03,\n                      1.2588e-03, 1.3907e-03, 7.5189e-04, 1.6553e-03, 2.0029e-03, 1.4717e-03,\n                      1.7948e-03, 1.5735e-03, 1.4581e-03, 1.6054e-03, 1.6048e-03, 1.4994e-03,\n                      1.6125e-03, 9.3880e-04, 1.4002e-03, 8.3596e-04, 1.6843e-03, 1.6054e-03,\n                      4.3525e-04, 1.6351e-03, 1.6376e-03, 1.7926e-03, 1.9042e-03, 1.1292e-03,\n                      1.6171e-03, 4.5946e-04, 9.8314e-04, 1.4364e-03, 3.0325e-04, 1.9620e-03,\n                      1.1384e-03, 1.1940e-03, 1.4970e-03, 2.0156e-03, 2.0526e-03, 1.3023e-03,\n                      1.6468e-03, 1.9346e-03, 1.6523e-03, 1.0509e-03, 2.3637e-03, 1.2979e-03,\n                      1.9769e-03, 1.3673e-03, 1.7033e-03, 8.4186e-04, 6.4660e-04, 1.4765e-03,\n                      9.9420e-04, 9.6635e-04, 1.7447e-03, 1.3105e-03, 1.7235e-03, 1.9557e-03,\n                      1.0631e-03, 1.9301e-03, 1.3585e-03, 1.6102e-03, 1.4793e-03, 1.7341e-03,\n                      1.9975e-03, 1.0790e-03, 1.6471e-03, 1.6461e-03, 1.6384e-03, 9.4902e-04,\n                      1.3997e-03, 1.7632e-03, 1.2853e-03, 6.4414e-04, 1.4156e-03, 1.0969e-03,\n                      1.0556e-05, 1.4435e-03, 1.6905e-03, 1.8468e-03, 1.2423e-03, 1.6387e-03,\n                      1.6292e-03, 1.0323e-03, 1.2516e-03, 1.5639e-03, 1.2288e-03, 1.3554e-03,\n                      1.4042e-03, 1.5790e-03, 1.6057e-03, 1.8263e-03, 1.5614e-03, 1.1764e-03,\n                      1.5562e-03, 2.1366e-03, 1.5483e-03, 2.0637e-03, 1.4198e-03, 1.3433e-03,\n                      1.9841e-03, 1.8619e-03, 1.3438e-03, 1.3289e-03, 1.5233e-03, 1.3560e-03,\n                      2.0870e-03, 1.2968e-03, 7.6361e-04, 7.7598e-04, 1.4565e-03, 1.0148e-03,\n                      1.7645e-03, 1.8700e-03, 1.5980e-03, 9.1894e-04, 1.9543e-03, 1.9079e-03,\n                      1.2669e-03, 1.2026e-03, 1.1298e-03, 1.5027e-03, 1.7844e-03, 1.2745e-03,\n                      1.6095e-03, 1.1591e-03, 1.7162e-03, 1.8746e-03, 1.6347e-03, 1.5656e-03,\n                      1.7922e-03, 1.4869e-03, 1.5900e-03, 1.5581e-03, 1.8342e-03, 9.5495e-04,\n                      1.6880e-03, 5.7829e-04, 1.3623e-03, 1.5981e-03, 1.8909e-03, 2.0674e-03,\n                      1.0660e-03, 1.9722e-03, 1.1832e-03, 7.7058e-04, 1.4584e-03, 1.8442e-03,\n                      1.4649e-03, 1.4731e-03, 1.0203e-03, 1.7108e-03, 1.4202e-03, 1.6844e-03,\n                      1.4234e-03, 1.8830e-03, 1.9820e-03, 1.2952e-03, 1.3418e-03, 1.6513e-03,\n                      8.1645e-04, 1.4088e-03, 1.8387e-03, 1.7850e-03, 1.1608e-03, 1.3162e-03,\n                      1.3695e-03, 1.1741e-03, 1.5892e-03, 1.7801e-03, 2.0121e-03, 1.1279e-03,\n                      1.9644e-03, 1.7101e-03, 1.2894e-03, 1.4688e-03, 1.7808e-03, 1.2997e-03,\n                      1.3351e-03, 1.4660e-03, 1.4358e-03, 1.0143e-03, 1.2585e-03, 8.0903e-04,\n                      8.3749e-04, 1.6978e-03, 7.6407e-04, 1.7551e-03, 1.2192e-03, 1.5669e-03,\n                      1.6953e-03, 2.0233e-03, 1.6482e-03, 1.4320e-03, 2.2745e-03, 1.1389e-03,\n                      1.6725e-03, 1.6115e-03, 1.2637e-03, 1.4064e-03, 2.1997e-03, 1.4103e-03,\n                      1.0536e-03, 1.6623e-03, 1.9294e-03, 1.2414e-03, 2.0167e-03, 1.4727e-03,\n                      1.4554e-03, 1.6772e-03, 1.6944e-03, 8.4636e-04, 1.5987e-03, 9.0625e-04,\n                      1.3672e-03, 1.7672e-03, 1.6406e-03, 2.0270e-03, 2.0036e-03, 1.9947e-03,\n                      1.7675e-03, 1.5415e-03, 6.2319e-04, 1.7495e-03, 2.0785e-03, 1.9731e-03,\n                      1.7469e-03, 1.7260e-03, 1.9384e-03, 1.5951e-03, 1.4034e-03, 1.1818e-03,\n                      1.4902e-03, 1.4050e-03, 1.8491e-03, 1.7937e-03, 1.5707e-03, 1.5369e-03,\n                      7.4185e-04, 1.7828e-03, 1.2310e-03, 1.6786e-03, 1.4176e-03, 7.9634e-04,\n                      1.3652e-03, 1.9377e-03, 1.3371e-03, 1.5741e-03, 1.3713e-03, 1.2214e-03,\n                      1.6962e-03, 1.5828e-03, 1.5578e-03, 1.3639e-03, 1.6308e-03, 1.6517e-03,\n                      1.3480e-03, 2.0648e-03, 2.0939e-03, 1.0216e-03, 2.0710e-03, 1.3443e-03,\n                      1.9843e-03, 1.2567e-03, 6.6369e-04, 1.5278e-03, 1.3320e-03, 2.0516e-03,\n                      1.1957e-03, 1.6447e-03, 1.9038e-03, 1.1745e-03, 1.2199e-03, 1.3577e-03,\n                      1.7853e-03, 2.2098e-03, 1.2731e-03, 1.5089e-03, 1.7010e-03, 1.4349e-03,\n                      2.0623e-03, 1.8388e-03, 1.2977e-03, 1.5677e-03, 8.6330e-04, 1.5215e-03,\n                      1.3929e-03, 1.6977e-03, 1.2933e-03, 1.5187e-03, 1.7850e-03, 1.3910e-03,\n                      1.3564e-03, 1.5565e-03, 1.5975e-03, 1.3125e-03, 1.2894e-03, 1.5396e-03,\n                      1.4849e-03, 1.1057e-03, 1.5678e-03, 1.9814e-03, 1.4535e-03, 1.6589e-03]), zero_point=tensor([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                      127,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n                min_val=tensor([-0.0508, -0.2271, -0.2097, -0.0670, -0.0513, -0.1868, -0.0434, -0.0274,\n                        -0.0310, -0.2423, -0.2506, -0.0091, -0.1395, -0.0410, -0.1954, -0.0512,\n                        -0.0689, -0.2051, -0.1069, -0.2186, -0.0582, -0.0599, -0.0717, -0.0447,\n                        -0.0592, -0.0366, -0.0562, -0.0687, -0.0545, -0.0338, -0.1239, -0.2524,\n                        -0.1834, -0.0176, -0.1613, -0.0205, -0.2222, -0.0236, -0.0175, -0.0284,\n                        -0.1769, -0.0680, -0.0751, -0.0518, -0.1602, -0.0711, -0.1411, -0.2037,\n                        -0.0504, -0.2034, -0.0505, -0.0918, -0.2319, -0.0125, -0.2320, -0.0786,\n                        -0.0087, -0.2220, -0.0426, -0.1972, -0.0650, -0.0041, -0.0552, -0.0402,\n                        -0.1744, -0.0852, -0.2615, -0.0962, -0.0742, -0.2506, -0.1199, -0.0524,\n                        -0.0884, -0.2577, -0.0474, -0.1842, -0.1154, -0.1498, -0.1807, -0.1962,\n                        -0.1933, -0.2036, -0.0625, -0.0140, -0.1611, -0.1780, -0.0962, -0.0354,\n                        -0.0562, -0.0405, -0.2297, -0.2014, -0.0281, -0.2055, -0.0442, -0.0474,\n                        -0.2064, -0.0562, -0.1792, -0.0978, -0.2156, -0.2055, -0.0392, -0.2093,\n                        -0.0843, -0.2294, -0.2437, -0.1445, -0.0636, -0.0570, -0.0164, -0.1839,\n                        -0.0773, -0.0634, -0.0789, -0.1528, -0.1916, -0.0050, -0.0509, -0.1667,\n                        -0.2108, -0.2476, -0.2115, -0.0088, -0.0135, -0.1661, -0.0518, -0.0702,\n                        -0.0309, -0.0918, -0.0774, -0.0527, -0.1273, -0.1237, -0.0570, -0.1677,\n                        -0.2206, -0.0264, -0.1361, -0.0620, -0.0724, -0.0203, -0.0484, -0.2220,\n                        -0.0620, -0.1381, -0.0214, -0.0579, -0.2097, -0.1215, -0.0302, -0.2257,\n                        -0.0405, -0.0207, -0.0281, -0.1404, -0.0014, -0.1848, -0.2164, -0.0570,\n                        -0.0560, -0.2098, -0.2085, -0.1321, -0.1602, -0.2002, -0.0518, -0.0456,\n                        -0.1797, -0.0394, -0.0561, -0.0101, -0.1999, -0.1506, -0.1992, -0.2735,\n                        -0.1982, -0.2642, -0.1817, -0.1719, -0.0629, -0.0086, -0.1720, -0.0316,\n                        -0.1950, -0.1736, -0.0556, -0.1660, -0.0977, -0.0993, -0.0114, -0.1299,\n                        -0.2259, -0.0563, -0.0459, -0.1176, -0.2501, -0.0314, -0.0248, -0.1539,\n                        -0.0984, -0.1923, -0.0582, -0.1415, -0.2060, -0.1484, -0.2197, -0.0524,\n                        -0.2092, -0.2004, -0.2294, -0.0717, -0.2035, -0.0511, -0.0555, -0.1222,\n                        -0.0710, -0.0722, -0.1744, -0.2046, -0.0660, -0.2646, -0.1205, -0.2524,\n                        -0.1515, -0.0932, -0.1867, -0.2361, -0.0294, -0.0640, -0.1306, -0.2190,\n                        -0.0467, -0.0622, -0.0159, -0.2410, -0.0606, -0.0196, -0.1718, -0.0396,\n                        -0.1045, -0.1803, -0.0477, -0.2285, -0.0069, -0.0741, -0.1753, -0.1399,\n                        -0.0223, -0.0626, -0.0571, -0.1271, -0.2514, -0.2189, -0.0155, -0.0513,\n                        -0.0477, -0.0323, -0.1709, -0.0494, -0.1838, -0.1298, -0.1611, -0.1036,\n                        -0.0405, -0.2173, -0.0978, -0.2247, -0.0898, -0.0575, -0.2170, -0.2590,\n                        -0.0081, -0.0553, -0.0197, -0.1458, -0.2141, -0.2063, -0.0999, -0.1800,\n                        -0.0237, -0.0445, -0.0630, -0.0720, -0.2470, -0.0445, -0.2581, -0.1885,\n                        -0.1863, -0.2147, -0.0695, -0.1083, -0.0425, -0.0957, -0.0278, -0.0488,\n                        -0.0568, -0.2595, -0.2565, -0.2553, -0.2262, -0.0548, -0.0050, -0.0136,\n                        -0.0516, -0.0555, -0.0456, -0.2209, -0.2481, -0.2042, -0.0433, -0.1513,\n                        -0.0281, -0.0475, -0.0305, -0.2296, -0.0353, -0.1967, -0.0841, -0.2282,\n                        -0.0263, -0.0530, -0.0701, -0.0105, -0.1747, -0.2480, -0.1503, -0.2015,\n                        -0.1755, -0.1563, -0.0588, -0.0219, -0.0296, -0.1746, -0.2087, -0.2114,\n                        -0.1725, -0.2643, -0.0641, -0.1308, -0.2651, -0.0293, -0.2540, -0.0830,\n                        -0.0850, -0.0321, -0.1705, -0.0518, -0.1531, -0.2105, -0.0680, -0.1503,\n                        -0.1561, -0.1324, -0.2285, -0.2829, -0.1630, -0.1931, -0.2177, -0.1837,\n                        -0.0570, -0.2354, -0.0118, -0.2007, -0.1105, -0.1948, -0.0369, -0.0507,\n                        -0.0719, -0.1944, -0.0470, -0.1780, -0.0366, -0.0682, -0.0411, -0.1680,\n                        -0.1650, -0.0534, -0.0374, -0.1415, -0.2007, -0.2536, -0.1860, -0.2123]), max_val=tensor([ 0.2527,  0.0851,  0.0518,  0.1880,  0.1815,  0.0298,  0.2576,  0.1895,\n                         0.1887,  0.0498,  0.0481,  0.2319,  0.2510,  0.0543,  0.0554,  0.2119,\n                         0.1624,  0.0460,  0.0321,  0.0494,  0.1579,  0.2161,  0.1639,  0.1738,\n                         0.2429,  0.2524,  0.2578,  0.2466,  0.1606,  0.2690,  0.0673,  0.0592,\n                         0.0524,  0.2174,  0.0223,  0.2230,  0.0619,  0.1826,  0.1692,  0.2216,\n                         0.0946,  0.2295,  0.1598,  0.1931,  0.0128,  0.1235,  0.0961,  0.0770,\n                         0.2419,  0.0652,  0.1468,  0.1098,  0.0475,  0.1726,  0.0487,  0.1945,\n                         0.1042,  0.0480,  0.2566,  0.0448,  0.2413,  0.1941,  0.2518,  0.2031,\n                         0.0324,  0.1142,  0.0466,  0.1343,  0.1066,  0.0175,  0.0166,  0.2359,\n                         0.1260,  0.0480,  0.2099,  0.0277,  0.1237,  0.1194,  0.0603,  0.0439,\n                         0.0410,  0.0626,  0.2139,  0.2630,  0.0099,  0.0840,  0.0478,  0.2102,\n                         0.2544,  0.1869,  0.0331,  0.0357,  0.1852,  0.0514,  0.2038,  0.1904,\n                         0.0953,  0.1192,  0.0399,  0.1062,  0.0341,  0.0199,  0.0553,  0.0580,\n                         0.2080,  0.0531,  0.0517,  0.0107,  0.2054,  0.0584,  0.1249,  0.0181,\n                        -0.0016,  0.2492,  0.1446,  0.0630,  0.0311,  0.2560,  0.2607,  0.0405,\n                         0.0511,  0.0546,  0.0431,  0.1335,  0.3002,  0.0598,  0.2511,  0.1736,\n                         0.2163,  0.1069,  0.0821,  0.1875,  0.0049,  0.0460,  0.2216,  0.0639,\n                         0.0495,  0.2484,  0.0986,  0.2451,  0.1725,  0.2045,  0.1879,  0.0772,\n                         0.2537,  0.1024,  0.2092,  0.2091,  0.0300,  0.1168,  0.1778,  0.0534,\n                         0.1632,  0.0818,  0.1798,  0.1288,  0.0010,  0.0685,  0.0580,  0.2345,\n                         0.1578,  0.0500,  0.0223,  0.0117,  0.0595,  0.0408,  0.1561,  0.1721,\n                         0.0216,  0.2005,  0.2039,  0.2319,  0.0389,  0.0669,  0.0221,  0.0289,\n                         0.0518,  0.0601,  0.0241,  0.0147,  0.2520,  0.2365,  0.0118,  0.1688,\n                         0.0522,  0.0272,  0.2650,  0.0168,  0.0645,  0.0800,  0.1850,  0.1100,\n                         0.0658,  0.2375,  0.2029,  0.0986,  0.0541,  0.2423,  0.1609,  0.0913,\n                         0.1435,  0.0443,  0.2266,  0.1619,  0.0588,  0.0878,  0.0559,  0.2381,\n                         0.0381,  0.0711,  0.0583,  0.1888,  0.0490,  0.1979,  0.2329,  0.0183,\n                         0.2144,  0.0734,  0.0307,  0.0082,  0.2401,  0.0519,  0.1354,  0.0488,\n                         0.0088,  0.0979,  0.0590,  0.0601,  0.1860,  0.1871,  0.0197,  0.0466,\n                         0.1804,  0.2139,  0.1808,  0.0557,  0.2517,  0.1645,  0.0100,  0.2097,\n                         0.0877,  0.0504,  0.2335,  0.0126,  0.1474,  0.1672,  0.0381,  0.1491,\n                         0.2018,  0.2261,  0.2555,  0.1432,  0.0489,  0.0572,  0.1637,  0.1865,\n                         0.2262,  0.1651,  0.0470,  0.1862,  0.0340,  0.0725,  0.0569,  0.0746,\n                         0.1064,  0.0586,  0.0798,  0.0593,  0.1548,  0.1990,  0.0577,  0.0532,\n                         0.2093,  0.1819,  0.2889,  0.0526,  0.0470,  0.0506,  0.1605,  0.0377,\n                         0.2794,  0.1791,  0.1338,  0.2111,  0.0493,  0.1577,  0.0537,  0.0539,\n                         0.0588,  0.0521,  0.2152,  0.0086,  0.2030,  0.1151,  0.1736,  0.2244,\n                         0.2084,  0.0545,  0.0450,  0.0470,  0.0349,  0.1958,  0.0791,  0.2222,\n                         0.2640,  0.2506,  0.2219,  0.0550,  0.0603,  0.0731,  0.1782,  0.0506,\n                         0.1893,  0.1784,  0.2348,  0.0328,  0.1995,  0.1032,  0.0942,  0.0634,\n                         0.1563,  0.2132,  0.1800,  0.1011,  0.0544,  0.0070,  0.1698,  0.0825,\n                         0.0704,  0.1096,  0.2154,  0.2010,  0.1978,  0.0338,  0.0467,  0.0780,\n                         0.0290,  0.0572,  0.2659,  0.0619,  0.0531,  0.1707,  0.0553,  0.1596,\n                         0.0758,  0.1940,  0.0566,  0.2605,  0.0353,  0.0655,  0.2418,  0.0282,\n                         0.0400,  0.1724,  0.0512,  0.0183,  0.0254,  0.0469,  0.0132,  0.0359,\n                         0.2619,  0.0537,  0.1648,  0.0466,  0.0268,  0.0235,  0.1769,  0.2156,\n                         0.1643,  0.0743,  0.2267,  0.0207,  0.1723,  0.1977,  0.2029,  0.0750,\n                         0.0131,  0.1955,  0.1886,  0.1173,  0.0401,  0.0603,  0.0512,  0.0194])\n              )\n            )\n          )\n          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(\n            in_features=384, out_features=1536, bias=True\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0015, 0.0015, 0.0015,  ..., 0.0013, 0.0020, 0.0016]), zero_point=tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([-0.1790, -0.1469, -0.1495,  ..., -0.1386, -0.2593, -0.1621]), max_val=tensor([0.1940, 0.1849, 0.1968,  ..., 0.1696, 0.1514, 0.2051]))\n            )\n          )\n          (4): GELU(approximate='none')\n          (5): Linear(\n            in_features=1536, out_features=384, bias=True\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0017, 0.0015, 0.0014, 0.0016, 0.0018, 0.0015, 0.0014, 0.0016, 0.0018,\n                      0.0015, 0.0020, 0.0015, 0.0026, 0.0030, 0.0017, 0.0017, 0.0021, 0.0015,\n                      0.0016, 0.0020, 0.0015, 0.0016, 0.0019, 0.0018, 0.0015, 0.0025, 0.0014,\n                      0.0014, 0.0017, 0.0015, 0.0021, 0.0015, 0.0017, 0.0016, 0.0015, 0.0016,\n                      0.0018, 0.0016, 0.0015, 0.0016, 0.0017, 0.0017, 0.0019, 0.0016, 0.0016,\n                      0.0019, 0.0025, 0.0014, 0.0016, 0.0015, 0.0018, 0.0023, 0.0015, 0.0018,\n                      0.0016, 0.0009, 0.0016, 0.0019, 0.0017, 0.0015, 0.0015, 0.0018, 0.0017,\n                      0.0017, 0.0016, 0.0018, 0.0017, 0.0017, 0.0016, 0.0015, 0.0017, 0.0015,\n                      0.0019, 0.0025, 0.0014, 0.0016, 0.0016, 0.0013, 0.0020, 0.0016, 0.0017,\n                      0.0017, 0.0014, 0.0019, 0.0016, 0.0016, 0.0021, 0.0016, 0.0014, 0.0017,\n                      0.0017, 0.0019, 0.0018, 0.0014, 0.0016, 0.0018, 0.0016, 0.0021, 0.0015,\n                      0.0018, 0.0015, 0.0014, 0.0026, 0.0016, 0.0016, 0.0015, 0.0017, 0.0022,\n                      0.0017, 0.0037, 0.0016, 0.0019, 0.0025, 0.0016, 0.0018, 0.0015, 0.0015,\n                      0.0028, 0.0015, 0.0016, 0.0018, 0.0018, 0.0015, 0.0018, 0.0023, 0.0018,\n                      0.0018, 0.0016, 0.0015, 0.0020, 0.0010, 0.0019, 0.0017, 0.0018, 0.0013,\n                      0.0016, 0.0015, 0.0015, 0.0016, 0.0015, 0.0020, 0.0017, 0.0017, 0.0018,\n                      0.0016, 0.0016, 0.0026, 0.0019, 0.0017, 0.0018, 0.0029, 0.0015, 0.0018,\n                      0.0019, 0.0016, 0.0018, 0.0099, 0.0022, 0.0018, 0.0016, 0.0018, 0.0013,\n                      0.0016, 0.0018, 0.0015, 0.0016, 0.0020, 0.0016, 0.0015, 0.0014, 0.0016,\n                      0.0016, 0.0016, 0.0013, 0.0016, 0.0019, 0.0018, 0.0016, 0.0015, 0.0016,\n                      0.0014, 0.0035, 0.0016, 0.0015, 0.0014, 0.0018, 0.0027, 0.0015, 0.0017,\n                      0.0035, 0.0016, 0.0016, 0.0016, 0.0014, 0.0018, 0.0015, 0.0019, 0.0018,\n                      0.0016, 0.0017, 0.0020, 0.0017, 0.0014, 0.0016, 0.0019, 0.0020, 0.0017,\n                      0.0015, 0.0021, 0.0023, 0.0017, 0.0026, 0.0013, 0.0019, 0.0015, 0.0022,\n                      0.0015, 0.0012, 0.0014, 0.0016, 0.0016, 0.0017, 0.0019, 0.0018, 0.0015,\n                      0.0019, 0.0017, 0.0016, 0.0016, 0.0017, 0.0020, 0.0015, 0.0016, 0.0016,\n                      0.0017, 0.0014, 0.0016, 0.0019, 0.0020, 0.0018, 0.0021, 0.0015, 0.0016,\n                      0.0021, 0.0016, 0.0019, 0.0017, 0.0016, 0.0021, 0.0016, 0.0025, 0.0023,\n                      0.0015, 0.0015, 0.0017, 0.0015, 0.0018, 0.0018, 0.0016, 0.0015, 0.0017,\n                      0.0016, 0.0017, 0.0017, 0.0033, 0.0023, 0.0018, 0.0016, 0.0016, 0.0016,\n                      0.0016, 0.0019, 0.0021, 0.0016, 0.0026, 0.0016, 0.0014, 0.0013, 0.0016,\n                      0.0014, 0.0015, 0.0018, 0.0016, 0.0015, 0.0016, 0.0015, 0.0016, 0.0013,\n                      0.0019, 0.0017, 0.0016, 0.0018, 0.0015, 0.0018, 0.0016, 0.0017, 0.0018,\n                      0.0019, 0.0014, 0.0013, 0.0015, 0.0016, 0.0019, 0.0017, 0.0015, 0.0015,\n                      0.0014, 0.0016, 0.0017, 0.0019, 0.0017, 0.0017, 0.0015, 0.0014, 0.0011,\n                      0.0017, 0.0018, 0.0016, 0.0024, 0.0017, 0.0018, 0.0016, 0.0019, 0.0019,\n                      0.0016, 0.0020, 0.0019, 0.0017, 0.0014, 0.0017, 0.0019, 0.0018, 0.0014,\n                      0.0020, 0.0017, 0.0015, 0.0015, 0.0020, 0.0010, 0.0020, 0.0018, 0.0018,\n                      0.0014, 0.0019, 0.0032, 0.0016, 0.0015, 0.0015, 0.0026, 0.0016, 0.0018,\n                      0.0015, 0.0016, 0.0026, 0.0016, 0.0010, 0.0017, 0.0015, 0.0021, 0.0014,\n                      0.0017, 0.0017, 0.0016, 0.0015, 0.0021, 0.0015, 0.0018, 0.0015, 0.0018,\n                      0.0019, 0.0015, 0.0016, 0.0017, 0.0015, 0.0016, 0.0018, 0.0014, 0.0017,\n                      0.0015, 0.0016, 0.0016, 0.0015, 0.0018, 0.0018]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                     dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n                min_val=tensor([-0.2148, -0.1610, -0.1826, -0.1874, -0.2046, -0.1909, -0.1542, -0.2111,\n                        -0.2201, -0.1696, -0.1946, -0.1924, -0.3330, -0.2357, -0.1932, -0.1801,\n                        -0.2637, -0.1944, -0.1548, -0.1823, -0.1937, -0.2065, -0.2177, -0.2322,\n                        -0.1912, -0.1716, -0.1564, -0.1766, -0.2039, -0.1870, -0.2703, -0.1786,\n                        -0.2090, -0.2054, -0.1902, -0.2104, -0.1924, -0.2087, -0.1888, -0.1998,\n                        -0.2011, -0.2229, -0.1896, -0.1725, -0.1739, -0.1891, -0.2217, -0.1807,\n                        -0.2068, -0.1858, -0.2115, -0.2990, -0.1966, -0.2243, -0.1997, -0.0937,\n                        -0.2014, -0.2446, -0.1654, -0.1790, -0.1956, -0.2292, -0.1701, -0.1589,\n                        -0.2035, -0.2269, -0.1764, -0.1727, -0.1560, -0.1929, -0.1794, -0.1924,\n                        -0.2441, -0.2192, -0.1809, -0.2018, -0.2003, -0.1583, -0.2550, -0.1558,\n                        -0.2117, -0.2146, -0.1766, -0.2414, -0.2063, -0.2010, -0.2437, -0.1588,\n                        -0.1792, -0.1873, -0.1933, -0.2137, -0.2258, -0.1834, -0.2109, -0.2249,\n                        -0.1918, -0.1926, -0.1654, -0.1976, -0.1916, -0.1796, -0.2100, -0.2041,\n                        -0.2021, -0.1703, -0.2198, -0.2823, -0.2216, -0.4716, -0.2032, -0.2189,\n                        -0.3040, -0.2086, -0.1924, -0.1969, -0.1854, -0.3261, -0.1702, -0.1827,\n                        -0.2040, -0.1829, -0.1804, -0.2345, -0.1966, -0.2294, -0.2351, -0.2031,\n                        -0.1923, -0.2555, -0.1314, -0.1878, -0.2013, -0.2295, -0.1718, -0.1988,\n                        -0.1795, -0.1978, -0.1363, -0.1875, -0.1868, -0.2132, -0.1710, -0.2257,\n                        -0.1922, -0.1797, -0.2544, -0.1937, -0.1875, -0.2294, -0.3753, -0.1970,\n                        -0.2284, -0.2173, -0.2006, -0.1766, -1.2669, -0.2842, -0.1840, -0.1706,\n                        -0.2360, -0.1726, -0.1709, -0.2311, -0.1829, -0.1528, -0.2577, -0.1964,\n                        -0.1715, -0.1774, -0.1971, -0.2075, -0.2078, -0.1718, -0.1948, -0.2412,\n                        -0.2247, -0.1686, -0.1946, -0.1841, -0.1824, -0.4472, -0.2010, -0.1794,\n                        -0.1661, -0.2301, -0.3422, -0.1879, -0.2191, -0.3310, -0.2032, -0.1974,\n                        -0.2044, -0.1817, -0.2280, -0.1947, -0.1920, -0.2105, -0.2001, -0.2156,\n                        -0.2133, -0.2222, -0.1817, -0.2075, -0.2441, -0.1989, -0.2238, -0.1866,\n                        -0.2676, -0.2133, -0.2211, -0.3376, -0.1589, -0.2465, -0.1940, -0.2760,\n                        -0.1936, -0.1556, -0.1814, -0.2063, -0.2079, -0.2136, -0.2015, -0.1945,\n                        -0.1799, -0.1709, -0.1846, -0.2086, -0.2111, -0.1834, -0.1851, -0.1895,\n                        -0.1996, -0.1873, -0.1729, -0.1696, -0.1528, -0.2068, -0.1864, -0.1808,\n                        -0.2749, -0.1917, -0.1964, -0.2700, -0.2004, -0.1846, -0.2205, -0.1860,\n                        -0.1836, -0.1553, -0.3209, -0.2932, -0.1976, -0.1938, -0.2207, -0.1769,\n                        -0.2243, -0.2351, -0.1689, -0.1879, -0.2177, -0.2032, -0.1778, -0.1997,\n                        -0.4177, -0.2775, -0.2216, -0.2018, -0.2053, -0.1845, -0.1850, -0.1820,\n                        -0.2203, -0.1797, -0.3325, -0.1672, -0.1568, -0.1696, -0.2066, -0.1824,\n                        -0.1951, -0.2330, -0.1982, -0.1685, -0.1652, -0.1738, -0.2010, -0.1678,\n                        -0.2494, -0.2158, -0.2009, -0.2151, -0.1929, -0.1552, -0.1791, -0.1906,\n                        -0.2101, -0.2395, -0.1444, -0.1566, -0.1856, -0.1998, -0.2425, -0.2157,\n                        -0.1743, -0.1650, -0.1728, -0.2053, -0.1556, -0.1788, -0.1799, -0.2130,\n                        -0.1714, -0.1850, -0.1369, -0.1890, -0.1857, -0.1990, -0.3016, -0.2162,\n                        -0.2297, -0.2049, -0.1779, -0.2369, -0.1729, -0.2075, -0.2137, -0.1957,\n                        -0.1822, -0.2149, -0.1802, -0.1962, -0.1799, -0.2541, -0.1898, -0.1875,\n                        -0.1932, -0.2505, -0.1201, -0.2399, -0.1638, -0.2293, -0.1727, -0.2082,\n                        -0.3198, -0.2024, -0.1630, -0.1882, -0.3362, -0.2097, -0.2031, -0.1795,\n                        -0.1872, -0.3295, -0.1991, -0.1239, -0.2150, -0.1876, -0.1908, -0.1846,\n                        -0.2135, -0.1857, -0.2026, -0.1959, -0.2726, -0.1951, -0.1887, -0.1708,\n                        -0.2166, -0.2021, -0.1870, -0.2021, -0.2136, -0.1802, -0.1993, -0.2299,\n                        -0.1799, -0.2119, -0.1870, -0.1888, -0.1989, -0.1581, -0.2322, -0.2264]), max_val=tensor([0.1884, 0.1869, 0.1800, 0.2066, 0.2254, 0.1846, 0.1809, 0.2078, 0.2308,\n                        0.1931, 0.2477, 0.1918, 0.2380, 0.3806, 0.2144, 0.2192, 0.1702, 0.1626,\n                        0.2091, 0.2510, 0.1956, 0.1925, 0.2355, 0.2015, 0.1785, 0.3234, 0.1833,\n                        0.1812, 0.2121, 0.1769, 0.1957, 0.1879, 0.2178, 0.1677, 0.1729, 0.1963,\n                        0.2228, 0.1884, 0.1783, 0.1743, 0.2177, 0.2025, 0.2411, 0.2091, 0.1995,\n                        0.2371, 0.3205, 0.1734, 0.1652, 0.1927, 0.2225, 0.2071, 0.1744, 0.2303,\n                        0.2083, 0.1134, 0.2090, 0.2311, 0.2106, 0.1923, 0.1651, 0.2145, 0.2130,\n                        0.2159, 0.1905, 0.1912, 0.2162, 0.2120, 0.1990, 0.1875, 0.2107, 0.1758,\n                        0.2434, 0.3231, 0.1747, 0.1701, 0.1828, 0.1616, 0.1689, 0.1972, 0.1910,\n                        0.1808, 0.1783, 0.2464, 0.1655, 0.2015, 0.2707, 0.2056, 0.1696, 0.2218,\n                        0.2154, 0.2370, 0.1877, 0.1786, 0.1951, 0.1586, 0.2047, 0.2711, 0.1938,\n                        0.2251, 0.1967, 0.1747, 0.3288, 0.1828, 0.1931, 0.1907, 0.1861, 0.1812,\n                        0.1597, 0.3915, 0.2012, 0.2371, 0.3128, 0.1936, 0.2314, 0.1538, 0.1890,\n                        0.3520, 0.1854, 0.2059, 0.2260, 0.2247, 0.1894, 0.2004, 0.2957, 0.2111,\n                        0.1773, 0.1974, 0.1760, 0.1531, 0.1270, 0.2412, 0.2183, 0.1933, 0.1616,\n                        0.1998, 0.1851, 0.1693, 0.1985, 0.1636, 0.2559, 0.1936, 0.2098, 0.1793,\n                        0.2051, 0.2003, 0.3323, 0.2474, 0.2215, 0.1960, 0.3587, 0.1862, 0.1734,\n                        0.2357, 0.1745, 0.2330, 0.6966, 0.2342, 0.2269, 0.2019, 0.1812, 0.1642,\n                        0.2044, 0.2320, 0.1956, 0.2067, 0.1721, 0.2078, 0.1952, 0.1698, 0.2094,\n                        0.1761, 0.1730, 0.1695, 0.1974, 0.2046, 0.1978, 0.2094, 0.1756, 0.2029,\n                        0.1823, 0.2780, 0.1822, 0.1859, 0.1833, 0.1682, 0.2437, 0.1832, 0.2064,\n                        0.4492, 0.1825, 0.2049, 0.1821, 0.1581, 0.2075, 0.1430, 0.2435, 0.2248,\n                        0.1749, 0.1963, 0.2569, 0.1972, 0.1798, 0.1894, 0.2082, 0.2574, 0.2053,\n                        0.1832, 0.1839, 0.2867, 0.1803, 0.2133, 0.1636, 0.1693, 0.1870, 0.2685,\n                        0.1906, 0.1581, 0.1665, 0.2093, 0.2001, 0.1614, 0.2407, 0.2241, 0.1922,\n                        0.2354, 0.2114, 0.1584, 0.1870, 0.2191, 0.2529, 0.1738, 0.2043, 0.2008,\n                        0.2098, 0.1766, 0.2003, 0.2439, 0.2582, 0.2271, 0.1953, 0.1526, 0.2095,\n                        0.2116, 0.1831, 0.2460, 0.2069, 0.2015, 0.2659, 0.2095, 0.1908, 0.1590,\n                        0.1968, 0.1759, 0.1974, 0.1925, 0.1797, 0.2210, 0.2041, 0.1617, 0.1614,\n                        0.2042, 0.2133, 0.2199, 0.3954, 0.2931, 0.2316, 0.1655, 0.2091, 0.2087,\n                        0.1971, 0.2464, 0.2653, 0.2022, 0.3111, 0.2011, 0.1817, 0.1626, 0.2084,\n                        0.1665, 0.1777, 0.2090, 0.2079, 0.1965, 0.2030, 0.1874, 0.2049, 0.1585,\n                        0.1930, 0.1787, 0.1661, 0.2337, 0.1751, 0.2301, 0.2021, 0.2205, 0.2228,\n                        0.1899, 0.1799, 0.1700, 0.1853, 0.1948, 0.2243, 0.1710, 0.1855, 0.1866,\n                        0.1754, 0.1914, 0.2158, 0.2459, 0.2106, 0.1993, 0.1912, 0.1737, 0.1248,\n                        0.2100, 0.2233, 0.1715, 0.2431, 0.1499, 0.1884, 0.1681, 0.2470, 0.1844,\n                        0.2090, 0.2563, 0.2364, 0.2121, 0.1786, 0.2022, 0.2470, 0.2257, 0.1611,\n                        0.2371, 0.2107, 0.1872, 0.1927, 0.1700, 0.1224, 0.2587, 0.2233, 0.1906,\n                        0.1839, 0.2395, 0.4047, 0.1961, 0.1917, 0.1889, 0.2131, 0.1875, 0.2262,\n                        0.1898, 0.2051, 0.3346, 0.1835, 0.1115, 0.1946, 0.1612, 0.2724, 0.1639,\n                        0.1763, 0.2170, 0.1994, 0.1740, 0.2045, 0.1912, 0.2263, 0.1951, 0.2317,\n                        0.2378, 0.1884, 0.1886, 0.1983, 0.1938, 0.1745, 0.2258, 0.1778, 0.1878,\n                        0.1735, 0.1976, 0.1991, 0.1936, 0.2190, 0.2018])\n              )\n            )\n          )\n        )\n      )\n    )\n    (6): Module(\n      (0): Module()\n      (1): Conv2d(\n        384, 768, kernel_size=(2, 2), stride=(2, 2)\n        (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0012, 0.0014, 0.0011, 0.0013, 0.0011, 0.0011, 0.0011, 0.0013, 0.0015,\n                  0.0020, 0.0022, 0.0014, 0.0019, 0.0015, 0.0037, 0.0011, 0.0011, 0.0015,\n                  0.0014, 0.0012, 0.0015, 0.0012, 0.0014, 0.0011, 0.0016, 0.0014, 0.0011,\n                  0.0011, 0.0012, 0.0011, 0.0012, 0.0013, 0.0015, 0.0011, 0.0014, 0.0012,\n                  0.0013, 0.0010, 0.0011, 0.0011, 0.0012, 0.0014, 0.0013, 0.0013, 0.0012,\n                  0.0014, 0.0011, 0.0024, 0.0013, 0.0010, 0.0015, 0.0015, 0.0025, 0.0012,\n                  0.0014, 0.0014, 0.0013, 0.0011, 0.0014, 0.0014, 0.0014, 0.0016, 0.0012,\n                  0.0012, 0.0013, 0.0012, 0.0010, 0.0015, 0.0013, 0.0010, 0.0013, 0.0014,\n                  0.0015, 0.0012, 0.0011, 0.0013, 0.0012, 0.0012, 0.0015, 0.0013, 0.0015,\n                  0.0009, 0.0015, 0.0011, 0.0012, 0.0013, 0.0013, 0.0017, 0.0012, 0.0010,\n                  0.0013, 0.0011, 0.0013, 0.0013, 0.0013, 0.0014, 0.0016, 0.0015, 0.0013,\n                  0.0013, 0.0013, 0.0014, 0.0010, 0.0017, 0.0015, 0.0011, 0.0014, 0.0014,\n                  0.0012, 0.0010, 0.0010, 0.0014, 0.0015, 0.0013, 0.0013, 0.0022, 0.0012,\n                  0.0015, 0.0019, 0.0025, 0.0012, 0.0009, 0.0013, 0.0022, 0.0012, 0.0015,\n                  0.0017, 0.0014, 0.0012, 0.0014, 0.0022, 0.0018, 0.0011, 0.0016, 0.0014,\n                  0.0011, 0.0014, 0.0016, 0.0012, 0.0012, 0.0010, 0.0014, 0.0015, 0.0014,\n                  0.0014, 0.0019, 0.0014, 0.0012, 0.0013, 0.0013, 0.0013, 0.0012, 0.0012,\n                  0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0013, 0.0016, 0.0012, 0.0012,\n                  0.0017, 0.0014, 0.0014, 0.0018, 0.0017, 0.0012, 0.0011, 0.0011, 0.0025,\n                  0.0016, 0.0014, 0.0013, 0.0014, 0.0015, 0.0015, 0.0015, 0.0013, 0.0017,\n                  0.0018, 0.0012, 0.0011, 0.0013, 0.0012, 0.0014, 0.0015, 0.0015, 0.0011,\n                  0.0012, 0.0012, 0.0018, 0.0011, 0.0014, 0.0011, 0.0016, 0.0010, 0.0012,\n                  0.0013, 0.0013, 0.0023, 0.0014, 0.0014, 0.0013, 0.0028, 0.0011, 0.0025,\n                  0.0015, 0.0012, 0.0016, 0.0010, 0.0011, 0.0013, 0.0029, 0.0013, 0.0027,\n                  0.0012, 0.0017, 0.0014, 0.0014, 0.0011, 0.0013, 0.0016, 0.0013, 0.0012,\n                  0.0018, 0.0014, 0.0015, 0.0012, 0.0013, 0.0027, 0.0015, 0.0013, 0.0012,\n                  0.0011, 0.0012, 0.0032, 0.0013, 0.0009, 0.0015, 0.0014, 0.0017, 0.0012,\n                  0.0014, 0.0015, 0.0013, 0.0012, 0.0013, 0.0013, 0.0028, 0.0016, 0.0011,\n                  0.0013, 0.0012, 0.0011, 0.0014, 0.0010, 0.0012, 0.0011, 0.0012, 0.0012,\n                  0.0013, 0.0014, 0.0011, 0.0070, 0.0013, 0.0033, 0.0014, 0.0014, 0.0013,\n                  0.0013, 0.0014, 0.0012, 0.0025, 0.0017, 0.0010, 0.0013, 0.0014, 0.0015,\n                  0.0014, 0.0013, 0.0012, 0.0018, 0.0014, 0.0013, 0.0021, 0.0012, 0.0014,\n                  0.0036, 0.0013, 0.0011, 0.0019, 0.0013, 0.0015, 0.0013, 0.0016, 0.0011,\n                  0.0014, 0.0017, 0.0017, 0.0014, 0.0014, 0.0012, 0.0012, 0.0013, 0.0012,\n                  0.0012, 0.0015, 0.0018, 0.0013, 0.0014, 0.0016, 0.0012, 0.0013, 0.0014,\n                  0.0012, 0.0010, 0.0013, 0.0017, 0.0012, 0.0010, 0.0014, 0.0012, 0.0023,\n                  0.0012, 0.0011, 0.0012, 0.0015, 0.0010, 0.0014, 0.0018, 0.0012, 0.0009,\n                  0.0014, 0.0011, 0.0014, 0.0009, 0.0012, 0.0014, 0.0021, 0.0014, 0.0017,\n                  0.0012, 0.0018, 0.0012, 0.0011, 0.0014, 0.0017, 0.0016, 0.0012, 0.0014,\n                  0.0014, 0.0010, 0.0013, 0.0015, 0.0016, 0.0014, 0.0012, 0.0008, 0.0016,\n                  0.0015, 0.0013, 0.0012, 0.0011, 0.0015, 0.0014, 0.0015, 0.0013, 0.0014,\n                  0.0011, 0.0016, 0.0012, 0.0013, 0.0010, 0.0012, 0.0014, 0.0013, 0.0013,\n                  0.0012, 0.0013, 0.0009, 0.0013, 0.0012, 0.0012, 0.0012, 0.0011, 0.0012,\n                  0.0017, 0.0014, 0.0010, 0.0013, 0.0015, 0.0013, 0.0013, 0.0016, 0.0028,\n                  0.0011, 0.0012, 0.0012, 0.0011, 0.0012, 0.0017, 0.0015, 0.0021, 0.0012,\n                  0.0014, 0.0015, 0.0011, 0.0011, 0.0010, 0.0013, 0.0010, 0.0012, 0.0014,\n                  0.0020, 0.0012, 0.0014, 0.0020, 0.0013, 0.0012, 0.0021, 0.0013, 0.0012,\n                  0.0011, 0.0010, 0.0013, 0.0010, 0.0014, 0.0013, 0.0012, 0.0010, 0.0012,\n                  0.0014, 0.0012, 0.0009, 0.0014, 0.0011, 0.0014, 0.0014, 0.0013, 0.0013,\n                  0.0013, 0.0016, 0.0016, 0.0012, 0.0015, 0.0023, 0.0012, 0.0015, 0.0028,\n                  0.0012, 0.0017, 0.0013, 0.0024, 0.0013, 0.0010, 0.0029, 0.0015, 0.0012,\n                  0.0018, 0.0012, 0.0016, 0.0018, 0.0012, 0.0013, 0.0010, 0.0010, 0.0012,\n                  0.0012, 0.0014, 0.0014, 0.0012, 0.0016, 0.0015, 0.0012, 0.0016, 0.0013,\n                  0.0020, 0.0011, 0.0012, 0.0012, 0.0010, 0.0013, 0.0012, 0.0009, 0.0012,\n                  0.0015, 0.0012, 0.0011, 0.0014, 0.0010, 0.0014, 0.0018, 0.0014, 0.0014,\n                  0.0017, 0.0015, 0.0020, 0.0012, 0.0012, 0.0024, 0.0012, 0.0011, 0.0013,\n                  0.0011, 0.0017, 0.0011, 0.0011, 0.0016, 0.0012, 0.0014, 0.0016, 0.0011,\n                  0.0011, 0.0022, 0.0015, 0.0011, 0.0014, 0.0014, 0.0014, 0.0012, 0.0020,\n                  0.0018, 0.0011, 0.0013, 0.0013, 0.0013, 0.0014, 0.0012, 0.0012, 0.0012,\n                  0.0022, 0.0015, 0.0014, 0.0012, 0.0025, 0.0017, 0.0014, 0.0012, 0.0017,\n                  0.0011, 0.0013, 0.0031, 0.0017, 0.0015, 0.0012, 0.0017, 0.0011, 0.0011,\n                  0.0012, 0.0012, 0.0019, 0.0012, 0.0010, 0.0012, 0.0011, 0.0018, 0.0013,\n                  0.0013, 0.0013, 0.0011, 0.0011, 0.0015, 0.0011, 0.0012, 0.0011, 0.0012,\n                  0.0013, 0.0009, 0.0092, 0.0012, 0.0011, 0.0012, 0.0014, 0.0012, 0.0014,\n                  0.0014, 0.0012, 0.0011, 0.0013, 0.0026, 0.0009, 0.0011, 0.0030, 0.0019,\n                  0.0015, 0.0015, 0.0013, 0.0019, 0.0016, 0.0016, 0.0013, 0.0013, 0.0013,\n                  0.0010, 0.0009, 0.0012, 0.0010, 0.0011, 0.0015, 0.0011, 0.0012, 0.0013,\n                  0.0011, 0.0009, 0.0015, 0.0010, 0.0024, 0.0015, 0.0016, 0.0011, 0.0016,\n                  0.0013, 0.0012, 0.0028, 0.0016, 0.0017, 0.0013, 0.0011, 0.0012, 0.0010,\n                  0.0010, 0.0012, 0.0013, 0.0010, 0.0014, 0.0011, 0.0013, 0.0014, 0.0014,\n                  0.0027, 0.0012, 0.0011, 0.0014, 0.0015, 0.0009, 0.0013, 0.0012, 0.0009,\n                  0.0014, 0.0011, 0.0023, 0.0011, 0.0012, 0.0016, 0.0010, 0.0013, 0.0014,\n                  0.0014, 0.0013, 0.0012, 0.0012, 0.0012, 0.0022, 0.0013, 0.0012, 0.0021,\n                  0.0012, 0.0014, 0.0016, 0.0012, 0.0013, 0.0018, 0.0016, 0.0011, 0.0012,\n                  0.0015, 0.0014, 0.0015, 0.0013, 0.0015, 0.0015, 0.0013, 0.0010, 0.0020,\n                  0.0013, 0.0018, 0.0017, 0.0014, 0.0011, 0.0014, 0.0014, 0.0012, 0.0013,\n                  0.0020, 0.0019, 0.0016, 0.0014, 0.0011, 0.0012, 0.0010, 0.0015, 0.0013,\n                  0.0013, 0.0011, 0.0012, 0.0011, 0.0019, 0.0013, 0.0011, 0.0074, 0.0011,\n                  0.0015, 0.0011, 0.0011, 0.0012, 0.0010, 0.0075, 0.0010, 0.0015, 0.0011,\n                  0.0012, 0.0010, 0.0014, 0.0024, 0.0011, 0.0011, 0.0014, 0.0014, 0.0010,\n                  0.0013, 0.0016, 0.0015, 0.0013, 0.0010, 0.0012, 0.0016, 0.0011, 0.0014,\n                  0.0013, 0.0015, 0.0013, 0.0015, 0.0015, 0.0015, 0.0012, 0.0013, 0.0010,\n                  0.0014, 0.0015, 0.0012, 0.0015, 0.0035, 0.0016, 0.0034, 0.0012, 0.0013,\n                  0.0013, 0.0010, 0.0037, 0.0009, 0.0014, 0.0013, 0.0013, 0.0013, 0.0011,\n                  0.0011, 0.0013, 0.0013, 0.0011, 0.0010, 0.0011, 0.0012, 0.0012, 0.0015,\n                  0.0013, 0.0019, 0.0013]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                 dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n          (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n            min_val=tensor([-0.1503, -0.1714, -0.1212, -0.1699, -0.1418, -0.1453, -0.1403, -0.1464,\n                    -0.1875, -0.1305, -0.2764, -0.1568, -0.2046, -0.1400, -0.2720, -0.1443,\n                    -0.1449, -0.1532, -0.1765, -0.1343, -0.1734, -0.1366, -0.1831, -0.1150,\n                    -0.1383, -0.1209, -0.1363, -0.1431, -0.1565, -0.1362, -0.1476, -0.1665,\n                    -0.1185, -0.1423, -0.1336, -0.1460, -0.1603, -0.1330, -0.1367, -0.1400,\n                    -0.1441, -0.1708, -0.1693, -0.1460, -0.1546, -0.1520, -0.1385, -0.3088,\n                    -0.1253, -0.1307, -0.1921, -0.1578, -0.1927, -0.1323, -0.1758, -0.1788,\n                    -0.1691, -0.1438, -0.1569, -0.1262, -0.1503, -0.2004, -0.1388, -0.1542,\n                    -0.1697, -0.1586, -0.1192, -0.1341, -0.1678, -0.1103, -0.1625, -0.1818,\n                    -0.1877, -0.1187, -0.1357, -0.1546, -0.1363, -0.1489, -0.1405, -0.1395,\n                    -0.1527, -0.1123, -0.1964, -0.1401, -0.1480, -0.1710, -0.1655, -0.2127,\n                    -0.1406, -0.1187, -0.1627, -0.1386, -0.1448, -0.1577, -0.1563, -0.1852,\n                    -0.1334, -0.1491, -0.1659, -0.1726, -0.1676, -0.1802, -0.1325, -0.2119,\n                    -0.1951, -0.1288, -0.1765, -0.1443, -0.1564, -0.1325, -0.1297, -0.1467,\n                    -0.1764, -0.1655, -0.1662, -0.2859, -0.1360, -0.1696, -0.2389, -0.3151,\n                    -0.1542, -0.1181, -0.1686, -0.2865, -0.1287, -0.1875, -0.2178, -0.1783,\n                    -0.1513, -0.1802, -0.2791, -0.1714, -0.1341, -0.1119, -0.1703, -0.1454,\n                    -0.1690, -0.1537, -0.1487, -0.1266, -0.1324, -0.1746, -0.1772, -0.1826,\n                    -0.1295, -0.2468, -0.1678, -0.1543, -0.1556, -0.1201, -0.1706, -0.1575,\n                    -0.1324, -0.1387, -0.1255, -0.1524, -0.1497, -0.1318, -0.1406, -0.1869,\n                    -0.1503, -0.1576, -0.1969, -0.1696, -0.1760, -0.1380, -0.1629, -0.1507,\n                    -0.1386, -0.1445, -0.3162, -0.1621, -0.1746, -0.1594, -0.1760, -0.1975,\n                    -0.1256, -0.1679, -0.1413, -0.1502, -0.2345, -0.1491, -0.1378, -0.1670,\n                    -0.1581, -0.1748, -0.1626, -0.1464, -0.1371, -0.1533, -0.1525, -0.1519,\n                    -0.1182, -0.1740, -0.1321, -0.1421, -0.1239, -0.1241, -0.1610, -0.1641,\n                    -0.2998, -0.1813, -0.1695, -0.1242, -0.1574, -0.1432, -0.3141, -0.1267,\n                    -0.1495, -0.2024, -0.1206, -0.1396, -0.1632, -0.2483, -0.1725, -0.3491,\n                    -0.1499, -0.2138, -0.1421, -0.1658, -0.1297, -0.1642, -0.2080, -0.1423,\n                    -0.1283, -0.1304, -0.1840, -0.1914, -0.1419, -0.1476, -0.3160, -0.1657,\n                    -0.1435, -0.1480, -0.1390, -0.1560, -0.1686, -0.1419, -0.0954, -0.1454,\n                    -0.1831, -0.1447, -0.1552, -0.1767, -0.1916, -0.1655, -0.1585, -0.1507,\n                    -0.1611, -0.3491, -0.1611, -0.1377, -0.1380, -0.1376, -0.1355, -0.1747,\n                    -0.1235, -0.1558, -0.1410, -0.1354, -0.1269, -0.1602, -0.1816, -0.1334,\n                    -0.3834, -0.1300, -0.4255, -0.1811, -0.1564, -0.1695, -0.1623, -0.1778,\n                    -0.1559, -0.3185, -0.1722, -0.1242, -0.1357, -0.1465, -0.1692, -0.1733,\n                    -0.1622, -0.1281, -0.1561, -0.1695, -0.1646, -0.2498, -0.1384, -0.1804,\n                    -0.3165, -0.1555, -0.1403, -0.1832, -0.1270, -0.1908, -0.1490, -0.1609,\n                    -0.1358, -0.1362, -0.2075, -0.2184, -0.1342, -0.1732, -0.1286, -0.1484,\n                    -0.1481, -0.1403, -0.1284, -0.1319, -0.2253, -0.1454, -0.1467, -0.2018,\n                    -0.1548, -0.1421, -0.1820, -0.1455, -0.1268, -0.1424, -0.2159, -0.1492,\n                    -0.1238, -0.1816, -0.1482, -0.2913, -0.1476, -0.1330, -0.1561, -0.1922,\n                    -0.1273, -0.1788, -0.2367, -0.1509, -0.1140, -0.1414, -0.1134, -0.1417,\n                    -0.1039, -0.1586, -0.1833, -0.2534, -0.1739, -0.1547, -0.1460, -0.2112,\n                    -0.1532, -0.1428, -0.1769, -0.2184, -0.1357, -0.1589, -0.1823, -0.1265,\n                    -0.1314, -0.1669, -0.1800, -0.1383, -0.1799, -0.1511, -0.1060, -0.1998,\n                    -0.1771, -0.1678, -0.1583, -0.1068, -0.1567, -0.1323, -0.1931, -0.1647,\n                    -0.1355, -0.1438, -0.1365, -0.1547, -0.1615, -0.1257, -0.1598, -0.1427,\n                    -0.1303, -0.1706, -0.1453, -0.1618, -0.1158, -0.1656, -0.1522, -0.1537,\n                    -0.1423, -0.1388, -0.1500, -0.1735, -0.1848, -0.1150, -0.1345, -0.1442,\n                    -0.1497, -0.1237, -0.2047, -0.2354, -0.1407, -0.1510, -0.1434, -0.1440,\n                    -0.1477, -0.1593, -0.1379, -0.2421, -0.1574, -0.1546, -0.1547, -0.1389,\n                    -0.1336, -0.1311, -0.1688, -0.1309, -0.1483, -0.1321, -0.1532, -0.1586,\n                    -0.1264, -0.1448, -0.1681, -0.1388, -0.1800, -0.1663, -0.1512, -0.1467,\n                    -0.1010, -0.1493, -0.1228, -0.1828, -0.1485, -0.1480, -0.1263, -0.1388,\n                    -0.1158, -0.1513, -0.1166, -0.1592, -0.1411, -0.1583, -0.1848, -0.1376,\n                    -0.1484, -0.1691, -0.1338, -0.1423, -0.1490, -0.1325, -0.1539, -0.1529,\n                    -0.1891, -0.2172, -0.1476, -0.2184, -0.1667, -0.2086, -0.1650, -0.1255,\n                    -0.3708, -0.1409, -0.1560, -0.1824, -0.1479, -0.1250, -0.1673, -0.1309,\n                    -0.1472, -0.1188, -0.1290, -0.1520, -0.1545, -0.1682, -0.1641, -0.1594,\n                    -0.2063, -0.1923, -0.1500, -0.1427, -0.1616, -0.1566, -0.1391, -0.1510,\n                    -0.1404, -0.1257, -0.1422, -0.1339, -0.0955, -0.1377, -0.1558, -0.1345,\n                    -0.1402, -0.1822, -0.1246, -0.1406, -0.2275, -0.1802, -0.1841, -0.2141,\n                    -0.1981, -0.1303, -0.1207, -0.1529, -0.2802, -0.1598, -0.1449, -0.1716,\n                    -0.1430, -0.1344, -0.1420, -0.1388, -0.1614, -0.1481, -0.1395, -0.1448,\n                    -0.1380, -0.1170, -0.2585, -0.1737, -0.1357, -0.1398, -0.1847, -0.1335,\n                    -0.1583, -0.2066, -0.1853, -0.1224, -0.1539, -0.1556, -0.1342, -0.1580,\n                    -0.1219, -0.1390, -0.1404, -0.2536, -0.1538, -0.1736, -0.1559, -0.1865,\n                    -0.2122, -0.1749, -0.1592, -0.2161, -0.1400, -0.1510, -0.4003, -0.2114,\n                    -0.1959, -0.1426, -0.1529, -0.1359, -0.1453, -0.1267, -0.1437, -0.1414,\n                    -0.1326, -0.1294, -0.1359, -0.1346, -0.2357, -0.1248, -0.1385, -0.1450,\n                    -0.1451, -0.1292, -0.1902, -0.1404, -0.1565, -0.1361, -0.1325, -0.1642,\n                    -0.1171, -0.7283, -0.1215, -0.1365, -0.1328, -0.1474, -0.1402, -0.1556,\n                    -0.1481, -0.1524, -0.1383, -0.1250, -0.3380, -0.1068, -0.1131, -0.3892,\n                    -0.2153, -0.1393, -0.1494, -0.1360, -0.2454, -0.2049, -0.2004, -0.1620,\n                    -0.1650, -0.1569, -0.1236, -0.1111, -0.1572, -0.1268, -0.1371, -0.1401,\n                    -0.1247, -0.1195, -0.1601, -0.1377, -0.1145, -0.1945, -0.1092, -0.2804,\n                    -0.1939, -0.1571, -0.1405, -0.1930, -0.1328, -0.1464, -0.2197, -0.1754,\n                    -0.2166, -0.1468, -0.1394, -0.1502, -0.1113, -0.1266, -0.1247, -0.1326,\n                    -0.1288, -0.1745, -0.1231, -0.1611, -0.1734, -0.1443, -0.1646, -0.1462,\n                    -0.1354, -0.1644, -0.1910, -0.1132, -0.1555, -0.1482, -0.1165, -0.1851,\n                    -0.1368, -0.1925, -0.1377, -0.1587, -0.2089, -0.1315, -0.1623, -0.1763,\n                    -0.1322, -0.1680, -0.1538, -0.1079, -0.1507, -0.1165, -0.1431, -0.1395,\n                    -0.1647, -0.1576, -0.1744, -0.1377, -0.1282, -0.1676, -0.1633, -0.1394,\n                    -0.1424, -0.1516, -0.1843, -0.1834, -0.1860, -0.1207, -0.1977, -0.1860,\n                    -0.1470, -0.1306, -0.1547, -0.1651, -0.1863, -0.1150, -0.1424, -0.1269,\n                    -0.1345, -0.1799, -0.1504, -0.1632, -0.2508, -0.2462, -0.1328, -0.1741,\n                    -0.1437, -0.1576, -0.1123, -0.1460, -0.1585, -0.1728, -0.1384, -0.1309,\n                    -0.1371, -0.1446, -0.1595, -0.1255, -0.9505, -0.1338, -0.1509, -0.1418,\n                    -0.1350, -0.1543, -0.1273, -0.9004, -0.1313, -0.1955, -0.1361, -0.1372,\n                    -0.1325, -0.1133, -0.1769, -0.1394, -0.1346, -0.1778, -0.1790, -0.1119,\n                    -0.1663, -0.1454, -0.1861, -0.1700, -0.1112, -0.1579, -0.2097, -0.1428,\n                    -0.1782, -0.1671, -0.1867, -0.1603, -0.1939, -0.1955, -0.1963, -0.1414,\n                    -0.1348, -0.1233, -0.1606, -0.1884, -0.1528, -0.1735, -0.2100, -0.1703,\n                    -0.2027, -0.1587, -0.1509, -0.1699, -0.1264, -0.4775, -0.1214, -0.1806,\n                    -0.1544, -0.1222, -0.1260, -0.1420, -0.1442, -0.1707, -0.1684, -0.1369,\n                    -0.1086, -0.1359, -0.1520, -0.1378, -0.1489, -0.1224, -0.2387, -0.1448]), max_val=tensor([0.1524, 0.1727, 0.1413, 0.1329, 0.1269, 0.1298, 0.1338, 0.1669, 0.1481,\n                    0.2504, 0.2605, 0.1780, 0.2476, 0.1892, 0.4755, 0.1386, 0.1338, 0.1905,\n                    0.1327, 0.1556, 0.1857, 0.1548, 0.1375, 0.1363, 0.2093, 0.1715, 0.1336,\n                    0.1354, 0.1359, 0.1306, 0.1461, 0.1189, 0.1950, 0.1336, 0.1820, 0.1477,\n                    0.1685, 0.1254, 0.1423, 0.1437, 0.1540, 0.1746, 0.1413, 0.1714, 0.1528,\n                    0.1831, 0.1331, 0.2541, 0.1592, 0.1293, 0.1632, 0.1912, 0.3215, 0.1511,\n                    0.1531, 0.1513, 0.1244, 0.1359, 0.1731, 0.1784, 0.1787, 0.1398, 0.1572,\n                    0.1554, 0.0937, 0.1563, 0.1230, 0.1865, 0.1612, 0.1246, 0.1360, 0.1363,\n                    0.1693, 0.1487, 0.1399, 0.1632, 0.1498, 0.1262, 0.1916, 0.1675, 0.1905,\n                    0.1125, 0.1419, 0.1457, 0.1226, 0.1706, 0.1298, 0.1350, 0.1526, 0.1249,\n                    0.1327, 0.1361, 0.1633, 0.1629, 0.1627, 0.1707, 0.2009, 0.1967, 0.1537,\n                    0.1418, 0.1496, 0.1386, 0.1206, 0.1459, 0.1726, 0.1442, 0.1600, 0.1826,\n                    0.1388, 0.1328, 0.1256, 0.1722, 0.1946, 0.1631, 0.1500, 0.1894, 0.1541,\n                    0.1898, 0.1480, 0.2607, 0.1327, 0.0985, 0.1347, 0.2787, 0.1540, 0.1676,\n                    0.1185, 0.1692, 0.1400, 0.1727, 0.2782, 0.2228, 0.1345, 0.2063, 0.1766,\n                    0.1280, 0.1739, 0.2009, 0.1410, 0.1582, 0.1147, 0.1421, 0.1873, 0.1655,\n                    0.1728, 0.1958, 0.1835, 0.1394, 0.1646, 0.1605, 0.1470, 0.1233, 0.1508,\n                    0.1529, 0.1489, 0.1407, 0.1559, 0.1524, 0.1622, 0.2022, 0.1555, 0.1507,\n                    0.2096, 0.1722, 0.1753, 0.2260, 0.2212, 0.1531, 0.1355, 0.1396, 0.2691,\n                    0.1980, 0.1686, 0.1593, 0.1646, 0.1180, 0.1875, 0.1921, 0.1589, 0.2166,\n                    0.1900, 0.1572, 0.1422, 0.1228, 0.1337, 0.1646, 0.1952, 0.1888, 0.1215,\n                    0.1552, 0.1441, 0.2239, 0.1426, 0.1136, 0.1436, 0.1993, 0.1250, 0.1584,\n                    0.1596, 0.1438, 0.1277, 0.1322, 0.1829, 0.1625, 0.3499, 0.1362, 0.1354,\n                    0.1853, 0.1257, 0.1904, 0.1328, 0.1374, 0.1522, 0.3742, 0.1127, 0.3150,\n                    0.1426, 0.1779, 0.1723, 0.1789, 0.1385, 0.1379, 0.1564, 0.1613, 0.1553,\n                    0.2304, 0.1501, 0.1730, 0.1531, 0.1616, 0.3454, 0.1883, 0.1697, 0.1434,\n                    0.1416, 0.1576, 0.4038, 0.1624, 0.1152, 0.1950, 0.1302, 0.2163, 0.1375,\n                    0.1815, 0.1853, 0.1363, 0.1585, 0.1706, 0.1635, 0.3601, 0.1979, 0.1391,\n                    0.1626, 0.1516, 0.1097, 0.1703, 0.1229, 0.1516, 0.1440, 0.1505, 0.1558,\n                    0.1387, 0.1499, 0.1360, 0.8907, 0.1604, 0.3995, 0.1726, 0.1725, 0.1154,\n                    0.1318, 0.1301, 0.1257, 0.2139, 0.2172, 0.1132, 0.1632, 0.1836, 0.1929,\n                    0.1346, 0.1520, 0.1497, 0.2312, 0.1826, 0.1266, 0.2607, 0.1505, 0.1545,\n                    0.4527, 0.1668, 0.1410, 0.2449, 0.1602, 0.1546, 0.1632, 0.2002, 0.1364,\n                    0.1752, 0.2219, 0.2086, 0.1756, 0.1414, 0.1582, 0.1319, 0.1707, 0.1501,\n                    0.1490, 0.1848, 0.1863, 0.1593, 0.1799, 0.1780, 0.1380, 0.1658, 0.1380,\n                    0.1585, 0.1190, 0.1597, 0.1358, 0.1441, 0.1297, 0.1388, 0.1386, 0.2252,\n                    0.1492, 0.1418, 0.1559, 0.1453, 0.1245, 0.1741, 0.1523, 0.1301, 0.1167,\n                    0.1736, 0.1422, 0.1818, 0.1203, 0.1456, 0.1594, 0.2653, 0.1199, 0.2143,\n                    0.1519, 0.2296, 0.1545, 0.1294, 0.1722, 0.1877, 0.2031, 0.1344, 0.1385,\n                    0.1737, 0.1187, 0.1610, 0.1960, 0.2073, 0.1721, 0.1226, 0.1048, 0.1510,\n                    0.1895, 0.1463, 0.1570, 0.1446, 0.1966, 0.1716, 0.1370, 0.1452, 0.1731,\n                    0.1358, 0.1990, 0.1193, 0.1448, 0.1317, 0.1282, 0.1792, 0.1632, 0.1550,\n                    0.1470, 0.1509, 0.1078, 0.1391, 0.1504, 0.1422, 0.1476, 0.1181, 0.1152,\n                    0.2102, 0.1297, 0.1281, 0.1696, 0.1918, 0.1692, 0.1693, 0.1695, 0.3586,\n                    0.1342, 0.0964, 0.1484, 0.1249, 0.1534, 0.2208, 0.1865, 0.2624, 0.1475,\n                    0.1803, 0.1878, 0.1380, 0.1386, 0.1317, 0.1293, 0.1108, 0.1553, 0.1790,\n                    0.2522, 0.1437, 0.1727, 0.2531, 0.1582, 0.1499, 0.2724, 0.1513, 0.1363,\n                    0.1373, 0.1270, 0.1621, 0.1297, 0.1517, 0.1601, 0.1406, 0.1256, 0.1554,\n                    0.1765, 0.1458, 0.1184, 0.1786, 0.1170, 0.1829, 0.1316, 0.1637, 0.1603,\n                    0.1464, 0.2023, 0.1989, 0.1278, 0.1906, 0.2953, 0.1288, 0.1724, 0.3596,\n                    0.1482, 0.1992, 0.1548, 0.3111, 0.1554, 0.1177, 0.2076, 0.1966, 0.1469,\n                    0.2326, 0.1473, 0.2050, 0.2229, 0.1559, 0.1695, 0.1252, 0.1248, 0.1544,\n                    0.1406, 0.1743, 0.1803, 0.1470, 0.1240, 0.1230, 0.1301, 0.1977, 0.1521,\n                    0.2590, 0.1338, 0.1281, 0.1573, 0.1267, 0.1629, 0.1470, 0.1110, 0.1462,\n                    0.1929, 0.1489, 0.1345, 0.1285, 0.1230, 0.1746, 0.1270, 0.1521, 0.1437,\n                    0.1611, 0.1737, 0.2576, 0.1544, 0.1224, 0.3038, 0.1441, 0.1414, 0.1378,\n                    0.1331, 0.2117, 0.1441, 0.1453, 0.2002, 0.1170, 0.1725, 0.1994, 0.1247,\n                    0.1450, 0.2777, 0.1967, 0.1350, 0.1793, 0.1448, 0.1795, 0.1520, 0.2577,\n                    0.2314, 0.1456, 0.1590, 0.1641, 0.1708, 0.1735, 0.1525, 0.1505, 0.1582,\n                    0.2751, 0.1863, 0.1665, 0.1522, 0.3173, 0.2134, 0.1746, 0.1295, 0.1377,\n                    0.1215, 0.1677, 0.2634, 0.1495, 0.1394, 0.1467, 0.2131, 0.1389, 0.1412,\n                    0.1473, 0.1584, 0.2396, 0.1557, 0.1165, 0.1478, 0.1353, 0.1535, 0.1600,\n                    0.1613, 0.1703, 0.1141, 0.1424, 0.1381, 0.1310, 0.1260, 0.1440, 0.1492,\n                    0.1627, 0.1093, 1.1665, 0.1471, 0.1398, 0.1522, 0.1795, 0.1585, 0.1732,\n                    0.1750, 0.1558, 0.1396, 0.1705, 0.1450, 0.1125, 0.1379, 0.2106, 0.2463,\n                    0.1869, 0.1947, 0.1633, 0.1552, 0.2022, 0.1360, 0.1480, 0.1676, 0.1592,\n                    0.1170, 0.1121, 0.1386, 0.1276, 0.1430, 0.1951, 0.1447, 0.1474, 0.1137,\n                    0.1438, 0.1137, 0.1753, 0.1289, 0.3010, 0.1917, 0.1990, 0.1352, 0.1997,\n                    0.1624, 0.1564, 0.3537, 0.2087, 0.1593, 0.1704, 0.1095, 0.1354, 0.1207,\n                    0.1262, 0.1532, 0.1609, 0.1274, 0.1539, 0.1362, 0.1191, 0.1668, 0.1799,\n                    0.3432, 0.1524, 0.1458, 0.1767, 0.1773, 0.1188, 0.1615, 0.1320, 0.1144,\n                    0.1519, 0.1206, 0.2884, 0.1375, 0.1566, 0.1662, 0.1239, 0.1363, 0.1503,\n                    0.1752, 0.1637, 0.1463, 0.1500, 0.1504, 0.2810, 0.1692, 0.1562, 0.2691,\n                    0.1331, 0.1612, 0.2089, 0.1576, 0.1540, 0.2276, 0.2036, 0.1433, 0.1357,\n                    0.1884, 0.1365, 0.1450, 0.1708, 0.1546, 0.1248, 0.1650, 0.1154, 0.2557,\n                    0.1589, 0.2299, 0.2170, 0.1802, 0.1449, 0.1754, 0.1791, 0.1177, 0.1569,\n                    0.1331, 0.1719, 0.2083, 0.1542, 0.1369, 0.1381, 0.1298, 0.1844, 0.1594,\n                    0.1565, 0.1251, 0.1547, 0.1457, 0.2446, 0.1670, 0.1404, 0.4713, 0.1432,\n                    0.1845, 0.1397, 0.1384, 0.1555, 0.1073, 0.9499, 0.1223, 0.1412, 0.1292,\n                    0.1586, 0.1091, 0.1769, 0.3028, 0.1396, 0.1221, 0.1259, 0.1300, 0.1229,\n                    0.1360, 0.2068, 0.1319, 0.1528, 0.1333, 0.1280, 0.1455, 0.1428, 0.1813,\n                    0.1234, 0.1397, 0.1448, 0.1665, 0.1205, 0.1740, 0.1496, 0.1607, 0.1297,\n                    0.1824, 0.1334, 0.1523, 0.1883, 0.4455, 0.2023, 0.4286, 0.1336, 0.1641,\n                    0.1375, 0.1146, 0.1471, 0.1148, 0.1803, 0.1634, 0.1663, 0.1703, 0.1383,\n                    0.1210, 0.1545, 0.1705, 0.1169, 0.1226, 0.1426, 0.1389, 0.1586, 0.1847,\n                    0.1703, 0.1554, 0.1604])\n          )\n        )\n      )\n    )\n    (7): Module(\n      (0): Module(\n        (block): Module(\n          (0): Conv2d(\n            768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0019, 0.0022, 0.0008, 0.0018, 0.0016, 0.0013, 0.0016, 0.0021, 0.0011,\n                      0.0009, 0.0019, 0.0018, 0.0019, 0.0013, 0.0010, 0.0007, 0.0006, 0.0018,\n                      0.0021, 0.0012, 0.0018, 0.0012, 0.0017, 0.0011, 0.0009, 0.0015, 0.0017,\n                      0.0013, 0.0015, 0.0014, 0.0019, 0.0013, 0.0010, 0.0020, 0.0013, 0.0012,\n                      0.0014, 0.0013, 0.0017, 0.0014, 0.0016, 0.0018, 0.0018, 0.0013, 0.0017,\n                      0.0013, 0.0014, 0.0014, 0.0016, 0.0021, 0.0011, 0.0018, 0.0009, 0.0016,\n                      0.0008, 0.0018, 0.0015, 0.0014, 0.0016, 0.0015, 0.0019, 0.0016, 0.0017,\n                      0.0019, 0.0015, 0.0018, 0.0008, 0.0016, 0.0015, 0.0012, 0.0018, 0.0011,\n                      0.0018, 0.0010, 0.0018, 0.0020, 0.0014, 0.0012, 0.0018, 0.0012, 0.0013,\n                      0.0014, 0.0013, 0.0017, 0.0013, 0.0014, 0.0013, 0.0016, 0.0013, 0.0014,\n                      0.0014, 0.0013, 0.0014, 0.0018, 0.0018, 0.0019, 0.0016, 0.0018, 0.0015,\n                      0.0021, 0.0014, 0.0013, 0.0012, 0.0017, 0.0017, 0.0008, 0.0017, 0.0018,\n                      0.0017, 0.0014, 0.0012, 0.0017, 0.0018, 0.0019, 0.0018, 0.0005, 0.0017,\n                      0.0019, 0.0018, 0.0016, 0.0014, 0.0014, 0.0016, 0.0024, 0.0017, 0.0018,\n                      0.0010, 0.0019, 0.0014, 0.0011, 0.0027, 0.0018, 0.0014, 0.0013, 0.0018,\n                      0.0018, 0.0011, 0.0015, 0.0012, 0.0008, 0.0014, 0.0021, 0.0021, 0.0018,\n                      0.0010, 0.0005, 0.0019, 0.0016, 0.0020, 0.0019, 0.0012, 0.0016, 0.0015,\n                      0.0018, 0.0014, 0.0010, 0.0018, 0.0011, 0.0018, 0.0015, 0.0019, 0.0017,\n                      0.0019, 0.0018, 0.0018, 0.0016, 0.0018, 0.0012, 0.0014, 0.0017, 0.0006,\n                      0.0010, 0.0019, 0.0021, 0.0018, 0.0008, 0.0019, 0.0018, 0.0010, 0.0016,\n                      0.0013, 0.0017, 0.0016, 0.0015, 0.0016, 0.0018, 0.0014, 0.0014, 0.0014,\n                      0.0014, 0.0018, 0.0019, 0.0017, 0.0015, 0.0012, 0.0008, 0.0014, 0.0014,\n                      0.0016, 0.0019, 0.0013, 0.0008, 0.0018, 0.0015, 0.0028, 0.0016, 0.0013,\n                      0.0007, 0.0009, 0.0009, 0.0012, 0.0015, 0.0010, 0.0011, 0.0014, 0.0018,\n                      0.0016, 0.0014, 0.0015, 0.0018, 0.0018, 0.0018, 0.0019, 0.0015, 0.0014,\n                      0.0017, 0.0012, 0.0008, 0.0016, 0.0015, 0.0014, 0.0019, 0.0010, 0.0013,\n                      0.0016, 0.0017, 0.0004, 0.0018, 0.0013, 0.0016, 0.0012, 0.0019, 0.0017,\n                      0.0019, 0.0013, 0.0018, 0.0019, 0.0018, 0.0015, 0.0026, 0.0017, 0.0007,\n                      0.0018, 0.0016, 0.0015, 0.0017, 0.0016, 0.0020, 0.0017, 0.0017, 0.0015,\n                      0.0017, 0.0018, 0.0013, 0.0014, 0.0009, 0.0037, 0.0013, 0.0015, 0.0010,\n                      0.0015, 0.0019, 0.0017, 0.0026, 0.0018, 0.0016, 0.0017, 0.0019, 0.0019,\n                      0.0018, 0.0019, 0.0013, 0.0013, 0.0018, 0.0015, 0.0018, 0.0012, 0.0018,\n                      0.0011, 0.0018, 0.0014, 0.0008, 0.0016, 0.0013, 0.0019, 0.0012, 0.0014,\n                      0.0017, 0.0025, 0.0021, 0.0019, 0.0014, 0.0010, 0.0019, 0.0019, 0.0012,\n                      0.0019, 0.0010, 0.0017, 0.0005, 0.0013, 0.0008, 0.0019, 0.0020, 0.0009,\n                      0.0016, 0.0013, 0.0011, 0.0013, 0.0019, 0.0007, 0.0012, 0.0016, 0.0024,\n                      0.0017, 0.0016, 0.0015, 0.0017, 0.0014, 0.0018, 0.0025, 0.0018, 0.0015,\n                      0.0016, 0.0017, 0.0011, 0.0011, 0.0011, 0.0020, 0.0018, 0.0009, 0.0017,\n                      0.0018, 0.0013, 0.0017, 0.0014, 0.0021, 0.0018, 0.0018, 0.0018, 0.0017,\n                      0.0013, 0.0016, 0.0015, 0.0018, 0.0011, 0.0017, 0.0015, 0.0008, 0.0018,\n                      0.0024, 0.0017, 0.0017, 0.0013, 0.0015, 0.0014, 0.0007, 0.0016, 0.0022,\n                      0.0015, 0.0017, 0.0013, 0.0012, 0.0013, 0.0018, 0.0020, 0.0012, 0.0015,\n                      0.0018, 0.0018, 0.0008, 0.0007, 0.0012, 0.0018, 0.0013, 0.0016, 0.0013,\n                      0.0007, 0.0015, 0.0012, 0.0019, 0.0018, 0.0018, 0.0010, 0.0014, 0.0019,\n                      0.0016, 0.0013, 0.0013, 0.0011, 0.0012, 0.0005, 0.0017, 0.0007, 0.0018,\n                      0.0017, 0.0018, 0.0013, 0.0013, 0.0016, 0.0012, 0.0011, 0.0019, 0.0018,\n                      0.0016, 0.0018, 0.0016, 0.0016, 0.0022, 0.0019, 0.0012, 0.0018, 0.0015,\n                      0.0011, 0.0011, 0.0018, 0.0013, 0.0017, 0.0008, 0.0011, 0.0016, 0.0018,\n                      0.0008, 0.0016, 0.0015, 0.0018, 0.0013, 0.0016, 0.0013, 0.0017, 0.0013,\n                      0.0016, 0.0005, 0.0019, 0.0016, 0.0019, 0.0005, 0.0010, 0.0014, 0.0004,\n                      0.0018, 0.0020, 0.0018, 0.0025, 0.0017, 0.0013, 0.0009, 0.0013, 0.0016,\n                      0.0013, 0.0017, 0.0018, 0.0021, 0.0015, 0.0018, 0.0012, 0.0016, 0.0014,\n                      0.0013, 0.0014, 0.0013, 0.0017, 0.0018, 0.0017, 0.0013, 0.0013, 0.0017,\n                      0.0019, 0.0012, 0.0011, 0.0013, 0.0013, 0.0018, 0.0018, 0.0014, 0.0010,\n                      0.0011, 0.0011, 0.0013, 0.0012, 0.0009, 0.0017, 0.0015, 0.0018, 0.0016,\n                      0.0007, 0.0020, 0.0012, 0.0012, 0.0014, 0.0019, 0.0019, 0.0016, 0.0021,\n                      0.0014, 0.0014, 0.0016, 0.0012, 0.0013, 0.0013, 0.0013, 0.0019, 0.0008,\n                      0.0015, 0.0024, 0.0016, 0.0013, 0.0018, 0.0018, 0.0019, 0.0017, 0.0004,\n                      0.0019, 0.0012, 0.0019, 0.0017, 0.0021, 0.0014, 0.0012, 0.0017, 0.0020,\n                      0.0014, 0.0023, 0.0017, 0.0018, 0.0017, 0.0008, 0.0022, 0.0018, 0.0017,\n                      0.0019, 0.0016, 0.0007, 0.0023, 0.0014, 0.0018, 0.0017, 0.0019, 0.0013,\n                      0.0013, 0.0016, 0.0020, 0.0017, 0.0014, 0.0015, 0.0008, 0.0016, 0.0011,\n                      0.0013, 0.0017, 0.0011, 0.0015, 0.0017, 0.0014, 0.0014, 0.0015, 0.0008,\n                      0.0018, 0.0013, 0.0013, 0.0013, 0.0016, 0.0019, 0.0012, 0.0015, 0.0017,\n                      0.0015, 0.0014, 0.0019, 0.0017, 0.0012, 0.0015, 0.0015, 0.0010, 0.0007,\n                      0.0015, 0.0019, 0.0019, 0.0020, 0.0022, 0.0018, 0.0017, 0.0015, 0.0009,\n                      0.0015, 0.0007, 0.0017, 0.0015, 0.0015, 0.0019, 0.0019, 0.0015, 0.0017,\n                      0.0014, 0.0015, 0.0007, 0.0015, 0.0018, 0.0017, 0.0018, 0.0017, 0.0009,\n                      0.0014, 0.0013, 0.0010, 0.0020, 0.0018, 0.0020, 0.0015, 0.0018, 0.0011,\n                      0.0013, 0.0011, 0.0016, 0.0017, 0.0019, 0.0015, 0.0014, 0.0018, 0.0018,\n                      0.0006, 0.0018, 0.0018, 0.0014, 0.0018, 0.0012, 0.0016, 0.0015, 0.0012,\n                      0.0017, 0.0014, 0.0018, 0.0014, 0.0012, 0.0025, 0.0014, 0.0013, 0.0018,\n                      0.0012, 0.0018, 0.0015, 0.0005, 0.0017, 0.0006, 0.0013, 0.0017, 0.0013,\n                      0.0013, 0.0015, 0.0011, 0.0012, 0.0018, 0.0007, 0.0018, 0.0011, 0.0015,\n                      0.0018, 0.0018, 0.0014, 0.0011, 0.0020, 0.0011, 0.0020, 0.0011, 0.0018,\n                      0.0018, 0.0021, 0.0014, 0.0012, 0.0016, 0.0016, 0.0023, 0.0012, 0.0016,\n                      0.0014, 0.0016, 0.0013, 0.0017, 0.0018, 0.0019, 0.0013, 0.0013, 0.0018,\n                      0.0021, 0.0015, 0.0018, 0.0011, 0.0021, 0.0015, 0.0013, 0.0015, 0.0013,\n                      0.0012, 0.0013, 0.0013, 0.0016, 0.0016, 0.0010, 0.0016, 0.0016, 0.0015,\n                      0.0012, 0.0013, 0.0012, 0.0023, 0.0020, 0.0015, 0.0013, 0.0019, 0.0015,\n                      0.0015, 0.0018, 0.0020, 0.0019, 0.0011, 0.0016, 0.0018, 0.0018, 0.0018,\n                      0.0014, 0.0016, 0.0015, 0.0017, 0.0004, 0.0016, 0.0014, 0.0013, 0.0011,\n                      0.0017, 0.0019, 0.0014, 0.0018, 0.0004, 0.0018, 0.0012, 0.0013, 0.0017,\n                      0.0018, 0.0015, 0.0006, 0.0012, 0.0007, 0.0018, 0.0016, 0.0018, 0.0012,\n                      0.0016, 0.0017, 0.0017, 0.0014, 0.0014, 0.0010, 0.0017, 0.0014, 0.0018,\n                      0.0008, 0.0018, 0.0020]), zero_point=tensor([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0, 127,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n                     dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n                min_val=tensor([-0.0210, -0.2754, -0.0800, -0.0178, -0.2050, -0.1645, -0.2057, -0.2729,\n                        -0.1274, -0.1211, -0.0325, -0.2303, -0.0181, -0.1641, -0.1140, -0.0666,\n                        -0.0726, -0.2268, -0.2752, -0.1444, -0.2248, -0.1567, -0.0182, -0.1394,\n                        -0.1089, -0.0729, -0.0388, -0.1608, -0.0715, -0.1820, -0.2452, -0.0822,\n                        -0.1242, -0.0136, -0.1618, -0.1600, -0.1345, -0.1615, -0.0115, -0.1757,\n                        -0.2077, -0.2254, -0.2241, -0.1636, -0.2229, -0.1700, -0.1747, -0.1527,\n                        -0.0119, -0.0167, -0.0416, -0.2266, -0.0348, -0.2032, -0.0488, -0.0207,\n                        -0.0543, -0.1813, -0.0051, -0.0170, -0.0200, -0.2066, -0.0182, -0.2383,\n                        -0.1126, -0.2272, -0.0972, -0.0497, -0.0066, -0.1567, -0.0338, -0.1460,\n                        -0.2324, -0.0885, -0.2336, -0.0128, -0.0685, -0.1485, -0.0198, -0.1491,\n                        -0.1468, -0.1761, -0.1702, -0.0443, -0.1480, -0.0153, -0.1722, -0.2018,\n                        -0.1569, -0.1801, -0.0778, -0.1646, -0.0547, -0.2302, -0.0151, -0.2490,\n                        -0.2083, -0.2273, -0.1899, -0.0318, -0.1732, -0.1670, -0.1386, -0.0333,\n                        -0.0173, -0.0996, -0.0585, -0.0161, -0.2181, -0.1703, -0.1240, -0.2124,\n                        -0.2313, -0.0225, -0.0235, -0.0579, -0.2169, -0.0194, -0.0134, -0.1365,\n                        -0.1785, -0.1735, -0.2069, -0.3030, -0.0545, -0.0154, -0.0422, -0.0192,\n                        -0.1792, -0.0460, -0.3404, -0.0250, -0.0703, -0.1651, -0.2307, -0.0201,\n                        -0.1467, -0.1926, -0.1508, -0.0835, -0.0988, -0.0137, -0.0136, -0.0722,\n                        -0.1311, -0.0579, -0.2441, -0.0168, -0.2532, -0.2485, -0.1349, -0.0408,\n                        -0.0122, -0.0109, -0.0234, -0.0684, -0.0196, -0.1457, -0.2353, -0.1859,\n                        -0.2411, -0.0178, -0.0168, -0.0148, -0.2323, -0.0401, -0.2251, -0.0585,\n                        -0.1749, -0.2133, -0.0757, -0.0432, -0.0126, -0.0181, -0.2347, -0.0836,\n                        -0.0143, -0.0212, -0.1245, -0.0467, -0.1385, -0.0167, -0.0529, -0.1862,\n                        -0.2055, -0.0195, -0.1432, -0.1835, -0.1790, -0.1850, -0.0379, -0.2483,\n                        -0.2227, -0.1295, -0.0203, -0.1064, -0.1712, -0.1811, -0.0193, -0.0155,\n                        -0.1411, -0.0972, -0.0062, -0.0571, -0.3572, -0.0526, -0.1609, -0.0852,\n                        -0.1127, -0.1139, -0.0795, -0.1951, -0.1055, -0.1432, -0.0483, -0.0395,\n                        -0.2064, -0.1749, -0.0464, -0.0198, -0.0582, -0.2251, -0.2396, -0.0460,\n                        -0.0646, -0.2194, -0.1589, -0.0770, -0.0028, -0.1983, -0.1750, -0.2427,\n                        -0.1305, -0.1667, -0.2024, -0.2221, -0.0489, -0.2297, -0.1719, -0.0462,\n                        -0.1432, -0.2404, -0.2130, -0.2467, -0.1672, -0.0223, -0.0152, -0.2281,\n                        -0.1938, -0.3344, -0.2223, -0.0456, -0.2348, -0.2018, -0.1454, -0.2125,\n                        -0.1985, -0.2497, -0.0162, -0.2146, -0.0589, -0.0103, -0.0174, -0.1641,\n                        -0.1780, -0.1144, -0.0089, -0.1711, -0.1929, -0.1287, -0.0550, -0.0179,\n                        -0.0109, -0.3265, -0.0276, -0.1280, -0.0189, -0.2408, -0.2427, -0.0216,\n                        -0.2475, -0.1291, -0.0556, -0.0203, -0.1888, -0.0317, -0.1515, -0.2355,\n                        -0.0523, -0.0136, -0.1813, -0.0799, -0.2022, -0.0120, -0.0141, -0.1562,\n                        -0.1741, -0.0529, -0.0177, -0.0134, -0.0184, -0.0877, -0.0074, -0.0175,\n                        -0.2394, -0.1475, -0.2423, -0.0290, -0.2164, -0.0669, -0.1719, -0.0876,\n                        -0.2462, -0.2500, -0.1190, -0.2105, -0.1663, -0.0871, -0.1057, -0.0151,\n                        -0.0934, -0.1252, -0.0459, -0.3100, -0.0221, -0.0671, -0.1866, -0.0173,\n                        -0.1733, -0.2308, -0.3253, -0.2320, -0.1884, -0.0680, -0.2226, -0.0977,\n                        -0.1447, -0.0657, -0.2572, -0.4713, -0.1151, -0.2174, -0.0230, -0.1327,\n                        -0.2214, -0.1738, -0.2645, -0.0169, -0.2283, -0.0186, -0.0144, -0.1704,\n                        -0.0650, -0.1961, -0.2262, -0.1350, -0.2190, -0.0435, -0.1025, -0.2319,\n                        -0.0064, -0.0388, -0.0148, -0.0528, -0.1905, -0.0247, -0.0949, -0.2026,\n                        -0.0191, -0.0585, -0.2222, -0.1470, -0.0499, -0.1658, -0.0195, -0.2594,\n                        -0.1521, -0.1898, -0.2331, -0.0147, -0.1030, -0.0935, -0.1534, -0.0310,\n                        -0.1621, -0.0471, -0.1623, -0.0859, -0.1937, -0.1511, -0.2391, -0.0726,\n                        -0.2250, -0.1221, -0.1772, -0.2411, -0.2043, -0.1634, -0.1615, -0.1336,\n                        -0.1448, -0.0633, -0.2236, -0.0388, -0.2256, -0.2224, -0.2328, -0.1714,\n                        -0.1472, -0.2011, -0.1259, -0.1464, -0.2384, -0.0192, -0.2052, -0.2279,\n                        -0.2084, -0.2066, -0.0126, -0.2418, -0.1565, -0.0167, -0.0137, -0.0136,\n                        -0.0973, -0.2312, -0.1681, -0.0179, -0.1070, -0.1415, -0.1995, -0.0142,\n                        -0.0709, -0.1988, -0.0613, -0.2274, -0.1653, -0.2027, -0.1719, -0.2158,\n                        -0.0020, -0.0213, -0.0594, -0.2425, -0.2099, -0.0198, -0.0573, -0.1246,\n                        -0.0420, -0.0445, -0.0095, -0.0364, -0.0200, -0.0167, -0.2206, -0.0759,\n                        -0.1143, -0.1713, -0.2103, -0.1549, -0.0244, -0.0239, -0.2713, -0.0551,\n                        -0.0102, -0.1583, -0.2040, -0.0463, -0.1635, -0.0751, -0.1703, -0.2181,\n                        -0.2242, -0.2154, -0.1567, -0.1285, -0.2119, -0.2406, -0.1552, -0.1322,\n                        -0.1719, -0.1692, -0.0181, -0.2267, -0.1655, -0.0150, -0.1418, -0.1086,\n                        -0.1712, -0.1479, -0.1208, -0.2189, -0.0436, -0.2303, -0.2039, -0.0666,\n                        -0.2548, -0.1506, -0.1598, -0.1734, -0.2476, -0.0203, -0.0564, -0.0164,\n                        -0.0169, -0.1793, -0.0503, -0.1487, -0.1607, -0.1679, -0.1476, -0.2448,\n                        -0.0991, -0.1882, -0.3033, -0.2098, -0.0429, -0.0163, -0.2276, -0.0149,\n                        -0.2206, -0.0442, -0.2379, -0.0132, -0.0170, -0.0330, -0.0149, -0.1478,\n                        -0.1522, -0.2144, -0.0140, -0.1656, -0.0178, -0.2215, -0.0203, -0.2118,\n                        -0.1009, -0.0220, -0.0150, -0.2200, -0.2487, -0.2076, -0.0834, -0.2920,\n                        -0.1740, -0.0159, -0.2213, -0.0149, -0.1646, -0.0126, -0.0625, -0.0193,\n                        -0.0136, -0.0051, -0.1867, -0.0965, -0.2096, -0.1330, -0.1601, -0.2231,\n                        -0.1304, -0.1953, -0.2184, -0.1744, -0.0403, -0.1974, -0.0981, -0.0122,\n                        -0.1643, -0.1438, -0.0518, -0.1984, -0.0139, -0.1580, -0.1920, -0.0183,\n                        -0.1906, -0.1630, -0.0148, -0.2203, -0.1518, -0.0593, -0.0548, -0.1217,\n                        -0.0663, -0.0097, -0.0215, -0.0220, -0.2619, -0.0195, -0.0173, -0.0164,\n                        -0.0408, -0.0578, -0.0637, -0.0705, -0.2151, -0.1925, -0.0174, -0.0129,\n                        -0.2403, -0.0629, -0.2169, -0.0456, -0.1866, -0.0904, -0.1390, -0.0422,\n                        -0.0200, -0.0118, -0.0173, -0.1151, -0.1797, -0.1524, -0.0356, -0.2555,\n                        -0.0178, -0.2556, -0.0211, -0.0204, -0.0647, -0.1333, -0.1378, -0.2075,\n                        -0.2175, -0.0143, -0.1461, -0.1447, -0.2272, -0.0221, -0.0820, -0.2353,\n                        -0.2297, -0.1832, -0.2276, -0.1591, -0.2070, -0.0548, -0.1531, -0.2208,\n                        -0.1833, -0.0422, -0.0744, -0.1485, -0.0134, -0.1780, -0.1706, -0.0150,\n                        -0.0822, -0.2261, -0.0659, -0.0565, -0.2160, -0.0761, -0.1678, -0.2206,\n                        -0.0521, -0.1109, -0.1938, -0.1464, -0.1534, -0.2366, -0.0893, -0.0191,\n                        -0.1395, -0.0510, -0.2250, -0.2359, -0.1301, -0.1462, -0.0149, -0.1137,\n                        -0.0213, -0.1323, -0.2275, -0.0120, -0.2669, -0.1808, -0.1140, -0.0511,\n                        -0.2035, -0.0137, -0.1445, -0.0393, -0.1827, -0.2015, -0.0482, -0.0170,\n                        -0.2293, -0.0148, -0.1647, -0.1678, -0.2249, -0.0119, -0.1414, -0.0416,\n                        -0.1412, -0.2665, -0.1974, -0.1563, -0.1839, -0.1374, -0.1590, -0.1707,\n                        -0.1546, -0.2066, -0.0689, -0.1216, -0.0668, -0.0196, -0.0531, -0.1474,\n                        -0.1679, -0.0085, -0.0103, -0.2569, -0.1921, -0.0509, -0.2479, -0.1873,\n                        -0.1931, -0.0207, -0.0176, -0.2452, -0.1445, -0.2022, -0.2275, -0.2255,\n                        -0.0247, -0.1846, -0.2066, -0.1425, -0.2221, -0.0559, -0.0359, -0.1531,\n                        -0.1609, -0.0796, -0.0263, -0.0187, -0.1754, -0.0154, -0.0569, -0.2283,\n                        -0.0770, -0.1687, -0.2150, -0.0161, -0.1931, -0.0694, -0.1507, -0.0926,\n                        -0.0241, -0.2060, -0.2250, -0.1540, -0.2072, -0.2160, -0.2189, -0.1739,\n                        -0.1508, -0.0823, -0.2129, -0.0170, -0.0202, -0.1040, -0.2354, -0.2570]), max_val=tensor([ 0.2417,  0.0202,  0.1055,  0.2299,  0.0587,  0.1563,  0.0412,  0.0094,\n                         0.1409,  0.1059,  0.2399,  0.0146,  0.2413,  0.1649,  0.1260,  0.0935,\n                         0.0522,  0.0130,  0.0150,  0.1508,  0.0336,  0.1298,  0.2110,  0.1303,\n                         0.0123,  0.1870,  0.2103,  0.1517,  0.1903,  0.0590,  0.0051,  0.1605,\n                         0.0681,  0.2531,  0.0822,  0.0611,  0.1734,  0.1367,  0.2190,  0.1554,\n                         0.0336,  0.0296,  0.0178,  0.1629,  0.0150,  0.0077,  0.1439,  0.1818,\n                         0.2016,  0.2665,  0.1435,  0.0178,  0.1120,  0.0111,  0.0977,  0.2284,\n                         0.1966,  0.1011,  0.2067,  0.1958,  0.2419,  0.0143,  0.2181,  0.0178,\n                         0.1952,  0.0187,  0.0942,  0.2095,  0.1951,  0.1482,  0.2247,  0.1012,\n                         0.0198,  0.1288,  0.0146,  0.2498,  0.1838,  0.1262,  0.2244,  0.1494,\n                         0.1628,  0.0633,  0.1285,  0.2125,  0.1631,  0.1796,  0.1561,  0.0235,\n                         0.1592,  0.0737,  0.1769,  0.1461,  0.1747,  0.0173,  0.2230,  0.0192,\n                         0.0697,  0.0116,  0.0489,  0.2660,  0.1341,  0.0615,  0.1505,  0.2131,\n                         0.2165,  0.0806,  0.2110,  0.2224,  0.0190,  0.1759,  0.1587,  0.0127,\n                         0.0101,  0.2355,  0.2269,  0.0685,  0.0218,  0.2407,  0.2257,  0.2079,\n                         0.0499,  0.1523,  0.0432,  0.0184,  0.2208,  0.2225,  0.1299,  0.2445,\n                         0.0489,  0.1386,  0.0211,  0.2309,  0.1752,  0.1332,  0.0156,  0.2239,\n                         0.0466,  0.0465,  0.0456,  0.1062,  0.1810,  0.2715,  0.2606,  0.2296,\n                         0.0838,  0.0538,  0.0113,  0.2062,  0.0150,  0.0156,  0.1516,  0.1970,\n                         0.1873,  0.2223,  0.1784,  0.1222,  0.2346,  0.1364,  0.0144,  0.1423,\n                         0.0194,  0.2172,  0.2386,  0.2281,  0.0134,  0.2018,  0.0394,  0.1538,\n                         0.1294,  0.0177,  0.0793,  0.1220,  0.2395,  0.2667,  0.0069,  0.0993,\n                         0.2403,  0.2340,  0.1286,  0.2082,  0.1626,  0.2120,  0.2002,  0.0865,\n                         0.0227,  0.2283,  0.1818,  0.0950,  0.0524,  0.0726,  0.2313,  0.0243,\n                         0.0405,  0.1856,  0.1556,  0.0948,  0.1745,  0.0619,  0.2092,  0.2386,\n                         0.1639,  0.0939,  0.2285,  0.1891,  0.0048,  0.1984,  0.1610,  0.0758,\n                         0.0750,  0.0965,  0.1579,  0.0158,  0.1253,  0.1221,  0.1827,  0.2274,\n                         0.0091,  0.0174,  0.1938,  0.2247,  0.2310,  0.0181,  0.0137,  0.1938,\n                         0.1754,  0.0351,  0.1492,  0.1051,  0.2095,  0.0174,  0.0585,  0.0149,\n                         0.1234,  0.0259,  0.0542,  0.0169,  0.0424,  0.0196,  0.1476,  0.1989,\n                         0.1586,  0.0206,  0.0192,  0.0187,  0.1681,  0.2349,  0.2425,  0.0174,\n                         0.0484,  0.0163,  0.0199,  0.0942,  0.0215,  0.0518,  0.1889,  0.0282,\n                         0.0522,  0.0187,  0.2130,  0.0349,  0.1942,  0.2143,  0.2228,  0.1486,\n                         0.1587,  0.0597,  0.4727,  0.1630,  0.0290,  0.0960,  0.1865,  0.2351,\n                         0.2119,  0.0118,  0.2235,  0.2080,  0.2142,  0.0151,  0.0255,  0.2227,\n                         0.0139,  0.1626,  0.1622,  0.2292,  0.0515,  0.2306,  0.1453,  0.0175,\n                         0.1415,  0.2332,  0.0561,  0.0957,  0.0536,  0.1653,  0.2428,  0.1395,\n                         0.1750,  0.2202,  0.3124,  0.2725,  0.2353,  0.1840,  0.1326,  0.2444,\n                         0.0188,  0.1517,  0.0142,  0.1315,  0.0576,  0.0569,  0.0108,  0.1014,\n                         0.0178,  0.0201,  0.1021,  0.0200,  0.1569,  0.1376,  0.1653,  0.2412,\n                         0.0950,  0.1467,  0.2008,  0.0136,  0.2174,  0.1989,  0.1476,  0.2154,\n                         0.1350,  0.0240,  0.0172,  0.0188,  0.0697,  0.2041,  0.0104,  0.1437,\n                         0.0787,  0.1378,  0.0053, -0.0045,  0.0720,  0.0377,  0.2302,  0.1684,\n                         0.0251,  0.0527,  0.0179,  0.2279,  0.0184,  0.2251,  0.2157,  0.0535,\n                         0.2075,  0.0292,  0.0172,  0.0124,  0.0483,  0.1935,  0.0890,  0.0173,\n                         0.3104,  0.2208,  0.2172,  0.1635,  0.0566,  0.1736,  0.0841,  0.0093,\n                         0.2818,  0.1921,  0.0376,  0.1702,  0.1498,  0.0757,  0.2315,  0.0222,\n                         0.0698,  0.0631,  0.0172,  0.2318,  0.0820,  0.0814,  0.1546,  0.2252,\n                         0.1604,  0.2000,  0.1520,  0.0484,  0.0510,  0.1517,  0.0208,  0.2247,\n                         0.0147,  0.0152,  0.1477,  0.0501,  0.0513,  0.1511,  0.0810,  0.1397,\n                         0.1488,  0.0524,  0.0165,  0.0911,  0.0143,  0.0147,  0.0161,  0.0941,\n                         0.1659,  0.0154,  0.1507,  0.1278,  0.0140,  0.2227,  0.0799,  0.0304,\n                         0.0498,  0.0668,  0.2760,  0.0196,  0.0380,  0.2307,  0.1934,  0.1395,\n                         0.1415,  0.0142,  0.1458,  0.2120,  0.0928,  0.1364,  0.0583,  0.2296,\n                         0.1035,  0.0629,  0.1915,  0.0213,  0.1569,  0.0584,  0.1367,  0.0219,\n                         0.1683,  0.2031,  0.0462,  0.0205,  0.0436,  0.2444,  0.0578,  0.1267,\n                         0.1752,  0.0445,  0.2237,  0.2514,  0.2241,  0.3205,  0.0154,  0.1680,\n                         0.1161,  0.1560,  0.0188,  0.1593,  0.2145,  0.2339,  0.0137,  0.1917,\n                         0.2338,  0.1533,  0.0484,  0.1836,  0.1614,  0.1784,  0.1585,  0.0127,\n                         0.0361,  0.0104,  0.1634,  0.1634,  0.0212,  0.0154,  0.1496,  0.1358,\n                         0.0120,  0.1551,  0.2289,  0.0195,  0.1772,  0.1269,  0.0129,  0.1422,\n                         0.1215,  0.1553,  0.0929,  0.0302,  0.1911,  0.0138,  0.0143,  0.0837,\n                         0.0353,  0.0713,  0.1494,  0.0524,  0.0274,  0.2422,  0.1980,  0.2615,\n                         0.1736,  0.1319,  0.2008,  0.1455,  0.1263,  0.1626,  0.1675,  0.0186,\n                         0.0982,  0.0657,  0.0135,  0.0369,  0.1705,  0.2335,  0.0121,  0.2405,\n                         0.0149,  0.0459,  0.0175,  0.1555,  0.2356,  0.2139,  0.2666,  0.1768,\n                         0.1428,  0.0344,  0.2496,  0.1793,  0.2916,  0.0102,  0.2293,  0.0468,\n                         0.0819,  0.2821,  0.2336,  0.0431,  0.0147,  0.0388,  0.0590,  0.0138,\n                         0.0533,  0.2229,  0.0089,  0.2389,  0.1622,  0.1687,  0.2015,  0.2518,\n                         0.2171,  0.1817,  0.0384,  0.0917,  0.0561,  0.1349,  0.0510,  0.0261,\n                         0.1450,  0.0138,  0.0212,  0.1618,  0.1735,  0.0504,  0.0852,  0.2254,\n                         0.1529,  0.1593,  0.1691,  0.0430,  0.2443,  0.1285,  0.0615,  0.2178,\n                         0.0735,  0.1798,  0.2387,  0.0189,  0.1518,  0.1853,  0.1943,  0.0112,\n                         0.0842,  0.1931,  0.2377,  0.2407,  0.0089,  0.2790,  0.2289,  0.2164,\n                         0.1962,  0.1196,  0.1883,  0.0864,  0.0164,  0.0395,  0.1965,  0.2427,\n                         0.0139,  0.1889,  0.0210,  0.1736,  0.0556,  0.0659,  0.1854,  0.2248,\n                         0.2205,  0.2342,  0.2170,  0.0066,  0.0700,  0.1613,  0.1321,  0.0173,\n                         0.2314,  0.0190,  0.1937,  0.2264,  0.1450,  0.1613,  0.0596,  0.0119,\n                         0.0284,  0.2396,  0.1947,  0.1811,  0.0188,  0.2323,  0.0547,  0.0150,\n                         0.0524,  0.0382,  0.0211,  0.0901,  0.0393,  0.1866,  0.1581,  0.0186,\n                         0.1329,  0.2247,  0.1767,  0.0377,  0.3114,  0.0622,  0.1344,  0.2300,\n                         0.1586,  0.0163,  0.1914,  0.0602,  0.0170,  0.0510,  0.1688,  0.0216,\n                         0.1593,  0.1601,  0.0497,  0.1212,  0.1408,  0.0181,  0.0665,  0.2345,\n                         0.0887,  0.1905,  0.0239,  0.0171,  0.1735,  0.0108,  0.2539,  0.1428,\n                         0.2598,  0.1413,  0.0382,  0.2309,  0.0118,  0.1665,  0.1490,  0.1999,\n                         0.0198,  0.2867,  0.1557,  0.2056,  0.1652,  0.0641,  0.1599,  0.2211,\n                         0.0194,  0.2385,  0.1520,  0.0880,  0.0209,  0.2612,  0.1844,  0.2264,\n                         0.1460,  0.0188,  0.0178,  0.1668,  0.1908,  0.1710,  0.1406,  0.0569,\n                         0.1659,  0.0227,  0.2004,  0.1094,  0.2067,  0.2084,  0.1846,  0.1460,\n                         0.1520,  0.1562,  0.2875,  0.0215,  0.0517,  0.1682,  0.0161,  0.0546,\n                         0.0162,  0.2315,  0.2536,  0.0165,  0.1309,  0.0175,  0.0163,  0.0159,\n                         0.2278,  0.0550,  0.0387,  0.1871,  0.0185,  0.0547,  0.2062,  0.1742,\n                         0.1613,  0.1350,  0.2182,  0.2387,  0.0499,  0.2338,  0.0539,  0.0183,\n                         0.1472,  0.1591,  0.0434,  0.2269,  0.0486,  0.0815,  0.1502,  0.0927,\n                         0.2260,  0.0176,  0.0379,  0.0539,  0.0514,  0.0175,  0.0123,  0.0474,\n                         0.1738,  0.1302,  0.0430,  0.1831,  0.2227,  0.0214,  0.0158,  0.0165])\n              )\n            )\n          )\n          (2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(\n            in_features=768, out_features=3072, bias=True\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0017, 0.0012, 0.0011,  ..., 0.0014, 0.0014, 0.0016]), zero_point=tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([-0.2183, -0.1555, -0.1459,  ..., -0.1676, -0.1546, -0.1998]), max_val=tensor([0.1423, 0.1541, 0.1313,  ..., 0.1727, 0.1789, 0.2061]))\n            )\n          )\n          (4): GELU(approximate='none')\n          (5): Linear(\n            in_features=3072, out_features=768, bias=True\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0014, 0.0018, 0.0015, 0.0015, 0.0017, 0.0018, 0.0015, 0.0014, 0.0015,\n                      0.0031, 0.0017, 0.0016, 0.0021, 0.0018, 0.0035, 0.0018, 0.0018, 0.0016,\n                      0.0016, 0.0018, 0.0016, 0.0016, 0.0020, 0.0017, 0.0017, 0.0017, 0.0017,\n                      0.0018, 0.0017, 0.0015, 0.0016, 0.0015, 0.0019, 0.0014, 0.0015, 0.0021,\n                      0.0014, 0.0015, 0.0016, 0.0016, 0.0016, 0.0016, 0.0014, 0.0019, 0.0019,\n                      0.0016, 0.0017, 0.0014, 0.0015, 0.0016, 0.0017, 0.0014, 0.0017, 0.0016,\n                      0.0015, 0.0015, 0.0016, 0.0014, 0.0017, 0.0016, 0.0018, 0.0016, 0.0018,\n                      0.0016, 0.0017, 0.0016, 0.0015, 0.0019, 0.0023, 0.0017, 0.0015, 0.0017,\n                      0.0014, 0.0020, 0.0015, 0.0015, 0.0016, 0.0018, 0.0015, 0.0014, 0.0018,\n                      0.0016, 0.0015, 0.0015, 0.0016, 0.0015, 0.0018, 0.0016, 0.0017, 0.0016,\n                      0.0016, 0.0015, 0.0016, 0.0015, 0.0015, 0.0016, 0.0016, 0.0014, 0.0014,\n                      0.0017, 0.0016, 0.0016, 0.0014, 0.0013, 0.0015, 0.0014, 0.0016, 0.0017,\n                      0.0016, 0.0015, 0.0016, 0.0018, 0.0017, 0.0014, 0.0018, 0.0027, 0.0015,\n                      0.0017, 0.0014, 0.0015, 0.0016, 0.0017, 0.0015, 0.0013, 0.0017, 0.0016,\n                      0.0021, 0.0017, 0.0016, 0.0016, 0.0014, 0.0015, 0.0016, 0.0017, 0.0015,\n                      0.0016, 0.0014, 0.0015, 0.0016, 0.0019, 0.0017, 0.0018, 0.0018, 0.0018,\n                      0.0028, 0.0019, 0.0014, 0.0015, 0.0015, 0.0019, 0.0015, 0.0015, 0.0015,\n                      0.0016, 0.0015, 0.0014, 0.0016, 0.0018, 0.0016, 0.0017, 0.0015, 0.0014,\n                      0.0017, 0.0017, 0.0017, 0.0015, 0.0016, 0.0017, 0.0016, 0.0014, 0.0017,\n                      0.0017, 0.0017, 0.0015, 0.0015, 0.0018, 0.0014, 0.0029, 0.0015, 0.0021,\n                      0.0013, 0.0013, 0.0015, 0.0017, 0.0014, 0.0014, 0.0019, 0.0014, 0.0016,\n                      0.0017, 0.0016, 0.0016, 0.0015, 0.0017, 0.0016, 0.0031, 0.0015, 0.0016,\n                      0.0017, 0.0014, 0.0017, 0.0017, 0.0014, 0.0017, 0.0016, 0.0019, 0.0015,\n                      0.0019, 0.0017, 0.0016, 0.0015, 0.0014, 0.0015, 0.0041, 0.0015, 0.0017,\n                      0.0015, 0.0015, 0.0013, 0.0016, 0.0016, 0.0015, 0.0018, 0.0016, 0.0015,\n                      0.0015, 0.0015, 0.0015, 0.0014, 0.0015, 0.0030, 0.0015, 0.0015, 0.0016,\n                      0.0016, 0.0016, 0.0014, 0.0015, 0.0029, 0.0014, 0.0013, 0.0016, 0.0015,\n                      0.0014, 0.0015, 0.0016, 0.0019, 0.0014, 0.0017, 0.0016, 0.0016, 0.0014,\n                      0.0014, 0.0016, 0.0015, 0.0022, 0.0018, 0.0017, 0.0016, 0.0014, 0.0016,\n                      0.0014, 0.0015, 0.0016, 0.0011, 0.0016, 0.0028, 0.0015, 0.0018, 0.0016,\n                      0.0016, 0.0015, 0.0015, 0.0016, 0.0015, 0.0015, 0.0016, 0.0014, 0.0014,\n                      0.0015, 0.0017, 0.0015, 0.0015, 0.0016, 0.0016, 0.0014, 0.0015, 0.0016,\n                      0.0039, 0.0015, 0.0016, 0.0019, 0.0015, 0.0017, 0.0014, 0.0017, 0.0017,\n                      0.0018, 0.0019, 0.0021, 0.0015, 0.0017, 0.0015, 0.0016, 0.0014, 0.0015,\n                      0.0017, 0.0016, 0.0021, 0.0017, 0.0016, 0.0023, 0.0014, 0.0015, 0.0018,\n                      0.0021, 0.0018, 0.0041, 0.0016, 0.0016, 0.0018, 0.0016, 0.0015, 0.0020,\n                      0.0016, 0.0021, 0.0016, 0.0017, 0.0014, 0.0017, 0.0015, 0.0016, 0.0019,\n                      0.0015, 0.0015, 0.0017, 0.0016, 0.0015, 0.0015, 0.0014, 0.0033, 0.0016,\n                      0.0016, 0.0015, 0.0014, 0.0021, 0.0014, 0.0012, 0.0014, 0.0017, 0.0015,\n                      0.0016, 0.0014, 0.0015, 0.0016, 0.0020, 0.0016, 0.0015, 0.0018, 0.0018,\n                      0.0020, 0.0017, 0.0014, 0.0018, 0.0015, 0.0014, 0.0017, 0.0014, 0.0016,\n                      0.0014, 0.0016, 0.0016, 0.0018, 0.0018, 0.0016, 0.0015, 0.0017, 0.0013,\n                      0.0018, 0.0016, 0.0015, 0.0019, 0.0019, 0.0015, 0.0016, 0.0015, 0.0014,\n                      0.0016, 0.0014, 0.0015, 0.0016, 0.0036, 0.0014, 0.0014, 0.0017, 0.0036,\n                      0.0015, 0.0017, 0.0015, 0.0016, 0.0016, 0.0020, 0.0017, 0.0019, 0.0019,\n                      0.0018, 0.0015, 0.0018, 0.0016, 0.0017, 0.0016, 0.0014, 0.0016, 0.0018,\n                      0.0019, 0.0017, 0.0016, 0.0023, 0.0016, 0.0015, 0.0028, 0.0017, 0.0015,\n                      0.0019, 0.0017, 0.0014, 0.0015, 0.0014, 0.0017, 0.0015, 0.0016, 0.0015,\n                      0.0015, 0.0017, 0.0015, 0.0017, 0.0018, 0.0014, 0.0016, 0.0016, 0.0015,\n                      0.0014, 0.0016, 0.0023, 0.0015, 0.0018, 0.0019, 0.0018, 0.0016, 0.0019,\n                      0.0016, 0.0029, 0.0016, 0.0015, 0.0015, 0.0016, 0.0031, 0.0016, 0.0019,\n                      0.0014, 0.0015, 0.0018, 0.0022, 0.0014, 0.0017, 0.0017, 0.0014, 0.0014,\n                      0.0016, 0.0036, 0.0017, 0.0017, 0.0016, 0.0014, 0.0018, 0.0015, 0.0018,\n                      0.0016, 0.0014, 0.0014, 0.0017, 0.0018, 0.0017, 0.0015, 0.0014, 0.0014,\n                      0.0017, 0.0017, 0.0018, 0.0016, 0.0016, 0.0016, 0.0020, 0.0018, 0.0016,\n                      0.0016, 0.0015, 0.0017, 0.0014, 0.0015, 0.0016, 0.0016, 0.0016, 0.0016,\n                      0.0016, 0.0015, 0.0014, 0.0017, 0.0016, 0.0017, 0.0015, 0.0015, 0.0015,\n                      0.0015, 0.0015, 0.0015, 0.0016, 0.0017, 0.0017, 0.0016, 0.0014, 0.0015,\n                      0.0018, 0.0015, 0.0015, 0.0016, 0.0018, 0.0020, 0.0019, 0.0015, 0.0015,\n                      0.0014, 0.0014, 0.0017, 0.0017, 0.0015, 0.0019, 0.0016, 0.0016, 0.0016,\n                      0.0015, 0.0016, 0.0017, 0.0015, 0.0016, 0.0014, 0.0016, 0.0015, 0.0014,\n                      0.0016, 0.0016, 0.0019, 0.0016, 0.0016, 0.0016, 0.0020, 0.0015, 0.0014,\n                      0.0015, 0.0015, 0.0017, 0.0016, 0.0016, 0.0014, 0.0015, 0.0017, 0.0016,\n                      0.0015, 0.0017, 0.0021, 0.0017, 0.0014, 0.0018, 0.0017, 0.0016, 0.0016,\n                      0.0020, 0.0017, 0.0015, 0.0016, 0.0016, 0.0015, 0.0015, 0.0034, 0.0036,\n                      0.0015, 0.0015, 0.0015, 0.0017, 0.0018, 0.0015, 0.0016, 0.0014, 0.0015,\n                      0.0017, 0.0050, 0.0016, 0.0015, 0.0017, 0.0016, 0.0015, 0.0015, 0.0020,\n                      0.0014, 0.0016, 0.0038, 0.0016, 0.0014, 0.0020, 0.0016, 0.0016, 0.0036,\n                      0.0020, 0.0017, 0.0028, 0.0016, 0.0015, 0.0016, 0.0016, 0.0017, 0.0015,\n                      0.0019, 0.0016, 0.0017, 0.0017, 0.0020, 0.0020, 0.0016, 0.0014, 0.0014,\n                      0.0050, 0.0016, 0.0016, 0.0016, 0.0018, 0.0015, 0.0019, 0.0023, 0.0018,\n                      0.0015, 0.0015, 0.0015, 0.0016, 0.0015, 0.0018, 0.0016, 0.0015, 0.0016,\n                      0.0016, 0.0017, 0.0021, 0.0016, 0.0017, 0.0027, 0.0016, 0.0016, 0.0050,\n                      0.0015, 0.0013, 0.0016, 0.0018, 0.0016, 0.0021, 0.0015, 0.0017, 0.0018,\n                      0.0016, 0.0016, 0.0015, 0.0016, 0.0017, 0.0014, 0.0014, 0.0018, 0.0013,\n                      0.0015, 0.0013, 0.0015, 0.0018, 0.0016, 0.0016, 0.0016, 0.0015, 0.0015,\n                      0.0014, 0.0019, 0.0015, 0.0018, 0.0015, 0.0015, 0.0016, 0.0017, 0.0016,\n                      0.0015, 0.0018, 0.0014, 0.0016, 0.0016, 0.0015, 0.0017, 0.0017, 0.0017,\n                      0.0016, 0.0016, 0.0014, 0.0013, 0.0016, 0.0034, 0.0014, 0.0015, 0.0020,\n                      0.0014, 0.0016, 0.0016, 0.0020, 0.0016, 0.0016, 0.0016, 0.0016, 0.0014,\n                      0.0014, 0.0014, 0.0017, 0.0015, 0.0016, 0.0019, 0.0016, 0.0015, 0.0014,\n                      0.0015, 0.0016, 0.0018, 0.0021, 0.0016, 0.0014, 0.0016, 0.0015, 0.0018,\n                      0.0014, 0.0016, 0.0017, 0.0019, 0.0014, 0.0020, 0.0016, 0.0019, 0.0017,\n                      0.0019, 0.0019, 0.0027, 0.0018, 0.0016, 0.0014, 0.0016, 0.0017, 0.0017,\n                      0.0016, 0.0016, 0.0018, 0.0015, 0.0016, 0.0016, 0.0016, 0.0015, 0.0015,\n                      0.0016, 0.0017, 0.0015]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                     dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n                min_val=tensor([-0.1688, -0.1786, -0.1886, -0.1977, -0.1909, -0.1867, -0.1742, -0.1663,\n                        -0.1663, -0.2599, -0.1915, -0.1676, -0.2084, -0.2343, -0.2047, -0.2339,\n                        -0.2315, -0.2109, -0.1898, -0.2259, -0.2038, -0.2002, -0.2621, -0.2085,\n                        -0.2137, -0.1872, -0.1685, -0.2325, -0.1777, -0.1926, -0.2042, -0.1877,\n                        -0.1844, -0.1787, -0.1951, -0.2636, -0.1779, -0.1911, -0.1863, -0.1937,\n                        -0.2024, -0.1992, -0.1848, -0.2393, -0.2188, -0.2012, -0.2191, -0.1816,\n                        -0.1938, -0.1867, -0.1637, -0.1783, -0.2124, -0.2080, -0.1659, -0.1817,\n                        -0.1873, -0.1850, -0.2147, -0.1762, -0.2251, -0.1569, -0.1639, -0.2041,\n                        -0.2191, -0.2023, -0.1867, -0.2487, -0.2964, -0.2170, -0.1880, -0.1985,\n                        -0.1849, -0.2604, -0.1812, -0.1856, -0.2049, -0.2182, -0.1760, -0.1786,\n                        -0.2310, -0.2015, -0.1952, -0.1729, -0.1789, -0.1862, -0.2270, -0.2034,\n                        -0.2178, -0.2075, -0.1795, -0.1835, -0.1667, -0.1942, -0.1722, -0.2009,\n                        -0.1924, -0.1808, -0.1813, -0.2171, -0.2029, -0.2018, -0.1712, -0.1600,\n                        -0.1905, -0.1799, -0.2020, -0.1588, -0.1629, -0.1972, -0.1775, -0.2277,\n                        -0.2168, -0.1840, -0.1986, -0.3245, -0.1858, -0.1872, -0.1847, -0.1975,\n                        -0.1591, -0.1779, -0.1925, -0.1710, -0.1956, -0.1843, -0.2726, -0.2162,\n                        -0.1722, -0.2034, -0.1747, -0.1815, -0.1896, -0.2024, -0.1912, -0.2066,\n                        -0.1664, -0.1913, -0.1984, -0.1978, -0.2114, -0.1912, -0.1735, -0.1582,\n                        -0.3595, -0.2455, -0.1803, -0.1925, -0.1736, -0.2368, -0.1858, -0.1956,\n                        -0.1906, -0.2079, -0.1963, -0.1787, -0.2004, -0.2113, -0.2007, -0.2161,\n                        -0.1913, -0.1844, -0.1769, -0.1924, -0.2214, -0.1874, -0.2040, -0.2238,\n                        -0.2086, -0.1615, -0.2205, -0.2138, -0.2234, -0.1947, -0.1926, -0.2086,\n                        -0.1661, -0.3350, -0.1909, -0.2662, -0.1692, -0.1528, -0.1945, -0.1627,\n                        -0.1814, -0.1844, -0.2485, -0.1791, -0.1733, -0.2029, -0.2064, -0.2027,\n                        -0.1881, -0.1879, -0.1930, -0.3738, -0.1899, -0.1889, -0.1999, -0.1712,\n                        -0.1611, -0.2000, -0.1826, -0.2075, -0.1688, -0.2250, -0.1709, -0.2060,\n                        -0.2200, -0.2057, -0.1869, -0.1823, -0.1788, -0.4134, -0.1983, -0.2135,\n                        -0.1884, -0.1938, -0.1660, -0.2010, -0.2024, -0.1954, -0.1996, -0.1964,\n                        -0.1876, -0.1911, -0.1859, -0.1777, -0.1761, -0.1920, -0.3840, -0.1871,\n                        -0.1827, -0.2054, -0.2078, -0.1798, -0.1814, -0.1937, -0.3654, -0.1654,\n                        -0.1725, -0.1703, -0.1828, -0.1811, -0.1846, -0.1980, -0.2386, -0.1726,\n                        -0.2160, -0.1909, -0.2043, -0.1810, -0.1846, -0.1966, -0.1658, -0.1909,\n                        -0.1936, -0.1786, -0.2093, -0.1729, -0.1997, -0.1716, -0.1660, -0.1957,\n                        -0.1423, -0.2072, -0.3616, -0.1543, -0.2350, -0.1993, -0.2040, -0.1979,\n                        -0.1955, -0.1943, -0.1969, -0.1845, -0.2066, -0.1797, -0.1753, -0.1856,\n                        -0.2153, -0.1829, -0.1858, -0.1858, -0.2108, -0.1832, -0.1893, -0.1937,\n                        -0.4952, -0.1870, -0.1637, -0.2120, -0.1891, -0.2216, -0.1802, -0.2202,\n                        -0.2145, -0.1855, -0.2337, -0.2745, -0.1801, -0.2226, -0.1825, -0.2077,\n                        -0.1826, -0.1973, -0.1687, -0.1997, -0.1816, -0.2239, -0.2085, -0.2315,\n                        -0.1767, -0.1977, -0.2268, -0.1734, -0.2296, -0.5193, -0.1779, -0.1808,\n                        -0.2311, -0.1689, -0.1902, -0.1837, -0.2099, -0.2499, -0.2059, -0.1728,\n                        -0.1852, -0.1680, -0.1961, -0.1612, -0.2453, -0.1714, -0.1887, -0.1746,\n                        -0.2022, -0.1815, -0.1954, -0.1741, -0.3661, -0.2045, -0.2078, -0.1886,\n                        -0.1755, -0.2640, -0.1633, -0.1591, -0.1805, -0.1814, -0.1832, -0.2017,\n                        -0.1653, -0.1871, -0.1998, -0.1865, -0.1788, -0.1881, -0.2278, -0.2265,\n                        -0.2078, -0.1664, -0.1824, -0.2327, -0.1875, -0.1802, -0.2205, -0.1843,\n                        -0.1845, -0.1705, -0.1993, -0.2047, -0.2288, -0.2113, -0.1897, -0.1770,\n                        -0.1854, -0.1651, -0.2321, -0.1887, -0.1862, -0.2190, -0.2363, -0.1927,\n                        -0.1942, -0.1976, -0.1843, -0.2002, -0.1766, -0.1787, -0.1922, -0.4623,\n                        -0.1713, -0.1784, -0.1702, -0.3493, -0.1981, -0.1530, -0.1696, -0.2045,\n                        -0.2067, -0.2497, -0.2119, -0.1989, -0.2428, -0.2246, -0.1765, -0.2194,\n                        -0.1893, -0.2118, -0.2032, -0.1826, -0.1990, -0.2352, -0.2390, -0.2112,\n                        -0.2055, -0.2894, -0.2078, -0.1947, -0.1807, -0.1706, -0.1738, -0.1655,\n                        -0.2180, -0.1775, -0.1754, -0.1844, -0.2203, -0.1901, -0.1791, -0.1865,\n                        -0.1860, -0.2195, -0.1916, -0.2091, -0.2299, -0.1834, -0.1830, -0.2017,\n                        -0.1867, -0.1619, -0.1837, -0.2905, -0.1873, -0.1858, -0.1901, -0.1913,\n                        -0.2037, -0.1941, -0.2058, -0.3753, -0.1800, -0.1840, -0.1927, -0.2015,\n                        -0.3968, -0.1875, -0.2061, -0.1794, -0.1904, -0.2264, -0.2853, -0.1789,\n                        -0.2142, -0.2133, -0.1801, -0.1842, -0.2112, -0.4575, -0.2204, -0.2181,\n                        -0.2072, -0.1766, -0.2362, -0.1924, -0.1925, -0.1788, -0.1769, -0.1765,\n                        -0.1983, -0.2298, -0.2143, -0.1904, -0.1731, -0.1750, -0.2188, -0.2194,\n                        -0.2346, -0.2039, -0.1882, -0.2026, -0.2170, -0.2256, -0.1602, -0.2039,\n                        -0.1952, -0.2199, -0.1697, -0.1909, -0.1620, -0.1884, -0.2003, -0.1731,\n                        -0.1891, -0.1545, -0.1733, -0.2173, -0.1854, -0.1981, -0.1863, -0.1917,\n                        -0.1864, -0.1942, -0.1865, -0.1730, -0.1917, -0.1582, -0.2113, -0.2017,\n                        -0.1659, -0.1873, -0.2300, -0.1653, -0.1909, -0.1915, -0.1947, -0.2321,\n                        -0.1919, -0.1771, -0.1840, -0.1575, -0.1760, -0.1901, -0.2073, -0.1977,\n                        -0.2383, -0.1860, -0.2076, -0.1860, -0.1934, -0.1793, -0.2189, -0.1831,\n                        -0.1986, -0.1796, -0.2009, -0.1899, -0.1739, -0.2054, -0.1762, -0.2411,\n                        -0.2111, -0.1809, -0.1847, -0.2604, -0.1924, -0.1726, -0.1822, -0.1747,\n                        -0.1855, -0.1939, -0.1759, -0.1753, -0.1703, -0.2186, -0.1997, -0.1948,\n                        -0.2128, -0.2722, -0.2155, -0.1705, -0.1751, -0.2127, -0.1758, -0.1978,\n                        -0.1987, -0.2133, -0.1967, -0.2079, -0.1703, -0.1911, -0.1821, -0.4351,\n                        -0.3629, -0.1835, -0.1901, -0.1892, -0.1864, -0.2276, -0.1909, -0.1770,\n                        -0.1629, -0.1848, -0.2218, -0.6371, -0.1705, -0.1688, -0.2202, -0.2089,\n                        -0.1728, -0.1893, -0.1924, -0.1765, -0.2025, -0.4724, -0.2059, -0.1840,\n                        -0.2525, -0.1837, -0.2027, -0.4657, -0.2549, -0.2136, -0.3593, -0.1994,\n                        -0.1917, -0.1594, -0.2083, -0.2152, -0.1898, -0.2385, -0.2030, -0.1811,\n                        -0.1875, -0.2506, -0.2161, -0.1947, -0.1767, -0.1751, -0.5350, -0.1839,\n                        -0.2023, -0.2031, -0.1681, -0.1948, -0.1947, -0.2902, -0.2142, -0.1918,\n                        -0.1973, -0.1837, -0.1934, -0.1971, -0.1873, -0.2084, -0.1872, -0.1597,\n                        -0.1986, -0.2119, -0.2677, -0.2104, -0.1895, -0.2563, -0.1984, -0.2043,\n                        -0.6399, -0.1892, -0.1604, -0.2002, -0.2215, -0.2010, -0.2718, -0.1880,\n                        -0.1499, -0.2246, -0.2052, -0.1719, -0.1670, -0.2049, -0.1944, -0.1734,\n                        -0.1849, -0.2218, -0.1595, -0.1868, -0.1687, -0.1865, -0.1681, -0.1628,\n                        -0.1755, -0.1815, -0.1902, -0.1832, -0.1813, -0.2473, -0.1931, -0.2299,\n                        -0.1784, -0.1873, -0.2086, -0.2088, -0.1985, -0.1871, -0.1759, -0.1837,\n                        -0.1959, -0.2049, -0.1968, -0.1836, -0.2022, -0.2132, -0.2009, -0.1890,\n                        -0.1790, -0.1722, -0.2049, -0.4290, -0.1791, -0.1928, -0.1670, -0.1765,\n                        -0.2070, -0.2017, -0.2566, -0.1861, -0.1930, -0.2044, -0.1784, -0.1700,\n                        -0.1697, -0.1771, -0.2040, -0.1838, -0.1746, -0.2453, -0.2091, -0.1952,\n                        -0.1836, -0.1859, -0.2069, -0.1704, -0.2157, -0.2000, -0.1849, -0.2004,\n                        -0.1834, -0.2360, -0.1833, -0.1729, -0.2234, -0.2415, -0.1704, -0.2552,\n                        -0.1786, -0.2445, -0.2229, -0.2374, -0.2429, -0.3469, -0.1755, -0.1640,\n                        -0.1842, -0.1728, -0.1924, -0.2106, -0.1813, -0.1816, -0.1542, -0.1853,\n                        -0.2002, -0.1890, -0.1755, -0.1831, -0.1893, -0.1936, -0.2151, -0.1796]), max_val=tensor([0.1769, 0.2261, 0.1707, 0.1818, 0.2154, 0.2330, 0.1925, 0.1831, 0.1861,\n                        0.3946, 0.2109, 0.1980, 0.2636, 0.2040, 0.4404, 0.2068, 0.2248, 0.1604,\n                        0.2071, 0.2315, 0.1771, 0.1745, 0.1842, 0.2138, 0.2092, 0.2197, 0.2203,\n                        0.2267, 0.2177, 0.1777, 0.1638, 0.1672, 0.2454, 0.1706, 0.1642, 0.1867,\n                        0.1698, 0.1804, 0.1990, 0.2060, 0.1836, 0.1891, 0.1699, 0.2325, 0.2385,\n                        0.1654, 0.2175, 0.1656, 0.1663, 0.2066, 0.2133, 0.1669, 0.1907, 0.1860,\n                        0.1893, 0.1928, 0.2030, 0.1792, 0.1810, 0.2015, 0.2084, 0.2026, 0.2296,\n                        0.1893, 0.1636, 0.1750, 0.1886, 0.1947, 0.2389, 0.1865, 0.1914, 0.2143,\n                        0.1694, 0.2163, 0.1844, 0.1911, 0.1789, 0.2246, 0.1961, 0.1799, 0.2117,\n                        0.1745, 0.1768, 0.1888, 0.2054, 0.1788, 0.1833, 0.1962, 0.1732, 0.1865,\n                        0.1983, 0.1944, 0.2042, 0.1802, 0.1963, 0.1995, 0.2005, 0.1692, 0.1797,\n                        0.1765, 0.1981, 0.1733, 0.1796, 0.1583, 0.1781, 0.1728, 0.1899, 0.2170,\n                        0.2088, 0.1894, 0.2083, 0.2074, 0.1859, 0.1798, 0.2290, 0.3406, 0.1770,\n                        0.2209, 0.1752, 0.1607, 0.2059, 0.2121, 0.1886, 0.1575, 0.2109, 0.2010,\n                        0.2493, 0.1889, 0.2091, 0.1603, 0.1742, 0.1914, 0.1988, 0.2137, 0.1884,\n                        0.1710, 0.1768, 0.1837, 0.1819, 0.2360, 0.1824, 0.2326, 0.2308, 0.2273,\n                        0.3462, 0.2035, 0.1818, 0.1948, 0.1948, 0.1864, 0.1644, 0.1829, 0.1945,\n                        0.1766, 0.1854, 0.1593, 0.1969, 0.2329, 0.1831, 0.2036, 0.1579, 0.1834,\n                        0.2148, 0.2168, 0.2090, 0.1659, 0.1757, 0.1881, 0.2007, 0.1719, 0.2113,\n                        0.1812, 0.1838, 0.1847, 0.1950, 0.2333, 0.1732, 0.3676, 0.1798, 0.1758,\n                        0.1604, 0.1700, 0.1908, 0.2186, 0.1660, 0.1638, 0.2473, 0.1807, 0.2025,\n                        0.2146, 0.1762, 0.1983, 0.1814, 0.2112, 0.1995, 0.3895, 0.1816, 0.2015,\n                        0.2156, 0.1785, 0.2167, 0.2172, 0.1841, 0.2109, 0.2011, 0.2462, 0.1948,\n                        0.2398, 0.2028, 0.1880, 0.1756, 0.1734, 0.1856, 0.5252, 0.1867, 0.2147,\n                        0.1822, 0.1842, 0.1563, 0.2065, 0.2025, 0.1684, 0.2287, 0.2055, 0.1835,\n                        0.1958, 0.1926, 0.1908, 0.1716, 0.1939, 0.2942, 0.1919, 0.1947, 0.1981,\n                        0.1771, 0.1996, 0.1704, 0.1846, 0.2998, 0.1804, 0.1652, 0.2063, 0.1865,\n                        0.1801, 0.1940, 0.2087, 0.1483, 0.1837, 0.2153, 0.2018, 0.1877, 0.1646,\n                        0.1820, 0.1974, 0.1884, 0.2778, 0.2284, 0.2137, 0.1951, 0.1765, 0.1704,\n                        0.1810, 0.1865, 0.2040, 0.1301, 0.1832, 0.2229, 0.1942, 0.2019, 0.1951,\n                        0.1882, 0.1879, 0.1774, 0.1973, 0.1699, 0.1943, 0.1584, 0.1715, 0.1712,\n                        0.1952, 0.2107, 0.1861, 0.1917, 0.2071, 0.2052, 0.1730, 0.1784, 0.1969,\n                        0.3145, 0.1700, 0.2056, 0.2471, 0.1750, 0.1750, 0.1645, 0.1991, 0.1820,\n                        0.2275, 0.2370, 0.1849, 0.1852, 0.2219, 0.1908, 0.1942, 0.1703, 0.1770,\n                        0.2163, 0.1931, 0.2667, 0.2092, 0.2093, 0.2886, 0.1841, 0.1922, 0.1578,\n                        0.2725, 0.1996, 0.2954, 0.2068, 0.1971, 0.1844, 0.2063, 0.1842, 0.2551,\n                        0.1916, 0.2665, 0.1827, 0.2190, 0.1832, 0.2149, 0.1762, 0.2094, 0.1547,\n                        0.1870, 0.1798, 0.2181, 0.1906, 0.1896, 0.1821, 0.1775, 0.4243, 0.1752,\n                        0.1918, 0.1860, 0.1718, 0.1885, 0.1727, 0.1516, 0.1665, 0.2133, 0.1934,\n                        0.1646, 0.1811, 0.1851, 0.1881, 0.2497, 0.2094, 0.1871, 0.1646, 0.1905,\n                        0.2590, 0.2148, 0.1696, 0.1644, 0.1761, 0.1668, 0.2178, 0.1805, 0.2090,\n                        0.1736, 0.1912, 0.2011, 0.2023, 0.2324, 0.1984, 0.1895, 0.2102, 0.1544,\n                        0.2226, 0.1999, 0.1961, 0.2376, 0.2463, 0.1655, 0.2046, 0.1734, 0.1739,\n                        0.1994, 0.1789, 0.1882, 0.2057, 0.3649, 0.1788, 0.1799, 0.2164, 0.4633,\n                        0.1881, 0.2211, 0.1886, 0.1721, 0.1933, 0.2165, 0.1875, 0.2360, 0.2137,\n                        0.2207, 0.1926, 0.2264, 0.2050, 0.1808, 0.1936, 0.1771, 0.1940, 0.1592,\n                        0.1935, 0.1648, 0.1834, 0.1976, 0.2084, 0.1696, 0.3596, 0.2140, 0.1906,\n                        0.2456, 0.2144, 0.1709, 0.1871, 0.1806, 0.2117, 0.1815, 0.2075, 0.1688,\n                        0.1824, 0.2039, 0.1711, 0.2114, 0.2198, 0.1761, 0.2036, 0.1632, 0.1762,\n                        0.1758, 0.2001, 0.1779, 0.1683, 0.2299, 0.2401, 0.2298, 0.1917, 0.2449,\n                        0.1693, 0.3039, 0.2048, 0.1941, 0.1806, 0.1828, 0.2400, 0.2022, 0.2360,\n                        0.1813, 0.1792, 0.2086, 0.1678, 0.1776, 0.1712, 0.2137, 0.1694, 0.1687,\n                        0.2045, 0.3923, 0.1736, 0.1934, 0.1674, 0.1831, 0.2266, 0.1702, 0.2279,\n                        0.1995, 0.1784, 0.1525, 0.2187, 0.1744, 0.1776, 0.1850, 0.1642, 0.1751,\n                        0.1711, 0.2061, 0.2111, 0.1608, 0.2026, 0.1794, 0.2505, 0.2006, 0.2079,\n                        0.1832, 0.1757, 0.2109, 0.1815, 0.1802, 0.2065, 0.2050, 0.1836, 0.2024,\n                        0.2090, 0.1937, 0.1728, 0.1675, 0.2001, 0.2206, 0.1753, 0.1869, 0.1904,\n                        0.1845, 0.1854, 0.1847, 0.1981, 0.2097, 0.1628, 0.1917, 0.1729, 0.1853,\n                        0.1708, 0.1938, 0.1623, 0.2049, 0.2264, 0.2499, 0.2456, 0.1923, 0.1883,\n                        0.1735, 0.1654, 0.2215, 0.2132, 0.1674, 0.1876, 0.2057, 0.1986, 0.1982,\n                        0.1717, 0.2055, 0.2205, 0.1850, 0.2070, 0.1787, 0.1781, 0.1774, 0.1760,\n                        0.2005, 0.2042, 0.1798, 0.1872, 0.2051, 0.2054, 0.2302, 0.1960, 0.1806,\n                        0.1894, 0.1845, 0.2138, 0.2047, 0.2022, 0.1631, 0.1948, 0.1767, 0.2030,\n                        0.1666, 0.2026, 0.2368, 0.1772, 0.1816, 0.2305, 0.1681, 0.2079, 0.2030,\n                        0.2536, 0.1904, 0.1775, 0.1732, 0.1989, 0.1915, 0.1853, 0.2702, 0.4576,\n                        0.1926, 0.1826, 0.1756, 0.2211, 0.2080, 0.1943, 0.1972, 0.1719, 0.1910,\n                        0.1695, 0.6297, 0.1982, 0.1915, 0.1868, 0.1803, 0.1892, 0.1925, 0.2499,\n                        0.1835, 0.1906, 0.4889, 0.1840, 0.1805, 0.1791, 0.2000, 0.1594, 0.2434,\n                        0.2416, 0.2040, 0.3123, 0.1701, 0.1795, 0.2025, 0.1792, 0.1787, 0.1761,\n                        0.2476, 0.1778, 0.2116, 0.2118, 0.1661, 0.2483, 0.2053, 0.1780, 0.1792,\n                        0.6352, 0.2064, 0.1835, 0.1890, 0.2313, 0.1829, 0.2385, 0.2214, 0.2300,\n                        0.1912, 0.1899, 0.1920, 0.2026, 0.1630, 0.2241, 0.1811, 0.1909, 0.1998,\n                        0.1772, 0.1758, 0.1778, 0.1919, 0.2184, 0.3420, 0.1785, 0.1749, 0.5728,\n                        0.1734, 0.1403, 0.1856, 0.2267, 0.1769, 0.2031, 0.1715, 0.2128, 0.1837,\n                        0.1824, 0.1975, 0.1931, 0.1955, 0.2161, 0.1822, 0.1547, 0.2229, 0.1627,\n                        0.1711, 0.1630, 0.1747, 0.2277, 0.2011, 0.2055, 0.2017, 0.1779, 0.1952,\n                        0.1644, 0.2240, 0.1844, 0.1923, 0.1844, 0.1755, 0.1692, 0.2175, 0.1665,\n                        0.1915, 0.2337, 0.1714, 0.1977, 0.1874, 0.1830, 0.2145, 0.2198, 0.1882,\n                        0.1708, 0.2084, 0.1693, 0.1530, 0.1629, 0.3258, 0.1776, 0.1872, 0.2484,\n                        0.1786, 0.1706, 0.1990, 0.2456, 0.2030, 0.2067, 0.2004, 0.2058, 0.1821,\n                        0.1831, 0.1785, 0.2172, 0.1895, 0.2083, 0.1925, 0.1825, 0.1890, 0.1621,\n                        0.1929, 0.1752, 0.2278, 0.2637, 0.1785, 0.1660, 0.1990, 0.1885, 0.1601,\n                        0.1746, 0.2005, 0.1688, 0.1615, 0.1831, 0.1868, 0.2006, 0.2181, 0.1838,\n                        0.1735, 0.2007, 0.3253, 0.2295, 0.2023, 0.1718, 0.2060, 0.2169, 0.2189,\n                        0.2090, 0.2059, 0.2259, 0.1958, 0.2051, 0.2022, 0.2060, 0.1845, 0.1817,\n                        0.2084, 0.1829, 0.1921])\n              )\n            )\n          )\n        )\n      )\n      (1): Module(\n        (block): Module(\n          (0): Conv2d(\n            768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0016, 0.0024, 0.0011, 0.0010, 0.0008, 0.0013, 0.0010, 0.0019, 0.0016,\n                      0.0012, 0.0020, 0.0010, 0.0021, 0.0014, 0.0029, 0.0010, 0.0011, 0.0014,\n                      0.0019, 0.0017, 0.0018, 0.0014, 0.0014, 0.0006, 0.0013, 0.0009, 0.0016,\n                      0.0009, 0.0010, 0.0014, 0.0014, 0.0019, 0.0012, 0.0019, 0.0012, 0.0018,\n                      0.0012, 0.0010, 0.0007, 0.0019, 0.0015, 0.0018, 0.0010, 0.0016, 0.0016,\n                      0.0017, 0.0017, 0.0014, 0.0015, 0.0018, 0.0021, 0.0017, 0.0027, 0.0011,\n                      0.0023, 0.0014, 0.0009, 0.0013, 0.0007, 0.0016, 0.0016, 0.0018, 0.0016,\n                      0.0013, 0.0010, 0.0015, 0.0008, 0.0017, 0.0011, 0.0014, 0.0014, 0.0010,\n                      0.0013, 0.0015, 0.0014, 0.0019, 0.0016, 0.0010, 0.0014, 0.0016, 0.0012,\n                      0.0011, 0.0015, 0.0009, 0.0018, 0.0010, 0.0012, 0.0011, 0.0008, 0.0013,\n                      0.0011, 0.0011, 0.0015, 0.0009, 0.0014, 0.0014, 0.0009, 0.0017, 0.0019,\n                      0.0013, 0.0012, 0.0018, 0.0014, 0.0017, 0.0014, 0.0015, 0.0011, 0.0016,\n                      0.0015, 0.0007, 0.0018, 0.0011, 0.0019, 0.0011, 0.0010, 0.0016, 0.0012,\n                      0.0011, 0.0009, 0.0014, 0.0017, 0.0010, 0.0013, 0.0023, 0.0012, 0.0014,\n                      0.0010, 0.0017, 0.0019, 0.0020, 0.0031, 0.0017, 0.0017, 0.0010, 0.0017,\n                      0.0011, 0.0020, 0.0018, 0.0025, 0.0019, 0.0014, 0.0018, 0.0016, 0.0009,\n                      0.0010, 0.0011, 0.0014, 0.0018, 0.0018, 0.0018, 0.0012, 0.0019, 0.0013,\n                      0.0009, 0.0009, 0.0007, 0.0014, 0.0008, 0.0015, 0.0009, 0.0015, 0.0018,\n                      0.0015, 0.0016, 0.0015, 0.0016, 0.0024, 0.0019, 0.0016, 0.0020, 0.0013,\n                      0.0013, 0.0018, 0.0013, 0.0019, 0.0009, 0.0012, 0.0020, 0.0013, 0.0019,\n                      0.0011, 0.0007, 0.0009, 0.0017, 0.0015, 0.0018, 0.0011, 0.0012, 0.0016,\n                      0.0018, 0.0016, 0.0014, 0.0014, 0.0010, 0.0013, 0.0010, 0.0008, 0.0017,\n                      0.0014, 0.0013, 0.0016, 0.0014, 0.0017, 0.0014, 0.0026, 0.0016, 0.0011,\n                      0.0017, 0.0007, 0.0019, 0.0016, 0.0014, 0.0013, 0.0006, 0.0015, 0.0019,\n                      0.0014, 0.0011, 0.0020, 0.0012, 0.0011, 0.0016, 0.0015, 0.0012, 0.0017,\n                      0.0014, 0.0018, 0.0012, 0.0023, 0.0013, 0.0018, 0.0015, 0.0021, 0.0016,\n                      0.0016, 0.0008, 0.0028, 0.0011, 0.0008, 0.0013, 0.0019, 0.0014, 0.0009,\n                      0.0011, 0.0012, 0.0015, 0.0018, 0.0012, 0.0013, 0.0025, 0.0018, 0.0017,\n                      0.0013, 0.0006, 0.0007, 0.0015, 0.0019, 0.0010, 0.0010, 0.0008, 0.0015,\n                      0.0011, 0.0013, 0.0016, 0.0010, 0.0021, 0.0028, 0.0012, 0.0016, 0.0016,\n                      0.0010, 0.0014, 0.0011, 0.0024, 0.0009, 0.0012, 0.0014, 0.0016, 0.0019,\n                      0.0011, 0.0015, 0.0014, 0.0024, 0.0009, 0.0007, 0.0016, 0.0016, 0.0011,\n                      0.0023, 0.0013, 0.0014, 0.0010, 0.0016, 0.0016, 0.0018, 0.0017, 0.0006,\n                      0.0011, 0.0021, 0.0021, 0.0016, 0.0016, 0.0015, 0.0014, 0.0013, 0.0014,\n                      0.0012, 0.0009, 0.0018, 0.0013, 0.0014, 0.0011, 0.0017, 0.0013, 0.0018,\n                      0.0013, 0.0019, 0.0018, 0.0013, 0.0015, 0.0015, 0.0017, 0.0018, 0.0022,\n                      0.0010, 0.0012, 0.0011, 0.0016, 0.0010, 0.0015, 0.0024, 0.0017, 0.0010,\n                      0.0007, 0.0018, 0.0013, 0.0014, 0.0017, 0.0012, 0.0011, 0.0008, 0.0016,\n                      0.0009, 0.0015, 0.0014, 0.0015, 0.0017, 0.0014, 0.0014, 0.0016, 0.0013,\n                      0.0011, 0.0014, 0.0017, 0.0011, 0.0018, 0.0017, 0.0014, 0.0006, 0.0016,\n                      0.0011, 0.0014, 0.0014, 0.0014, 0.0017, 0.0014, 0.0014, 0.0010, 0.0015,\n                      0.0014, 0.0020, 0.0012, 0.0023, 0.0015, 0.0016, 0.0019, 0.0019, 0.0012,\n                      0.0013, 0.0015, 0.0009, 0.0015, 0.0010, 0.0018, 0.0009, 0.0020, 0.0015,\n                      0.0027, 0.0019, 0.0017, 0.0016, 0.0008, 0.0014, 0.0014, 0.0013, 0.0017,\n                      0.0010, 0.0012, 0.0009, 0.0023, 0.0023, 0.0021, 0.0016, 0.0032, 0.0014,\n                      0.0015, 0.0014, 0.0014, 0.0017, 0.0014, 0.0011, 0.0012, 0.0015, 0.0014,\n                      0.0016, 0.0019, 0.0007, 0.0015, 0.0013, 0.0011, 0.0023, 0.0011, 0.0008,\n                      0.0016, 0.0008, 0.0016, 0.0013, 0.0011, 0.0013, 0.0015, 0.0013, 0.0012,\n                      0.0014, 0.0007, 0.0016, 0.0012, 0.0012, 0.0014, 0.0012, 0.0014, 0.0011,\n                      0.0012, 0.0024, 0.0017, 0.0006, 0.0010, 0.0021, 0.0016, 0.0020, 0.0034,\n                      0.0012, 0.0022, 0.0016, 0.0025, 0.0016, 0.0013, 0.0015, 0.0015, 0.0011,\n                      0.0017, 0.0009, 0.0016, 0.0016, 0.0015, 0.0012, 0.0011, 0.0007, 0.0021,\n                      0.0016, 0.0012, 0.0013, 0.0016, 0.0018, 0.0013, 0.0009, 0.0011, 0.0014,\n                      0.0014, 0.0014, 0.0021, 0.0016, 0.0013, 0.0012, 0.0020, 0.0010, 0.0010,\n                      0.0013, 0.0010, 0.0015, 0.0015, 0.0006, 0.0012, 0.0010, 0.0013, 0.0012,\n                      0.0013, 0.0012, 0.0010, 0.0019, 0.0017, 0.0021, 0.0013, 0.0010, 0.0017,\n                      0.0019, 0.0013, 0.0016, 0.0017, 0.0014, 0.0010, 0.0016, 0.0017, 0.0013,\n                      0.0009, 0.0026, 0.0019, 0.0018, 0.0013, 0.0015, 0.0010, 0.0008, 0.0028,\n                      0.0009, 0.0009, 0.0016, 0.0011, 0.0011, 0.0015, 0.0016, 0.0016, 0.0020,\n                      0.0015, 0.0020, 0.0012, 0.0017, 0.0021, 0.0006, 0.0018, 0.0014, 0.0015,\n                      0.0017, 0.0018, 0.0011, 0.0017, 0.0019, 0.0018, 0.0017, 0.0014, 0.0008,\n                      0.0012, 0.0009, 0.0016, 0.0012, 0.0012, 0.0017, 0.0018, 0.0019, 0.0018,\n                      0.0017, 0.0011, 0.0011, 0.0013, 0.0014, 0.0015, 0.0020, 0.0015, 0.0020,\n                      0.0010, 0.0007, 0.0009, 0.0018, 0.0015, 0.0015, 0.0009, 0.0011, 0.0017,\n                      0.0009, 0.0007, 0.0013, 0.0015, 0.0011, 0.0010, 0.0014, 0.0026, 0.0018,\n                      0.0012, 0.0015, 0.0018, 0.0018, 0.0019, 0.0007, 0.0008, 0.0022, 0.0021,\n                      0.0005, 0.0007, 0.0010, 0.0015, 0.0013, 0.0015, 0.0019, 0.0016, 0.0015,\n                      0.0025, 0.0009, 0.0008, 0.0015, 0.0019, 0.0012, 0.0010, 0.0016, 0.0007,\n                      0.0012, 0.0018, 0.0024, 0.0017, 0.0014, 0.0020, 0.0010, 0.0006, 0.0009,\n                      0.0011, 0.0018, 0.0013, 0.0006, 0.0015, 0.0012, 0.0008, 0.0016, 0.0017,\n                      0.0006, 0.0015, 0.0021, 0.0018, 0.0013, 0.0009, 0.0016, 0.0010, 0.0019,\n                      0.0010, 0.0007, 0.0022, 0.0007, 0.0019, 0.0023, 0.0016, 0.0017, 0.0013,\n                      0.0015, 0.0012, 0.0020, 0.0012, 0.0007, 0.0004, 0.0016, 0.0012, 0.0013,\n                      0.0015, 0.0017, 0.0018, 0.0013, 0.0016, 0.0007, 0.0015, 0.0014, 0.0010,\n                      0.0015, 0.0017, 0.0012, 0.0009, 0.0018, 0.0016, 0.0016, 0.0009, 0.0017,\n                      0.0015, 0.0010, 0.0009, 0.0018, 0.0018, 0.0012, 0.0022, 0.0013, 0.0019,\n                      0.0009, 0.0016, 0.0021, 0.0014, 0.0021, 0.0015, 0.0015, 0.0015, 0.0012,\n                      0.0010, 0.0011, 0.0012, 0.0019, 0.0027, 0.0013, 0.0016, 0.0006, 0.0017,\n                      0.0017, 0.0023, 0.0013, 0.0012, 0.0006, 0.0008, 0.0012, 0.0013, 0.0019,\n                      0.0017, 0.0012, 0.0010, 0.0012, 0.0018, 0.0015, 0.0010, 0.0020, 0.0009,\n                      0.0017, 0.0016, 0.0013, 0.0014, 0.0005, 0.0011, 0.0016, 0.0014, 0.0016,\n                      0.0021, 0.0019, 0.0009, 0.0013, 0.0009, 0.0018, 0.0008, 0.0016, 0.0015,\n                      0.0015, 0.0009, 0.0008, 0.0012, 0.0029, 0.0012, 0.0011, 0.0014, 0.0009,\n                      0.0015, 0.0010, 0.0011, 0.0016, 0.0008, 0.0013, 0.0014, 0.0016, 0.0023,\n                      0.0007, 0.0017, 0.0014, 0.0015, 0.0013, 0.0020, 0.0016, 0.0015, 0.0017,\n                      0.0013, 0.0015, 0.0012]), zero_point=tensor([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0, 127,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                        0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n                     dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n                min_val=tensor([-0.2031, -0.3104, -0.1033, -0.0296, -0.1077, -0.1136, -0.1228, -0.2447,\n                        -0.2046, -0.1571, -0.1121, -0.0234, -0.1468, -0.1833, -0.3670, -0.1329,\n                        -0.0320, -0.1386, -0.2390, -0.0500, -0.2314, -0.1438, -0.1676, -0.0793,\n                        -0.0329, -0.0811, -0.1209, -0.1110, -0.1270, -0.0304, -0.0693, -0.2495,\n                        -0.1534, -0.1485, -0.1473, -0.2357, -0.1179, -0.1219, -0.0193, -0.0328,\n                        -0.1586, -0.2309, -0.0342, -0.2015, -0.2087, -0.2139, -0.1252, -0.0838,\n                        -0.1966, -0.2270, -0.0471, -0.0254, -0.3479, -0.0258, -0.0571, -0.1767,\n                        -0.0413, -0.1659, -0.0864, -0.0454, -0.1255, -0.1150, -0.2005, -0.0206,\n                        -0.1104, -0.1950, -0.1053, -0.1165, -0.1448, -0.0965, -0.1845, -0.1082,\n                        -0.1714, -0.1931, -0.1299, -0.2387, -0.2049, -0.1321, -0.1802, -0.0237,\n                        -0.1559, -0.1454, -0.0254, -0.1207, -0.0292, -0.0947, -0.1573, -0.1454,\n                        -0.0214, -0.1604, -0.0431, -0.1382, -0.1947, -0.1171, -0.0163, -0.1816,\n                        -0.0926, -0.1314, -0.1283, -0.1640, -0.1569, -0.0508, -0.1807, -0.1455,\n                        -0.1755, -0.0522, -0.0531, -0.1098, -0.1903, -0.0789, -0.2270, -0.1128,\n                        -0.1236, -0.0925, -0.1287, -0.2014, -0.1315, -0.1470, -0.1168, -0.0816,\n                        -0.0223, -0.1229, -0.0258, -0.2995, -0.0635, -0.1855, -0.1307, -0.1334,\n                        -0.1104, -0.2412, -0.0426, -0.2201, -0.0446, -0.1330, -0.1198, -0.1126,\n                        -0.0393, -0.1496, -0.3223, -0.1909, -0.1791, -0.0385, -0.2053, -0.1098,\n                        -0.0895, -0.0618, -0.0345, -0.0767, -0.1341, -0.2349, -0.1194, -0.2422,\n                        -0.1608, -0.0188, -0.1128, -0.0916, -0.1787, -0.0991, -0.1862, -0.0920,\n                        -0.1956, -0.2325, -0.1898, -0.2076, -0.1923, -0.0259, -0.0581, -0.0408,\n                        -0.2093, -0.0634, -0.0267, -0.1724, -0.1163, -0.1721, -0.2409, -0.1194,\n                        -0.1476, -0.2504, -0.1603, -0.0344, -0.1415, -0.0515, -0.1159, -0.0353,\n                        -0.1937, -0.2267, -0.1254, -0.1525, -0.2039, -0.0175, -0.2049, -0.1768,\n                        -0.1830, -0.1268, -0.1674, -0.1280, -0.1027, -0.2137, -0.0317, -0.1431,\n                        -0.1988, -0.1758, -0.1167, -0.0166, -0.1822, -0.2061, -0.1397, -0.2191,\n                        -0.0269, -0.2450, -0.0381, -0.1347, -0.1660, -0.0569, -0.1926, -0.2455,\n                        -0.1832, -0.0222, -0.0549, -0.0704, -0.1382, -0.1328, -0.1217, -0.1382,\n                        -0.2229, -0.0244, -0.2304, -0.0638, -0.0535, -0.0309, -0.0457, -0.1366,\n                        -0.2698, -0.2059, -0.0761, -0.0230, -0.0440, -0.1212, -0.0739, -0.1613,\n                        -0.2394, -0.1818, -0.1138, -0.1439, -0.1262, -0.1367, -0.0308, -0.1545,\n                        -0.1009, -0.0371, -0.2251, -0.2206, -0.1624, -0.0270, -0.0882, -0.1423,\n                        -0.1300, -0.1095, -0.1280, -0.0912, -0.0455, -0.1355, -0.1607, -0.2093,\n                        -0.1261, -0.2723, -0.3606, -0.1556, -0.2062, -0.0270, -0.0275, -0.0284,\n                        -0.0228, -0.3033, -0.1206, -0.1275, -0.1810, -0.1473, -0.1279, -0.1431,\n                        -0.1934, -0.0330, -0.0511, -0.0588, -0.0217, -0.2033, -0.1446, -0.1394,\n                        -0.2985, -0.1627, -0.0349, -0.0416, -0.0164, -0.0287, -0.2258, -0.2221,\n                        -0.0832, -0.1114, -0.2643, -0.1779, -0.1164, -0.0229, -0.1941, -0.1758,\n                        -0.1438, -0.0749, -0.0317, -0.0171, -0.0302, -0.1629, -0.1837, -0.1441,\n                        -0.2146, -0.1624, -0.0249, -0.1526, -0.1192, -0.1919, -0.1627, -0.0201,\n                        -0.1875, -0.1288, -0.2361, -0.2753, -0.1047, -0.0332, -0.1180, -0.0566,\n                        -0.0221, -0.1409, -0.3094, -0.2223, -0.0282, -0.0906, -0.0887, -0.0496,\n                        -0.1816, -0.1468, -0.1308, -0.0745, -0.0772, -0.1360, -0.0591, -0.0504,\n                        -0.1785, -0.1876, -0.2170, -0.1599, -0.1510, -0.1382, -0.1655, -0.0212,\n                        -0.1827, -0.2237, -0.1424, -0.0278, -0.0345, -0.1852, -0.0713, -0.1407,\n                        -0.1403, -0.0224, -0.1741, -0.0230, -0.2115, -0.1822, -0.1770, -0.1304,\n                        -0.1861, -0.1838, -0.2593, -0.0315, -0.2959, -0.1902, -0.2009, -0.1375,\n                        -0.2423, -0.0419, -0.1649, -0.1982, -0.1138, -0.1921, -0.1208, -0.1269,\n                        -0.1140, -0.0705, -0.0240, -0.3471, -0.2454, -0.2140, -0.2083, -0.0983,\n                        -0.1437, -0.1795, -0.0863, -0.0794, -0.1250, -0.1547, -0.0302, -0.2976,\n                        -0.2931, -0.0151, -0.2101, -0.0411, -0.0277, -0.1275, -0.1573, -0.0195,\n                        -0.0251, -0.1793, -0.1401, -0.1594, -0.1513, -0.0286, -0.0472, -0.2496,\n                        -0.0928, -0.1217, -0.1421, -0.0959, -0.1652, -0.1196, -0.1079, -0.2069,\n                        -0.0975, -0.2032, -0.0221, -0.1353, -0.1663, -0.1898, -0.0989, -0.1375,\n                        -0.0199, -0.0895, -0.0329, -0.1474, -0.1481, -0.0402, -0.1506, -0.1378,\n                        -0.1463, -0.1518, -0.0596, -0.1427, -0.0691, -0.1318, -0.0645, -0.0305,\n                        -0.2583, -0.4398, -0.0317, -0.2856, -0.0179, -0.0469, -0.1999, -0.0460,\n                        -0.1864, -0.1404, -0.0235, -0.2183, -0.1151, -0.2076, -0.0273, -0.0192,\n                        -0.0566, -0.1467, -0.0153, -0.0438, -0.0282, -0.1495, -0.1606, -0.2061,\n                        -0.1046, -0.1613, -0.1169, -0.1397, -0.0257, -0.1401, -0.0216, -0.1216,\n                        -0.0389, -0.1641, -0.1412, -0.0469, -0.0426, -0.0383, -0.1618, -0.1234,\n                        -0.0356, -0.0268, -0.0384, -0.1589, -0.1341, -0.1344, -0.1197, -0.0361,\n                        -0.1572, -0.0812, -0.2430, -0.0171, -0.2639, -0.0856, -0.0225, -0.1182,\n                        -0.0394, -0.1180, -0.0789, -0.0275, -0.1805, -0.1205, -0.0825, -0.2169,\n                        -0.1373, -0.1203, -0.0475, -0.2445, -0.2274, -0.0428, -0.1858, -0.1334,\n                        -0.1064, -0.0407, -0.0326, -0.1145, -0.2059, -0.1235, -0.0455, -0.0702,\n                        -0.0402, -0.0930, -0.1368, -0.1031, -0.0558, -0.1506, -0.1434, -0.2704,\n                        -0.0669, -0.2016, -0.1793, -0.1944, -0.2208, -0.2322, -0.1348, -0.2193,\n                        -0.0389, -0.2252, -0.2193, -0.1839, -0.0841, -0.1294, -0.0869, -0.1229,\n                        -0.0191, -0.0993, -0.2222, -0.0465, -0.0355, -0.2262, -0.2129, -0.1239,\n                        -0.1466, -0.0342, -0.1758, -0.0717, -0.0237, -0.0362, -0.0426, -0.1233,\n                        -0.0877, -0.1189, -0.2353, -0.1978, -0.1963, -0.1202, -0.0284, -0.2229,\n                        -0.1186, -0.0789, -0.1532, -0.0264, -0.0735, -0.1315, -0.1730, -0.1019,\n                        -0.1486, -0.1494, -0.0373, -0.1017, -0.1589, -0.2477, -0.0427, -0.0987,\n                        -0.2760, -0.2696, -0.0640, -0.0945, -0.1343, -0.1165, -0.0316, -0.1982,\n                        -0.2431, -0.2041, -0.0395, -0.3248, -0.0515, -0.0693, -0.1949, -0.1411,\n                        -0.0347, -0.1282, -0.0244, -0.0935, -0.1535, -0.0357, -0.3065, -0.2220,\n                        -0.1463, -0.1390, -0.0251, -0.0312, -0.1204, -0.1427, -0.0459, -0.0216,\n                        -0.0321, -0.1173, -0.0332, -0.1015, -0.1171, -0.1565, -0.0557, -0.1294,\n                        -0.2643, -0.2321, -0.1606, -0.0958, -0.2037, -0.1241, -0.2485, -0.0615,\n                        -0.0878, -0.1238, -0.0884, -0.2374, -0.2932, -0.0751, -0.1074, -0.1237,\n                        -0.1632, -0.1509, -0.0500, -0.1554, -0.0624, -0.0996, -0.0208, -0.1576,\n                        -0.0617, -0.0236, -0.1308, -0.2256, -0.0294, -0.1167, -0.0648, -0.1453,\n                        -0.0051, -0.0277, -0.1944, -0.1249, -0.1585, -0.1134, -0.1734, -0.2012,\n                        -0.2103, -0.0109, -0.2128, -0.0427, -0.1280, -0.1096, -0.2256, -0.0711,\n                        -0.0241, -0.2834, -0.1525, -0.1377, -0.1002, -0.0415, -0.0326, -0.1731,\n                        -0.2737, -0.1437, -0.1907, -0.1859, -0.1498, -0.1188, -0.1389, -0.0549,\n                        -0.0285, -0.2326, -0.1685, -0.2052, -0.0351, -0.0305, -0.0194, -0.2906,\n                        -0.0790, -0.0233, -0.0780, -0.0755, -0.0362, -0.1604, -0.2369, -0.2186,\n                        -0.1591, -0.1236, -0.0533, -0.1483, -0.1941, -0.0330, -0.0935, -0.1139,\n                        -0.2158, -0.1437, -0.1361, -0.1319, -0.0656, -0.1468, -0.1679, -0.1038,\n                        -0.2010, -0.0432, -0.2376, -0.0226, -0.0376, -0.1207, -0.1373, -0.0781,\n                        -0.2017, -0.1877, -0.1859, -0.1185, -0.1059, -0.1260, -0.3767, -0.1554,\n                        -0.1429, -0.1811, -0.1121, -0.1893, -0.0335, -0.0591, -0.0277, -0.1023,\n                        -0.1349, -0.0254, -0.2048, -0.0336, -0.0313, -0.2126, -0.0202, -0.1921,\n                        -0.0420, -0.2566, -0.2057, -0.1873, -0.2186, -0.1627, -0.1413, -0.1475]), max_val=tensor([ 0.1618,  0.1807,  0.1335,  0.1289,  0.0738,  0.1712,  0.0349,  0.0450,\n                         0.1305,  0.0884,  0.2547,  0.1255,  0.2614,  0.0360,  0.0748,  0.1270,\n                         0.1442,  0.1816,  0.1517,  0.2122,  0.0298,  0.1788,  0.1786,  0.0339,\n                         0.1680,  0.1169,  0.1979,  0.0447,  0.0262,  0.1745,  0.1768,  0.0925,\n                         0.0400,  0.2429,  0.1448,  0.0574,  0.1580,  0.0220,  0.0904,  0.2389,\n                         0.1891,  0.1237,  0.1233,  0.1521,  0.1916,  0.0348,  0.2148,  0.1716,\n                         0.0275,  0.0445,  0.2613,  0.2138,  0.0340,  0.1416,  0.2924,  0.0180,\n                         0.1190,  0.0936,  0.0336,  0.2080,  0.2081,  0.2323,  0.1365,  0.1589,\n                         0.1278,  0.1206,  0.0370,  0.2120,  0.1199,  0.1771,  0.0999,  0.1263,\n                         0.0199,  0.0388,  0.1789,  0.1463,  0.1035,  0.0770,  0.1400,  0.2078,\n                         0.1288,  0.0439,  0.1900,  0.0307,  0.2261,  0.1276,  0.0339,  0.0134,\n                         0.1038,  0.0481,  0.1339,  0.0445,  0.0224,  0.1174,  0.1774,  0.1148,\n                         0.1178,  0.2126,  0.2399,  0.0312,  0.0824,  0.2250,  0.0338,  0.2211,\n                         0.0275,  0.1912,  0.1387,  0.1990,  0.1353,  0.0916,  0.1275,  0.1409,\n                         0.2406,  0.1395,  0.1330,  0.0478,  0.1544,  0.0339,  0.0193,  0.1758,\n                         0.2175,  0.0133,  0.1660,  0.0466,  0.1558,  0.1438,  0.0740,  0.2131,\n                         0.2476,  0.2566,  0.3894,  0.0241,  0.2154,  0.0936,  0.2150,  0.1455,\n                         0.2521,  0.2227,  0.0409,  0.2399,  0.0733,  0.2345,  0.1579,  0.0377,\n                         0.1231,  0.1424,  0.1806,  0.2345,  0.2310,  0.0968,  0.1491,  0.0217,\n                         0.0217,  0.1155,  0.0302,  0.0301,  0.1251,  0.0619,  0.0181,  0.1199,\n                         0.1338,  0.0540,  0.0220,  0.1151,  0.0824,  0.2034,  0.3059,  0.2453,\n                         0.0169,  0.2568,  0.1604,  0.0562,  0.2246,  0.1534,  0.0446,  0.0679,\n                         0.1291,  0.0311,  0.0445,  0.2432,  0.1349,  0.0865,  0.0352,  0.2156,\n                         0.0958,  0.1097,  0.1387,  0.0439,  0.0401,  0.2267,  0.0994,  0.0288,\n                         0.0246,  0.0281,  0.0295,  0.0590,  0.0726,  0.0279,  0.1831,  0.1672,\n                         0.0900,  0.0786,  0.2208,  0.1730,  0.3317,  0.0428,  0.0987,  0.0555,\n                         0.0903,  0.1856,  0.2048,  0.1801,  0.0494,  0.0746,  0.0252,  0.1231,\n                         0.0311,  0.1432,  0.2558,  0.1515,  0.1405,  0.2075,  0.1872,  0.1548,\n                         0.0189,  0.1776,  0.0603,  0.1570,  0.2943,  0.1656,  0.2227,  0.1885,\n                         0.1049,  0.0317,  0.2021,  0.1061,  0.3541,  0.1346,  0.1024,  0.0218,\n                         0.0803,  0.1429,  0.0155,  0.0926,  0.1514,  0.1948,  0.2253,  0.0211,\n                         0.1613,  0.3233,  0.1262,  0.0503,  0.1220,  0.0823,  0.0540,  0.1857,\n                         0.2437,  0.1236,  0.0321,  0.1050,  0.1869,  0.0355,  0.1563,  0.0320,\n                         0.1087,  0.2066,  0.0448,  0.1133,  0.1835,  0.1989,  0.1329,  0.1814,\n                         0.1410,  0.2853,  0.0208,  0.1528,  0.0167,  0.2000,  0.2398,  0.0611,\n                         0.0291,  0.1728,  0.2987,  0.1109,  0.0902,  0.1690,  0.2023,  0.0949,\n                         0.1858,  0.0235,  0.1750,  0.1287,  0.2039,  0.2061,  0.1404,  0.1854,\n                         0.0193,  0.1373,  0.1582,  0.2691,  0.2017,  0.2044,  0.0170,  0.0561,\n                         0.1707,  0.1755,  0.1553,  0.1114,  0.2273,  0.0431,  0.0202,  0.0468,\n                         0.1669,  0.1422,  0.2338,  0.1663,  0.2463,  0.2260,  0.0218,  0.1863,\n                         0.0211,  0.2142,  0.1110,  0.1599,  0.1207,  0.1523,  0.1362,  0.2015,\n                         0.1310,  0.1861,  0.1573,  0.1203,  0.1298,  0.0268,  0.2297,  0.1621,\n                         0.0141,  0.2176,  0.1536,  0.1404,  0.1040,  0.2045,  0.1122,  0.1903,\n                         0.1447,  0.0461,  0.1521,  0.1771,  0.1743,  0.1998,  0.0274,  0.1357,\n                         0.0416,  0.1056,  0.1148,  0.2285,  0.2219,  0.1183,  0.0177,  0.2038,\n                         0.1220,  0.1807,  0.1213,  0.1797,  0.0177,  0.0262,  0.1607,  0.1056,\n                         0.1737,  0.1442,  0.0968,  0.1557,  0.0426,  0.0221,  0.1467,  0.2385,\n                         0.0481,  0.1467,  0.1650,  0.1327,  0.0117,  0.0327,  0.1282,  0.2247,\n                         0.0307,  0.2584,  0.1951,  0.0339,  0.1383,  0.0434,  0.0209,  0.0578,\n                         0.1800,  0.0226,  0.1644,  0.2112,  0.1152,  0.0081,  0.1150,  0.1344,\n                         0.0462,  0.2631,  0.0383,  0.4005,  0.1751,  0.1896,  0.1805,  0.1802,\n                         0.2210,  0.0118,  0.0176,  0.0426,  0.1870,  0.1816,  0.2077,  0.0493,\n                         0.0869,  0.1922,  0.1612,  0.1354,  0.2884,  0.1386,  0.0214,  0.0398,\n                         0.0286,  0.1368,  0.1682,  0.0224,  0.0879,  0.0378,  0.1667,  0.1462,\n                         0.1836,  0.0940,  0.2014,  0.0244,  0.0343,  0.1780,  0.0206,  0.1731,\n                         0.0454,  0.1128,  0.3047,  0.2174,  0.0714,  0.1049,  0.2640,  0.2082,\n                         0.1233,  0.0266,  0.1484,  0.1322,  0.2035,  0.3235,  0.0806,  0.1664,\n                         0.0620,  0.1937,  0.1359,  0.0516,  0.0287,  0.1106,  0.2046,  0.1855,\n                         0.1481,  0.0285,  0.0847,  0.2694,  0.2033,  0.0742,  0.0692,  0.0301,\n                         0.2267,  0.0256,  0.1136,  0.0524,  0.1734,  0.1840,  0.1822,  0.2669,\n                         0.2026,  0.0212,  0.1578,  0.2597,  0.1314,  0.1281,  0.1475,  0.1026,\n                         0.1928,  0.1968,  0.0756,  0.0332,  0.1140,  0.1709,  0.1479,  0.1617,\n                         0.1334,  0.1287,  0.1057,  0.2201,  0.1415,  0.1589,  0.1310,  0.2219,\n                         0.2369,  0.1682,  0.2062,  0.2198,  0.0870,  0.1290,  0.2043,  0.1802,\n                         0.1634,  0.1095,  0.3358,  0.1097,  0.1466,  0.1680,  0.1318,  0.1233,\n                         0.0549,  0.3597,  0.1196,  0.0193,  0.1080,  0.1409,  0.1432,  0.1846,\n                         0.1997,  0.1991,  0.2552,  0.1915,  0.2526,  0.1060,  0.2100,  0.0956,\n                         0.0716,  0.2283,  0.1276,  0.1177,  0.1210,  0.1433,  0.0358,  0.0456,\n                         0.2353,  0.1073,  0.1205,  0.0149,  0.0995,  0.1515,  0.1102,  0.2048,\n                         0.1486,  0.1582,  0.1282,  0.2255,  0.2387,  0.1199,  0.0370,  0.1432,\n                         0.0267,  0.1664,  0.1488,  0.1874,  0.2522,  0.1906,  0.2483,  0.0434,\n                         0.0311,  0.1038,  0.0260,  0.0151,  0.1196,  0.1044,  0.1397,  0.0481,\n                         0.0244,  0.0833,  0.1674,  0.1915,  0.1376,  0.0257,  0.0217,  0.3346,\n                         0.2297,  0.1492,  0.1908,  0.2317,  0.2247,  0.0442,  0.0941,  0.0355,\n                         0.1024,  0.0384,  0.0401,  0.0662,  0.1204,  0.1936,  0.1674,  0.1376,\n                         0.0435,  0.0243,  0.1904,  0.0478,  0.1114,  0.1014,  0.0656,  0.2426,\n                         0.1573,  0.1224,  0.2069,  0.0565,  0.1259,  0.2283,  0.0998,  0.1467,\n                         0.1783,  0.2550,  0.1226,  0.0722,  0.0488,  0.1411,  0.2347,  0.1663,\n                         0.0814,  0.1860,  0.1488,  0.0353,  0.2056,  0.2173,  0.0754,  0.1888,\n                         0.0999,  0.1926,  0.1307,  0.1196,  0.1012,  0.1140,  0.0610,  0.1294,\n                         0.0280,  0.2767,  0.0203,  0.1737,  0.2029,  0.2045,  0.2209,  0.1626,\n                         0.1933,  0.1544,  0.2528,  0.0497,  0.0888, -0.0044,  0.2024,  0.0857,\n                         0.1669,  0.1896,  0.2147,  0.1188,  0.1675,  0.2082,  0.0866,  0.1869,\n                         0.1727,  0.1322,  0.1300,  0.2103,  0.0719,  0.0302,  0.2295,  0.0286,\n                         0.1117,  0.1086,  0.1324,  0.1882,  0.0412,  0.0945,  0.0291,  0.2254,\n                         0.1508,  0.0491,  0.1638,  0.2447,  0.1110,  0.1972,  0.2670,  0.1264,\n                         0.0452,  0.1898,  0.0930,  0.0392,  0.1388,  0.1320,  0.0978,  0.1586,\n                         0.2365,  0.3439,  0.0240,  0.0623,  0.0771,  0.2125,  0.2191,  0.0354,\n                         0.1628,  0.1482,  0.0452,  0.1062,  0.1524,  0.0200,  0.1917,  0.0856,\n                         0.0520,  0.0403,  0.1558,  0.2314,  0.0250,  0.1217,  0.2477,  0.0135,\n                         0.0875,  0.1985,  0.1591,  0.1810,  0.0298,  0.1291,  0.2073,  0.1818,\n                         0.1254,  0.2684,  0.1298,  0.1140,  0.1700,  0.0727,  0.2249,  0.1056,\n                         0.0263,  0.0279,  0.1633,  0.0688,  0.0485,  0.1536,  0.3454,  0.1419,\n                         0.0982,  0.0463,  0.1164,  0.1330,  0.1294,  0.1355,  0.2038,  0.0879,\n                         0.1628,  0.1730,  0.1204,  0.2894,  0.0869,  0.1293,  0.1836,  0.0376,\n                         0.1641,  0.0355,  0.1146,  0.0162,  0.1180,  0.0092,  0.1845,  0.1108])\n              )\n            )\n          )\n          (2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(\n            in_features=768, out_features=3072, bias=True\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0014, 0.0012, 0.0018,  ..., 0.0014, 0.0012, 0.0014]), zero_point=tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([-0.1372, -0.1250, -0.1629,  ..., -0.1328, -0.1445, -0.1469]), max_val=tensor([0.1800, 0.1498, 0.2260,  ..., 0.1772, 0.1585, 0.1836]))\n            )\n          )\n          (4): GELU(approximate='none')\n          (5): Linear(\n            in_features=3072, out_features=768, bias=True\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0014, 0.0016, 0.0014, 0.0017, 0.0017, 0.0015, 0.0014, 0.0015, 0.0015,\n                      0.0026, 0.0018, 0.0016, 0.0019, 0.0018, 0.0025, 0.0023, 0.0024, 0.0015,\n                      0.0015, 0.0015, 0.0015, 0.0018, 0.0014, 0.0014, 0.0019, 0.0014, 0.0016,\n                      0.0015, 0.0016, 0.0016, 0.0015, 0.0014, 0.0016, 0.0013, 0.0016, 0.0016,\n                      0.0017, 0.0017, 0.0015, 0.0014, 0.0018, 0.0015, 0.0015, 0.0021, 0.0018,\n                      0.0017, 0.0014, 0.0015, 0.0016, 0.0017, 0.0013, 0.0014, 0.0016, 0.0016,\n                      0.0018, 0.0014, 0.0016, 0.0017, 0.0015, 0.0019, 0.0017, 0.0016, 0.0015,\n                      0.0015, 0.0014, 0.0015, 0.0014, 0.0017, 0.0019, 0.0015, 0.0014, 0.0013,\n                      0.0016, 0.0016, 0.0018, 0.0015, 0.0014, 0.0014, 0.0015, 0.0017, 0.0017,\n                      0.0016, 0.0017, 0.0017, 0.0017, 0.0015, 0.0016, 0.0026, 0.0014, 0.0016,\n                      0.0014, 0.0019, 0.0016, 0.0018, 0.0020, 0.0014, 0.0017, 0.0015, 0.0018,\n                      0.0015, 0.0016, 0.0015, 0.0015, 0.0017, 0.0016, 0.0016, 0.0018, 0.0017,\n                      0.0016, 0.0019, 0.0017, 0.0014, 0.0016, 0.0015, 0.0016, 0.0022, 0.0016,\n                      0.0016, 0.0018, 0.0016, 0.0014, 0.0017, 0.0015, 0.0014, 0.0017, 0.0013,\n                      0.0023, 0.0015, 0.0015, 0.0015, 0.0016, 0.0014, 0.0016, 0.0016, 0.0016,\n                      0.0016, 0.0014, 0.0016, 0.0014, 0.0015, 0.0014, 0.0015, 0.0015, 0.0017,\n                      0.0017, 0.0021, 0.0014, 0.0017, 0.0019, 0.0014, 0.0016, 0.0016, 0.0017,\n                      0.0015, 0.0017, 0.0015, 0.0015, 0.0017, 0.0015, 0.0016, 0.0017, 0.0015,\n                      0.0020, 0.0016, 0.0015, 0.0014, 0.0018, 0.0017, 0.0014, 0.0015, 0.0021,\n                      0.0015, 0.0018, 0.0015, 0.0017, 0.0017, 0.0013, 0.0013, 0.0016, 0.0016,\n                      0.0020, 0.0017, 0.0016, 0.0016, 0.0016, 0.0015, 0.0021, 0.0016, 0.0017,\n                      0.0017, 0.0017, 0.0016, 0.0016, 0.0016, 0.0016, 0.0017, 0.0015, 0.0016,\n                      0.0017, 0.0014, 0.0019, 0.0016, 0.0018, 0.0020, 0.0016, 0.0015, 0.0014,\n                      0.0015, 0.0016, 0.0015, 0.0015, 0.0016, 0.0018, 0.0055, 0.0018, 0.0014,\n                      0.0016, 0.0023, 0.0014, 0.0016, 0.0016, 0.0016, 0.0014, 0.0018, 0.0015,\n                      0.0019, 0.0017, 0.0019, 0.0016, 0.0015, 0.0020, 0.0015, 0.0015, 0.0015,\n                      0.0015, 0.0014, 0.0015, 0.0016, 0.0026, 0.0017, 0.0016, 0.0016, 0.0014,\n                      0.0015, 0.0015, 0.0016, 0.0014, 0.0014, 0.0017, 0.0020, 0.0019, 0.0017,\n                      0.0014, 0.0017, 0.0016, 0.0015, 0.0015, 0.0017, 0.0016, 0.0017, 0.0016,\n                      0.0017, 0.0015, 0.0016, 0.0013, 0.0019, 0.0044, 0.0016, 0.0014, 0.0016,\n                      0.0017, 0.0020, 0.0016, 0.0015, 0.0017, 0.0027, 0.0014, 0.0017, 0.0015,\n                      0.0017, 0.0015, 0.0015, 0.0016, 0.0022, 0.0019, 0.0018, 0.0015, 0.0015,\n                      0.0018, 0.0019, 0.0013, 0.0022, 0.0019, 0.0015, 0.0013, 0.0016, 0.0018,\n                      0.0017, 0.0022, 0.0018, 0.0016, 0.0017, 0.0017, 0.0015, 0.0015, 0.0016,\n                      0.0014, 0.0019, 0.0020, 0.0020, 0.0015, 0.0021, 0.0015, 0.0015, 0.0013,\n                      0.0014, 0.0016, 0.0068, 0.0016, 0.0014, 0.0017, 0.0015, 0.0014, 0.0018,\n                      0.0018, 0.0020, 0.0015, 0.0016, 0.0016, 0.0017, 0.0015, 0.0017, 0.0016,\n                      0.0018, 0.0015, 0.0016, 0.0015, 0.0019, 0.0017, 0.0014, 0.0018, 0.0014,\n                      0.0014, 0.0015, 0.0017, 0.0016, 0.0016, 0.0014, 0.0015, 0.0015, 0.0016,\n                      0.0016, 0.0014, 0.0015, 0.0015, 0.0014, 0.0015, 0.0017, 0.0016, 0.0015,\n                      0.0028, 0.0016, 0.0017, 0.0016, 0.0014, 0.0015, 0.0019, 0.0019, 0.0013,\n                      0.0017, 0.0018, 0.0014, 0.0014, 0.0016, 0.0020, 0.0017, 0.0016, 0.0017,\n                      0.0014, 0.0015, 0.0016, 0.0017, 0.0026, 0.0016, 0.0018, 0.0015, 0.0017,\n                      0.0019, 0.0014, 0.0016, 0.0016, 0.0021, 0.0016, 0.0015, 0.0016, 0.0027,\n                      0.0018, 0.0019, 0.0017, 0.0016, 0.0014, 0.0037, 0.0015, 0.0018, 0.0018,\n                      0.0017, 0.0015, 0.0019, 0.0016, 0.0022, 0.0016, 0.0013, 0.0014, 0.0015,\n                      0.0017, 0.0016, 0.0015, 0.0014, 0.0016, 0.0016, 0.0019, 0.0014, 0.0015,\n                      0.0018, 0.0014, 0.0016, 0.0016, 0.0017, 0.0016, 0.0015, 0.0018, 0.0016,\n                      0.0017, 0.0019, 0.0016, 0.0015, 0.0022, 0.0016, 0.0015, 0.0016, 0.0017,\n                      0.0015, 0.0015, 0.0015, 0.0016, 0.0015, 0.0016, 0.0019, 0.0018, 0.0019,\n                      0.0015, 0.0021, 0.0015, 0.0015, 0.0015, 0.0018, 0.0035, 0.0017, 0.0016,\n                      0.0015, 0.0017, 0.0018, 0.0017, 0.0017, 0.0017, 0.0017, 0.0018, 0.0016,\n                      0.0014, 0.0018, 0.0014, 0.0015, 0.0017, 0.0015, 0.0026, 0.0016, 0.0016,\n                      0.0016, 0.0016, 0.0015, 0.0016, 0.0020, 0.0016, 0.0016, 0.0014, 0.0018,\n                      0.0016, 0.0016, 0.0014, 0.0015, 0.0016, 0.0014, 0.0024, 0.0013, 0.0016,\n                      0.0014, 0.0016, 0.0016, 0.0014, 0.0015, 0.0017, 0.0014, 0.0017, 0.0015,\n                      0.0015, 0.0015, 0.0015, 0.0014, 0.0014, 0.0019, 0.0015, 0.0015, 0.0015,\n                      0.0015, 0.0015, 0.0015, 0.0014, 0.0017, 0.0018, 0.0020, 0.0015, 0.0025,\n                      0.0014, 0.0016, 0.0015, 0.0016, 0.0017, 0.0016, 0.0016, 0.0015, 0.0021,\n                      0.0013, 0.0017, 0.0015, 0.0015, 0.0017, 0.0016, 0.0016, 0.0017, 0.0015,\n                      0.0015, 0.0016, 0.0023, 0.0016, 0.0015, 0.0015, 0.0015, 0.0018, 0.0016,\n                      0.0015, 0.0014, 0.0017, 0.0017, 0.0025, 0.0014, 0.0014, 0.0014, 0.0015,\n                      0.0018, 0.0016, 0.0016, 0.0017, 0.0014, 0.0016, 0.0014, 0.0014, 0.0015,\n                      0.0015, 0.0017, 0.0026, 0.0015, 0.0016, 0.0015, 0.0016, 0.0016, 0.0015,\n                      0.0019, 0.0015, 0.0014, 0.0016, 0.0020, 0.0018, 0.0016, 0.0026, 0.0021,\n                      0.0017, 0.0018, 0.0015, 0.0014, 0.0019, 0.0017, 0.0018, 0.0015, 0.0015,\n                      0.0016, 0.0016, 0.0016, 0.0015, 0.0015, 0.0015, 0.0016, 0.0016, 0.0014,\n                      0.0019, 0.0019, 0.0019, 0.0019, 0.0015, 0.0018, 0.0015, 0.0015, 0.0017,\n                      0.0018, 0.0016, 0.0034, 0.0015, 0.0014, 0.0014, 0.0016, 0.0016, 0.0016,\n                      0.0025, 0.0014, 0.0016, 0.0017, 0.0014, 0.0025, 0.0020, 0.0015, 0.0014,\n                      0.0045, 0.0013, 0.0014, 0.0016, 0.0014, 0.0017, 0.0016, 0.0022, 0.0017,\n                      0.0016, 0.0015, 0.0017, 0.0019, 0.0015, 0.0015, 0.0016, 0.0015, 0.0018,\n                      0.0015, 0.0016, 0.0019, 0.0016, 0.0015, 0.0021, 0.0015, 0.0014, 0.0028,\n                      0.0016, 0.0016, 0.0018, 0.0020, 0.0015, 0.0021, 0.0015, 0.0018, 0.0016,\n                      0.0017, 0.0014, 0.0016, 0.0016, 0.0015, 0.0016, 0.0017, 0.0016, 0.0015,\n                      0.0015, 0.0015, 0.0016, 0.0015, 0.0015, 0.0014, 0.0015, 0.0015, 0.0017,\n                      0.0015, 0.0017, 0.0014, 0.0016, 0.0017, 0.0013, 0.0015, 0.0017, 0.0014,\n                      0.0014, 0.0014, 0.0017, 0.0015, 0.0014, 0.0016, 0.0017, 0.0015, 0.0017,\n                      0.0017, 0.0014, 0.0016, 0.0015, 0.0017, 0.0049, 0.0016, 0.0017, 0.0019,\n                      0.0017, 0.0014, 0.0015, 0.0020, 0.0015, 0.0015, 0.0016, 0.0014, 0.0016,\n                      0.0015, 0.0015, 0.0016, 0.0015, 0.0016, 0.0016, 0.0017, 0.0015, 0.0015,\n                      0.0016, 0.0018, 0.0018, 0.0017, 0.0016, 0.0016, 0.0015, 0.0017, 0.0019,\n                      0.0014, 0.0015, 0.0021, 0.0016, 0.0016, 0.0017, 0.0019, 0.0016, 0.0016,\n                      0.0015, 0.0018, 0.0031, 0.0014, 0.0020, 0.0016, 0.0014, 0.0016, 0.0014,\n                      0.0016, 0.0014, 0.0018, 0.0015, 0.0015, 0.0016, 0.0016, 0.0017, 0.0017,\n                      0.0021, 0.0016, 0.0017]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                     dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n                min_val=tensor([-0.1804, -0.2036, -0.1737, -0.1998, -0.1841, -0.1959, -0.1829, -0.1905,\n                        -0.1887, -0.3307, -0.2283, -0.1993, -0.2093, -0.1945, -0.2241, -0.2933,\n                        -0.2649, -0.1650, -0.1844, -0.1621, -0.1733, -0.1644, -0.1818, -0.1824,\n                        -0.2491, -0.1684, -0.1859, -0.1959, -0.1701, -0.2049, -0.1836, -0.1734,\n                        -0.2087, -0.1599, -0.1885, -0.1879, -0.1926, -0.2227, -0.1900, -0.1840,\n                        -0.2357, -0.1796, -0.1623, -0.2300, -0.2255, -0.2124, -0.1745, -0.1924,\n                        -0.2016, -0.2005, -0.1654, -0.1608, -0.1996, -0.1853, -0.1732, -0.1771,\n                        -0.1926, -0.1735, -0.1874, -0.1764, -0.2186, -0.2019, -0.1950, -0.1794,\n                        -0.1775, -0.1572, -0.1829, -0.1877, -0.2438, -0.1680, -0.1750, -0.1688,\n                        -0.1806, -0.2020, -0.2248, -0.1896, -0.1624, -0.1813, -0.1888, -0.1821,\n                        -0.2162, -0.1993, -0.1998, -0.2152, -0.2199, -0.1927, -0.2014, -0.2932,\n                        -0.1826, -0.1993, -0.1832, -0.1935, -0.1981, -0.2060, -0.1695, -0.1745,\n                        -0.2211, -0.1840, -0.2319, -0.1838, -0.2031, -0.1719, -0.1703, -0.2023,\n                        -0.1697, -0.2076, -0.2335, -0.2186, -0.2002, -0.1709, -0.1750, -0.1819,\n                        -0.2004, -0.1826, -0.1973, -0.2712, -0.1607, -0.1788, -0.1790, -0.1741,\n                        -0.1591, -0.2203, -0.1786, -0.1747, -0.2236, -0.1665, -0.2999, -0.1603,\n                        -0.1886, -0.1841, -0.2031, -0.1661, -0.1889, -0.1773, -0.1868, -0.2003,\n                        -0.1830, -0.2039, -0.1791, -0.1916, -0.1758, -0.1918, -0.1554, -0.2169,\n                        -0.2147, -0.2652, -0.1841, -0.2190, -0.1619, -0.1827, -0.1760, -0.2096,\n                        -0.2122, -0.1932, -0.1995, -0.1743, -0.1953, -0.2141, -0.1974, -0.1996,\n                        -0.1946, -0.1933, -0.1789, -0.1989, -0.1959, -0.1788, -0.2325, -0.2177,\n                        -0.1843, -0.1823, -0.2710, -0.1815, -0.2354, -0.1905, -0.2170, -0.2176,\n                        -0.1645, -0.1719, -0.1838, -0.2007, -0.2546, -0.1778, -0.2004, -0.2051,\n                        -0.1913, -0.1701, -0.2304, -0.1775, -0.2223, -0.2204, -0.1776, -0.2034,\n                        -0.1936, -0.2067, -0.2009, -0.2127, -0.1936, -0.1634, -0.1867, -0.1854,\n                        -0.2485, -0.1956, -0.1830, -0.2611, -0.1698, -0.1984, -0.1829, -0.1750,\n                        -0.2058, -0.1930, -0.1554, -0.2009, -0.2037, -0.7075, -0.2290, -0.1749,\n                        -0.1748, -0.1838, -0.1794, -0.1797, -0.1839, -0.2024, -0.1651, -0.1813,\n                        -0.1921, -0.2450, -0.1931, -0.2013, -0.1826, -0.1929, -0.2559, -0.1919,\n                        -0.1705, -0.1654, -0.1806, -0.1776, -0.1917, -0.1985, -0.3382, -0.2126,\n                        -0.1993, -0.2093, -0.1746, -0.1963, -0.1973, -0.1827, -0.1715, -0.1828,\n                        -0.2084, -0.2586, -0.2407, -0.2137, -0.1848, -0.1696, -0.1821, -0.1859,\n                        -0.1904, -0.2119, -0.2038, -0.2145, -0.1710, -0.2199, -0.1914, -0.1730,\n                        -0.1505, -0.2422, -0.5585, -0.2022, -0.1803, -0.1880, -0.2074, -0.1999,\n                        -0.1843, -0.1775, -0.1891, -0.3018, -0.1655, -0.2004, -0.1856, -0.2117,\n                        -0.1759, -0.1878, -0.2033, -0.2070, -0.2421, -0.2270, -0.1961, -0.1691,\n                        -0.2341, -0.2450, -0.1722, -0.2866, -0.2055, -0.1835, -0.1674, -0.1948,\n                        -0.1911, -0.1664, -0.2807, -0.1664, -0.2007, -0.2165, -0.1750, -0.1787,\n                        -0.1978, -0.2040, -0.1732, -0.2397, -0.2520, -0.2585, -0.1910, -0.2634,\n                        -0.1875, -0.1853, -0.1613, -0.1832, -0.2008, -0.8658, -0.2020, -0.1791,\n                        -0.1662, -0.1924, -0.1847, -0.1700, -0.2072, -0.2505, -0.1789, -0.1994,\n                        -0.1836, -0.2215, -0.1758, -0.1552, -0.2058, -0.1773, -0.1984, -0.1843,\n                        -0.1978, -0.1854, -0.2169, -0.1850, -0.2275, -0.1678, -0.1722, -0.1949,\n                        -0.1734, -0.2098, -0.1842, -0.1812, -0.1715, -0.1904, -0.1994, -0.1899,\n                        -0.1834, -0.1936, -0.1843, -0.1615, -0.1584, -0.2198, -0.1941, -0.1931,\n                        -0.2286, -0.1787, -0.2112, -0.1916, -0.1773, -0.1911, -0.2408, -0.1905,\n                        -0.1632, -0.1740, -0.2039, -0.1837, -0.1819, -0.1702, -0.2552, -0.2158,\n                        -0.1984, -0.1670, -0.1764, -0.1877, -0.1901, -0.2152, -0.3355, -0.1627,\n                        -0.1916, -0.1942, -0.2154, -0.2143, -0.1645, -0.2014, -0.2051, -0.2683,\n                        -0.2048, -0.1902, -0.1801, -0.3463, -0.1600, -0.1907, -0.1774, -0.2025,\n                        -0.1718, -0.3966, -0.1952, -0.2249, -0.2340, -0.2011, -0.1943, -0.1971,\n                        -0.2012, -0.2871, -0.2007, -0.1680, -0.1751, -0.1865, -0.2235, -0.1991,\n                        -0.1789, -0.1736, -0.2017, -0.2056, -0.1793, -0.1795, -0.1872, -0.2030,\n                        -0.1782, -0.2087, -0.2035, -0.2057, -0.1944, -0.1874, -0.2254, -0.1871,\n                        -0.1872, -0.2391, -0.1654, -0.1964, -0.2625, -0.1835, -0.1920, -0.2007,\n                        -0.2168, -0.1747, -0.1782, -0.1626, -0.2068, -0.1926, -0.2084, -0.2404,\n                        -0.2296, -0.1711, -0.1907, -0.2311, -0.1787, -0.1810, -0.1741, -0.2361,\n                        -0.4475, -0.2055, -0.1783, -0.1841, -0.2140, -0.1879, -0.1896, -0.2185,\n                        -0.1912, -0.2215, -0.1927, -0.1744, -0.1714, -0.2296, -0.1854, -0.1708,\n                        -0.1964, -0.1769, -0.3145, -0.1885, -0.2005, -0.1815, -0.1933, -0.1447,\n                        -0.1948, -0.1987, -0.1672, -0.1819, -0.1674, -0.1736, -0.2024, -0.1873,\n                        -0.1788, -0.1633, -0.1852, -0.1765, -0.3102, -0.1718, -0.2019, -0.1769,\n                        -0.1986, -0.2082, -0.1743, -0.1850, -0.1867, -0.1814, -0.2167, -0.1831,\n                        -0.1863, -0.1727, -0.1857, -0.1756, -0.1738, -0.2373, -0.1868, -0.1749,\n                        -0.1907, -0.1961, -0.1961, -0.1856, -0.1829, -0.2240, -0.1740, -0.2566,\n                        -0.1885, -0.3197, -0.1811, -0.2057, -0.1894, -0.2073, -0.2117, -0.1832,\n                        -0.2023, -0.1858, -0.2703, -0.1682, -0.2180, -0.1965, -0.1863, -0.1741,\n                        -0.2080, -0.1757, -0.1860, -0.1876, -0.1870, -0.2072, -0.2908, -0.1518,\n                        -0.1755, -0.1927, -0.1961, -0.2297, -0.2037, -0.1865, -0.1712, -0.2134,\n                        -0.2182, -0.3205, -0.1750, -0.1711, -0.1806, -0.1857, -0.2186, -0.1844,\n                        -0.1799, -0.2076, -0.1766, -0.2085, -0.1832, -0.1782, -0.1829, -0.1831,\n                        -0.1742, -0.3321, -0.1965, -0.2066, -0.1752, -0.1754, -0.2096, -0.1979,\n                        -0.1772, -0.1797, -0.1796, -0.2009, -0.1995, -0.2283, -0.2010, -0.2469,\n                        -0.2066, -0.2138, -0.2263, -0.1911, -0.1593, -0.2442, -0.1989, -0.2248,\n                        -0.1933, -0.1885, -0.1791, -0.1945, -0.2056, -0.1936, -0.1785, -0.1941,\n                        -0.2036, -0.2102, -0.1713, -0.2456, -0.1689, -0.2483, -0.2477, -0.1775,\n                        -0.1672, -0.1980, -0.1729, -0.2058, -0.2342, -0.2081, -0.4410, -0.1673,\n                        -0.1795, -0.1757, -0.2002, -0.2043, -0.2023, -0.2940, -0.1802, -0.2044,\n                        -0.1709, -0.1771, -0.3144, -0.2532, -0.1864, -0.1691, -0.4312, -0.1711,\n                        -0.1820, -0.1682, -0.1812, -0.2078, -0.2038, -0.2769, -0.2072, -0.1960,\n                        -0.1728, -0.1789, -0.2456, -0.1921, -0.1982, -0.1924, -0.1929, -0.2325,\n                        -0.1866, -0.1751, -0.2033, -0.1848, -0.1930, -0.2715, -0.1807, -0.1823,\n                        -0.2219, -0.2040, -0.1774, -0.1941, -0.2617, -0.1868, -0.2703, -0.1924,\n                        -0.1728, -0.1914, -0.2235, -0.1771, -0.1897, -0.1859, -0.1809, -0.2052,\n                        -0.1771, -0.2006, -0.1895, -0.1797, -0.1812, -0.1792, -0.1921, -0.1782,\n                        -0.1781, -0.1697, -0.1701, -0.2120, -0.1887, -0.2175, -0.1643, -0.1607,\n                        -0.1724, -0.1668, -0.1620, -0.2052, -0.1728, -0.1752, -0.1774, -0.2117,\n                        -0.1677, -0.1786, -0.1840, -0.1833, -0.1916, -0.1713, -0.1758, -0.1660,\n                        -0.2025, -0.1923, -0.2113, -0.6216, -0.2059, -0.1747, -0.2409, -0.2064,\n                        -0.1843, -0.1972, -0.1711, -0.1947, -0.1864, -0.1977, -0.1755, -0.2097,\n                        -0.1743, -0.1756, -0.1745, -0.1933, -0.1947, -0.2036, -0.1756, -0.1737,\n                        -0.1751, -0.1811, -0.2338, -0.2279, -0.2220, -0.1965, -0.1778, -0.1878,\n                        -0.2193, -0.1796, -0.1734, -0.1951, -0.2066, -0.2110, -0.1924, -0.2060,\n                        -0.2399, -0.2024, -0.1855, -0.1904, -0.1837, -0.3428, -0.1696, -0.1900,\n                        -0.2069, -0.1852, -0.1783, -0.1840, -0.2019, -0.1599, -0.2310, -0.1857,\n                        -0.1979, -0.2006, -0.1934, -0.2209, -0.2151, -0.2630, -0.1986, -0.1779]), max_val=tensor([0.1834, 0.1697, 0.1702, 0.2099, 0.2163, 0.1729, 0.1783, 0.1632, 0.1823,\n                        0.3104, 0.1551, 0.1953, 0.2372, 0.2239, 0.3162, 0.2774, 0.3095, 0.1957,\n                        0.1913, 0.1872, 0.1914, 0.2258, 0.1730, 0.1704, 0.1990, 0.1819, 0.1981,\n                        0.1913, 0.2067, 0.1950, 0.1865, 0.1761, 0.1758, 0.1615, 0.2005, 0.2052,\n                        0.2159, 0.1982, 0.1890, 0.1677, 0.1793, 0.1932, 0.1871, 0.2677, 0.2201,\n                        0.1913, 0.1818, 0.1583, 0.1885, 0.2193, 0.1621, 0.1725, 0.1551, 0.1994,\n                        0.2272, 0.1788, 0.1995, 0.2116, 0.1801, 0.2425, 0.1923, 0.1726, 0.1755,\n                        0.1852, 0.1818, 0.1845, 0.1805, 0.2147, 0.2188, 0.1861, 0.1821, 0.1666,\n                        0.2082, 0.2088, 0.1986, 0.1922, 0.1753, 0.1753, 0.1721, 0.2101, 0.2194,\n                        0.1718, 0.2145, 0.1907, 0.2056, 0.1827, 0.2040, 0.3319, 0.1630, 0.1648,\n                        0.1836, 0.2360, 0.1979, 0.2230, 0.2487, 0.1560, 0.1825, 0.1945, 0.1627,\n                        0.1944, 0.1875, 0.1860, 0.1909, 0.2117, 0.2005, 0.1950, 0.1937, 0.1766,\n                        0.1886, 0.2409, 0.2147, 0.1839, 0.2022, 0.1901, 0.2030, 0.2755, 0.2006,\n                        0.2022, 0.2245, 0.2017, 0.1719, 0.2013, 0.1872, 0.1833, 0.1973, 0.1704,\n                        0.2519, 0.1905, 0.1608, 0.1932, 0.1861, 0.1731, 0.2019, 0.2023, 0.2015,\n                        0.1841, 0.1823, 0.1857, 0.1789, 0.1909, 0.1754, 0.1947, 0.1889, 0.1518,\n                        0.1815, 0.2520, 0.1819, 0.2220, 0.2375, 0.1813, 0.2058, 0.1823, 0.2059,\n                        0.1881, 0.2120, 0.1847, 0.1889, 0.2123, 0.1804, 0.1904, 0.2195, 0.1802,\n                        0.2554, 0.1948, 0.1720, 0.1587, 0.2077, 0.1878, 0.1805, 0.1959, 0.2510,\n                        0.1869, 0.1601, 0.1687, 0.2118, 0.1927, 0.1604, 0.1410, 0.2002, 0.1766,\n                        0.1819, 0.2146, 0.1935, 0.1775, 0.1977, 0.1869, 0.2630, 0.2064, 0.1810,\n                        0.2060, 0.2112, 0.1861, 0.1984, 0.1933, 0.1873, 0.1604, 0.1914, 0.2042,\n                        0.2172, 0.1790, 0.1861, 0.2031, 0.2263, 0.1766, 0.2071, 0.1885, 0.1800,\n                        0.1889, 0.1668, 0.1931, 0.1861, 0.1899, 0.2266, 0.2900, 0.2265, 0.1499,\n                        0.2079, 0.2933, 0.1828, 0.2045, 0.2007, 0.1888, 0.1801, 0.2225, 0.1800,\n                        0.1756, 0.2114, 0.2413, 0.1983, 0.1689, 0.2329, 0.1830, 0.1909, 0.1946,\n                        0.1883, 0.1837, 0.1783, 0.1755, 0.3127, 0.2052, 0.1983, 0.1783, 0.1804,\n                        0.1908, 0.1940, 0.2044, 0.1729, 0.1671, 0.2107, 0.1795, 0.1930, 0.1766,\n                        0.1740, 0.2194, 0.2082, 0.1946, 0.1880, 0.1896, 0.1851, 0.1865, 0.2031,\n                        0.1633, 0.1794, 0.1992, 0.1631, 0.1818, 0.4160, 0.1735, 0.1665, 0.1975,\n                        0.2121, 0.2554, 0.2011, 0.1889, 0.2149, 0.3376, 0.1761, 0.2110, 0.1955,\n                        0.1866, 0.1853, 0.1864, 0.1903, 0.2831, 0.1988, 0.1723, 0.1899, 0.1941,\n                        0.2246, 0.1799, 0.1579, 0.2416, 0.2403, 0.1848, 0.1620, 0.2038, 0.2231,\n                        0.2146, 0.1650, 0.2275, 0.1909, 0.1899, 0.2112, 0.1943, 0.1863, 0.2031,\n                        0.1760, 0.2275, 0.1880, 0.2174, 0.1844, 0.1974, 0.1729, 0.1904, 0.1672,\n                        0.1700, 0.1898, 0.4612, 0.1931, 0.1770, 0.2156, 0.1917, 0.1773, 0.2342,\n                        0.2332, 0.2190, 0.1857, 0.1888, 0.2084, 0.1969, 0.1958, 0.2149, 0.1764,\n                        0.2256, 0.1862, 0.2032, 0.1866, 0.2467, 0.2157, 0.1684, 0.1937, 0.1749,\n                        0.1782, 0.1809, 0.2108, 0.1926, 0.2041, 0.1671, 0.1884, 0.1854, 0.1870,\n                        0.2094, 0.1602, 0.1696, 0.1855, 0.1787, 0.1878, 0.1980, 0.1970, 0.1555,\n                        0.3544, 0.2038, 0.1737, 0.2072, 0.1790, 0.1729, 0.2019, 0.2397, 0.1685,\n                        0.2103, 0.2323, 0.1763, 0.1670, 0.2039, 0.1884, 0.2026, 0.1933, 0.2121,\n                        0.1757, 0.1922, 0.2043, 0.1967, 0.3096, 0.1983, 0.2241, 0.1885, 0.2074,\n                        0.2453, 0.1718, 0.1982, 0.2044, 0.2013, 0.1994, 0.1755, 0.1996, 0.2660,\n                        0.2233, 0.2355, 0.2140, 0.1816, 0.1789, 0.4747, 0.1633, 0.1743, 0.2306,\n                        0.2096, 0.1641, 0.2460, 0.1736, 0.1980, 0.1815, 0.1677, 0.1809, 0.1593,\n                        0.2010, 0.1766, 0.1891, 0.1772, 0.1846, 0.1949, 0.2475, 0.1758, 0.1760,\n                        0.2289, 0.1668, 0.1846, 0.1783, 0.2149, 0.2015, 0.1880, 0.2203, 0.1970,\n                        0.2132, 0.2059, 0.2044, 0.1716, 0.2819, 0.2002, 0.1734, 0.1538, 0.2166,\n                        0.1963, 0.1874, 0.1882, 0.1997, 0.1918, 0.1830, 0.1779, 0.1729, 0.2388,\n                        0.1650, 0.2626, 0.1938, 0.1883, 0.1964, 0.2123, 0.2248, 0.2115, 0.2028,\n                        0.1962, 0.2022, 0.2343, 0.2209, 0.2011, 0.2149, 0.1675, 0.2232, 0.2023,\n                        0.1818, 0.1817, 0.1818, 0.1903, 0.2108, 0.1871, 0.3281, 0.2093, 0.1730,\n                        0.2030, 0.1992, 0.1934, 0.2084, 0.2553, 0.2007, 0.2056, 0.1835, 0.2286,\n                        0.2043, 0.2027, 0.1804, 0.1918, 0.2053, 0.1837, 0.2972, 0.1672, 0.2000,\n                        0.1756, 0.1853, 0.2039, 0.1771, 0.1884, 0.2170, 0.1555, 0.2034, 0.1890,\n                        0.1914, 0.1898, 0.1863, 0.1781, 0.1630, 0.1988, 0.1793, 0.1907, 0.1793,\n                        0.1918, 0.1841, 0.1678, 0.1836, 0.1683, 0.2232, 0.1983, 0.1626, 0.1873,\n                        0.1739, 0.1804, 0.1866, 0.1842, 0.1758, 0.2004, 0.1771, 0.1674, 0.1892,\n                        0.1702, 0.2193, 0.1833, 0.1664, 0.2125, 0.1961, 0.2026, 0.2168, 0.1842,\n                        0.1790, 0.1666, 0.2565, 0.2010, 0.1874, 0.1951, 0.1562, 0.1812, 0.1895,\n                        0.1804, 0.1817, 0.2080, 0.1835, 0.3044, 0.1820, 0.1830, 0.1673, 0.1852,\n                        0.2326, 0.2069, 0.2052, 0.2098, 0.1738, 0.1769, 0.1785, 0.1766, 0.1889,\n                        0.1855, 0.2098, 0.3178, 0.1743, 0.1957, 0.1842, 0.2047, 0.1881, 0.1692,\n                        0.2415, 0.1932, 0.1840, 0.1760, 0.2521, 0.1971, 0.1913, 0.3314, 0.2657,\n                        0.1705, 0.1974, 0.1913, 0.1796, 0.1869, 0.2161, 0.1742, 0.1828, 0.1574,\n                        0.1975, 0.2019, 0.1942, 0.1949, 0.1863, 0.1601, 0.2016, 0.1752, 0.1807,\n                        0.1790, 0.2373, 0.1977, 0.1845, 0.1954, 0.2236, 0.1632, 0.1919, 0.2125,\n                        0.2269, 0.1686, 0.3910, 0.1885, 0.1784, 0.1799, 0.2002, 0.1734, 0.1781,\n                        0.3193, 0.1837, 0.1939, 0.2178, 0.1645, 0.2164, 0.2127, 0.1785, 0.1812,\n                        0.5766, 0.1646, 0.1728, 0.1991, 0.1808, 0.2103, 0.1919, 0.2729, 0.2137,\n                        0.2003, 0.1956, 0.2166, 0.1975, 0.1597, 0.1787, 0.1973, 0.1905, 0.2020,\n                        0.1772, 0.2068, 0.2451, 0.2076, 0.1963, 0.2043, 0.1951, 0.1704, 0.3541,\n                        0.1995, 0.2059, 0.2345, 0.2434, 0.1846, 0.2708, 0.1791, 0.2276, 0.2065,\n                        0.1974, 0.1815, 0.2058, 0.2032, 0.1876, 0.1817, 0.2194, 0.1775, 0.1624,\n                        0.1946, 0.1952, 0.2093, 0.1713, 0.1936, 0.1737, 0.1869, 0.1844, 0.2074,\n                        0.1904, 0.2043, 0.1728, 0.1969, 0.2106, 0.1534, 0.1900, 0.2152, 0.1731,\n                        0.1835, 0.1837, 0.1835, 0.1932, 0.1688, 0.2019, 0.2179, 0.1786, 0.2182,\n                        0.2179, 0.1836, 0.1980, 0.1798, 0.1766, 0.5120, 0.1681, 0.2186, 0.1713,\n                        0.2142, 0.1677, 0.1768, 0.2532, 0.1870, 0.1920, 0.2008, 0.1787, 0.1803,\n                        0.1941, 0.1961, 0.2051, 0.1781, 0.1969, 0.1844, 0.2175, 0.1866, 0.1931,\n                        0.2014, 0.1797, 0.2210, 0.2158, 0.1990, 0.2025, 0.1840, 0.1809, 0.2440,\n                        0.1674, 0.1691, 0.2724, 0.1915, 0.2093, 0.2107, 0.1940, 0.2023, 0.2095,\n                        0.1941, 0.2348, 0.3884, 0.1836, 0.2481, 0.1858, 0.1823, 0.2026, 0.1714,\n                        0.1712, 0.1840, 0.1905, 0.1855, 0.1762, 0.1855, 0.1995, 0.1726, 0.1735,\n                        0.2440, 0.1966, 0.2132])\n              )\n            )\n          )\n        )\n      )\n      (2): Module(\n        (block): Module(\n          (0): Conv2d(\n            768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0020, 0.0022, 0.0016, 0.0016, 0.0013, 0.0008, 0.0017, 0.0020, 0.0017,\n                      0.0008, 0.0019, 0.0011, 0.0015, 0.0007, 0.0045, 0.0009, 0.0009, 0.0016,\n                      0.0018, 0.0006, 0.0011, 0.0014, 0.0018, 0.0006, 0.0010, 0.0007, 0.0014,\n                      0.0011, 0.0010, 0.0007, 0.0008, 0.0005, 0.0028, 0.0012, 0.0010, 0.0014,\n                      0.0009, 0.0008, 0.0013, 0.0004, 0.0017, 0.0017, 0.0017, 0.0013, 0.0013,\n                      0.0006, 0.0005, 0.0010, 0.0014, 0.0015, 0.0018, 0.0012, 0.0023, 0.0017,\n                      0.0006, 0.0017, 0.0013, 0.0015, 0.0006, 0.0014, 0.0018, 0.0009, 0.0015,\n                      0.0017, 0.0005, 0.0016, 0.0007, 0.0014, 0.0009, 0.0009, 0.0010, 0.0020,\n                      0.0016, 0.0012, 0.0019, 0.0024, 0.0011, 0.0011, 0.0017, 0.0003, 0.0013,\n                      0.0002, 0.0008, 0.0006, 0.0010, 0.0008, 0.0011, 0.0015, 0.0012, 0.0008,\n                      0.0009, 0.0010, 0.0014, 0.0016, 0.0016, 0.0014, 0.0011, 0.0016, 0.0016,\n                      0.0020, 0.0009, 0.0016, 0.0006, 0.0019, 0.0015, 0.0002, 0.0013, 0.0015,\n                      0.0015, 0.0009, 0.0012, 0.0015, 0.0017, 0.0019, 0.0016, 0.0014, 0.0017,\n                      0.0008, 0.0015, 0.0014, 0.0007, 0.0008, 0.0013, 0.0027, 0.0008, 0.0018,\n                      0.0006, 0.0019, 0.0012, 0.0025, 0.0031, 0.0017, 0.0006, 0.0009, 0.0016,\n                      0.0015, 0.0019, 0.0016, 0.0006, 0.0016, 0.0011, 0.0014, 0.0023, 0.0013,\n                      0.0008, 0.0007, 0.0018, 0.0010, 0.0016, 0.0017, 0.0017, 0.0014, 0.0011,\n                      0.0013, 0.0012, 0.0006, 0.0017, 0.0010, 0.0013, 0.0008, 0.0018, 0.0005,\n                      0.0016, 0.0017, 0.0014, 0.0016, 0.0016, 0.0013, 0.0010, 0.0016, 0.0010,\n                      0.0023, 0.0016, 0.0025, 0.0018, 0.0008, 0.0018, 0.0020, 0.0009, 0.0017,\n                      0.0012, 0.0009, 0.0012, 0.0008, 0.0015, 0.0017, 0.0008, 0.0006, 0.0012,\n                      0.0011, 0.0016, 0.0020, 0.0006, 0.0008, 0.0006, 0.0007, 0.0008, 0.0005,\n                      0.0015, 0.0017, 0.0008, 0.0011, 0.0012, 0.0007, 0.0013, 0.0013, 0.0009,\n                      0.0012, 0.0005, 0.0025, 0.0010, 0.0009, 0.0007, 0.0008, 0.0013, 0.0015,\n                      0.0010, 0.0010, 0.0022, 0.0019, 0.0011, 0.0016, 0.0016, 0.0018, 0.0008,\n                      0.0015, 0.0014, 0.0009, 0.0006, 0.0014, 0.0026, 0.0017, 0.0011, 0.0019,\n                      0.0012, 0.0015, 0.0026, 0.0014, 0.0016, 0.0013, 0.0008, 0.0019, 0.0015,\n                      0.0017, 0.0009, 0.0017, 0.0014, 0.0019, 0.0016, 0.0031, 0.0019, 0.0008,\n                      0.0017, 0.0010, 0.0007, 0.0019, 0.0015, 0.0015, 0.0016, 0.0011, 0.0010,\n                      0.0013, 0.0018, 0.0011, 0.0013, 0.0009, 0.0019, 0.0009, 0.0021, 0.0013,\n                      0.0013, 0.0018, 0.0005, 0.0028, 0.0018, 0.0015, 0.0015, 0.0020, 0.0015,\n                      0.0018, 0.0012, 0.0011, 0.0020, 0.0015, 0.0014, 0.0020, 0.0016, 0.0018,\n                      0.0018, 0.0017, 0.0008, 0.0008, 0.0008, 0.0013, 0.0017, 0.0015, 0.0010,\n                      0.0011, 0.0016, 0.0026, 0.0018, 0.0010, 0.0007, 0.0022, 0.0019, 0.0010,\n                      0.0017, 0.0010, 0.0016, 0.0008, 0.0006, 0.0008, 0.0009, 0.0019, 0.0018,\n                      0.0018, 0.0006, 0.0026, 0.0014, 0.0019, 0.0011, 0.0019, 0.0012, 0.0014,\n                      0.0012, 0.0009, 0.0009, 0.0013, 0.0010, 0.0016, 0.0019, 0.0018, 0.0007,\n                      0.0009, 0.0008, 0.0020, 0.0004, 0.0019, 0.0013, 0.0006, 0.0007, 0.0017,\n                      0.0015, 0.0008, 0.0017, 0.0006, 0.0019, 0.0018, 0.0018, 0.0018, 0.0013,\n                      0.0013, 0.0009, 0.0017, 0.0014, 0.0015, 0.0014, 0.0011, 0.0004, 0.0018,\n                      0.0009, 0.0014, 0.0016, 0.0009, 0.0010, 0.0010, 0.0013, 0.0010, 0.0023,\n                      0.0019, 0.0013, 0.0005, 0.0019, 0.0008, 0.0019, 0.0021, 0.0014, 0.0010,\n                      0.0019, 0.0013, 0.0006, 0.0011, 0.0008, 0.0017, 0.0012, 0.0010, 0.0006,\n                      0.0006, 0.0016, 0.0006, 0.0016, 0.0009, 0.0018, 0.0012, 0.0009, 0.0016,\n                      0.0013, 0.0009, 0.0013, 0.0007, 0.0018, 0.0004, 0.0009, 0.0028, 0.0011,\n                      0.0016, 0.0019, 0.0008, 0.0011, 0.0007, 0.0008, 0.0007, 0.0018, 0.0014,\n                      0.0004, 0.0016, 0.0013, 0.0012, 0.0016, 0.0019, 0.0031, 0.0018, 0.0012,\n                      0.0014, 0.0004, 0.0018, 0.0010, 0.0017, 0.0005, 0.0005, 0.0011, 0.0017,\n                      0.0008, 0.0010, 0.0012, 0.0017, 0.0009, 0.0008, 0.0009, 0.0018, 0.0005,\n                      0.0008, 0.0020, 0.0022, 0.0015, 0.0019, 0.0019, 0.0012, 0.0016, 0.0029,\n                      0.0008, 0.0018, 0.0018, 0.0026, 0.0012, 0.0007, 0.0009, 0.0008, 0.0017,\n                      0.0009, 0.0012, 0.0018, 0.0022, 0.0013, 0.0016, 0.0004, 0.0009, 0.0016,\n                      0.0004, 0.0012, 0.0010, 0.0014, 0.0017, 0.0014, 0.0008, 0.0007, 0.0016,\n                      0.0019, 0.0008, 0.0015, 0.0013, 0.0009, 0.0018, 0.0007, 0.0005, 0.0009,\n                      0.0017, 0.0008, 0.0012, 0.0007, 0.0009, 0.0014, 0.0008, 0.0017, 0.0019,\n                      0.0006, 0.0023, 0.0008, 0.0008, 0.0005, 0.0018, 0.0016, 0.0010, 0.0014,\n                      0.0014, 0.0012, 0.0007, 0.0017, 0.0011, 0.0010, 0.0009, 0.0019, 0.0017,\n                      0.0005, 0.0025, 0.0017, 0.0015, 0.0017, 0.0017, 0.0017, 0.0015, 0.0026,\n                      0.0016, 0.0007, 0.0017, 0.0018, 0.0007, 0.0011, 0.0007, 0.0017, 0.0014,\n                      0.0010, 0.0026, 0.0013, 0.0017, 0.0016, 0.0006, 0.0023, 0.0016, 0.0014,\n                      0.0017, 0.0020, 0.0009, 0.0029, 0.0017, 0.0007, 0.0017, 0.0016, 0.0007,\n                      0.0012, 0.0008, 0.0017, 0.0010, 0.0015, 0.0017, 0.0013, 0.0013, 0.0017,\n                      0.0018, 0.0018, 0.0006, 0.0008, 0.0018, 0.0006, 0.0020, 0.0013, 0.0016,\n                      0.0015, 0.0006, 0.0007, 0.0007, 0.0012, 0.0016, 0.0015, 0.0007, 0.0020,\n                      0.0010, 0.0008, 0.0017, 0.0017, 0.0011, 0.0007, 0.0009, 0.0012, 0.0017,\n                      0.0009, 0.0016, 0.0016, 0.0021, 0.0027, 0.0010, 0.0006, 0.0019, 0.0017,\n                      0.0005, 0.0008, 0.0017, 0.0013, 0.0014, 0.0018, 0.0013, 0.0011, 0.0014,\n                      0.0007, 0.0007, 0.0007, 0.0011, 0.0018, 0.0018, 0.0012, 0.0016, 0.0013,\n                      0.0010, 0.0012, 0.0030, 0.0018, 0.0017, 0.0015, 0.0012, 0.0012, 0.0004,\n                      0.0009, 0.0022, 0.0007, 0.0007, 0.0016, 0.0009, 0.0004, 0.0016, 0.0016,\n                      0.0013, 0.0016, 0.0009, 0.0024, 0.0017, 0.0008, 0.0013, 0.0008, 0.0005,\n                      0.0014, 0.0011, 0.0018, 0.0007, 0.0026, 0.0034, 0.0011, 0.0012, 0.0014,\n                      0.0017, 0.0020, 0.0015, 0.0005, 0.0011, 0.0005, 0.0010, 0.0017, 0.0014,\n                      0.0017, 0.0018, 0.0018, 0.0010, 0.0017, 0.0018, 0.0017, 0.0009, 0.0011,\n                      0.0017, 0.0017, 0.0010, 0.0007, 0.0019, 0.0010, 0.0015, 0.0006, 0.0018,\n                      0.0019, 0.0006, 0.0007, 0.0008, 0.0013, 0.0011, 0.0006, 0.0012, 0.0021,\n                      0.0009, 0.0014, 0.0016, 0.0016, 0.0007, 0.0020, 0.0006, 0.0012, 0.0015,\n                      0.0016, 0.0011, 0.0007, 0.0011, 0.0029, 0.0009, 0.0012, 0.0003, 0.0012,\n                      0.0008, 0.0018, 0.0007, 0.0010, 0.0005, 0.0011, 0.0006, 0.0015, 0.0007,\n                      0.0015, 0.0006, 0.0007, 0.0010, 0.0018, 0.0004, 0.0010, 0.0007, 0.0007,\n                      0.0013, 0.0019, 0.0017, 0.0018, 0.0006, 0.0016, 0.0022, 0.0017, 0.0017,\n                      0.0016, 0.0018, 0.0009, 0.0011, 0.0006, 0.0018, 0.0007, 0.0009, 0.0006,\n                      0.0020, 0.0016, 0.0006, 0.0018, 0.0036, 0.0018, 0.0011, 0.0012, 0.0015,\n                      0.0018, 0.0014, 0.0007, 0.0008, 0.0007, 0.0016, 0.0007, 0.0016, 0.0019,\n                      0.0005, 0.0017, 0.0010, 0.0015, 0.0012, 0.0013, 0.0009, 0.0007, 0.0018,\n                      0.0003, 0.0016, 0.0020]), zero_point=tensor([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0, -128,    0,  127,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,  127,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                      -128,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0, -128,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0, -128,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         0,    0,    0,    0,    0,    0,    0,    0,    0, -128,    0,    0],\n                     dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n                min_val=tensor([-2.6229e-01, -2.8273e-01, -2.0138e-01, -2.0729e-01, -3.1248e-02,\n                        -9.8238e-02, -2.1814e-01, -2.1236e-02, -2.1373e-01, -1.0133e-01,\n                        -2.7605e-02, -1.3187e-02, -4.8217e-02, -8.6507e-02, -5.7622e-01,\n                        -3.4155e-02, -3.0446e-02, -2.0863e-01, -2.8954e-02, -4.9081e-02,\n                        -1.4327e-01, -1.7433e-01, -1.9014e-02, -7.6877e-02, -4.0324e-02,\n                        -4.0091e-02, -1.7997e-01, -2.0734e-02, -1.2298e-01, -6.7598e-03,\n                        -7.9254e-02, -6.3864e-02, -3.5839e-01, -3.1516e-02, -1.2290e-01,\n                        -3.2824e-02, -6.1555e-02, -9.9771e-02, -1.6437e-01, -5.4413e-02,\n                        -2.5667e-02, -2.5787e-02, -2.1628e-01, -3.2011e-02, -1.7259e-01,\n                        -5.2864e-02, -6.4420e-02, -1.3230e-01, -1.7942e-01, -1.9581e-01,\n                        -2.1807e-02, -1.5412e-01, -3.2815e-02, -1.9736e-02, -8.2041e-02,\n                        -2.4055e-02, -1.6270e-01, -2.0395e-02, -7.3588e-02, -1.8274e-01,\n                        -1.9096e-02, -1.1006e-01, -1.8427e-02, -2.1923e-01, -4.7168e-02,\n                        -2.0206e-01, -1.8466e-02, -1.8471e-01, -2.9466e-02, -1.1117e-01,\n                        -5.1273e-02, -2.5768e-01, -2.5921e-02, -2.9875e-02, -2.3884e-01,\n                        -2.5137e-02, -1.4530e-01, -1.4062e-01, -1.8831e-02,  5.2001e-03,\n                        -1.6019e-01, -5.8078e-02, -1.0835e-01, -3.8442e-02, -4.8965e-02,\n                        -9.5939e-02, -2.7589e-02, -6.5326e-02, -2.1577e-02, -6.9927e-02,\n                        -1.1134e-01, -5.1728e-02, -1.7986e-02, -2.0359e-01, -2.0621e-01,\n                        -2.6368e-02, -1.3609e-01, -2.0600e-01, -4.1363e-02, -2.9557e-02,\n                        -1.1600e-01, -1.7074e-02, -3.7306e-02, -2.4002e-01, -1.9551e-01,\n                        -5.6362e-02, -2.1529e-02, -1.9687e-01, -2.1500e-02, -1.6499e-02,\n                        -1.5983e-01, -1.9621e-01, -2.2048e-02, -2.6474e-02, -2.3876e-02,\n                        -7.2297e-02, -2.1997e-01, -4.2091e-02, -1.9380e-01, -1.0815e-01,\n                        -2.9830e-03, -1.0488e-01, -1.6111e-01, -3.4637e-01, -7.0410e-03,\n                        -2.2638e-01, -4.4641e-02, -2.2950e-02, -1.5568e-01, -3.2629e-01,\n                        -3.1284e-02, -2.2196e-01, -7.3847e-02, -1.1493e-01, -2.7841e-02,\n                        -2.2569e-02, -1.4786e-02, -3.0704e-02, -3.5488e-02, -4.0430e-02,\n                        -1.4037e-01, -3.4074e-02, -2.6161e-02, -4.4354e-02, -6.2269e-02,\n                        -5.3195e-02, -2.3810e-02, -4.4594e-02, -2.0450e-01, -2.3713e-02,\n                        -2.2637e-02, -1.8192e-01, -2.0224e-02, -1.6362e-01, -1.4901e-01,\n                        -4.2264e-02, -2.2079e-01, -1.2368e-01, -1.9126e-02, -9.9882e-02,\n                        -2.4601e-02, -4.3475e-02, -2.0519e-02, -2.0955e-02, -1.8153e-01,\n                        -2.0996e-01, -2.0887e-01, -1.6930e-01, -1.2468e-01, -2.6021e-02,\n                        -1.2390e-01, -1.8675e-02, -2.3637e-02, -3.1915e-01, -4.4094e-02,\n                        -2.3535e-02, -2.3638e-02, -2.5753e-01, -1.1074e-01, -3.3664e-02,\n                        -1.9505e-02, -1.1475e-01, -6.6290e-03, -1.0820e-01, -1.9446e-01,\n                        -2.5793e-02, -3.8152e-02, -4.1653e-02, -1.5291e-01, -1.3930e-01,\n                        -2.0205e-01, -2.5841e-01, -7.8719e-02, -1.6635e-02, -7.2117e-02,\n                        -4.5967e-02, -1.8020e-02, -6.8135e-02, -2.0942e-02, -2.3678e-02,\n                        -1.0571e-01, -1.3993e-01, -1.5990e-01, -6.5862e-02, -1.6968e-01,\n                        -3.9446e-02, -1.1721e-01, -1.5850e-01, -6.4125e-02, -3.2443e-01,\n                        -1.2366e-01, -1.0995e-01, -4.3571e-02, -9.3761e-02, -1.6191e-01,\n                        -5.0750e-02, -1.2964e-01, -1.3429e-01, -2.3039e-02, -3.0762e-02,\n                        -1.3472e-01, -2.0825e-01, -2.6156e-02, -2.2734e-01, -4.9741e-03,\n                        -2.0942e-02, -3.2764e-02, -1.1860e-01, -3.4061e-02, -2.0407e-02,\n                        -3.3103e-01, -2.1660e-01, -1.4459e-01, -2.2379e-02, -1.5078e-01,\n                        -1.8688e-01, -3.2674e-01, -1.7691e-01, -6.9339e-02, -1.2436e-02,\n                        -8.3952e-02, -2.1404e-02, -1.9317e-01, -2.5345e-02, -1.0551e-01,\n                        -2.1272e-02, -1.8308e-01, -2.4013e-01, -1.8418e-02, -3.9718e-01,\n                        -2.4083e-01, -3.2866e-02, -2.1245e-01, -1.2701e-02, -8.4259e-02,\n                        -1.7161e-02, -1.8649e-01, -2.6578e-02, -2.1665e-02, -1.3535e-01,\n                        -4.5254e-02, -1.6091e-01, -2.3286e-01, -3.5228e-02, -1.4624e-01,\n                        -1.1816e-01, -4.7350e-02, -1.1524e-01, -2.6498e-01, -1.6257e-01,\n                        -2.0947e-02, -2.2264e-02, -6.5773e-02, -3.5585e-02, -3.2510e-02,\n                        -6.6401e-02, -1.9212e-01, -2.6290e-02, -4.2743e-02, -2.2470e-01,\n                        -1.5112e-01, -2.1453e-02, -2.6034e-01, -1.9583e-01, -1.8501e-01,\n                        -2.1483e-02, -1.9970e-01, -2.3516e-01, -5.6096e-02, -2.1598e-01,\n                        -1.0594e-01, -3.2209e-02, -4.2230e-02, -1.6662e-01, -2.3446e-02,\n                        -3.6206e-02, -1.2743e-01, -1.4385e-01, -3.0274e-02, -3.3039e-01,\n                        -2.5007e-02, -1.2441e-01, -9.2876e-02, -2.3915e-02, -2.4647e-01,\n                        -1.2181e-01, -2.1753e-01, -7.7353e-02, -2.1000e-01, -1.0813e-01,\n                        -2.7707e-03, -1.0843e-01, -3.4717e-02, -2.4273e-01, -2.3068e-02,\n                        -2.4074e-02, -7.4542e-02, -3.3465e-01, -1.7725e-01, -2.3006e-02,\n                        -1.3001e-02, -2.5682e-02, -4.5428e-02, -2.9939e-02, -1.5535e-01,\n                        -4.4423e-02, -9.6449e-02, -1.6987e-01, -2.4727e-02, -2.0454e-01,\n                        -3.2694e-02, -2.3145e-01, -9.0160e-02, -1.5805e-02, -9.1922e-02,\n                        -3.4071e-02,  5.5907e-03, -2.1592e-02, -1.7267e-01, -7.2134e-02,\n                        -7.5888e-02, -1.9548e-02, -2.3334e-02, -9.0955e-02, -2.2283e-01,\n                        -3.6972e-02, -2.4941e-01, -2.4430e-02, -2.2453e-02, -1.9007e-02,\n                        -1.8954e-02, -2.9394e-02, -4.2531e-02, -2.5137e-02, -1.7593e-01,\n                        -1.9403e-02, -3.7575e-02, -1.3896e-01, -4.7049e-02, -2.3021e-01,\n                        -3.2229e-02, -2.7593e-02, -2.0367e-01, -6.9675e-03, -2.4483e-02,\n                        -4.9221e-02, -1.6212e-01, -1.2540e-01, -2.9248e-01, -2.3777e-01,\n                        -1.6788e-01, -5.1572e-02, -2.4044e-01, -1.2362e-02, -2.4299e-02,\n                        -2.2554e-02, -1.8027e-01, -5.9555e-02, -2.3786e-01, -1.6772e-01,\n                        -7.6780e-02, -2.7520e-02, -2.8624e-02, -2.9601e-02, -1.4189e-02,\n                        -4.2252e-02, -2.6913e-03, -8.2322e-02, -4.1961e-02, -7.2820e-02,\n                        -2.0746e-01, -1.1008e-01, -2.4131e-02, -1.5474e-01, -1.1320e-01,\n                        -5.8903e-02, -1.1772e-02, -3.0112e-02, -2.1716e-02, -8.9084e-02,\n                        -2.7726e-02, -1.0813e-02, -3.8632e-02, -3.5459e-01, -3.3517e-02,\n                        -2.0827e-01, -2.4717e-01, -2.2285e-02, -1.3061e-02, -9.1760e-02,\n                        -9.7378e-02, -4.0155e-02, -2.1427e-02, -2.1962e-02, -4.0225e-02,\n                        -3.1921e-02, -1.6693e-01, -4.6674e-02, -2.1127e-02, -2.4556e-01,\n                        -3.9095e-01, -2.0662e-02, -1.6976e-02, -2.0135e-02, -5.6694e-02,\n                        -2.3469e-01, -1.6409e-02, -2.3815e-02, -3.3832e-02, -3.7000e-02,\n                        -1.4129e-01, -2.2276e-01, -1.0070e-01, -1.2540e-01, -1.5239e-01,\n                        -2.1831e-01, -1.1611e-01, -9.9803e-02, -1.1396e-01, -2.2446e-01,\n                        -6.3122e-02, -1.0276e-01, -3.9754e-02, -2.8783e-02, -1.9179e-01,\n                        -2.5561e-02, -4.4626e-02, -1.1902e-02, -2.0774e-01, -2.7360e-02,\n                        -1.0275e-01, -4.9854e-02, -2.2503e-01, -2.6894e-02, -3.8277e-02,\n                        -8.5815e-02, -1.1930e-01, -7.7546e-02, -2.1241e-01, -1.1307e-01,\n                        -2.3381e-02, -2.2600e-01, -1.8287e-02, -1.2111e-02, -2.1024e-01,\n                         1.6185e-03, -1.8855e-02, -1.8354e-02, -5.6211e-02, -1.5386e-01,\n                        -9.7294e-02, -3.3353e-02, -2.1345e-02, -1.7404e-01, -9.8900e-02,\n                        -9.5379e-02, -2.0924e-01, -2.3790e-01, -1.3073e-02, -1.8995e-01,\n                        -2.0925e-02, -1.7915e-02, -2.2724e-02, -3.6063e-02, -3.8142e-02,\n                        -1.0916e-01, -1.5581e-02, -9.8667e-02, -1.5759e-01, -9.3112e-02,\n                        -1.2406e-02, -1.7371e-01, -1.0304e-01, -2.1849e-01, -2.7528e-02,\n                        -7.7915e-02, -2.6544e-02, -7.3780e-02, -1.0368e-01, -6.2708e-02,\n                        -2.2785e-01, -2.0611e-01, -1.3187e-01, -1.8467e-01, -2.3803e-02,\n                        -9.6593e-02, -5.9070e-02, -2.2899e-02, -4.4112e-02, -1.2461e-01,\n                        -6.2514e-02, -2.4283e-01, -2.2320e-01, -3.6848e-02, -2.2401e-02,\n                        -2.1861e-01, -1.9464e-01, -2.6906e-02, -2.1441e-01, -2.1474e-01,\n                        -1.8815e-01, -3.3478e-01, -2.0793e-01, -1.4321e-02, -2.0384e-02,\n                        -2.2448e-01, -8.8397e-02, -1.3924e-01, -3.6057e-02, -2.1759e-01,\n                        -3.2026e-02, -1.2907e-01, -3.2897e-01, -1.8122e-02, -2.1007e-02,\n                        -2.0393e-01, -4.3895e-03, -3.3159e-02, -2.0972e-02, -1.6704e-02,\n                        -2.1915e-01, -2.5294e-01, -3.0181e-02, -3.7567e-01, -2.1456e-01,\n                        -9.1265e-02, -2.1606e-01, -2.0209e-01, -8.4303e-02, -1.5755e-01,\n                        -1.6831e-02, -2.1560e-01, -1.3057e-01, -1.9408e-01, -2.1432e-01,\n                        -1.7183e-01, -1.6364e-01, -3.1102e-02, -2.2843e-01, -2.3511e-01,\n                        -7.8698e-02, -3.5283e-02, -2.2846e-01, -8.1381e-02, -2.6862e-02,\n                        -1.7002e-01, -3.2680e-02, -1.3569e-02, -7.1711e-02, -4.4168e-02,\n                        -4.0491e-03, -2.2150e-02, -2.2661e-02, -2.4541e-02, -1.1146e-02,\n                        -2.5813e-01, -1.2976e-01, -1.6351e-02, -2.2347e-02, -2.7704e-02,\n                        -1.2696e-01, -8.6807e-02, -1.1792e-01, -1.5065e-01, -2.1782e-01,\n                        -6.7473e-02, -2.0735e-02, -2.4781e-02, -2.6860e-01, -2.6793e-02,\n                        -1.3422e-01, -8.2946e-02, -2.3183e-02, -2.0504e-02, -4.3275e-02,\n                        -5.8761e-02, -2.1929e-01, -3.5507e-02, -1.7987e-01, -2.2817e-01,\n                        -1.6802e-01, -1.3841e-01, -1.7578e-01, -3.7806e-02, -3.7693e-02,\n                        -8.9039e-02, -1.3468e-01, -4.4203e-02, -2.3519e-01, -1.5423e-01,\n                        -1.9283e-02, -7.7735e-02, -3.9187e-02, -1.6833e-02, -3.7932e-01,\n                        -2.3551e-01, -2.1639e-01, -3.4305e-02, -1.5374e-01, -1.5705e-01,\n                        -5.5424e-02, -3.5121e-02, -2.8226e-01, -8.7157e-02, -8.4942e-02,\n                        -1.9927e-02, -1.1824e-01, -4.7707e-02, -2.0266e-01, -3.0323e-02,\n                        -6.9595e-02, -2.0460e-01, -6.7214e-02, -3.0488e-02, -2.5045e-02,\n                        -1.0255e-01, -1.9234e-02, -3.1696e-02, -6.1051e-02, -1.8276e-01,\n                        -1.3972e-01, -2.3612e-01, -1.7688e-02, -3.3476e-01, -4.3007e-01,\n                        -1.8436e-02, -3.5096e-02, -1.7620e-01, -2.1274e-01, -2.5807e-01,\n                        -5.1684e-02, -6.9209e-02, -1.6744e-02, -1.8206e-02, -1.3399e-01,\n                        -2.5453e-02, -5.5905e-02, -1.6458e-02, -2.3197e-01, -2.2765e-01,\n                        -1.2529e-01, -2.1741e-01, -2.3323e-01, -2.1385e-01, -6.8205e-03,\n                        -1.3912e-01, -2.4156e-02, -2.2150e-01, -1.3190e-01, -6.5123e-03,\n                        -2.4709e-02, -2.8592e-02, -3.1155e-02, -8.1437e-03, -1.7585e-02,\n                        -2.2482e-02, -3.7200e-02, -8.3371e-02, -6.6807e-02, -4.9702e-02,\n                        -1.4428e-01, -3.5514e-02, -2.3156e-02, -2.6486e-01, -1.0460e-01,\n                        -6.3442e-02, -1.9695e-02, -2.0885e-01, -9.1996e-02, -2.5006e-01,\n                        -4.4802e-02, -4.6433e-02, -1.9525e-01, -2.0119e-01, -1.3514e-01,\n                        -9.1686e-02, -1.4674e-01, -3.7480e-01, -1.1104e-01, -2.6958e-02,\n                        -3.6283e-02, -2.1139e-02, -7.7331e-02, -2.2811e-01, -3.8563e-02,\n                        -1.3180e-01, -4.4622e-02, -1.4092e-01, -7.2046e-02, -1.9684e-01,\n                        -8.9205e-02, -2.3523e-02, -8.1666e-02, -9.1662e-02, -1.2754e-01,\n                        -2.2553e-01,  4.8028e-04, -1.2494e-01, -3.7272e-02, -9.3462e-03,\n                        -1.7028e-01, -2.1971e-02, -2.1492e-01, -2.2847e-01, -3.4495e-02,\n                        -2.0544e-01, -2.4708e-02, -2.6416e-02, -2.5981e-02, -2.8717e-02,\n                        -2.3891e-02, -1.1435e-01, -3.2579e-02, -4.4501e-02, -2.3292e-01,\n                        -4.7041e-02, -6.5035e-02, -7.1722e-02, -2.5707e-01, -2.4105e-02,\n                        -4.3910e-02, -2.4751e-02, -3.3290e-02, -2.2440e-02, -7.2251e-02,\n                        -1.4761e-01, -1.5386e-02, -2.3281e-01, -1.9715e-02, -4.4423e-02,\n                        -2.4313e-02, -9.5650e-02, -2.0783e-01, -5.3938e-03, -2.1006e-01,\n                        -2.4015e-01, -4.4548e-02, -2.2374e-01, -1.2405e-01, -1.9274e-01,\n                        -2.9802e-02, -1.7154e-01, -4.4316e-02, -9.1702e-02, -2.2598e-01,\n                         2.7219e-04, -2.0956e-01, -2.6990e-02]), max_val=tensor([ 0.0288,  0.0248,  0.0183,  0.0198,  0.1695,  0.0926,  0.0189,  0.2501,\n                         0.0205,  0.1056,  0.2449,  0.1346,  0.1954,  0.0842,  0.1121,  0.1094,\n                         0.1080,  0.0197,  0.2243,  0.0726,  0.0547,  0.0255,  0.2265,  0.0377,\n                         0.1333,  0.0922,  0.0428,  0.1380,  0.0202,  0.0828,  0.0962,  0.0414,\n                         0.0214,  0.1477,  0.1109,  0.1811,  0.1189,  0.0116,  0.0185,  0.0439,\n                         0.2197,  0.2190,  0.0221,  0.1644,  0.0364,  0.0802,  0.0491,  0.1001,\n                         0.0228,  0.0302,  0.2319,  0.0399,  0.2937,  0.2119,  0.0352,  0.2133,\n                         0.0283,  0.1934,  0.0414,  0.0216,  0.2241,  0.0369,  0.1868,  0.0227,\n                         0.0577,  0.0195,  0.0905,  0.0174,  0.1090,  0.0417,  0.1263,  0.0154,\n                         0.2051,  0.1482,  0.0261,  0.3023,  0.0362,  0.0267,  0.2138,  0.0643,\n                         0.0616, -0.0008,  0.0740,  0.0764,  0.1287,  0.1030,  0.1387,  0.1918,\n                         0.1500,  0.1035,  0.0831,  0.1211,  0.1747,  0.0221,  0.0233,  0.1715,\n                         0.0278,  0.0223,  0.2092,  0.2521,  0.0662,  0.2060,  0.0810,  0.0213,\n                         0.0237, -0.0073,  0.1623,  0.0198,  0.1916,  0.1114,  0.0407,  0.0203,\n                         0.2147,  0.2443,  0.2041,  0.1826,  0.0206,  0.1024,  0.0181,  0.1760,\n                         0.0928,  0.0284,  0.0160,  0.0336,  0.1008,  0.0227,  0.0702,  0.2470,\n                         0.0361,  0.0312,  0.3975,  0.0228,  0.0358,  0.0597,  0.2055,  0.1936,\n                         0.2358,  0.2054,  0.0748,  0.2003,  0.0219,  0.1763,  0.2983,  0.1622,\n                         0.1076,  0.0871,  0.2245,  0.1305,  0.0269,  0.2117,  0.2113,  0.0199,\n                         0.1431,  0.0233,  0.0223,  0.0702,  0.0246,  0.0165,  0.1601,  0.0881,\n                         0.2286,  0.0667,  0.2091,  0.2185,  0.0224,  0.0169,  0.0501,  0.0139,\n                         0.0402,  0.2073,  0.0346,  0.2945,  0.2035,  0.0237,  0.2349,  0.0982,\n                         0.2242,  0.0294,  0.0445,  0.2198,  0.1466,  0.0393,  0.1535,  0.0970,\n                         0.0268,  0.2135,  0.1067,  0.0825,  0.0138,  0.0225,  0.0407,  0.0255,\n                         0.0555,  0.1047,  0.0150,  0.0935,  0.1073,  0.0556,  0.1847,  0.2183,\n                         0.1035,  0.0331,  0.0211,  0.0941,  0.0552,  0.1639,  0.0928,  0.0159,\n                         0.0019,  0.0345,  0.0435,  0.0450,  0.0829,  0.0961,  0.0193,  0.1958,\n                         0.0206,  0.0136,  0.2747,  0.2442,  0.0237,  0.0199,  0.2040,  0.0242,\n                         0.1023,  0.1954,  0.1727,  0.1039,  0.0701,  0.1718,  0.0321,  0.0241,\n                         0.0400,  0.2451,  0.0303,  0.0226,  0.0215,  0.0359,  0.1997,  0.1593,\n                         0.1053,  0.2449,  0.0280,  0.2220,  0.1112,  0.2157,  0.0252,  0.0194,\n                         0.2033,  0.0276,  0.0230,  0.1055,  0.0211,  0.1257,  0.0376,  0.2404,\n                         0.0338,  0.1893,  0.2075,  0.0135,  0.1229,  0.0111,  0.0235,  0.1387,\n                         0.1691,  0.0376,  0.2391,  0.1038,  0.0212,  0.0330,  0.1678,  0.2244,\n                         0.0660,  0.3618,  0.2238,  0.1926,  0.0217,  0.2508,  0.1958,  0.0241,\n                         0.0178,  0.1426,  0.0440,  0.0273,  0.0221,  0.2597,  0.0219,  0.0218,\n                         0.2325,  0.0216,  0.0086,  0.1058,  0.0963,  0.0229,  0.2189,  0.1881,\n                         0.0235,  0.0240,  0.2054,  0.0250,  0.2281,  0.0159,  0.0874,  0.2780,\n                         0.0226,  0.0843,  0.0223,  0.1289,  0.0185,  0.0400,  0.0707,  0.0771,\n                         0.1095,  0.0275,  0.2267,  0.2335,  0.0348,  0.1953,  0.0258,  0.2448,\n                         0.1357,  0.2421,  0.1551,  0.1792,  0.0237,  0.1157,  0.1195,  0.0120,\n                         0.1319,  0.0208,  0.2473,  0.0243,  0.0212,  0.1097,  0.0967,  0.2538,\n                         0.1126,  0.2405,  0.0189,  0.0384,  0.0841,  0.2211,  0.1921,  0.0997,\n                         0.0242,  0.0738,  0.0259,  0.2292,  0.2228,  0.2284,  0.1707,  0.1634,\n                         0.1197,  0.2220,  0.0174,  0.1865,  0.1813,  0.0176,  0.0409,  0.0222,\n                         0.1110,  0.1829,  0.0215,  0.1198,  0.1231,  0.1248,  0.0352,  0.0648,\n                         0.0235,  0.0258,  0.0524,  0.0581,  0.0309,  0.1040,  0.2402,  0.2672,\n                         0.0175,  0.1306,  0.0269,  0.0365,  0.0181,  0.1425,  0.0990,  0.2104,\n                         0.1581,  0.1254,  0.0742,  0.0353,  0.2019,  0.0395,  0.0238,  0.0586,\n                         0.2244,  0.0233,  0.0984,  0.2085,  0.1652,  0.1080,  0.1614,  0.0365,\n                         0.2248,  0.0526,  0.1135,  0.0286,  0.1340,  0.0193,  0.0228,  0.1018,\n                         0.1434,  0.0791,  0.0132,  0.0941,  0.2284,  0.1752,  0.0536,  0.2081,\n                         0.0204,  0.1474,  0.2021,  0.0189,  0.0267,  0.2344,  0.1469,  0.1717,\n                         0.0567,  0.0225,  0.1248,  0.2170,  0.0613,  0.0698,  0.0143,  0.0189,\n                         0.0064,  0.0872,  0.0309,  0.0247,  0.0266,  0.0583,  0.0087,  0.0265,\n                         0.0393,  0.0817,  0.2512,  0.2808,  0.0210,  0.2372,  0.2362,  0.1467,\n                         0.0333,  0.3719,  0.0404,  0.2342,  0.0201,  0.3261,  0.1532,  0.0437,\n                         0.1081,  0.1036,  0.0268,  0.0981,  0.1541,  0.0273,  0.2781,  0.1682,\n                         0.0191,  0.0937,  0.1082,  0.2014,  0.0421,  0.0621,  0.1309,  0.1790,\n                         0.2101,  0.0176,  0.0278,  0.0618,  0.0207,  0.0245,  0.1073,  0.0358,\n                         0.1647,  0.1184,  0.2239,  0.0869,  0.0612,  0.0114,  0.2140,  0.0959,\n                         0.0423,  0.0926,  0.1102,  0.0240,  0.0383,  0.0219,  0.2380,  0.0415,\n                         0.2874,  0.1041,  0.0406,  0.0689,  0.0472,  0.0205,  0.0225,  0.0290,\n                         0.1836,  0.1481,  0.0931,  0.2125,  0.1383,  0.0195,  0.1107,  0.0324,\n                         0.0207,  0.0697,  0.3150,  0.0325,  0.0306,  0.2114,  0.0210,  0.0205,\n                         0.0187,  0.0267,  0.0242,  0.0928,  0.2165,  0.0212,  0.0400,  0.0136,\n                         0.0934,  0.0169,  0.1840,  0.1212,  0.0438,  0.1688,  0.2140,  0.0570,\n                         0.0820,  0.2934,  0.2042,  0.1824,  0.0233,  0.0210,  0.1083,  0.0348,\n                         0.0182,  0.0785,  0.0247,  0.0319,  0.0430,  0.0157,  0.1069,  0.0211,\n                         0.0265,  0.0748,  0.0318,  0.0360,  0.0536,  0.2139,  0.0261,  0.0179,\n                         0.0090,  0.1012,  0.0229,  0.0688,  0.2585,  0.0194,  0.2062,  0.1961,\n                         0.0409,  0.0909,  0.0871,  0.1523,  0.2086,  0.1876,  0.0846,  0.0375,\n                         0.0248,  0.1073,  0.2212,  0.2163,  0.1433,  0.0088,  0.0162,  0.0434,\n                         0.0703,  0.1099,  0.2041,  0.2038,  0.0241,  0.3411,  0.0178,  0.0296,\n                         0.2468,  0.2114,  0.0637,  0.1051,  0.0269,  0.1649,  0.0290,  0.0192,\n                         0.0103,  0.0153,  0.0197,  0.0936,  0.0909,  0.0905,  0.0158,  0.2288,\n                         0.0300,  0.0249,  0.2053,  0.1671,  0.1213,  0.1492,  0.1563,  0.0211,\n                         0.0248,  0.1890,  0.0135,  0.0164,  0.0444,  0.1118,  0.0207,  0.0074,\n                         0.0455,  0.2074,  0.0371,  0.0553,  0.0238,  0.2084,  0.1594,  0.0208,\n                         0.1080,  0.3082,  0.2179,  0.0106,  0.1688,  0.1027,  0.0341,  0.0251,\n                         0.0234,  0.0485,  0.0911,  0.0250,  0.0422,  0.1371,  0.1464,  0.0328,\n                         0.0276,  0.0203,  0.1842,  0.0425,  0.1391,  0.0651,  0.0301,  0.2154,\n                         0.1824,  0.2109,  0.0238,  0.0246,  0.0361,  0.0200,  0.2089,  0.0181,\n                         0.1161,  0.0191,  0.2117,  0.0280,  0.0355,  0.0915,  0.2401,  0.1275,\n                         0.1868,  0.0789,  0.2347,  0.2375,  0.0805,  0.0940,  0.0953,  0.1698,\n                         0.0217,  0.0776,  0.1558,  0.0208,  0.1183,  0.1829,  0.2033,  0.0203,\n                         0.0372,  0.0199,  0.0808,  0.1475,  0.0213,  0.0227,  0.0261,  0.0494,\n                         0.0177,  0.0331,  0.0921,  0.1508,  0.0356,  0.1520,  0.0953,  0.0311,\n                         0.0907,  0.0223,  0.0614,  0.0558,  0.0103,  0.0243,  0.0410,  0.1924,\n                         0.0433,  0.0461,  0.0404,  0.0240,  0.0976,  0.0168,  0.0908,  0.0933,\n                         0.0427,  0.2417,  0.0291,  0.0183,  0.0779,  0.0187,  0.2746,  0.2165,\n                         0.2108,  0.2059,  0.2284,  0.0164,  0.1345,  0.0750,  0.0253,  0.0826,\n                         0.1131,  0.0789,  0.0237,  0.2088,  0.0750,  0.2253,  0.4529,  0.2305,\n                         0.1358,  0.0360,  0.1908,  0.0221,  0.1715,  0.0945,  0.1053,  0.0170,\n                         0.0203,  0.0868,  0.0255,  0.0365,  0.0587,  0.0298,  0.0242,  0.0194,\n                         0.1565,  0.0242,  0.1145,  0.0875,  0.0265,  0.0853,  0.0213,  0.2538])\n              )\n            )\n          )\n          (2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(\n            in_features=768, out_features=3072, bias=True\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0016, 0.0013, 0.0013,  ..., 0.0013, 0.0014, 0.0015]), zero_point=tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([-0.2095, -0.1601, -0.1697,  ..., -0.1639, -0.1611, -0.1891]), max_val=tensor([0.1115, 0.1547, 0.1408,  ..., 0.1192, 0.1746, 0.1544]))\n            )\n          )\n          (4): GELU(approximate='none')\n          (5): Linear(\n            in_features=3072, out_features=768, bias=True\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0023, 0.0015, 0.0021, 0.0015, 0.0014, 0.0016, 0.0014, 0.0016, 0.0015,\n                      0.0025, 0.0016, 0.0017, 0.0020, 0.0016, 0.0030, 0.0020, 0.0019, 0.0026,\n                      0.0016, 0.0016, 0.0015, 0.0024, 0.0025, 0.0015, 0.0016, 0.0014, 0.0016,\n                      0.0015, 0.0018, 0.0019, 0.0017, 0.0014, 0.0015, 0.0014, 0.0022, 0.0017,\n                      0.0017, 0.0016, 0.0014, 0.0015, 0.0016, 0.0018, 0.0017, 0.0014, 0.0013,\n                      0.0015, 0.0017, 0.0016, 0.0017, 0.0021, 0.0015, 0.0015, 0.0020, 0.0016,\n                      0.0017, 0.0016, 0.0016, 0.0017, 0.0016, 0.0016, 0.0023, 0.0018, 0.0015,\n                      0.0019, 0.0016, 0.0015, 0.0017, 0.0014, 0.0018, 0.0014, 0.0013, 0.0017,\n                      0.0016, 0.0017, 0.0014, 0.0015, 0.0014, 0.0014, 0.0016, 0.0013, 0.0021,\n                      0.0016, 0.0017, 0.0015, 0.0017, 0.0019, 0.0017, 0.0023, 0.0014, 0.0019,\n                      0.0016, 0.0017, 0.0017, 0.0015, 0.0016, 0.0014, 0.0020, 0.0016, 0.0016,\n                      0.0016, 0.0014, 0.0016, 0.0014, 0.0016, 0.0016, 0.0015, 0.0016, 0.0018,\n                      0.0016, 0.0015, 0.0016, 0.0016, 0.0025, 0.0016, 0.0016, 0.0018, 0.0016,\n                      0.0015, 0.0017, 0.0022, 0.0015, 0.0015, 0.0016, 0.0015, 0.0016, 0.0016,\n                      0.0022, 0.0017, 0.0015, 0.0014, 0.0017, 0.0015, 0.0017, 0.0017, 0.0017,\n                      0.0017, 0.0018, 0.0020, 0.0014, 0.0015, 0.0016, 0.0015, 0.0017, 0.0014,\n                      0.0022, 0.0021, 0.0015, 0.0015, 0.0016, 0.0016, 0.0017, 0.0017, 0.0016,\n                      0.0017, 0.0016, 0.0016, 0.0016, 0.0015, 0.0017, 0.0016, 0.0016, 0.0014,\n                      0.0015, 0.0015, 0.0015, 0.0018, 0.0021, 0.0015, 0.0016, 0.0019, 0.0016,\n                      0.0015, 0.0018, 0.0017, 0.0014, 0.0016, 0.0014, 0.0043, 0.0014, 0.0015,\n                      0.0017, 0.0015, 0.0014, 0.0018, 0.0016, 0.0016, 0.0017, 0.0015, 0.0016,\n                      0.0015, 0.0022, 0.0014, 0.0020, 0.0017, 0.0016, 0.0018, 0.0016, 0.0015,\n                      0.0016, 0.0017, 0.0020, 0.0016, 0.0018, 0.0017, 0.0016, 0.0015, 0.0015,\n                      0.0015, 0.0017, 0.0017, 0.0015, 0.0015, 0.0015, 0.0021, 0.0021, 0.0016,\n                      0.0018, 0.0015, 0.0024, 0.0015, 0.0016, 0.0019, 0.0019, 0.0015, 0.0015,\n                      0.0015, 0.0015, 0.0021, 0.0015, 0.0017, 0.0015, 0.0020, 0.0016, 0.0015,\n                      0.0016, 0.0014, 0.0015, 0.0018, 0.0023, 0.0019, 0.0018, 0.0014, 0.0020,\n                      0.0017, 0.0016, 0.0018, 0.0015, 0.0022, 0.0016, 0.0016, 0.0015, 0.0014,\n                      0.0018, 0.0014, 0.0015, 0.0019, 0.0014, 0.0015, 0.0014, 0.0019, 0.0016,\n                      0.0015, 0.0025, 0.0015, 0.0020, 0.0018, 0.0033, 0.0018, 0.0020, 0.0014,\n                      0.0017, 0.0017, 0.0017, 0.0016, 0.0020, 0.0021, 0.0022, 0.0016, 0.0017,\n                      0.0014, 0.0019, 0.0016, 0.0014, 0.0014, 0.0017, 0.0023, 0.0017, 0.0017,\n                      0.0024, 0.0016, 0.0013, 0.0018, 0.0016, 0.0015, 0.0017, 0.0013, 0.0016,\n                      0.0017, 0.0020, 0.0017, 0.0016, 0.0016, 0.0016, 0.0015, 0.0016, 0.0023,\n                      0.0017, 0.0016, 0.0025, 0.0017, 0.0017, 0.0025, 0.0015, 0.0015, 0.0014,\n                      0.0016, 0.0016, 0.0034, 0.0017, 0.0017, 0.0019, 0.0014, 0.0016, 0.0017,\n                      0.0015, 0.0016, 0.0014, 0.0018, 0.0015, 0.0015, 0.0014, 0.0015, 0.0017,\n                      0.0016, 0.0016, 0.0016, 0.0014, 0.0016, 0.0017, 0.0014, 0.0022, 0.0016,\n                      0.0015, 0.0014, 0.0014, 0.0015, 0.0017, 0.0017, 0.0016, 0.0017, 0.0014,\n                      0.0014, 0.0016, 0.0015, 0.0016, 0.0016, 0.0018, 0.0017, 0.0019, 0.0016,\n                      0.0017, 0.0015, 0.0017, 0.0016, 0.0020, 0.0015, 0.0016, 0.0019, 0.0019,\n                      0.0016, 0.0018, 0.0014, 0.0015, 0.0016, 0.0017, 0.0015, 0.0015, 0.0018,\n                      0.0015, 0.0016, 0.0016, 0.0015, 0.0019, 0.0015, 0.0017, 0.0017, 0.0015,\n                      0.0016, 0.0021, 0.0016, 0.0015, 0.0016, 0.0018, 0.0016, 0.0020, 0.0027,\n                      0.0015, 0.0014, 0.0018, 0.0015, 0.0018, 0.0030, 0.0016, 0.0017, 0.0015,\n                      0.0018, 0.0021, 0.0018, 0.0015, 0.0017, 0.0016, 0.0015, 0.0017, 0.0016,\n                      0.0017, 0.0015, 0.0014, 0.0018, 0.0015, 0.0015, 0.0019, 0.0018, 0.0015,\n                      0.0017, 0.0014, 0.0021, 0.0015, 0.0017, 0.0017, 0.0016, 0.0015, 0.0016,\n                      0.0016, 0.0022, 0.0017, 0.0016, 0.0017, 0.0018, 0.0015, 0.0018, 0.0016,\n                      0.0020, 0.0016, 0.0016, 0.0014, 0.0015, 0.0016, 0.0015, 0.0015, 0.0016,\n                      0.0016, 0.0021, 0.0015, 0.0016, 0.0013, 0.0015, 0.0022, 0.0015, 0.0015,\n                      0.0017, 0.0015, 0.0015, 0.0018, 0.0016, 0.0015, 0.0013, 0.0015, 0.0016,\n                      0.0016, 0.0023, 0.0018, 0.0016, 0.0017, 0.0020, 0.0019, 0.0016, 0.0015,\n                      0.0018, 0.0016, 0.0017, 0.0020, 0.0018, 0.0016, 0.0014, 0.0016, 0.0017,\n                      0.0016, 0.0019, 0.0024, 0.0016, 0.0015, 0.0016, 0.0017, 0.0015, 0.0016,\n                      0.0014, 0.0015, 0.0025, 0.0016, 0.0020, 0.0024, 0.0016, 0.0016, 0.0016,\n                      0.0017, 0.0015, 0.0014, 0.0018, 0.0022, 0.0015, 0.0017, 0.0016, 0.0015,\n                      0.0014, 0.0016, 0.0014, 0.0017, 0.0023, 0.0018, 0.0018, 0.0016, 0.0017,\n                      0.0014, 0.0016, 0.0014, 0.0015, 0.0017, 0.0015, 0.0016, 0.0017, 0.0015,\n                      0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0020, 0.0019,\n                      0.0015, 0.0016, 0.0019, 0.0016, 0.0015, 0.0017, 0.0015, 0.0016, 0.0015,\n                      0.0017, 0.0016, 0.0017, 0.0015, 0.0034, 0.0017, 0.0016, 0.0015, 0.0017,\n                      0.0017, 0.0014, 0.0014, 0.0016, 0.0015, 0.0016, 0.0016, 0.0016, 0.0017,\n                      0.0014, 0.0014, 0.0020, 0.0014, 0.0017, 0.0014, 0.0023, 0.0014, 0.0014,\n                      0.0017, 0.0016, 0.0018, 0.0017, 0.0016, 0.0015, 0.0016, 0.0021, 0.0024,\n                      0.0018, 0.0017, 0.0014, 0.0014, 0.0014, 0.0017, 0.0015, 0.0017, 0.0014,\n                      0.0014, 0.0018, 0.0013, 0.0016, 0.0016, 0.0020, 0.0017, 0.0017, 0.0015,\n                      0.0014, 0.0015, 0.0024, 0.0016, 0.0021, 0.0016, 0.0014, 0.0014, 0.0016,\n                      0.0015, 0.0015, 0.0032, 0.0016, 0.0018, 0.0016, 0.0018, 0.0014, 0.0015,\n                      0.0019, 0.0014, 0.0015, 0.0014, 0.0014, 0.0015, 0.0015, 0.0017, 0.0015,\n                      0.0017, 0.0016, 0.0015, 0.0014, 0.0015, 0.0015, 0.0019, 0.0017, 0.0013,\n                      0.0015, 0.0015, 0.0021, 0.0016, 0.0019, 0.0016, 0.0016, 0.0017, 0.0015,\n                      0.0015, 0.0016, 0.0021, 0.0014, 0.0016, 0.0017, 0.0018, 0.0018, 0.0020,\n                      0.0017, 0.0018, 0.0017, 0.0020, 0.0020, 0.0020, 0.0017, 0.0017, 0.0019,\n                      0.0018, 0.0015, 0.0017, 0.0016, 0.0018, 0.0014, 0.0015, 0.0016, 0.0017,\n                      0.0016, 0.0017, 0.0020, 0.0016, 0.0016, 0.0014, 0.0015, 0.0016, 0.0016,\n                      0.0017, 0.0020, 0.0015, 0.0016, 0.0016, 0.0015, 0.0015, 0.0018, 0.0017,\n                      0.0015, 0.0016, 0.0013, 0.0017, 0.0021, 0.0014, 0.0019, 0.0018, 0.0017,\n                      0.0014, 0.0015, 0.0017, 0.0014, 0.0015, 0.0049, 0.0017, 0.0017, 0.0016,\n                      0.0015, 0.0015, 0.0015, 0.0016, 0.0018, 0.0015, 0.0015, 0.0015, 0.0015,\n                      0.0016, 0.0016, 0.0019, 0.0015, 0.0015, 0.0017, 0.0014, 0.0018, 0.0017,\n                      0.0016, 0.0015, 0.0015, 0.0016, 0.0019, 0.0014, 0.0015, 0.0018, 0.0016,\n                      0.0019, 0.0017, 0.0019, 0.0015, 0.0020, 0.0020, 0.0016, 0.0016, 0.0015,\n                      0.0021, 0.0016, 0.0024, 0.0016, 0.0017, 0.0016, 0.0017, 0.0017, 0.0015,\n                      0.0014, 0.0016, 0.0015, 0.0014, 0.0015, 0.0014, 0.0016, 0.0016, 0.0015,\n                      0.0022, 0.0016, 0.0016]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                     dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n                min_val=tensor([-0.2080, -0.1845, -0.2701, -0.1830, -0.1815, -0.2007, -0.1756, -0.1991,\n                        -0.1896, -0.2713, -0.2079, -0.2201, -0.2331, -0.1682, -0.3861, -0.2510,\n                        -0.2103, -0.2031, -0.1985, -0.2004, -0.1983, -0.2818, -0.3041, -0.1750,\n                        -0.1976, -0.1761, -0.1892, -0.1920, -0.2254, -0.2376, -0.1811, -0.1712,\n                        -0.1845, -0.1771, -0.2439, -0.1820, -0.1919, -0.1930, -0.1738, -0.1869,\n                        -0.1746, -0.2056, -0.1759, -0.1726, -0.1721, -0.1948, -0.1599, -0.2086,\n                        -0.1892, -0.2526, -0.1753, -0.1930, -0.1824, -0.2110, -0.2139, -0.1775,\n                        -0.1808, -0.2204, -0.2090, -0.2096, -0.2995, -0.2159, -0.1728, -0.2415,\n                        -0.1839, -0.1708, -0.1977, -0.1778, -0.2313, -0.1689, -0.1727, -0.1682,\n                        -0.1648, -0.2156, -0.1806, -0.1673, -0.1823, -0.1625, -0.2054, -0.1673,\n                        -0.2668, -0.2092, -0.1818, -0.1832, -0.2232, -0.2421, -0.2211, -0.2467,\n                        -0.1780, -0.2414, -0.2044, -0.1737, -0.1866, -0.1741, -0.1685, -0.1750,\n                        -0.2522, -0.1866, -0.2044, -0.1957, -0.1814, -0.1706, -0.1736, -0.2107,\n                        -0.2020, -0.1805, -0.2080, -0.2303, -0.1761, -0.1888, -0.2086, -0.1824,\n                        -0.2747, -0.2004, -0.1797, -0.2188, -0.2040, -0.1787, -0.2052, -0.2655,\n                        -0.1909, -0.1923, -0.1969, -0.1970, -0.2064, -0.1998, -0.2766, -0.1811,\n                        -0.1941, -0.1841, -0.1899, -0.1858, -0.2226, -0.1793, -0.1618, -0.2119,\n                        -0.2267, -0.2609, -0.1631, -0.1875, -0.1617, -0.1902, -0.2197, -0.1782,\n                        -0.2840, -0.2663, -0.1864, -0.1872, -0.1778, -0.1827, -0.2201, -0.1989,\n                        -0.2062, -0.1943, -0.1906, -0.2069, -0.2062, -0.1889, -0.1793, -0.1829,\n                        -0.2060, -0.1524, -0.1771, -0.1933, -0.1909, -0.1960, -0.2749, -0.1905,\n                        -0.2109, -0.1749, -0.1968, -0.1720, -0.1992, -0.2198, -0.1856, -0.2046,\n                        -0.1624, -0.5540, -0.1677, -0.1771, -0.2107, -0.1967, -0.1821, -0.2267,\n                        -0.1873, -0.2081, -0.1908, -0.1694, -0.2101, -0.1902, -0.2618, -0.1635,\n                        -0.2514, -0.1935, -0.1993, -0.2339, -0.1820, -0.1879, -0.1990, -0.2208,\n                        -0.2518, -0.1827, -0.1602, -0.2174, -0.2028, -0.1911, -0.1799, -0.1711,\n                        -0.2100, -0.2144, -0.1877, -0.1856, -0.1812, -0.2598, -0.2674, -0.2108,\n                        -0.2278, -0.1896, -0.3013, -0.1810, -0.2020, -0.2436, -0.1714, -0.1939,\n                        -0.1917, -0.1954, -0.1909, -0.2726, -0.1963, -0.2189, -0.1981, -0.1716,\n                        -0.1714, -0.1948, -0.1989, -0.1788, -0.1961, -0.2265, -0.2751, -0.2380,\n                        -0.1941, -0.1659, -0.1948, -0.1841, -0.2085, -0.2018, -0.1732, -0.2776,\n                        -0.1744, -0.1719, -0.1773, -0.1647, -0.2309, -0.1820, -0.1697, -0.2455,\n                        -0.1609, -0.1826, -0.1811, -0.1921, -0.1749, -0.1811, -0.3185, -0.1814,\n                        -0.2578, -0.1919, -0.3065, -0.2253, -0.2558, -0.1740, -0.1850, -0.2185,\n                        -0.2121, -0.1960, -0.2525, -0.2649, -0.2560, -0.1991, -0.1673, -0.1771,\n                        -0.2082, -0.2043, -0.1848, -0.1776, -0.2178, -0.2649, -0.1898, -0.1982,\n                        -0.2517, -0.2080, -0.1726, -0.2263, -0.1829, -0.1722, -0.2141, -0.1727,\n                        -0.2043, -0.1934, -0.2376, -0.2147, -0.1899, -0.2111, -0.2000, -0.1763,\n                        -0.2011, -0.2624, -0.2030, -0.1650, -0.3192, -0.1978, -0.2012, -0.3140,\n                        -0.1894, -0.1777, -0.1803, -0.1869, -0.1863, -0.2614, -0.2136, -0.2190,\n                        -0.2407, -0.1809, -0.2001, -0.2085, -0.1903, -0.2012, -0.1765, -0.2364,\n                        -0.1863, -0.1696, -0.1760, -0.1969, -0.2116, -0.2033, -0.2051, -0.2046,\n                        -0.1589, -0.2096, -0.2177, -0.1594, -0.2750, -0.2104, -0.1761, -0.1813,\n                        -0.1703, -0.1771, -0.1985, -0.1647, -0.1956, -0.2174, -0.1773, -0.1810,\n                        -0.1796, -0.1867, -0.2092, -0.2083, -0.1823, -0.1702, -0.1871, -0.1807,\n                        -0.2012, -0.1710, -0.2189, -0.1982, -0.2554, -0.1981, -0.2035, -0.2202,\n                        -0.2436, -0.1748, -0.2325, -0.1762, -0.1925, -0.2110, -0.2142, -0.1742,\n                        -0.1764, -0.1849, -0.1909, -0.2102, -0.2048, -0.1950, -0.2233, -0.1788,\n                        -0.2114, -0.2136, -0.1760, -0.2083, -0.2203, -0.2100, -0.1934, -0.2026,\n                        -0.1736, -0.1771, -0.1818, -0.2294, -0.1810, -0.1811, -0.2324, -0.1864,\n                        -0.2310, -0.3230, -0.1985, -0.1773, -0.1838, -0.2263, -0.2682, -0.2287,\n                        -0.1928, -0.2077, -0.1869, -0.1869, -0.1788, -0.2020, -0.1877, -0.1750,\n                        -0.1739, -0.1761, -0.1851, -0.1880, -0.2434, -0.1981, -0.1728, -0.1783,\n                        -0.1663, -0.2439, -0.1953, -0.2163, -0.2183, -0.2055, -0.1939, -0.2001,\n                        -0.1642, -0.2833, -0.2023, -0.2091, -0.1872, -0.2285, -0.1921, -0.2261,\n                        -0.1924, -0.2010, -0.2092, -0.2045, -0.1783, -0.1764, -0.2010, -0.1728,\n                        -0.1769, -0.2012, -0.2003, -0.2642, -0.1945, -0.1768, -0.1621, -0.1858,\n                        -0.2772, -0.1806, -0.1722, -0.1772, -0.1943, -0.1964, -0.2259, -0.2100,\n                        -0.1731, -0.1682, -0.1698, -0.1765, -0.1893, -0.2551, -0.2250, -0.2021,\n                        -0.1735, -0.2177, -0.2476, -0.1659, -0.1926, -0.1939, -0.1911, -0.2178,\n                        -0.2588, -0.1775, -0.1809, -0.1785, -0.1859, -0.1919, -0.2017, -0.2407,\n                        -0.2692, -0.2035, -0.1975, -0.1996, -0.2193, -0.1939, -0.2022, -0.1562,\n                        -0.1933, -0.3142, -0.2000, -0.2548, -0.3076, -0.2049, -0.1873, -0.1999,\n                        -0.2149, -0.1928, -0.1791, -0.2366, -0.1817, -0.1883, -0.2239, -0.1987,\n                        -0.1966, -0.1782, -0.2014, -0.1821, -0.2127, -0.2413, -0.2071, -0.2356,\n                        -0.2080, -0.2230, -0.1625, -0.1773, -0.1828, -0.1918, -0.2088, -0.1889,\n                        -0.1785, -0.1929, -0.1976, -0.2108, -0.2001, -0.2013, -0.2063, -0.1992,\n                        -0.2084, -0.1921, -0.1898, -0.2381, -0.1968, -0.1977, -0.1971, -0.1996,\n                        -0.1901, -0.2166, -0.1635, -0.2072, -0.1926, -0.1899, -0.1849, -0.2143,\n                        -0.1675, -0.3524, -0.2190, -0.1938, -0.1800, -0.1990, -0.1774, -0.1752,\n                        -0.1840, -0.2097, -0.1669, -0.2077, -0.1606, -0.1660, -0.2143, -0.1834,\n                        -0.1725, -0.2594, -0.1821, -0.2126, -0.1786, -0.1827, -0.1827, -0.1751,\n                        -0.1977, -0.2097, -0.2248, -0.1803, -0.2039, -0.1858, -0.1773, -0.2729,\n                        -0.2382, -0.2247, -0.1862, -0.1831, -0.1559, -0.1832, -0.2017, -0.1913,\n                        -0.2004, -0.1782, -0.1805, -0.2247, -0.1699, -0.1725, -0.1797, -0.2553,\n                        -0.1892, -0.1922, -0.1875, -0.1795, -0.1896, -0.3065, -0.1986, -0.2667,\n                        -0.2073, -0.1751, -0.1813, -0.1984, -0.1957, -0.1867, -0.4158, -0.2064,\n                        -0.1668, -0.2045, -0.2247, -0.1777, -0.1711, -0.2383, -0.1854, -0.1932,\n                        -0.1582, -0.1818, -0.1963, -0.1871, -0.2217, -0.1790, -0.2217, -0.1904,\n                        -0.1863, -0.1788, -0.1973, -0.1860, -0.2441, -0.1998, -0.1612, -0.1645,\n                        -0.1756, -0.2600, -0.2105, -0.2463, -0.1948, -0.2077, -0.1813, -0.1855,\n                        -0.1821, -0.1879, -0.2732, -0.1721, -0.1911, -0.2173, -0.2305, -0.1660,\n                        -0.2517, -0.2161, -0.1913, -0.2217, -0.2500, -0.2611, -0.2599, -0.1781,\n                        -0.2116, -0.2047, -0.2267, -0.1950, -0.2144, -0.1990, -0.2265, -0.1816,\n                        -0.1959, -0.2029, -0.1929, -0.2046, -0.1916, -0.1995, -0.1790, -0.2097,\n                        -0.1854, -0.1687, -0.2097, -0.1856, -0.2219, -0.2554, -0.1573, -0.1945,\n                        -0.1841, -0.1944, -0.1887, -0.2026, -0.2209, -0.1922, -0.1715, -0.1604,\n                        -0.2121, -0.2639, -0.1736, -0.2489, -0.2271, -0.2230, -0.1689, -0.1967,\n                        -0.2138, -0.1763, -0.1654, -0.4035, -0.2195, -0.2042, -0.2048, -0.1926,\n                        -0.1842, -0.1899, -0.2018, -0.2269, -0.1893, -0.1849, -0.1716, -0.1682,\n                        -0.1926, -0.2019, -0.2426, -0.1678, -0.1828, -0.2217, -0.1694, -0.2266,\n                        -0.2112, -0.2025, -0.1908, -0.1729, -0.2005, -0.2462, -0.1542, -0.1771,\n                        -0.2016, -0.2043, -0.1734, -0.1952, -0.2114, -0.1878, -0.2042, -0.2622,\n                        -0.1959, -0.2018, -0.1967, -0.2085, -0.2046, -0.3086, -0.2010, -0.2178,\n                        -0.2017, -0.2166, -0.2141, -0.1900, -0.1785, -0.1950, -0.1980, -0.1809,\n                        -0.1870, -0.1646, -0.1702, -0.2084, -0.1765, -0.2301, -0.2067, -0.2001]), max_val=tensor([0.2971, 0.1952, 0.2366, 0.1884, 0.1828, 0.1985, 0.1694, 0.1772, 0.1693,\n                        0.3187, 0.2060, 0.2077, 0.2524, 0.2017, 0.1799, 0.2255, 0.2375, 0.3338,\n                        0.1705, 0.1835, 0.1967, 0.3040, 0.3178, 0.1866, 0.2094, 0.1719, 0.2064,\n                        0.1936, 0.1778, 0.1862, 0.2116, 0.1814, 0.1864, 0.1766, 0.2776, 0.2109,\n                        0.2137, 0.2048, 0.1721, 0.1799, 0.1983, 0.2235, 0.2101, 0.1755, 0.1641,\n                        0.1683, 0.2163, 0.1956, 0.2217, 0.2647, 0.1850, 0.1711, 0.2542, 0.1915,\n                        0.1807, 0.1972, 0.1991, 0.1884, 0.1841, 0.1788, 0.2933, 0.2247, 0.1879,\n                        0.1963, 0.2093, 0.1847, 0.2100, 0.1685, 0.2220, 0.1716, 0.1709, 0.2133,\n                        0.2082, 0.1936, 0.1744, 0.1858, 0.1627, 0.1751, 0.2032, 0.1658, 0.2635,\n                        0.2024, 0.2205, 0.1844, 0.1883, 0.2299, 0.1875, 0.2871, 0.1797, 0.2073,\n                        0.2081, 0.2211, 0.2182, 0.1938, 0.2042, 0.1626, 0.1932, 0.2009, 0.1871,\n                        0.2063, 0.1840, 0.1999, 0.1779, 0.1711, 0.1876, 0.1858, 0.1813, 0.2117,\n                        0.2095, 0.1707, 0.1950, 0.2004, 0.3168, 0.1805, 0.2054, 0.2235, 0.2061,\n                        0.1906, 0.2216, 0.2754, 0.1871, 0.1832, 0.1976, 0.1875, 0.1792, 0.1938,\n                        0.2103, 0.2109, 0.1924, 0.1645, 0.2219, 0.1788, 0.2185, 0.2147, 0.2131,\n                        0.1972, 0.1579, 0.1925, 0.1831, 0.1839, 0.1981, 0.1832, 0.1935, 0.1596,\n                        0.2394, 0.2456, 0.1739, 0.1811, 0.1991, 0.1987, 0.1729, 0.2148, 0.1795,\n                        0.2148, 0.1971, 0.1654, 0.1793, 0.1961, 0.2139, 0.2038, 0.1858, 0.1765,\n                        0.1951, 0.1946, 0.1676, 0.2349, 0.2480, 0.1768, 0.1953, 0.2472, 0.2052,\n                        0.1893, 0.2336, 0.1929, 0.1838, 0.2078, 0.1717, 0.1996, 0.1786, 0.1961,\n                        0.2108, 0.1824, 0.1751, 0.1928, 0.2042, 0.1839, 0.2152, 0.1843, 0.2053,\n                        0.1829, 0.2791, 0.1732, 0.1839, 0.2176, 0.1937, 0.1708, 0.2006, 0.1862,\n                        0.1663, 0.2099, 0.1799, 0.2087, 0.2272, 0.1943, 0.1833, 0.1835, 0.1901,\n                        0.1859, 0.2106, 0.1878, 0.1916, 0.1856, 0.1907, 0.2629, 0.2331, 0.1760,\n                        0.2032, 0.1739, 0.2362, 0.1929, 0.1953, 0.1909, 0.2360, 0.1874, 0.1784,\n                        0.1836, 0.1623, 0.2700, 0.1696, 0.1876, 0.1897, 0.2534, 0.2008, 0.1710,\n                        0.1850, 0.1695, 0.1878, 0.2141, 0.2875, 0.1869, 0.2294, 0.1798, 0.2513,\n                        0.2113, 0.1816, 0.2302, 0.1874, 0.2012, 0.2008, 0.1994, 0.1959, 0.1836,\n                        0.1803, 0.1657, 0.1922, 0.1755, 0.1840, 0.1968, 0.1736, 0.2453, 0.2094,\n                        0.1937, 0.2551, 0.1953, 0.2489, 0.2287, 0.4171, 0.1913, 0.2425, 0.1781,\n                        0.2118, 0.2158, 0.2189, 0.2061, 0.1843, 0.2587, 0.2779, 0.1929, 0.2122,\n                        0.1821, 0.2367, 0.1771, 0.1673, 0.1768, 0.1756, 0.2889, 0.2191, 0.2129,\n                        0.3046, 0.1777, 0.1712, 0.2074, 0.2037, 0.1846, 0.1624, 0.1553, 0.1923,\n                        0.2172, 0.2491, 0.1796, 0.2040, 0.1939, 0.2069, 0.1911, 0.2050, 0.2868,\n                        0.2187, 0.1978, 0.2357, 0.2108, 0.2185, 0.3004, 0.1754, 0.1901, 0.1799,\n                        0.2071, 0.2012, 0.4362, 0.1888, 0.1947, 0.2079, 0.1834, 0.1898, 0.2120,\n                        0.1840, 0.1936, 0.1822, 0.1652, 0.1906, 0.1959, 0.1792, 0.1734, 0.1733,\n                        0.1831, 0.1910, 0.1753, 0.1833, 0.1826, 0.2221, 0.1828, 0.2825, 0.2072,\n                        0.1845, 0.1748, 0.1763, 0.1858, 0.2178, 0.2185, 0.2092, 0.1863, 0.1793,\n                        0.1786, 0.2011, 0.1953, 0.1606, 0.1957, 0.2309, 0.2107, 0.2441, 0.2091,\n                        0.2192, 0.1960, 0.1865, 0.1997, 0.1642, 0.1903, 0.1905, 0.2391, 0.2062,\n                        0.1993, 0.1990, 0.1537, 0.1927, 0.1530, 0.2177, 0.1871, 0.1926, 0.2323,\n                        0.1794, 0.1983, 0.1840, 0.1960, 0.2411, 0.1888, 0.1772, 0.1775, 0.1904,\n                        0.1974, 0.2681, 0.1797, 0.1882, 0.1896, 0.2296, 0.2064, 0.2535, 0.3470,\n                        0.1887, 0.1763, 0.1995, 0.1734, 0.1823, 0.3857, 0.1669, 0.2196, 0.1870,\n                        0.2211, 0.1893, 0.2074, 0.1740, 0.2169, 0.2009, 0.1788, 0.2182, 0.1752,\n                        0.2099, 0.1888, 0.1832, 0.2253, 0.1952, 0.1682, 0.2185, 0.2242, 0.1857,\n                        0.2167, 0.1773, 0.2674, 0.1751, 0.1793, 0.1984, 0.1802, 0.1845, 0.1673,\n                        0.2031, 0.2838, 0.2157, 0.1876, 0.2176, 0.2079, 0.1778, 0.1966, 0.2000,\n                        0.2523, 0.1793, 0.1699, 0.1685, 0.1842, 0.1832, 0.1864, 0.1907, 0.1811,\n                        0.2036, 0.2427, 0.1880, 0.2060, 0.1655, 0.1717, 0.2673, 0.1863, 0.1849,\n                        0.2159, 0.1959, 0.1695, 0.1750, 0.1886, 0.1965, 0.1527, 0.1868, 0.2059,\n                        0.1985, 0.2887, 0.1903, 0.1726, 0.2183, 0.2535, 0.2259, 0.2016, 0.1940,\n                        0.2244, 0.1982, 0.1713, 0.1748, 0.2275, 0.2042, 0.1782, 0.2043, 0.2195,\n                        0.2005, 0.1961, 0.3047, 0.1855, 0.1695, 0.1895, 0.2213, 0.1772, 0.1969,\n                        0.1809, 0.1800, 0.2553, 0.1816, 0.1790, 0.2445, 0.1770, 0.2084, 0.1947,\n                        0.1987, 0.1724, 0.1831, 0.1852, 0.2767, 0.1717, 0.1998, 0.1559, 0.1963,\n                        0.1674, 0.2054, 0.1746, 0.1930, 0.2863, 0.2267, 0.1928, 0.1924, 0.2133,\n                        0.1756, 0.1979, 0.1607, 0.1916, 0.2220, 0.1854, 0.1985, 0.2136, 0.1968,\n                        0.1935, 0.1678, 0.1818, 0.1891, 0.2095, 0.1773, 0.2015, 0.2599, 0.1893,\n                        0.1893, 0.2057, 0.2456, 0.1605, 0.1801, 0.1737, 0.1925, 0.1655, 0.1754,\n                        0.2128, 0.2041, 0.1709, 0.1868, 0.4261, 0.2118, 0.2046, 0.1943, 0.2096,\n                        0.2167, 0.1769, 0.1609, 0.2013, 0.1868, 0.2051, 0.2009, 0.2018, 0.1766,\n                        0.1808, 0.1797, 0.2514, 0.1768, 0.2067, 0.1681, 0.2950, 0.1827, 0.1808,\n                        0.2182, 0.1769, 0.2019, 0.2184, 0.2092, 0.1757, 0.2083, 0.2487, 0.3020,\n                        0.2103, 0.2143, 0.1806, 0.1774, 0.1839, 0.2172, 0.1675, 0.2185, 0.1587,\n                        0.1821, 0.2268, 0.1705, 0.2016, 0.1974, 0.2290, 0.2107, 0.2210, 0.1757,\n                        0.1730, 0.1943, 0.2688, 0.1979, 0.2612, 0.1927, 0.1772, 0.1771, 0.1992,\n                        0.1823, 0.1845, 0.2531, 0.1859, 0.2247, 0.1813, 0.1842, 0.1785, 0.1928,\n                        0.2176, 0.1653, 0.1850, 0.1729, 0.1753, 0.1885, 0.1762, 0.1840, 0.1904,\n                        0.2020, 0.2063, 0.1870, 0.1784, 0.1873, 0.1881, 0.1719, 0.2102, 0.1700,\n                        0.1858, 0.1965, 0.2638, 0.2015, 0.1885, 0.2024, 0.1649, 0.2218, 0.1913,\n                        0.1884, 0.2042, 0.2637, 0.1785, 0.2084, 0.1737, 0.2001, 0.2273, 0.2222,\n                        0.1756, 0.2254, 0.1808, 0.2075, 0.1901, 0.2529, 0.2170, 0.2123, 0.2410,\n                        0.1961, 0.1863, 0.1764, 0.1795, 0.2107, 0.1763, 0.1924, 0.1802, 0.2115,\n                        0.1779, 0.2098, 0.2542, 0.2027, 0.1693, 0.1811, 0.1917, 0.1894, 0.1971,\n                        0.1793, 0.2500, 0.1930, 0.2056, 0.2016, 0.1967, 0.1882, 0.2235, 0.2078,\n                        0.1596, 0.1989, 0.1676, 0.1853, 0.1812, 0.1836, 0.1956, 0.1627, 0.1939,\n                        0.1831, 0.1815, 0.1742, 0.1828, 0.1883, 0.6272, 0.1990, 0.2130, 0.1927,\n                        0.1965, 0.1928, 0.1774, 0.2046, 0.1906, 0.1781, 0.1874, 0.1894, 0.1923,\n                        0.2072, 0.1915, 0.2286, 0.1968, 0.1929, 0.1810, 0.1806, 0.2040, 0.1627,\n                        0.2011, 0.1742, 0.1867, 0.2025, 0.2354, 0.1783, 0.1941, 0.2286, 0.1824,\n                        0.2392, 0.2148, 0.2438, 0.1886, 0.2577, 0.1835, 0.2006, 0.1814, 0.1660,\n                        0.2642, 0.1839, 0.2643, 0.1713, 0.2196, 0.2015, 0.1846, 0.2049, 0.1895,\n                        0.1806, 0.2006, 0.1717, 0.1619, 0.1761, 0.1792, 0.2074, 0.1938, 0.1873,\n                        0.2822, 0.1971, 0.1975])\n              )\n            )\n          )\n        )\n      )\n    )\n  )\n  (activation_post_process_1): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0176]), zero_point=tensor([60], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0616124868392944, max_val=1.1766728162765503)\n  )\n  (activation_post_process_2): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0176]), zero_point=tensor([60], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0616124868392944, max_val=1.1766728162765503)\n  )\n  (activation_post_process_3): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0110]), zero_point=tensor([62], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.6833268404006958, max_val=0.7143961191177368)\n  )\n  (activation_post_process_4): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0110]), zero_point=tensor([62], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.6833268404006958, max_val=0.7143961191177368)\n  )\n  (activation_post_process_11): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0034]), zero_point=tensor([67], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2266329526901245, max_val=0.2015158087015152)\n  )\n  (activation_post_process_5): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0031]), zero_point=tensor([62], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.1949658840894699, max_val=0.20361801981925964)\n  )\n  (activation_post_process_6): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0031]), zero_point=tensor([62], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.19016119837760925, max_val=0.19924528896808624)\n  )\n  (activation_post_process_7): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.3802]), zero_point=tensor([63], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-23.824138641357422, max_val=24.4601993560791)\n  )\n  (activation_post_process_8): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.1440]), zero_point=tensor([66], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-9.487757682800293, max_val=8.801295280456543)\n  )\n  (activation_post_process_9): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0687]), zero_point=tensor([2], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.16973841190338135, max_val=8.553435325622559)\n  )\n  (activation_post_process_10): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0641]), zero_point=tensor([65], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-4.188440799713135, max_val=3.956249952316284)\n  )\n  (activation_post_process_12): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0631]), zero_point=tensor([65], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-4.122890949249268, max_val=3.8902158737182617)\n  )\n  (activation_post_process_13): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0114]), zero_point=tensor([55], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.6275590658187866, max_val=0.8192094564437866)\n  )\n  (activation_post_process_14): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0112]), zero_point=tensor([55], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.618950366973877, max_val=0.80401211977005)\n  )\n  (activation_post_process_15): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0133]), zero_point=tensor([51], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.671795129776001, max_val=1.013983130455017)\n  )\n  (activation_post_process_22): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0063]), zero_point=tensor([62], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.38834795355796814, max_val=0.4091598391532898)\n  )\n  (activation_post_process_16): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0047]), zero_point=tensor([66], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.31070682406425476, max_val=0.2862929403781891)\n  )\n  (activation_post_process_17): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0046]), zero_point=tensor([66], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3044951260089874, max_val=0.2844727039337158)\n  )\n  (activation_post_process_18): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.3640]), zero_point=tensor([66], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-24.13080596923828, max_val=22.0911865234375)\n  )\n  (activation_post_process_19): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.1470]), zero_point=tensor([71], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-10.43090534210205, max_val=8.242938041687012)\n  )\n  (activation_post_process_20): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0652]), zero_point=tensor([3], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.16989874839782715, max_val=8.105669975280762)\n  )\n  (activation_post_process_21): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0581]), zero_point=tensor([86], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-5.015749454498291, max_val=2.3580427169799805)\n  )\n  (activation_post_process_23): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0569]), zero_point=tensor([87], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-4.97128963470459, max_val=2.2600886821746826)\n  )\n  (activation_post_process_24): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0193]), zero_point=tensor([43], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.8363053202629089, max_val=1.613936185836792)\n  )\n  (activation_post_process_25): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0193]), zero_point=tensor([43], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.8185942769050598, max_val=1.6268445253372192)\n  )\n  (activation_post_process_26): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0228]), zero_point=tensor([28], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.6493717432022095, max_val=2.248940944671631)\n  )\n  (activation_post_process_33): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0072]), zero_point=tensor([64], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4615464508533478, max_val=0.45073816180229187)\n  )\n  (activation_post_process_27): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0027]), zero_point=tensor([64], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.17627736926078796, max_val=0.1713627427816391)\n  )\n  (activation_post_process_28): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0027]), zero_point=tensor([64], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.1714409440755844, max_val=0.16800741851329803)\n  )\n  (activation_post_process_29): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.2986]), zero_point=tensor([67], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-20.067913055419922, max_val=17.84999656677246)\n  )\n  (activation_post_process_30): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.1082]), zero_point=tensor([73], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-7.88311767578125, max_val=5.8632659912109375)\n  )\n  (activation_post_process_31): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0467]), zero_point=tensor([4], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.16996371746063232, max_val=5.755808353424072)\n  )\n  (activation_post_process_32): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0694]), zero_point=tensor([71], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-4.933929920196533, max_val=3.873918294906616)\n  )\n  (activation_post_process_34): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0684]), zero_point=tensor([72], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-4.889355659484863, max_val=3.7919342517852783)\n  )\n  (activation_post_process_35): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0225]), zero_point=tensor([71], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.6072697639465332, max_val=1.2498490810394287)\n  )\n  (activation_post_process_36): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0221]), zero_point=tensor([73], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.6183632612228394, max_val=1.1875859498977661)\n  )\n  (activation_post_process_37): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0352]), zero_point=tensor([53], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.8649588823318481, max_val=2.607884168624878)\n  )\n  (activation_post_process_38): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0352]), zero_point=tensor([53], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.8649588823318481, max_val=2.607884168624878)\n  )\n  (activation_post_process_39): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0089]), zero_point=tensor([69], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.615966796875, max_val=0.5153531432151794)\n  )\n  (activation_post_process_40): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0089]), zero_point=tensor([69], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.615966796875, max_val=0.5153531432151794)\n  )\n  (activation_post_process_41): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0115]), zero_point=tensor([80], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.9211301207542419, max_val=0.5347496867179871)\n  )\n  (activation_post_process_48): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0047]), zero_point=tensor([37], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.17486688494682312, max_val=0.4224856197834015)\n  )\n  (activation_post_process_42): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0043]), zero_point=tensor([69], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.29466167092323303, max_val=0.24944216012954712)\n  )\n  (activation_post_process_43): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0041]), zero_point=tensor([69], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.28275635838508606, max_val=0.23918011784553528)\n  )\n  (activation_post_process_44): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.2174]), zero_point=tensor([78], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-17.052263259887695, max_val=10.55688762664795)\n  )\n  (activation_post_process_45): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.1001]), zero_point=tensor([68], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-6.7549638748168945, max_val=5.953451156616211)\n  )\n  (activation_post_process_46): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0471]), zero_point=tensor([4], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.16948941349983215, max_val=5.817306995391846)\n  )\n  (activation_post_process_47): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0847]), zero_point=tensor([94], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-7.967654705047607, max_val=2.787677764892578)\n  )\n  (activation_post_process_49): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0822]), zero_point=tensor([94], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-7.75468111038208, max_val=2.678812026977539)\n  )\n  (activation_post_process_50): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0288]), zero_point=tensor([112], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.2423288822174072, max_val=0.41967305541038513)\n  )\n  (activation_post_process_51): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0291]), zero_point=tensor([113], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.280379295349121, max_val=0.41371047496795654)\n  )\n  (activation_post_process_52): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0358]), zero_point=tensor([111], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.976576089859009, max_val=0.5723773241043091)\n  )\n  (activation_post_process_59): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0065]), zero_point=tensor([87], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5665181875228882, max_val=0.2622144818305969)\n  )\n  (activation_post_process_53): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0038]), zero_point=tensor([75], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.28065720200538635, max_val=0.19714167714118958)\n  )\n  (activation_post_process_54): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0037]), zero_point=tensor([75], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2766946256160736, max_val=0.1892133206129074)\n  )\n  (activation_post_process_55): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.2192]), zero_point=tensor([60], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-13.0967378616333, max_val=14.741006851196289)\n  )\n  (activation_post_process_56): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.1018]), zero_point=tensor([69], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-6.989694118499756, max_val=5.944207668304443)\n  )\n  (activation_post_process_57): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0457]), zero_point=tensor([4], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.16965427994728088, max_val=5.633297920227051)\n  )\n  (activation_post_process_58): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0894]), zero_point=tensor([46], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-4.145941257476807, max_val=7.203052043914795)\n  )\n  (activation_post_process_60): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0854]), zero_point=tensor([47], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-4.03858757019043, max_val=6.805595397949219)\n  )\n  (activation_post_process_61): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0367]), zero_point=tensor([102], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.754591464996338, max_val=0.9001493453979492)\n  )\n  (activation_post_process_62): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0367]), zero_point=tensor([103], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.771451473236084, max_val=0.8895657658576965)\n  )\n  (activation_post_process_63): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0636]), zero_point=tensor([112], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-7.143592357635498, max_val=0.9333856701850891)\n  )\n  (activation_post_process_70): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0125]), zero_point=tensor([66], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.8168216943740845, max_val=0.765492856502533)\n  )\n  (activation_post_process_64): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0071]), zero_point=tensor([70], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4951398968696594, max_val=0.40079060196876526)\n  )\n  (activation_post_process_65): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0069]), zero_point=tensor([71], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.48492106795310974, max_val=0.3867117166519165)\n  )\n  (activation_post_process_66): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.1592]), zero_point=tensor([71], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-11.26034164428711, max_val=8.953231811523438)\n  )\n  (activation_post_process_67): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.1016]), zero_point=tensor([78], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-7.9179768562316895, max_val=4.987223148345947)\n  )\n  (activation_post_process_68): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0397]), zero_point=tensor([4], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.16960373520851135, max_val=4.8744611740112305)\n  )\n  (activation_post_process_69): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.1117]), zero_point=tensor([58], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-6.424079418182373, max_val=7.7572340965271)\n  )\n  (activation_post_process_71): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.1082]), zero_point=tensor([58], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-6.226460933685303, max_val=7.520759582519531)\n  )\n  (activation_post_process_72): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0900]), zero_point=tensor([66], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-5.9113030433654785, max_val=5.513059139251709)\n  )\n  (activation_post_process_73): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0909]), zero_point=tensor([66], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-5.97604513168335, max_val=5.5643157958984375)\n  )\n  (activation_post_process_74): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.1231]), zero_point=tensor([81], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-9.987297058105469, max_val=5.6496195793151855)\n  )\n  (activation_post_process_75): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.1231]), zero_point=tensor([81], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-9.987297058105469, max_val=5.6496195793151855)\n  )\n  (activation_post_process_76): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0053]), zero_point=tensor([68], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3577841818332672, max_val=0.313448429107666)\n  )\n  (activation_post_process_77): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0053]), zero_point=tensor([68], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3577841818332672, max_val=0.313448429107666)\n  )\n  (activation_post_process_78): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0080]), zero_point=tensor([58], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4603448510169983, max_val=0.5549927353858948)\n  )\n  (activation_post_process_85): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0016]), zero_point=tensor([73], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.11644890904426575, max_val=0.0863666832447052)\n  )\n  (activation_post_process_79): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0025]), zero_point=tensor([59], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.14775703847408295, max_val=0.1698305606842041)\n  )\n  (activation_post_process_80): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0024]), zero_point=tensor([59], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.14244037866592407, max_val=0.16395226120948792)\n  )\n  (activation_post_process_81): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.1786]), zero_point=tensor([43], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-7.702210903167725, max_val=14.977051734924316)\n  )\n  (activation_post_process_82): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.1089]), zero_point=tensor([75], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-8.160797119140625, max_val=5.670104503631592)\n  )\n  (activation_post_process_83): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0445]), zero_point=tensor([4], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.1699424833059311, max_val=5.480155944824219)\n  )\n  (activation_post_process_84): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0638]), zero_point=tensor([64], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-4.062187194824219, max_val=4.035497665405273)\n  )\n  (activation_post_process_86): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0619]), zero_point=tensor([64], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.9371135234832764, max_val=3.9277443885803223)\n  )\n  (activation_post_process_87): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0048]), zero_point=tensor([57], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.27138420939445496, max_val=0.3332441449165344)\n  )\n  (activation_post_process_88): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0048]), zero_point=tensor([56], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.27032387256622314, max_val=0.33798056840896606)\n  )\n  (activation_post_process_89): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0082]), zero_point=tensor([59], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4866878092288971, max_val=0.5551044344902039)\n  )\n  (activation_post_process_96): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0035]), zero_point=tensor([96], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.33539870381355286, max_val=0.10977963358163834)\n  )\n  (activation_post_process_90): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0037]), zero_point=tensor([60], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.22176603972911835, max_val=0.2469877153635025)\n  )\n  (activation_post_process_91): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0036]), zero_point=tensor([60], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.21561923623085022, max_val=0.23905214667320251)\n  )\n  (activation_post_process_92): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.1461]), zero_point=tensor([67], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-9.80976390838623, max_val=8.746265411376953)\n  )\n  (activation_post_process_93): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.1049]), zero_point=tensor([79], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-8.254278182983398, max_val=5.067026138305664)\n  )\n  (activation_post_process_94): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0398]), zero_point=tensor([4], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.16990695893764496, max_val=4.885670185089111)\n  )\n  (activation_post_process_95): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0617]), zero_point=tensor([63], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.918161153793335, max_val=3.920179605484009)\n  )\n  (activation_post_process_97): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0602]), zero_point=tensor([64], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.8247694969177246, max_val=3.820336103439331)\n  )\n  (activation_post_process_98): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0118]), zero_point=tensor([54], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.6324271559715271, max_val=0.8685418963432312)\n  )\n  (activation_post_process_99): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0117]), zero_point=tensor([52], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.6044521331787109, max_val=0.880778431892395)\n  )\n  (activation_post_process_100): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0130]), zero_point=tensor([41], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5355253219604492, max_val=1.1133581399917603)\n  )\n  (activation_post_process_107): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0057]), zero_point=tensor([27], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.15731991827487946, max_val=0.570029079914093)\n  )\n  (activation_post_process_101): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0051]), zero_point=tensor([57], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2917209565639496, max_val=0.35819336771965027)\n  )\n  (activation_post_process_102): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0050]), zero_point=tensor([57], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.28335970640182495, max_val=0.35107341408729553)\n  )\n  (activation_post_process_103): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.1478]), zero_point=tensor([53], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-7.766916275024414, max_val=11.005539894104004)\n  )\n  (activation_post_process_104): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.1229]), zero_point=tensor([89], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-10.895971298217773, max_val=4.709015369415283)\n  )\n  (activation_post_process_105): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0374]), zero_point=tensor([5], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.16992956399917603, max_val=4.57649040222168)\n  )\n  (activation_post_process_106): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0688]), zero_point=tensor([54], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.7426304817199707, max_val=4.9929704666137695)\n  )\n  (activation_post_process_108): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0665]), zero_point=tensor([53], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.5477852821350098, max_val=4.893321990966797)\n  )\n  (activation_post_process_109): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0251]), zero_point=tensor([17], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.41697603464126587, max_val=2.7654495239257812)\n  )\n  (activation_post_process_110): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0256]), zero_point=tensor([15], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3842437267303467, max_val=2.866992235183716)\n  )\n  (activation_post_process_111): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0314]), zero_point=tensor([21], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.6684264540672302, max_val=3.321056604385376)\n  )\n  (activation_post_process_118): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0083]), zero_point=tensor([26], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2142772674560547, max_val=0.8360745310783386)\n  )\n  (activation_post_process_112): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0044]), zero_point=tensor([64], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.27664539217948914, max_val=0.2764536440372467)\n  )\n  (activation_post_process_113): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0042]), zero_point=tensor([63], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.26361706852912903, max_val=0.2697003185749054)\n  )\n  (activation_post_process_114): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.2226]), zero_point=tensor([47], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-10.53746509552002, max_val=17.73316192626953)\n  )\n  (activation_post_process_115): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.1123]), zero_point=tensor([74], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-8.355232238769531, max_val=5.907013893127441)\n  )\n  (activation_post_process_116): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0466]), zero_point=tensor([4], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.16972242295742035, max_val=5.74878454208374)\n  )\n  (activation_post_process_117): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0707]), zero_point=tensor([64], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-4.513115882873535, max_val=4.466853141784668)\n  )\n  (activation_post_process_119): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0682]), zero_point=tensor([64], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-4.353522300720215, max_val=4.308958053588867)\n  )\n  (activation_post_process_120): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0296]), zero_point=tensor([22], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.6469994783401489, max_val=3.112727642059326)\n  )\n  (activation_post_process_121): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0304]), zero_point=tensor([21], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.6459388732910156, max_val=3.217318058013916)\n  )\n  (activation_post_process_122): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0528]), zero_point=tensor([17], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.8952133655548096, max_val=5.805991172790527)\n  )\n  (activation_post_process_129): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0098]), zero_point=tensor([106], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.044601559638977, max_val=0.2061680108308792)\n  )\n  (activation_post_process_123): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0061]), zero_point=tensor([67], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4075455367565155, max_val=0.36469951272010803)\n  )\n  (activation_post_process_124): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0059]), zero_point=tensor([67], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.39538201689720154, max_val=0.35461121797561646)\n  )\n  (activation_post_process_125): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.2055]), zero_point=tensor([58], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-12.020212173461914, max_val=14.081425666809082)\n  )\n  (activation_post_process_126): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.1277]), zero_point=tensor([77], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-9.77492618560791, max_val=6.447988986968994)\n  )\n  (activation_post_process_127): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0503]), zero_point=tensor([3], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.1699296236038208, max_val=6.218933582305908)\n  )\n  (activation_post_process_128): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0632]), zero_point=tensor([68], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-4.273190498352051, max_val=3.7523961067199707)\n  )\n  (activation_post_process_130): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0613]), zero_point=tensor([68], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-4.19120979309082, max_val=3.596569299697876)\n  )\n  (activation_post_process_131): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0389]), zero_point=tensor([16], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.6251667737960815, max_val=4.319167613983154)\n  )\n  (activation_post_process_132): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0409]), zero_point=tensor([15], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.6293637156486511, max_val=4.56070613861084)\n  )\n  (activation_post_process_133): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0835]), zero_point=tensor([13], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0523627996444702, max_val=9.551192283630371)\n  )\n  (activation_post_process_140): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0119]), zero_point=tensor([27], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.31839004158973694, max_val=1.1897728443145752)\n  )\n  (activation_post_process_134): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0132]), zero_point=tensor([76], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.00372314453125, max_val=0.6686846017837524)\n  )\n  (activation_post_process_135): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0128]), zero_point=tensor([77], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.9879545569419861, max_val=0.6325427293777466)\n  )\n  (activation_post_process_136): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.1910]), zero_point=tensor([74], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-14.160364151000977, max_val=10.09770679473877)\n  )\n  (activation_post_process_137): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.1298]), zero_point=tensor([73], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-9.486958503723145, max_val=6.994064807891846)\n  )\n  (activation_post_process_138): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0547]), zero_point=tensor([3], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.16983433067798615, max_val=6.774710655212402)\n  )\n  (activation_post_process_139): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0932]), zero_point=tensor([62], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-5.7904744148254395, max_val=6.049445629119873)\n  )\n  (activation_post_process_141): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0887]), zero_point=tensor([62], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-5.510374069213867, max_val=5.755146503448486)\n  )\n  (activation_post_process_142): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0585]), zero_point=tensor([24], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.4043569564819336, max_val=6.02310037612915)\n  )\n  (activation_post_process_143): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0611]), zero_point=tensor([23], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.407608151435852, max_val=6.351842880249023)\n  )\n  (activation_post_process_144): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.1215]), zero_point=tensor([12], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.4746894836425781, max_val=13.955227851867676)\n  )\n  (activation_post_process_151): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0148]), zero_point=tensor([101], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.492621660232544, max_val=0.38475316762924194)\n  )\n  (activation_post_process_145): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0098]), zero_point=tensor([60], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.583331823348999, max_val=0.6573845744132996)\n  )\n  (activation_post_process_146): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0094]), zero_point=tensor([60], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5594462752342224, max_val=0.6337649822235107)\n  )\n  (activation_post_process_147): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.2671]), zero_point=tensor([74], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-19.886945724487305, max_val=14.034634590148926)\n  )\n  (activation_post_process_148): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.1229]), zero_point=tensor([78], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-9.632854461669922, max_val=5.981197357177734)\n  )\n  (activation_post_process_149): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0472]), zero_point=tensor([4], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.16992712020874023, max_val=5.824703216552734)\n  )\n  (activation_post_process_150): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.1348]), zero_point=tensor([94], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-12.676762580871582, max_val=4.4393110275268555)\n  )\n  (activation_post_process_152): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.1326]), zero_point=tensor([94], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-12.497160911560059, max_val=4.346286773681641)\n  )\n  (activation_post_process_153): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.1582]), zero_point=tensor([10], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.5896772146224976, max_val=18.49589729309082)\n  )\n  (activation_post_process_154): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.1674]), zero_point=tensor([9], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.4805763959884644, max_val=19.777406692504883)\n  )\n  (activation_post_process_155): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.2625]), zero_point=tensor([7], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.8679838180541992, max_val=31.47182273864746)\n  )\n  (activation_post_process_162): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0187]), zero_point=tensor([28], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5267910957336426, max_val=1.845348596572876)\n  )\n  (activation_post_process_156): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0189]), zero_point=tensor([65], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.233229160308838, max_val=1.16639244556427)\n  )\n  (activation_post_process_157): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0183]), zero_point=tensor([65], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.1908321380615234, max_val=1.1356836557388306)\n  )\n  (activation_post_process_158): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.3678]), zero_point=tensor([68], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-25.12060546875, max_val=21.59488868713379)\n  )\n  (activation_post_process_159): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.1428]), zero_point=tensor([77], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-11.032332420349121, max_val=7.098635196685791)\n  )\n  (activation_post_process_160): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0553]), zero_point=tensor([3], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.16962121427059174, max_val=6.856800556182861)\n  )\n  (activation_post_process_161): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.1984]), zero_point=tensor([17], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.4552814960479736, max_val=21.744922637939453)\n  )\n  (activation_post_process_163): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.1910]), zero_point=tensor([17], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.299546480178833, max_val=20.960126876831055)\n  )\n  (activation_post_process_164): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.3444]), zero_point=tensor([17], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-5.897017002105713, max_val=37.846771240234375)\n  )\n  (activation_post_process_165): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.3643]), zero_point=tensor([17], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-6.172945499420166, max_val=40.09518814086914)\n  )\n  (activation_post_process_166): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.5418]), zero_point=tensor([5], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.5936408042907715, max_val=66.21417236328125)\n  )\n  (activation_post_process_173): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0202]), zero_point=tensor([98], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.9757620096206665, max_val=0.5883321762084961)\n  )\n  (activation_post_process_167): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0184]), zero_point=tensor([56], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0249128341674805, max_val=1.3111673593521118)\n  )\n  (activation_post_process_168): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0178]), zero_point=tensor([55], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.9838797450065613, max_val=1.277819275856018)\n  )\n  (activation_post_process_169): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.4188]), zero_point=tensor([65], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-27.36811637878418, max_val=25.819477081298828)\n  )\n  (activation_post_process_170): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.1777]), zero_point=tensor([81], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-14.344317436218262, max_val=8.220355033874512)\n  )\n  (activation_post_process_171): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0635]), zero_point=tensor([3], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.16956710815429688, max_val=7.896186828613281)\n  )\n  (activation_post_process_172): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.2873]), zero_point=tensor([100], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-28.694677352905273, max_val=7.786972999572754)\n  )\n  (activation_post_process_174): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.2712]), zero_point=tensor([100], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-27.23874282836914, max_val=7.208102703094482)\n  )\n  (activation_post_process_175): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.4960]), zero_point=tensor([22], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-10.982097625732422, max_val=52.01322937011719)\n  )\n  (activation_post_process_176): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.5228]), zero_point=tensor([21], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-11.12863540649414, max_val=55.26945114135742)\n  )\n  (activation_post_process_177): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.8252]), zero_point=tensor([3], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.700928211212158, max_val=102.09810638427734)\n  )\n  (activation_post_process_178): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.8252]), zero_point=tensor([3], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.700928211212158, max_val=102.09810638427734)\n  )\n  (activation_post_process_179): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0164]), zero_point=tensor([63], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.032258152961731, max_val=1.0469088554382324)\n  )\n  (activation_post_process_180): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0164]), zero_point=tensor([63], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.032258152961731, max_val=1.0469088554382324)\n  )\n  (activation_post_process_181): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0257]), zero_point=tensor([58], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.484893798828125, max_val=1.7841823101043701)\n  )\n  (activation_post_process_188): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0137]), zero_point=tensor([65], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.8951234817504883, max_val=0.8404833078384399)\n  )\n  (activation_post_process_182): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0165]), zero_point=tensor([77], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.263350009918213, max_val=0.8261895775794983)\n  )\n  (activation_post_process_183): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0159]), zero_point=tensor([78], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.2393046617507935, max_val=0.7856418490409851)\n  )\n  (activation_post_process_184): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.2976]), zero_point=tensor([64], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-18.972686767578125, max_val=18.825490951538086)\n  )\n  (activation_post_process_185): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.3104]), zero_point=tensor([89], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-27.68568229675293, max_val=11.732460975646973)\n  )\n  (activation_post_process_186): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0883]), zero_point=tensor([2], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.16606716811656952, max_val=11.050043106079102)\n  )\n  (activation_post_process_187): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.1059]), zero_point=tensor([68], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-7.193780422210693, max_val=6.253548622131348)\n  )\n  (activation_post_process_189): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0954]), zero_point=tensor([67], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-6.432901859283447, max_val=5.685900688171387)\n  )\n  (activation_post_process_190): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0380]), zero_point=tensor([60], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.2727317810058594, max_val=2.5477454662323)\n  )\n  (activation_post_process_191): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0378]), zero_point=tensor([61], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.28790283203125, max_val=2.5118966102600098)\n  )\n  (activation_post_process_192): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0449]), zero_point=tensor([56], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.5073747634887695, max_val=3.193829298019409)\n  )\n  (activation_post_process_199): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0144]), zero_point=tensor([64], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.9255371689796448, max_val=0.9062274694442749)\n  )\n  (activation_post_process_193): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0307]), zero_point=tensor([69], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.1059954166412354, max_val=1.794583797454834)\n  )\n  (activation_post_process_194): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0298]), zero_point=tensor([68], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.0321662425994873, max_val=1.7487369775772095)\n  )\n  (activation_post_process_195): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.2232]), zero_point=tensor([54], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-12.069433212280273, max_val=16.272293090820312)\n  )\n  (activation_post_process_196): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.1843]), zero_point=tensor([89], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-16.43532371520996, max_val=6.9756693840026855)\n  )\n  (activation_post_process_197): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0532]), zero_point=tensor([3], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.16992786526679993, max_val=6.582432270050049)\n  )\n  (activation_post_process_198): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0942]), zero_point=tensor([48], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-4.567194938659668, max_val=7.399138927459717)\n  )\n  (activation_post_process_200): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0896]), zero_point=tensor([49], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-4.3543620109558105, max_val=7.026246547698975)\n  )\n  (activation_post_process_201): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0561]), zero_point=tensor([66], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.683408498764038, max_val=3.439636707305908)\n  )\n  (activation_post_process_202): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0600]), zero_point=tensor([65], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.9162631034851074, max_val=3.707587242126465)\n  )\n  (activation_post_process_203): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0739]), zero_point=tensor([65], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-4.842237949371338, max_val=4.547823905944824)\n  )\n  (activation_post_process_210): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0138]), zero_point=tensor([64], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.8775014877319336, max_val=0.8755592703819275)\n  )\n  (activation_post_process_204): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0495]), zero_point=tensor([53], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.627852439880371, max_val=3.658982276916504)\n  )\n  (activation_post_process_205): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0480]), zero_point=tensor([53], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.5686416625976562, max_val=3.531486749649048)\n  )\n  (activation_post_process_206): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.3365]), zero_point=tensor([52], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-17.610885620117188, max_val=25.12705421447754)\n  )\n  (activation_post_process_207): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.2343]), zero_point=tensor([77], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-17.940353393554688, max_val=11.811502456665039)\n  )\n  (activation_post_process_208): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0885]), zero_point=tensor([2], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.16935108602046967, max_val=11.069193840026855)\n  )\n  (activation_post_process_209): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.1646]), zero_point=tensor([64], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-10.591503143310547, max_val=10.317668914794922)\n  )\n  (activation_post_process_211): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.1503]), zero_point=tensor([63], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-9.40211296081543, max_val=9.690129280090332)\n  )\n  (activation_post_process_212): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0772]), zero_point=tensor([69], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-5.307229995727539, max_val=4.4956841468811035)\n  )\n  (activation_post_process_213): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0818]), zero_point=tensor([70], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-5.706943035125732, max_val=4.6872100830078125)\n  )\n  (activation_post_process_214): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0699]), zero_point=tensor([68], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-4.786128997802734, max_val=4.090804100036621)\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=1)\n  (activation_post_process_215): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0699]), zero_point=tensor([68], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-4.786128997802734, max_val=4.090804100036621)\n  )\n  (activation_post_process_216): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0699]), zero_point=tensor([68], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-4.786128997802734, max_val=4.090804100036621)\n  )\n  (classifier): Module(\n    (0): Module()\n    (1): Flatten(start_dim=1, end_dim=-1)\n    (2): Linear(\n      in_features=768, out_features=10, bias=True\n      (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n        fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0018, 0.0015, 0.0024, 0.0016, 0.0015, 0.0017, 0.0016, 0.0017, 0.0018,\n                0.0024]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n        (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n          min_val=tensor([-0.2340, -0.1910, -0.3075, -0.2053, -0.1917, -0.2227, -0.2095, -0.2118,\n                  -0.2337, -0.3081]), max_val=tensor([0.2295, 0.1887, 0.2498, 0.2037, 0.1907, 0.2197, 0.1960, 0.2101, 0.2307,\n                  0.2573])\n        )\n      )\n    )\n  )\n  (activation_post_process_217): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0701]), zero_point=tensor([51], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.5606067180633545, max_val=5.346323013305664)\n  )\n  (activation_post_process_218): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0701]), zero_point=tensor([51], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.5606067180633545, max_val=5.346323013305664)\n  )\n  (activation_post_process_219): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0653]), zero_point=tensor([51], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.3537960052490234, max_val=4.938382148742676)\n  )\n  (activation_post_process_220): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0968]), zero_point=tensor([57], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-5.482699871063232, max_val=6.814818382263184)\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"model_quantized_trained = quantize_fx.convert_fx(model_prepared)","metadata":{"execution":{"iopub.status.busy":"2024-04-09T11:08:13.314374Z","iopub.execute_input":"2024-04-09T11:08:13.314766Z","iopub.status.idle":"2024-04-09T11:08:18.690677Z","shell.execute_reply.started":"2024-04-09T11:08:13.314736Z","shell.execute_reply":"2024-04-09T11:08:18.689729Z"},"trusted":true},"execution_count":98,"outputs":[]},{"cell_type":"code","source":"model_quantized_trained.to('cpu')","metadata":{"execution":{"iopub.status.busy":"2024-04-09T11:08:18.692523Z","iopub.execute_input":"2024-04-09T11:08:18.693361Z","iopub.status.idle":"2024-04-09T11:08:18.884502Z","shell.execute_reply.started":"2024-04-09T11:08:18.693324Z","shell.execute_reply":"2024-04-09T11:08:18.883259Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":99,"outputs":[{"execution_count":99,"output_type":"execute_result","data":{"text/plain":"GraphModule(\n  (features): Module(\n    (0): Module(\n      (0): QuantizedConv2d(3, 96, kernel_size=(4, 4), stride=(4, 4), scale=0.017624294385313988, zero_point=60)\n      (1): Module()\n    )\n    (1): Module(\n      (0): Module(\n        (block): Module(\n          (0): QuantizedConv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), scale=0.003138455795124173, zero_point=62, padding=(3, 3), groups=96)\n          (2): QuantizedLayerNorm((96,), eps=1e-06, elementwise_affine=True)\n          (3): QuantizedLinear(in_features=96, out_features=384, scale=0.14400829374790192, zero_point=66, qscheme=torch.per_channel_affine)\n          (4): GELU(approximate='none')\n          (5): QuantizedLinear(in_features=384, out_features=96, scale=0.06413142383098602, zero_point=65, qscheme=torch.per_channel_affine)\n        )\n      )\n      (1): Module(\n        (block): Module(\n          (0): QuantizedConv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), scale=0.004700785502791405, zero_point=66, padding=(3, 3), groups=96)\n          (2): QuantizedLayerNorm((96,), eps=1e-06, elementwise_affine=True)\n          (3): QuantizedLinear(in_features=96, out_features=384, scale=0.14703813195228577, zero_point=71, qscheme=torch.per_channel_affine)\n          (4): GELU(approximate='none')\n          (5): QuantizedLinear(in_features=384, out_features=96, scale=0.058061353862285614, zero_point=86, qscheme=torch.per_channel_affine)\n        )\n      )\n      (2): Module(\n        (block): Module(\n          (0): QuantizedConv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), scale=0.002737323520705104, zero_point=64, padding=(3, 3), groups=96)\n          (2): QuantizedLayerNorm((96,), eps=1e-06, elementwise_affine=True)\n          (3): QuantizedLinear(in_features=96, out_features=384, scale=0.10823924094438553, zero_point=73, qscheme=torch.per_channel_affine)\n          (4): GELU(approximate='none')\n          (5): QuantizedLinear(in_features=384, out_features=96, scale=0.0693531334400177, zero_point=71, qscheme=torch.per_channel_affine)\n        )\n      )\n    )\n    (2): Module(\n      (0): Module()\n      (1): QuantizedConv2d(96, 192, kernel_size=(2, 2), stride=(2, 2), scale=0.011463620699942112, zero_point=80)\n    )\n    (3): Module(\n      (0): Module(\n        (block): Module(\n          (0): QuantizedConv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), scale=0.004284282214939594, zero_point=69, padding=(3, 3), groups=192)\n          (2): QuantizedLayerNorm((192,), eps=1e-06, elementwise_affine=True)\n          (3): QuantizedLinear(in_features=192, out_features=768, scale=0.10006625950336456, zero_point=68, qscheme=torch.per_channel_affine)\n          (4): GELU(approximate='none')\n          (5): QuantizedLinear(in_features=768, out_features=192, scale=0.08468765765428543, zero_point=94, qscheme=torch.per_channel_affine)\n        )\n      )\n      (1): Module(\n        (block): Module(\n          (0): QuantizedConv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), scale=0.0037621958181262016, zero_point=75, padding=(3, 3), groups=192)\n          (2): QuantizedLayerNorm((192,), eps=1e-06, elementwise_affine=True)\n          (3): QuantizedLinear(in_features=192, out_features=768, scale=0.1018417477607727, zero_point=69, qscheme=torch.per_channel_affine)\n          (4): GELU(approximate='none')\n          (5): QuantizedLinear(in_features=768, out_features=192, scale=0.08936215192079544, zero_point=46, qscheme=torch.per_channel_affine)\n        )\n      )\n      (2): Module(\n        (block): Module(\n          (0): QuantizedConv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), scale=0.007054571062326431, zero_point=70, padding=(3, 3), groups=192)\n          (2): QuantizedLayerNorm((192,), eps=1e-06, elementwise_affine=True)\n          (3): QuantizedLinear(in_features=192, out_features=768, scale=0.10161574929952621, zero_point=78, qscheme=torch.per_channel_affine)\n          (4): GELU(approximate='none')\n          (5): QuantizedLinear(in_features=768, out_features=192, scale=0.11166388541460037, zero_point=58, qscheme=torch.per_channel_affine)\n        )\n      )\n    )\n    (4): Module(\n      (0): Module()\n      (1): QuantizedConv2d(192, 384, kernel_size=(2, 2), stride=(2, 2), scale=0.00799478404223919, zero_point=58)\n    )\n    (5): Module(\n      (0): Module(\n        (block): Module(\n          (0): QuantizedConv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), scale=0.002500689821317792, zero_point=59, padding=(3, 3), groups=384)\n          (2): QuantizedLayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (3): QuantizedLinear(in_features=384, out_features=1536, scale=0.10890474170446396, zero_point=75, qscheme=torch.per_channel_affine)\n          (4): GELU(approximate='none')\n          (5): QuantizedLinear(in_features=1536, out_features=384, scale=0.06376130133867264, zero_point=64, qscheme=torch.per_channel_affine)\n        )\n      )\n      (1): Module(\n        (block): Module(\n          (0): QuantizedConv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), scale=0.0036909745540469885, zero_point=60, padding=(3, 3), groups=384)\n          (2): QuantizedLayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (3): QuantizedLinear(in_features=384, out_features=1536, scale=0.10489215701818466, zero_point=79, qscheme=torch.per_channel_affine)\n          (4): GELU(approximate='none')\n          (5): QuantizedLinear(in_features=1536, out_features=384, scale=0.061719220131635666, zero_point=63, qscheme=torch.per_channel_affine)\n        )\n      )\n      (2): Module(\n        (block): Module(\n          (0): QuantizedConv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), scale=0.005117435473948717, zero_point=57, padding=(3, 3), groups=384)\n          (2): QuantizedLayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (3): QuantizedLinear(in_features=384, out_features=1536, scale=0.12287390977144241, zero_point=89, qscheme=torch.per_channel_affine)\n          (4): GELU(approximate='none')\n          (5): QuantizedLinear(in_features=1536, out_features=384, scale=0.06878426671028137, zero_point=54, qscheme=torch.per_channel_affine)\n        )\n      )\n      (3): Module(\n        (block): Module(\n          (0): QuantizedConv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), scale=0.004355110693722963, zero_point=64, padding=(3, 3), groups=384)\n          (2): QuantizedLayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (3): QuantizedLinear(in_features=384, out_features=1536, scale=0.11230114847421646, zero_point=74, qscheme=torch.per_channel_affine)\n          (4): GELU(approximate='none')\n          (5): QuantizedLinear(in_features=1536, out_features=384, scale=0.07070841640233994, zero_point=64, qscheme=torch.per_channel_affine)\n        )\n      )\n      (4): Module(\n        (block): Module(\n          (0): QuantizedConv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), scale=0.006080669816583395, zero_point=67, padding=(3, 3), groups=384)\n          (2): QuantizedLayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (3): QuantizedLinear(in_features=384, out_features=1536, scale=0.12773948907852173, zero_point=77, qscheme=torch.per_channel_affine)\n          (4): GELU(approximate='none')\n          (5): QuantizedLinear(in_features=1536, out_features=384, scale=0.06319359689950943, zero_point=68, qscheme=torch.per_channel_affine)\n        )\n      )\n      (5): Module(\n        (block): Module(\n          (0): QuantizedConv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), scale=0.01316856499761343, zero_point=76, padding=(3, 3), groups=384)\n          (2): QuantizedLayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (3): QuantizedLinear(in_features=384, out_features=1536, scale=0.12977184355258942, zero_point=73, qscheme=torch.per_channel_affine)\n          (4): GELU(approximate='none')\n          (5): QuantizedLinear(in_features=1536, out_features=384, scale=0.09322771430015564, zero_point=62, qscheme=torch.per_channel_affine)\n        )\n      )\n      (6): Module(\n        (block): Module(\n          (0): QuantizedConv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), scale=0.009769421070814133, zero_point=60, padding=(3, 3), groups=384)\n          (2): QuantizedLayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (3): QuantizedLinear(in_features=384, out_features=1536, scale=0.12294528633356094, zero_point=78, qscheme=torch.per_channel_affine)\n          (4): GELU(approximate='none')\n          (5): QuantizedLinear(in_features=1536, out_features=384, scale=0.13477222621440887, zero_point=94, qscheme=torch.per_channel_affine)\n        )\n      )\n      (7): Module(\n        (block): Module(\n          (0): QuantizedConv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), scale=0.018894657492637634, zero_point=65, padding=(3, 3), groups=384)\n          (2): QuantizedLayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (3): QuantizedLinear(in_features=384, out_features=1536, scale=0.14276352524757385, zero_point=77, qscheme=torch.per_channel_affine)\n          (4): GELU(approximate='none')\n          (5): QuantizedLinear(in_features=1536, out_features=384, scale=0.19842681288719177, zero_point=17, qscheme=torch.per_channel_affine)\n        )\n      )\n      (8): Module(\n        (block): Module(\n          (0): QuantizedConv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), scale=0.018394330516457558, zero_point=56, padding=(3, 3), groups=384)\n          (2): QuantizedLayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (3): QuantizedLinear(in_features=384, out_features=1536, scale=0.17767459154129028, zero_point=81, qscheme=torch.per_channel_affine)\n          (4): GELU(approximate='none')\n          (5): QuantizedLinear(in_features=1536, out_features=384, scale=0.2872571051120758, zero_point=100, qscheme=torch.per_channel_affine)\n        )\n      )\n    )\n    (6): Module(\n      (0): Module()\n      (1): QuantizedConv2d(384, 768, kernel_size=(2, 2), stride=(2, 2), scale=0.02574075758457184, zero_point=58)\n    )\n    (7): Module(\n      (0): Module(\n        (block): Module(\n          (0): QuantizedConv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), scale=0.01645306684076786, zero_point=77, padding=(3, 3), groups=768)\n          (2): QuantizedLayerNorm((768,), eps=1e-06, elementwise_affine=True)\n          (3): QuantizedLinear(in_features=768, out_features=3072, scale=0.3103790879249573, zero_point=89, qscheme=torch.per_channel_affine)\n          (4): GELU(approximate='none')\n          (5): QuantizedLinear(in_features=3072, out_features=768, scale=0.10588447749614716, zero_point=68, qscheme=torch.per_channel_affine)\n        )\n      )\n      (1): Module(\n        (block): Module(\n          (0): QuantizedConv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), scale=0.030713222920894623, zero_point=69, padding=(3, 3), groups=768)\n          (2): QuantizedLayerNorm((768,), eps=1e-06, elementwise_affine=True)\n          (3): QuantizedLinear(in_features=768, out_features=3072, scale=0.1843385249376297, zero_point=89, qscheme=torch.per_channel_affine)\n          (4): GELU(approximate='none')\n          (5): QuantizedLinear(in_features=3072, out_features=768, scale=0.09422309696674347, zero_point=48, qscheme=torch.per_channel_affine)\n        )\n      )\n      (2): Module(\n        (block): Module(\n          (0): QuantizedConv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), scale=0.04950263723731041, zero_point=53, padding=(3, 3), groups=768)\n          (2): QuantizedLayerNorm((768,), eps=1e-06, elementwise_affine=True)\n          (3): QuantizedLinear(in_features=768, out_features=3072, scale=0.23426657915115356, zero_point=77, qscheme=torch.per_channel_affine)\n          (4): GELU(approximate='none')\n          (5): QuantizedLinear(in_features=3072, out_features=768, scale=0.16463914513587952, zero_point=64, qscheme=torch.per_channel_affine)\n        )\n      )\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=1)\n  (classifier): Module(\n    (0): Module()\n    (1): Flatten(start_dim=1, end_dim=-1)\n    (2): QuantizedLinear(in_features=768, out_features=10, scale=0.09683085232973099, zero_point=57, qscheme=torch.per_channel_affine)\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"all_predictions_int8 = []\nall_labels_int8 = []\ncorrect_pred = 0\ntotal_pred = 0\nstart_time_int8 = time()\nwith torch.no_grad():\n    model_quantized_trained.eval()\n    for data in testloader:\n        images, labels = data\n        all_labels_int8.extend(labels.numpy())\n        #images, labels = images.to(device), labels.to(device)\n        outputs = model_quantized_trained(images.to('cpu'))\n        _, predicted = torch.max(outputs.data, 1)\n        total_pred += labels.size(0)\n        correct_pred += (predicted == labels).sum().item()\n        predicted_tensor_cpu = predicted.to('cpu')\n        all_predictions_int8.extend(predicted_tensor_cpu.numpy())\nend_time_int8 = time()\nprint(\"Time: \",end_time_int8 - start_time_int8)\nprint('Accuracy achieved by the network on test images is: %d%%' % (100 * correct_pred / total_pred))","metadata":{"execution":{"iopub.status.busy":"2024-04-09T11:08:25.244616Z","iopub.execute_input":"2024-04-09T11:08:25.245020Z","iopub.status.idle":"2024-04-09T11:18:20.254195Z","shell.execute_reply.started":"2024-04-09T11:08:25.244990Z","shell.execute_reply":"2024-04-09T11:18:20.252994Z"},"trusted":true},"execution_count":100,"outputs":[{"name":"stdout","text":"Time:  595.0004639625549\nAccuracy achieved by the network on test images is: 72%\n","output_type":"stream"}]},{"cell_type":"code","source":"metrics(all_predictions_int8,all_labels_int8)","metadata":{"execution":{"iopub.status.busy":"2024-04-09T11:19:29.058242Z","iopub.execute_input":"2024-04-09T11:19:29.058653Z","iopub.status.idle":"2024-04-09T11:19:30.310462Z","shell.execute_reply.started":"2024-04-09T11:19:29.058622Z","shell.execute_reply":"2024-04-09T11:19:30.309182Z"},"trusted":true},"execution_count":104,"outputs":[{"name":"stdout","text":"Confusion Matrix:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 800x600 with 2 Axes>","image/png":"iVBORw0KGgoAAAANSUhEUgAABjIAAAVjCAYAAABjYLYHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuNSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/xnp5ZAAAACXBIWXMAAB7CAAAewgFu0HU+AAEAAElEQVR4nOzdd3gUVRfH8d+mEAKhJCQQeqjSe0eqICC9qjRpokhRUZoo5QUVUQEFQQGRjoJ0KdJ7kd5CJ6GFQAIphCSQ9v4RsiSkhyS7ge/neXyc3blz5yy7mZ2dM/dcQ0RERIQAAAAAAAAAAADMkIWpAwAAAAAAAAAAAIgPiQwAAAAAAAAAAGC2SGQAAAAAAAAAAACzRSIDAAAAAAAAAACYLRIZAAAAAAAAAADAbJHIAAAAAAAAAAAAZotEBgAAAAAAAAAAMFskMgAAAAAAAAAAgNkikQEAAAAAAAAAAMwWiQwAAAAAAAAAAGC2SGQAAAAAAAAAAACzRSIDAAAAAAAAAACYLRIZAAAAAAAAAADAbJHIAAAAAAAAAAAAZotEBgAAAAAAAAAAMFskMgAAAAAAAAAAgNkikQEAAAAAAAAAAMwWiQwAAAAAAAAAAGC2SGQAAAAAAAAAAACzRSIDAAAAQIZ08+ZNDRs2TJUqVVKOHDlkYWEhg8Egg8GgXbt2mTq8RDVs2DBDxYv0N3/+fONnpFevXqYOBwAAwGSsTB0AAAAAgBfj7++vTZs2aevWrTp69Ki8vLzk7e2tTJkyyd7eXiVLllT16tXVpk0b1a5d29ThporDhw+refPm8vX1NXUoSAZ3d3cVKVIkxnNOTk7y8PCQlVXSfp6GhYWpQIEC8vT0jPG8m5ubXFxcUitUAAAAmBESGQAAAEAGFRgYqGnTpumHH36Qj49PrPVPnjxRQECAbt68qe3bt2vSpEkqWbKkxo0bp3feeUcGg8EEUb+4iIgI9ezZ05jEyJkzpxo3bqw8efLIwiJy0Hn+/PlNGCGSw8vLS5s2bVLr1q2T1P7ff/+NlcRIT9GTMYULF5a7u7vJYgEAAHhVkMgAAAAAMqAbN26odevWOn36dIznCxUqpAoVKsjJyUlhYWHy9PTUqVOndPfuXUnSpUuX1LVrV928eVPDhw83Regv7PDhw7p06ZKkyLv5XV1d5ejoaOKo8CIWLlyY5ETGwoUL0zgaAAAAmBsSGQAAAEAG4+7urtq1axvvSjcYDHr33Xf1xRdfqGzZsrHaR0RE6OjRo5o+fbqWLFmi8PBwBQYGpnfYqeb48ePG5bZt22bYJAbzYkhlypSRq6ur1q9fL19fX+XMmTPB9n5+flq7dm2MbV9mvXr1Ym4MAAAAMdk3AAAAkKE8efJEnTt3NiYxMmfOrFWrVmnJkiVxJjGkyERH9erVtXDhQp06dUrlypVLz5BTXfQyWnnz5jVhJHhRPXr0kCQ9fvxYf/31V6Ltly9fruDgYElSz5490zQ2AAAAmA8SGQAAAEAGMnnyZB09etT4eMGCBWrXrl2Sty9XrpwOHTqkpk2bpkF06SMkJMS4HDUnBjKmrl27Gif5TkrJqKg21tbW6tq1a5rGBgAAAPPBWT8AAACQQQQFBennn382Pu7QoYO6dOmS7H6yZs2qunXrJtjm+vXrGjNmjGrVqqU8efIoU6ZMypMnj2rVqqWxY8fq5s2bie5n165dMhgMMhgMatiwofH5HTt26J133lHRokWVOXNm5cqVS/Xr19eMGTNiJCmimz9/vrGv8ePHG58fP3688fmo/8aNG2dcP27cuDifT27McTly5IgGDRqkKlWqyN7eXlZWVrK1tVXevHlVq1YtDRgwQMuXL9ejR4/i3L5hw4bGfSWlzJS3t7cmTZqkBg0aKG/evLKxsZGjo6MqV66sYcOGJanMkru7u3GfLi4uxuePHj2qfv36qWTJksqSJYvs7e1Vo0YNffPNN/HGnxpy586t5s2bS5IOHDigq1evxtvWzc1N+/fvlyQ1b95cTk5OSd5PUFCQ1qxZoyFDhuj11183fqbt7Ozk4uKi9u3b6/fff9eTJ0/i7SPqMxg10bcU+Xfy/Ocv6r/o4vtcbdy4Ue+++65KlCghOzs7GQwGTZs2LdY+DQZDnCWm/v77b+N6KysrHThwIMF/hydPnqhq1arGbVq1apXwPxwAAICZYI4MAAAAIIP4+++/5eXlZXw8dOjQNNnP119/rYkTJxpL+ES5d++e7t27p8OHD2vy5MkaN26cRowYkeR+nzx5okGDBmnOnDkxnn/8+LH27t2rvXv36o8//tC///5r1vNehIaGauDAgZo9e3asdVETrHt6eurw4cP69ddfNXr0aE2cOPGF9jlv3jwNHTpUfn5+MZ6/f/++7t+/r5MnT2rq1KkaPHiwfvjhB1laWiap34iICI0bN04TJ05UeHi48fmgoCAdOXJER44c0dy5c7Vt2zYVLVr0hV5DfHr27Kl//vlHUuSIi+iJqugWLlyoiIgI4zZJdfjwYTVp0kQBAQGx1oWEhOjRo0e6fv261qxZo4kTJ2rVqlWqXLlyCl5J0vn5+al3795avXr1C/XTqVMn9enTR/PmzVNYWJi6d++ukydPKnv27HG2Hz16tHGOmTx58uiPP/54of0DAACkFxIZAAAAQAaxY8cO43KhQoUSHVWREoMGDdIvv/xifGxnZ6dGjRrJ2dlZnp6e2rlzpwICAhQcHKyRI0fK09NTU6dOTVLf/fv314IFC2RhYaGaNWuqVKlSCg8P16FDh3Tx4kVJkRN59+zZUxs3boyxbenSpTVw4EBJ0n///acjR45IkqpXr64aNWrEaPv849Q2bNiwGEmM/Pnzq0aNGnJyclJ4eLju378vV1dX42t6UT/88IOGDRtmfGxjY6MGDRqoUKFC8vHx0c6dO/XgwQOFhYVp2rRpunHjhvFO/cSMHz9e//vf/yRJlSpVUvny5WVtba2TJ08aL3i7ubmpXbt2On78uLEMVGpq06aNcubMKV9fXy1evNg4iuZ5UWWl7O3t1bp1a2NSIzE+Pj7GJEbu3LlVtmxZFShQQFmzZlVgYKCuXLmi//77T6GhoXJ3d1eDBg10/PhxFS9ePEY/UZ/Bhw8fGmPJli1bsufqiIiIUPfu3fXPP//IYDCoWrVqKlOmjCIiInT27NkkvW/R/fzzz9q7d68uX74sNzc3ffTRR1q8eHGsdtu2bdOPP/4oKXLenPnz5ydrVAsAAIBJRQAAAADIEIoVKxYhKUJSROfOnVO9/7/++svYv6SIXr16Rfj5+cVo4+fnF9G9e/cY7VauXBlnfzt37jS2sbGxiZAUUb169Yjz58/HaBceHh4xbdq0GH3u3r073jjHjh1rbDd27NgEX1Ny2j4fc4MGDWKt9/b2jrCysoqQFGFpaRkxf/78iPDw8Dj78vDwiPj5558j5s6dG+f6Bg0aGPe1c+fOONvs378/wtLS0tiuRYsWEZ6enjHaBAcHRwwbNizGv9+PP/4YZ39ubm7GNpkyZYowGAwRxYoVizh8+HCstsuXL4+wtrY2tl+wYEGcfSZH9P1LiggKCoqIiIiI6N+/v/G5PXv2xNpu7969xvUffPBBRERERERQUFCMvtzc3OLc56FDhyK++OKLiDNnzsQb1927dyN69Ohh7OuNN95I0msoXLhwkl539M9V1OenfPnyEadPn47VNjg42Lj8xx9/GLd777334u3/yJEjMd6rxYsXx1jv7e0dkS9fPuP6IUOGJCluAAAAc8EcGQAAAEAGcf36deNy2bJlU7Xv8PBwjRw50vi4c+fOmjdvXqwSNdmzZ9fChQvVtm1b43PDhw+PUZYoLo8fP1aJEiW0Y8cOlSpVKsY6g8Ggjz/+WJ06dTI+t2zZshd5OWnm4MGDCg0NlSS98847eu+99+K9gz5v3rwaPHiw+vbtm+L9jRo1SmFhYZKkOnXqaM2aNcqTJ0+MNjY2Npo8ebKGDBlifG78+PF6+PBhgn0/efJEDg4O2rNnT5yjWDp37qyPP/7Y+Dgt35P33nvPuBzXpN/Rn4veNilq1qypr7/+WuXKlYu3Te7cubVw4UK1aNFCkrR9+3adP38+WftJqtDQUDk7O2vHjh0qX758rPU2NjbJ7rNatWrGkTWSNHDgQLm7uxsf9+3bVx4eHpKk8uXL67vvvkt+4AAAACZEIgMAAADIAPz9/Y0X0CUpZ86cqdr/li1b5ObmJknKlCmTfv7553gv0BsMBv3yyy+ytraWJF29elVbt25NdB+TJk2SnZ1dvOv79OljXP7vv/+SE3668ff3Ny6ndVme8+fPa8+ePcbHM2bMUKZMmeJt/8033xjnFvH399fSpUsT3ccXX3yhfPnyxbs++nsSVc4rLdSpU8dYymnFihUx5mcJDg7WihUrJEklSpRQ7dq10yyO6BNqb9u2Lc32M2bMmFSfB2b48OFq1KiRpMg5OLp3766wsDD9+uuvWrt2rSQpc+bMWrp0qTJnzpyq+wYAAEhrJDIAAACADOD5u+sTSgikRPT5N9566y05Ozsn2D5//vxq3ry58fHOnTsTbJ85c2a1bt06wTbRJ1iOfje5OSlYsKBxedWqVbp3716a7Sv6v2mlSpUSnYA6a9asevfdd+PcPj6dO3dOcH2pUqVka2srKXJi8cRGebyIHj16SIq8CB914V2S1q5dK19f3xhtUiowMFA7duzQTz/9pC+//FIff/yxBg0aZPwv+qiTkydPvtC+EvL222+nep8WFhZauHChHBwcJEn79+/X+++/r6FDhxrbTJ48OcGRKQAAAOaKyb4BAACADCBbtmwxHkdNXpxaTpw4YVyuU6dOkrapW7eu1q9fL0nGiaHj89prrxlHcMQnV65cxuXoIx/MSa1atVSwYEHdvHlTN27cUNmyZdW7d2+1bt1aNWvWTHDERHKl9D2ZPn26pMTfkxw5csRIzMTFYDDI3t5eQUFBkiLfl+c/i6mlR48eGjdunCIiIrRw4ULjxf6oslIGgyHFiYwHDx5ozJgxWrhwYZKTMd7e3inaV2KKFCliTDaktgIFCmjOnDnq2LGjJOmPP/4wrmvRooUGDx6cJvsFAABIa4zIAAAAADKA7Nmzy8rq2X1IUXeopxYvLy/jcuHChZO0jYuLi3E5sYu+OXLkSLS/6ImO6GW0zIm1tbUWLVpkHBHj7e2t77//XvXr11eOHDlUr149jR49Wvv371dERMQL7csc3hMp5vsSEhKSpG1SokiRInr99dclRZY6u3v3ru7evastW7ZIkurVqxfj9SXV9evXVblyZf3yyy/JGlGSVqNP0rokWYcOHdSvX78Yz+XOnTtGUgMAACCjIZEBAAAAZBDRL2a7urqmat/RR3hkzZo1SdtEb5fYRd/45tvIiBo0aKBTp06pZ8+exrJLUuRcDvv27dM333yj119/XaVKldKaNWtSvJ9X8T2Jmsg7NDRUS5cu1dKlS41JreRO8h2la9euunHjhqTIkU2ffvqpNm/erGvXrikgIEBhYWGKiIhQREREjHJciU1gn1LRPzNp5fkJ4WvXrh3rOQAAgIyERAYAAACQQUTdrS5Jhw8fTtW+o8+58ejRoyRtE71dWpUbSm9JvXhdtGhRLViwQF5eXtq8ebO+/PJLNWrUKMZF6kuXLql9+/aaMmVKimJ5Fd+Tzp07G/8NFy5cqAULFkiKvPif2HwecTlw4IAOHDggKfLf89ChQ5oyZYqaNWumIkWKKGvWrLKwePazOC3nAEkve/fu1aRJk2I8t3btWi1ZssREEQEAALw4EhkAAABABtG4cWPj8vXr140XaFND9HI3UXevJyb6hNyOjo6pFktqSm65Kj8/v2T1nzVrVjVr1kwTJkzQjh07dP/+fa1YsULly5c3thk1apRu376drH6ll/c9SUj27NnVtm1bSZGTbZ86dUqS1K5duxQlZrZv325cfu+991SmTJkE21+/fj3Z+zAnfn5+6tGjh8LCwiRFTtYeZeDAgRn+9QEAgFcXiQwAAAAgg+jcuXOMi9MpvdM/LpUrVzYuJzVBEr1dlSpVUi2W1JQ9e3bj8v379xNtf+bMmRfan62trTp16qRdu3YZS/k8efJE//77b7L7elnfk8T07NkzSc8lhYeHh3E5enIpPnv27Em0jTmW5IoyYMAAY7KiTJkyOnr0qBo1aiQpMsnRvXt3Y5IDAAAgIyGRAQAAAGQQtra2GjJkiPHxypUrtXLlymT38+jRo1gXxqOP9ti4caPu3buXYB8eHh7atGlTnNubk+iTQ588eTLR9suXL0+V/To4OKhu3brGx3fv3k12H9H/TU+cOKHTp08n2D4wMFB//vlnnNtnJG+++aacnZ2Nj/PmzaumTZumqK/oZaMCAwMTbOvh4aG1a9cm2mfmzJmNy2k5+XlyLVq0SMuWLZMkZcqUSUuXLlXWrFm1cOFC2dvbS5L27dunr7/+2pRhAgAApAiJDAAAACADGT58eIw77Xv06KH169cnefuzZ8+qVq1a2rJlS4zn33zzTRUpUkSS9PjxY33yySfx9hEREaHBgwcbL+IWK1ZMTZo0ScarSD/Vq1c33kF/+PBhnT9/Pt62M2fO1Llz5xLsLymjOqLcvHnTuJw7d+4kbxelVKlSql+/vvHxoEGDErxw/uWXXxoTUNmzZ1fXrl2TvU9zYGlpqb179+rIkSM6cuSI9uzZI0tLyxT1VbRoUePyunXr4m0XFham/v3768mTJ4n2mTNnTmOCxMvLyyySGW5ubho4cKDx8TfffKOKFStKkgoUKKDZs2cb102YMEGHDh1K9xgBAABeBIkMAAAAIAOxsbHRihUrjBfGg4KC1K5dO/Xs2TPei/QRERE6cuSI3nvvPVWsWFFnz56N1cbCwiLGBMHLli3T+++/r4CAgBjtHj58qN69e2vVqlXG5yZPnhzjzndz4uzsbByZEBERoXfffVe3bt2K0SY0NFQ//vijhgwZIhsbmwT7mz59uipVqqRZs2bJ09MzzjYBAQEaPXq0jhw5Iinywvybb76Zovi//fZb40X8vXv3qmPHjrFGyzx58kSjRo3S1KlTjc+NHTs2xmThGU3x4sVVrVo1VatWTcWLF09xPy1btjQmsnbt2qXPP/9cQUFBMdp4enqqY8eO2rBhg7JmzZponzY2NipRooSkyBEZa9asSXF8qSEsLEzdunUzTlTepEkTDR06NEabTp06qXfv3pIiP+/du3d/KSY2BwAArw4rUwcAAAAAIHmKFi2qw4cPq3Xr1jp79qzCw8O1aNEiLVq0SC4uLqpQoYIcHR0VFhYmT09PnTx5MlZpo7gmTu7SpYv27NmjX375RZI0d+5c/fXXX2rUqJHy5Mmje/fuafv27TGSG5988ok6dOiQti/4BX399dfauXOnwsPDderUKZUsWVKNGzdW/vz59eDBA+3Zs0f37t2TnZ2dvv32Ww0ePDjB/k6dOqWPPvpIAwcOVLFixVSuXDk5OjoqJCREd+7c0YEDB2L8G40cOVIFCxZMUex16tTRpEmTNGzYMEnS+vXrVahQITVq1EgFCxaUj4+Pdu7cGWOkSPv27fXpp5+maH8vm1KlSqlHjx5auHChJOnHH3/U0qVLVb16deXOnVvu7u7as2ePnjx5omzZsun777/Xhx9+mGi/HTt21DfffCNJ6tatm+bPn6/ixYvHmFz+hx9+SJsX9ZwJEybo4MGDkqRcuXJpwYIFcc7j8fPPP2vv3r26cuWKrl69qsGDB2v+/PnpEiMAAMCLIpEBAAAAZEAuLi46ePCgpk6dqilTpsjX11eS5O7uLnd393i3q1ixosaNG6d27drFuX7GjBlydnbWxIkT9fjxYz18+DDOkjyZM2fWmDFjNGrUqFR4NWmrZs2amjNnjvr376+wsDAFBQVpw4YNMdrkzZtXf/31V6ITIUdPAEVEROjKlSu6cuVKnG0zZcqk0aNHa8yYMS8U/+effy57e3sNHTpU/v7+evz4sTZv3hyrnaWlpQYNGqQff/zRrCekTm9Ro2eiyqnduXMn1me6QIEC+vPPP5NcJmr48OFatWqVLly4oJCQEG3cuDFWm/RIZBw4cEATJ040Pp4zZ47y5csXZ1s7OzstWbJEdevWVWhoqBYsWKCWLVuqc+fOaR4nAADAizLP8d8AAAAAEmVnZ6evvvpK7u7uWrp0qXr37q0KFSrI2dlZmTJlkp2dnQoVKqQ333xTX331lY4dO6aTJ0/Gm8SI8uWXX+rixYv68ssvVb16dTk6OsrKykqOjo6qUaOGvvrqK128eDFDJDGi9OnTR6dPn1bfvn1VpEgRZc6cWTlz5lTlypU1ceJEnT59WvXq1Uu0n88++0xubm6aPXu2evXqpapVqypXrlyytraWjY2N8uTJo4YNG+p///ufLl269MJJjCh9+/bV1atX9c0336hevXrKkyePrK2t5eDgoIoVK+qzzz7T6dOnNW3atBTPJ/GyypIlizZt2qRFixapSZMmxvcrb968qlu3rqZMmaLTp0/HmJw9MTly5NCRI0f03XffqX79+nJycooxGiM9+Pv7q3v37sbkW79+/dS+ffsEt6lRo4bGjRtnfPzBBx/EmMsFAADAXBkiIiIiTB0EAAAAAAAAAABAXBiRAQAAAAAAAAAAzBaJDAAAAAAAAAAAYLZIZAAAAAAAAAAAALNFIgMAAAAAAAAAAJgtEhkAAAAAAAAAAMBskcgAAAAAAAAAAABmi0QGAAAAAAAAAAAwWyQyAAAAAAAAAACA2SKRAQAAAAAAAAAAzBaJDAAAAAAAAAAAYLZIZAAAAAAAAAAAALNFIgMAAAAAAAAAAJgtEhkAAAAAAAAAAMBskcgAAAAAAAAAAABmi0QGAAAAAAAAAAAwW1amDgAAYL6yv7PQ1CEgGs9FPUwdAqJEmDoARAkKCTN1CHgqs7WlqUPAUyFh4aYOAdE8esxxylzYZ7U2dQh4yi8wxNQh4KkctvxdmIssmQymDsEkbCsPMnUIKRZ0YoapQ3ilMCIDAAAAAAAAAACYLRIZAAAAAAAAAADAbFFaCgAAAAAAAACQ/gzcZ4+k4ZMCAAAAAAAAAADMFokMAAAAAAAAAABgtkhkAAAAAAAAAAAAs8UcGQAAAAAAAACA9GcwmDoCZBCMyAAAAAAAAAAAAGaLRAYAAAAAAAAAADBbJDIAAAAAAAAAAIDZYo4MAAAAAAAAAED6M3CfPZKGTwoAAAAAAAAAADBbJDIAAAAAAAAAAIDZorQUAAAAAAAAACD9GQymjgAZBCMyAAAAAAAAAACA2SKRAQAAAAAAAAAAzBaJDAAAAAAAAAAAYLaYIwMAAAAAAAAAkP4M3GePpOGTAgAAAAAAAAAAzBaJDAAAAAAAAAAAYLYoLQUAAAAAAAAASH8Gg6kjQAbBiAwAAAAAAAAAAGC2SGQAAAAAAAAAAACzRSIDAAAAAAAAAACYLebIAAAAAAAAAACkPwP32SNp+KQAAAAAAAAAAACzRSIDAAAAAAAAAACYLRIZAAAAAAAAAADAbDFHBgAAAAAAAAAg/RkMpo4AGQQjMgAAAAAAAAAAgNkikQEAAAAAAAAAAMwWpaUAAAAAAAAAAOnPwH32SBo+KQAypHHjxslgMMhALUUAAAAAAADgpcaIDCAd7d69Ww0bNjQ+3r9/v+rUqWO6gF4hN27c0J9//qmtW7fq8uXL8vLyUnh4uBwcHFSuXDnVq1dP3bp1U5EiRUwdKp4zvmsVfdqmnPHxW//7V/tc78Zq17VBMf06oG6S+vxw1n4t3X01znXNKudXlWKOqlI0l1zy2MkxW2Zlz5JJAcEhcr8XoH2unvpj+2VdueOfshf0iqhcrlSS2lWtVl1z5y9K42ggSSEhT7R+3Vpt27JZly9dkp+fr6ysrJU7T25VrFhZ7Tt1VqVKVUwdZob1KCBAB/bv0flzZ3XB9Zy87t2Vr6+PHgcHyy5bdhUpWky169ZT63YdlSNnznj7uXXjulxdz+r82TNyPXdGly5e0OPgYEnS6HET1bJN+3R6Ra+Ox48fa+3qldq+bYsuX7qogIcBymmfU6+9Vlqt2rRVsxYtTR1ihnb+3Fnt37dHp04cl9u1q/L1eSArKys5OuVWhUqV1aZ9R1WqXDXBPsLDw+Xudk2uZ89E/nfurK5cvqiQkBBJ0sw581W1eo30eDkZXuOa5ZPUrmKVapo6649Yz4eHh+uGu5suuJ7RBdezuuh6VteuXDK+F1NmzlOlqtVTNWZEunPHQ2tW/q29e3brzh0PBT56JHt7B+XLn1/VatTUm82aq3iJkqYO86Xy2/QpWrbo2d/B1FnzVDkJn++j/x3U1k3/6MypE3rg7S1LS0vZO+RS0RIlVbVaTTV9q7WyZMmSlqG/tPr17qFjR48ka5s58xaoWvWaaRQRgPREIgNIRwsWLIjxeOHChSQy0lhwcLBGjRqlWbNm6fHjx7HWe3h4yMPDQ1u2bNGYMWPUuXNn/fDDDypYsKAJosXzyhe216C3yqTb/iwtDFox4o0419nb2cjezkaVi+bSB81L6evlpzR13dl0iw14ER4etzVk4Ie6euVyjOdDQkJ03d1d193dtW7tar3TtbuGjxzNaLcUcD13RmNHDYtzna/PA5049kAnjh3R0oV/aMzESapV5/VY7U4cO6KB7/dK40gRnbvbNQ0dMlDu7m4xnvf28pK3l5f279ujtWtW6YepPytLlqwmijLj+qBPD508fizW8yEhIbp547pu3riuDevW6K1WbfXF2PGyts4UZz+b/lmn/435Iq3DRRJs3bRe3/3vS1OH8cpZtmSRpk+bqqCgwBjP373rqbt3PXXi+DE9CgjQsJH8naSWy5cuaPnS5N1s89DfT5P+95X279kZa92jRwG6dfO69uzYqjIVKqpEyaTd9IMXY2FhoUKFXEwdBoBUQiIDSCdBQUH6+++/JUl2dnYKCAjQ8uXL9dNPP8nGxsbE0b2cvL291bp1ax06dEiSlC1bNnXt2lWNGzdWgQIFZG1tLU9PT+3fv1+rVq3S5cuXtXz5ctWuXVuffPKJaYOHDAbp5/dry9rKQvd8g5Q7p22St233zVbdeRAU73qPB4/iXef76In2uXrq6BVvud8LkKdPoIKehMnZ3lb1yjire8Piypk1k8Z3rSK/wCeat+1Ssl7Xq6bz2++qyzvvxrve1pa70dJaSEhIjCRGiZKvqXvPXnJxKaLAR4904sQxLVowX0FBgfpz6WI5OeVWn379TRx1xpTH2VlVqtXQa6XLKk8eZ+VydFJERLju3b2rndu3aPeObfL19dGITwdp7qI/Y13EiIiIMC5bWFiocJGisrW1levZM+n9Ul4JD+7f10f9+8rT844kqembzdWqbTs5OeWWl9c9/bN2jbZu2axDB/Zr5LCh+vmX30wcccbj7XVPkuTklFuNmzZTpSpV5eycV2Hh4Tp76qSWLJovr3t3tfGftQoNDdWESd/H2U/0vw0rKysVK1FSYaGhunKZ7+CUatPxbbXt+Ha86zPbxn3e9fx7UaRYCYWFherac4lypJ45v83SzOk/SZIKu7ioQ8fOKlOuvLJlyyZfX19dPO+qHdu3yWDBTQipJTw8XD9+M15hYaGyd3CQz4MHiW4TEPBQnw3qr0sXXCVJ9Rq+oQaNmypfgYKysLCU111PnTxxVHt2bE3r8F9q4yd8Gyuh97xrV69qxLBPJUk1atZS7jx50iM0vAhuokISkcgA0snq1av18OFDSdLPP/+sPn36yMfHR+vXr1enTp1MHN3LJzw8XF26dDEmMVq1aqXff/9duXPnjtW2devW+uabb7RkyRJ9/vnn6R0q4jGgeWlVLe6oi7d9tf7ITX3eLmmlECTpyh1/3fCKP1kRn7DwCLn0+0vh0X6kR7fp2C39tvmC9nzbUvZ2NhrduaLmb78cb3tIDg4OlDkwsV07txuTGBUqVtK8BUtkaWlpXF+rTl01aNhY73V/V6GhIZo/b6569uojKytOE5OjSrUaWr1xe7zr33izuXbv3K5Rnw1RSEiI5v02S9/++FOMNk6582jQJ5+rdNlyeq10GWXJklUb1q0mkZFGZv/6izGJ0X/AQH340WDjulKly6he/YZy+eVnzfl1pvbt2a1tWzaryZvNTRVuhlTYpagGDPpEjZq8GeO4I0nlK1RUi1Zt9H6vbrpx3V1bNm9Qh85vq3LVarH6KVK0mD4b8YVKly2vkq+Vko2NjebMmkEi4wXktHdQkWIlkr1d4SLFNOizkSpVupyKlyylTDY2mj9nJomMNHL40EFjEqNVm7YaM36irK2tY7SpWau2evbuq5CQJ6YI8aW08q8luuB6VoVciqhewze0ZP7cRLf5+ftvdemCq6wzZdK4b35Q3fqNYqwvVaas6jV6Q4M+Ha6wsLC0Cv2ll79AgUTbbFi/zrjcqnW7NIwGQHpjsm8gnSxcuFCSVKFCBfXu3VuvvfZajOeRun766Sft3Bk5pLdZs2ZavXp1nEmMKBYWFurRo4eOHTumChUqpFeYiEeBXFk1ukslSdIncw8rJDQ83fadWFLiuleAVh10lyQ55bBVyfzZ0yEqIOVOnTxhXO7Tr3+si4mSVKZsOdVv0FCS9PChv9yuxT2HDOIX17/r8xo0ekOFXCLnYjp1Ina5nYKFCqtrz96qXLU6ZYzSWFhYmDZuWC9Jypsvn97/4KM42/X/cKCc8+aTJP3x+5x0i+9lMWX6LDVp1iLev4+c9vb6+LPhxsc7tv0bZ7uy5Suoy7vdVb5CRUYym1jpsuXVoUs3lSlfUZl4L9JUeHi4vpkwTpJU8rVSGvu/r2MlMaKLrzQbkueu5x3N+22GJGnoyK9kZRX/v3mU0yePa8umyO+Uvh8OjpXEiM5gMHCzSBoKDw83fr9nyZJFjZs0NXFEAFITiQwgHdy5c0fbtm2TJHXv3j3G/zdv3iwvL694tx03bpwMBoOxXnlwcLC+//57ValSRdmyZVO2bNlUo0YNzZgxQ6GhofH24+LiIoPBoF69ekmSLl68qPfff18uLi6ysbFRnjx51L59e+MIhrjMnz/fGIu7u3u87dzd3Y3t5s+fH2ebQ4cO6csvv1TDhg3l7OysTJkyKXv27CpTpowGDBggV1fXePtPzJMnT/TDDz9IkjJnzqx58+Yl+WSxQIECaty4cYznnn8P/Pz8NGHCBFWuXFk5c+aM83UGBARo0qRJql27thwcHGRjY6MCBQqoU6dO+ueffxKMoWHDhjIYDMaJ4S9evKj+/furSJEiypw5s/LmzRtjtMnL6Mc+NZTN1lpLdl/R/vOxJ/Y2tYDgZ39rma0Tv3gJmFLo0wlYJalAgfjn/ykQbW6gkGjbIHVFTe755EnseZuQfm5cv66ApyNla9WuG++FdktLS9WqHTmf2XnXc7p961a6xfiqiD5J962bN00YCWBeDh7YrxvXr0uSevXtx8XvdDJt8tcKCgxUs5ZtVKlK0iauX71imSQpq102te8cf0lVpL3/Dh3UvXuRvx+bNG0m23jK5MHMGCwy7n9IV3wTAulgyZIlCgsLk4WFhbp27SpJ6tatm8aMGaOQkBAtW7ZMQ4YMSbSfu3fvqnnz5jp58mSM548cOaIjR45oy5YtWrNmjSwsEj6Yrl69Wt27d1dg4LPakvfu3dOaNWu0fv16LVmyRG+/HX/N3Bc1f/589e7dO9bzISEhOn/+vM6fP685c+bo559/1kcfxX2HZEL+/fdfeXh4SJI6d+6sfPnyvXDMUS5fvqw333wzwUTOiRMn1KpVK2MMUW7fvq2VK1dq5cqV6tChg5YsWaLMmTMnuL9Nmzapc+fOevToWZkkT09PrVixQitXrtSPP/740s3n0b5WYbWoWlAPHj7W6MWx71g2tczWlmpZLfKCb1h4uK7c8TdxREDCCj8dASBJt27dVLHicZcSibqAaDAYVKiwS3qE9sq57u6my5cuSpJxZAZMw8/P17js4JArwbYOuZ6tP3H8aJLKWiDpnjx5Vg4nKSObgFfF1n83S4r8Xo4aNSlFHr98fX2VM2dO5ciR0zTBvaR2bt2sg/t2K3v2HBrwcdJKDoeEhBgn965Wo5Zx1FhYWJjue3spLCxMDrkcGU2WTv5Zv9a43KpNWxNGAiAtkMgA0sGiRYskRd5pnz9/fklSkSJFVKdOHe3fv18LFy5MUiKjQ4cOcnV11ZAhQ9S6dWs5ODjo4sWLmjBhgs6fP6/169drzpw5+uCDD+Lt48yZM/rrr7+UN29effbZZ6pWrZoiIiL077//atKkSQoODlb//v3VuHFjOTk5pc4/wHNCQ0Nlb2+vtm3bqn79+ipRooSyZs0qDw8PHT9+XD///LO8vb01aNAglSpVKtYIicTs3r3buNyyZctUjb1Tp066ffu2Bg8erDZt2sje3l6XL19W4cKFJUUmK9544w35+PgYR8C88847ypUrl1xdXfXjjz/q1KlTWrVqlXr16qU///wz3n15eHioa9eusrKy0jfffGMcobFz505999138vf316effioXFxe1a9cuVV+nqeTIYq3v3ou882nM0mN68DBldyzP/LCuSuTNrlzZbfQwMETX7j7UrjN3NHfrRd3xiX8S8PhYWRrknDOLapZ00idtyql43shyUot2Xo0xOgOxbd3yr7b8u1l3PG7LwsJCuRydVLFSJbVp117Va9QydXivhOZvtdLMGT8pICBA8+fN1ev1GsS6WHjhvKv27tklSWrxVivZ2dmZINKXU3BQkLy87mnfnp1asmCewp6Onny7a08TR/ZqixoZI0VO0JqQqJEbUuQEokhdJ44dNS67FClqwkheLbu3b9Hubf/K846HLCwt5ODgqLIVKqlZy7aqXK1G4h0gzZ05fUqSlC9/fmXNaqdNG9Zr3tzZunL52XwkUZN/v9OthzJlorTUi3j40F/Tp3wnSeo/6FPlzGmfpO2uXr6oJ48jf7MULV5CjwICNG/2L/p3w1rj94e1tbUqVK6q7r37q3LVpI3yQPIFBj7Sju2RlTDy5sunatVrmjgiAKmNRAaQxk6ePKnTp09LelZOKkr37t21f/9+HTt2TK6uripTpkyCfUWNuoi6oC1JVapUUbNmzVSmTBndvXtXM2fOTDCRcfz4cVWtWlU7duxQ9uzPavvXqlVLxYsXV/fu3eXv76/Fixfr008/TcErTlyLFi3UtWvXGBcRJKly5cpq2bKlhgwZovr16+v06dMaO3ZsshMZp06dMi5XrVo1VWKOcvbsWW3atElvvvlmnPv45JNP5OPjI0maM2eO+vbtG6Ndly5d1KJFC+3cuVN//fWX3nvvPbVo0SLOfV2+fFk5cuTQwYMHVbp0aePztWvXVtu2bVWnTh35+/tr0KBBatmyZYI1czOK/3WrKmf7LDp44Z4W7ryS4n7ql3U2LufKbqlc2TOregknDWpVRiMXHNEf2xOfkLKQU1adnd4x3vXbTt7W6MVH412PSNeuxnwfA29c180b1/XPurVq1LiJxn/9rbJly2ai6F4N9vb2mvDNZI0a8ZlOnjiu7u92VtfuPVW4sIsCAwN16uRxLVrwh0JCQlS6dBkN/XyEqUPO8DasW62vx30Z7/oevfvpzRapm2hH8hQsWEhWVtYKDQ3R8WMJH8ujr/e845FASyRXeHi4Fs57NvfIG0ymnm6uu8VMyt0OvKHbt25oy8Z1qtugsUaMmSg7O76fTSU8PFzubtckSTlz2mvyt19r2ZJFsdpdd3fX1B+/147t2zR95m/Klp2521Lqt+lT9OC+t8pVrKyWbTskeTv3aPOKhYdH6IP33tGtm9djtAkJCdGx/w7p+JHDev+jj9X1vb7Pd4NUsG3rFgUFRVadaNmyjbE0NICXB8W8gDQWNZm3ra2tOnaMeVG0S5cuxjtnkjLp9+DBg2MkMaI4ODgYSzWdOXNGfn5+CfYzb968GEmMKF27djWWYdq7d2+i8aRU/vz5YyUxosuRI4f+97//SZL27dun+/fvJ6v/6O0TmuA7JXr16hUjiRGdh4eHVq9eLUlq3rx5jCRGFBsbmxhzdsyYMSPB/X311VcxkhhRypYtq9GjR0uKHAWydu3aWG0ymtqlcuu9RiUUEhquT+ambP4Pt7sP9dP6c+r24y41/GKDGn6xQb1+2qNVB90VHh4h20xW+un92ur1RtyldZLC2z9Y703brU7f7dDDIOYRiE9mW1s1a/GWvho3QfMWLtGff6/WrNm/q1//D5UzZ05J0s4d2/Tp4I+YjyEdNGzUWEv/XKn2HTvr4oXzGjN6pN7r/o4G9O+jX2fOUObMtho24gv9vmCJcjk6mjrcl1aJ10pp7qI/NWDwp/y4NjHbLFlUvWbknZqXL13U5o1xz1+1eeM/unL5kvHxo8BHcbZDyixbvEDnzp6RJDV8o6lKlylr4ohefpkz26pR0xb67Itx+um3BZq9aIUm//ybuvV+X9mflinav3uHvho2RKGhfD+bSsDDhwoPD5ckXbl8ScuWLJKjk5O+nvS9du8/rINHT2ru/EUqX7GiJOnUyRMa99VoU4acoZ0+cUwb1q6SpaWVho78Klnf0Q/9n/32XrZonm7dvK4atevq1/nLtGXfMa35d7c+HfGVstplU0REhGb/Mk37du9Ii5fxyttAWamMy2DIuP8hXTEiA0hDoaGhWrp0qSSpdevWsZIHDg4Oeuutt7RmzRotWbJE33zzTYLzW3Tr1i3edVGjAiIiIuTm5qZKlSrF2a58+fKqUKFCnOsMBoMqV64sDw8PXbt2LaGXlqoePXokLy8vPXr0SBEREZIUY3TBqVOnkjUq42G0EhBZs2ZNvUCV8Huwa9cuhYWFSVKcSYwoLi4uatq0qTZt2mTcJq6a0AaDQe+99168/fTu3VsjR45URESEtm3bpk6dOiX5ddwys8lKrS0t9PP7tWRhYdDP/5zT+Vu+ye7jn/9uaOnu2CU/jl+7r1UH3dW8Sn4tHtpQmaws9W2Patp49Kbu+QXH25/Hg0DV/HydpMjSUvkcsqhJxfzq0ai4pvWrpSJ5smnK2rPJjvNVsWX77jjvCqxVp67e6dpdgwb014Xzrjp29IhW/LVMXbtTZicthYQ80T/r12jXzu3G42x09+97a8M/65QvfwE1bJS8UXCIrX6jN1SqTDlJ0uPHwbp966Z2bPlXu3du09hRw/TJ5yNVt35D0wYJfTBgkI4cPqTQ0FCNGT1Kt27eVMs2beXo6CRvby9tWLdWs3+dKWtra2PC9XEwk7SnluNHj+iXn6dKkuwdcmnE6DEmjujVsPyfbbLLFvv7uVrNOmrfuatGfvqRrlw8r1PHj2rdyuXq8Hb8575IO0FBz0qhPn78WJltbTVn3oIY5deqVquu2b8v0Hvd3tGlixe0Y/tWnTl9SuUrVDRFyBlWSEiIfvh2vCIiItT53R4qWix5NzxFf6+ePH6sajVr69spvxh/32XK5KC2HbuoSLHi+uTD3goPD9ecmT+pbv1G3NSQiu56eurokf8kSeUrVIwxRxyAlwcjMoA09O+//+ru3buSYpeVihL1/K1bt7Rz584E+ytVqlS86xwcHIzL0S/kJ6eP6P0k1Edq8Pb21hdffKHXXntN2bJlU5EiRVSuXDmVL19e5cuXjzG3hbe3d7L6jl6mJvok2akhviSQFFl2KkrNmgnX44xaHxgYGG/SqEiRInJM4M5oJycnubi4SIociZMcBQsWTNJ/6eXz9uX1Wv6cuuEVoEkrT6eoD/9ERkdsPn5b3z3tO2tma/VslPCPlNCwCJ2/5avzt3x15rqP/j1xW8Pm/6c3vtqoiAhp3LtV9MsHdVIU66sgodIGuRwd9f2Un2RlFZmw/HPpkvQK65UUFBioD/r10by5s+Xv56devftp1dqN+u/4ae09eFSzfvtdlatUleu5sxr68UAtWvCHqUPO8LJly65ixUuoWPESKlO2vJo2e0vf/viTvvrft/K4fUsjhg7WhnWrTR3mK69CxUr6Ysx4WVlZKTQ0RDNn/KSWbzZWzSrl1fLNxpo54ydZWVlq6LCRxm1S+waJV9W1K5c1YuhghYWGysbGRt9+PzXRSdeROuJKYkRxyOWocd/+aBw5vHr50vQKC8/J9NzE0O07dIpzDpnMmTNr0JBPjI//3bwxrUN76Sz+Y45uuLspj3Nevff+h8ne/vn36oNBn8Z5k1qFSlVUr+EbkqTrbtd07cqlWG2Qchv+WWccxdS6bXsTRwMgrZDIANJQVLmoXLlyqXnzuGv+tmrVylhmJbHyUgmVY4o+kiNqVEBy+4jeT0J9vKhjx46pVKlS+vbbb3Xp0qU47w6OLvpdLkmRK9ezH8JRiaTUYm8f/6RvDx48MC4nVtLK2fnZHA7Rt4suKWWx8uTJk2AfGUGJfNk1tG3kncvD5v+nwMdpN3n2H9svKzw88vP2epk8Kerj3A1fTVh+QpLUo1FxNa6QN9Xie5UUKFhQtWpHJoJu3riue/dS928Vz/w6a4ZOHI+s8T9m/ER9PPRzFSlaVNbWmWRnZ6dadepq9u8LVL1GTUVERGjalO918eIFE0f9cmrRqo0aN2mm8PBwTfnua/n7+Zo6pFdeu/YdtWDJX2r0RlPZ2j47R7KyslKDho215K9VKlO2nPF56s+/OI/btzRkwPvy9/eXpaWlJkz6QZWrVjN1WHgqX/6CqlqjtiTp9q0b8va6Z+KIXk3PJ01r16kbb9satWobk0+uZxktnBzX3a9p6YK5kqQhn4+K8T2QVNF/X+e0d1CJ12KXBY5Svdaz9/GC67lk7wvx2/BP5Ej6TJkyqVmzuOegBDISg8GQpP/iKj//vE2bNql9+/YqUKCAbGxsVKBAAbVv316bNm1KcjyhoaH69ddfVa9ePTk5OcnW1lbFihXTBx98oHPn0u94RmkpII34+flp3brIL9P79+8b58JIyKpVqzRz5syX+m6/J0+eqEuXLrp//76sra01ePBgtW3bViVLlpS9vb1snt7Rcu3aNRUrVkySEk10PK9ixYratm2bpMjJzUuUSPl8CM+L6+6auKTGMOG0HGp88+bNJLUr83na128d+FYZ2Vhbyu3uQ2XJZKWOtV1itSldMKdxuUFZZ+XJYStJ2nT8VrISH97+wXoQ8FiO2TMrr33yf6hE2XD0pqb2rSVJaluzsHacvpPivl5lRYsV0769uyVJXnfvKXfulCWXEL+IiAitXb1SklTYxUVt4rlDzcrKSh8N+li9e3ZVeHi41q9ZrddGjErPUF8Z9Ro20vatmxUUFKRDB/bpzRatTB3SK690mbL6cdp0hYaGytvbSyEhIcqdO4/xnGTD+nXGtsWKFzdVmC8Fr3v3NOiDvvLyuieDwaAvx01Ug0ZvmDosPKdwkWI6fCByvjxvr3tydErdOeeQuEyZMsnewUE+T29WyuMc/40zNjY2ypnTXt7eXvLxybg3N5nC38sWKSQkRPnyF1BwcLC2b4l9Uc/t2hXj8omjh/XgfmS1gDr1GsjWNoty53l2g5pTIuey0dv6+vJepZZz587o2tXI96leg4bKniOHiSNCshm4zz4thIeHq3///vr9999jPH/79m3dvn1ba9asUb9+/fTbb78lWObe29tbb731lo4cORLj+WvXrmn27NlasGCBZsyYoX79+qXJ64iORAaQRpYvX67g4Pjr78clICBAq1atUo8ePdIoqhcT/cAWNWwzLgmVc9qxY4exlNLMmTPjPdC9yAiDBg0a6Mcff5QkbdiwQW+//XaK+0qO6OW97t69m2BpJk9Pzzi3iy4po0mi2sTXR3wKFCiQrPZpycY68nNVJE82/fFx/UTbj+j4rO5vucErdcMreSM4kpsYi4u3/7O/7UKOdi/c36uKusBp7/59b/n5RU5C+VqpMgm2jT7Jrptb+s2T9KrJaf/seO15hySoObGyspJzHBcLz0e7a7ZsufhLTCJhvj4+GvxhX92+FXkzxWcjRuut1kzGao74fjYPxYoV19EHkTX/w8MTHi0f9nS9pSWXeJLjyZMnkiJHik34cnii7Rf+/ptxedmazbK1zSKXos8S3OGJVDWI/j7yXqWef9Y9m+S7dZt2pgsESAMDBgzQRx99FO/6hG6EHj16tDGJUblyZQ0fPlzFihXT1atXNXnyZJ04cUJz586Vk5OTvvnmmzj7CAsLU/v27Y1JjA4dOuj999+Xg4ODDh8+rIkTJ+revXv64IMPlD9/frVokbYjojhyAmkkqkxU3rx5NWXKlETbDxs2TLdu3dLChQvNNpERfe4JHx+feNtduhR/vc/oQ84SSjAcPXo0mdE906xZM+XLl08eHh5asWKFvv32W+XPnz/F/SVVuXLPSk8cPnw4wUTGf/9F/ijJkiWLihaNXe9Wktzc3HT//v0YpbKi8/Lykru7e6x9I365stkoV7bMkqQ7PoEp7iefw7PRHAHBCc/NgfhF3TklSU5JKKWG5Iv+IzksLOGkX2jos8+ylVXSRp8h+byilVGzTaTcI0wvLCxMO7ZvlSQ5O+dVxUqVTRxRxhTw8KGGfPS+3K5dlSQN/HioOr/T1cRRIT7X3a4al3M5OpkwkldblarVjJMX37p1U6VKx31DQkBAgHyf/jZLSmlapC7nvPmUxzmv7nrekecdD0VERMSbDPS49WxUvBMjnVJFSEiIcW4YewcH1X098RvjgIwkd+7cKbrec+nSJf3www+SpGrVqmnPnj2ytY2sbFG9enW1adNGDRo00NGjR/X999+rT58+Kh7HyOMFCxZo3759kqSPPvpIv/zyi3FdjRo11KJFC1WtWlX+/v4aMmSIzp8/byx3mBZIZABpwM3NTfv375ckdezYUe+8806i2xw6dEg//fSTduzYodu3b6fLhffkKlKkiHH56NGjqlq1apztli1bFm8foaHPLqQ9evQoRnIkSnh4uObMmZPiODNlyqTPP/9cQ4cOVXBwsPr27asNGzYkqSzU7du3dfHiRTVu3DjZ+23YsKEsLS0VFhamefPmqVOnTnG2u3HjhrZu3Rpjm7hERERo4cKF+vTTT+NcP3/+fOPogiZNmiQ7XnMxYNYBDZh1IME2ozpV1KhOkSMx3vrfv9rnmrL5FHq/UVIWFpE/LPafT/mcDO1quhiXXW/6prifV9ntW7d06GDk+16wYCHlzkNZqbSQI0cO2dnZKSAgQKdPnVRoaGi8J5bHjj4bKpwvv/mM2nrZ7Ni2xbhcrHjqlT5E2liz6m953vGQJHXo/HaSS0zimeCgIH06eIAunneVJPXu94F69k770gNImTset3Tsv4OSpHwFCiZaKgdp542mzTT715mSpJ3btqlJ02ZxttuxfavxNwHzzSTPqLFfa9TYrxNs88fsmVowd5YkaeqseapctXqsNvUbNdGKZYv06FGAjh05pGpP55l53p5d243L5StVeYHIEWX/vr3GEmwt3mqVphdQkYYoLZXqpk2bZrz+Nn36dGMSI0qWLFk0ffp01a5dW6GhoZo6dWqMJEWUqGSIg4ODvv/++1jrixcvrlGjRmnUqFG6cuWKVq9erc6dO6fBK4rEJwVIAwsXLjSeTMZ3Mft5Ue3Cw8O1ePHiNIvtRZQrV85YwmjGjBl6/PhxrDbLly/XihUr4u0j+nwV8+fPj7PNqFGjdPz48ReK9eOPP1ajRo0kSf/++6/at28vLy+veNtHRERo6dKlqlq1qk6fPp2ifebLl0/t20fWn9+0aZMWLFgQq82TJ0/Up08fhYRE3vk8aNCgBPucMGGCLl68GOv58+fP6+uvI0+68+bNq7ZtX+3SDIWcsqqCS8LltZpXya8RHSNLggQ+DtXiXVditWlZraDy5LSN9Xx0dUrlNvYTEhquFfvdUhj1y2v3rh0xkpbPu+/trc8/HWL8O+j8zrvpFdorx8LCQq/XayApsjb973N+jbOdv5+ffpr6o/Fx/QYN0yO8l8qGdavj/F6M7s/FC3Rw3x5JkcmiipXjviEA6edeAmUc/zt8SD9O/lZS5BwzPd7rnV5hvTRCQp5o+NAhOn0y8rzu7a499OGgj00c1avrwN5dCkvg+/nBfW+NHTnU+P3ctmP6lGdF3Eq+9prq1ou8u3zzpg06fOhgrDbe3l6a+fNPkiRra2u1bdchXWNEpE7v9lCmp/MqzZz2vR4FBMRqs2XTep08FnnTSK269WPMl4GU+2f9GuNyK8oVApKezpO4NrLkWqlSpVSrVq0429WqVUuvvfaaJGnt2rWxynBfunRJ58+flyR16dJFWeIZTd6rVy/j8urVq180/ASRqgTSwKJFiyRFDgGrV69ekrapU6eO8ubNqzt37mjRokUaMWJEWoaYIlZWVvrggw/07bff6uzZs2rcuLGGDx+uQoUK6e7du1qxYoXmz5+vOnXq6MCBuO+wb9asmXLnzq179+7pyy+/lLu7u9q3by9HR0dduXJFc+bM0fbt21W3bl3jqJaUsLCw0PLly9WqVSsdPnxY69evV7FixdStWzc1btxYBQoUkLW1tTw9PXXo0CGtXLlSFy5cSPH+okydOlXbt2+Xj4+P+vTpo3379untt9+Wvb29Lly4oB9++EEnT56UFPlFkFD9wOLFi8vLy0u1atXSiBEj1LBhQ0nSrl27NGnSJGPd++nTpydpMvmXWSEnO20c00yHL93TpmO3dPa6j7yezmPhkttObWsWVruahY2jMb5cfEx3fIJi9dOqekHN/7i+/j1xS7vPeur8LV/5PXoiG2tLFcmTTS2qFFD72oVl+XS+mO9WndKVO/7p90IziO++majQ0FC90eRNVahUSfny5VfmzJnl4+OjY0f+098r/jKWQKhcparefrebiSN+ufX/cKB27dqh4KAg/Tpzhlxdz6l1m3YqUKCgHj9+rDOnT2nJ4oXGu85r1Kyt2nVeN3HUGc/vv83U9Knfq2HjpqpYuYryFygo2yxZFPjoka5euawtm/7R6ZMnJEVebBrx5bg47+7fse1fBQU+K30Xtc3zy5KUK5ejatVN2nkG4ta5fWtVrVZdr9dvoGLFi8vaOpM8Pe9o5/Zt2rRhvcLDw5UjRw5998M04+TfSLovRw7T4YOR53PVatRUm/YddfXK5XjbW1tbq1BhlzjX/bM25g/jSxefnbcdOrBPdzxuGx8XKFRIlUgUxjL9x281NTRU9Rs1UZlyFeWcL59sbDLLz9dHJ48f0T+r/5afb+T3c/mKVdS2U9w3Gmz+Z02Mx1cvPXsv/ju0T553nr0X+QsU4s7zFzBsxCidPnVSD/399fHAD9W1e0+9Xr+BbGxsdO7sGc2bM1t370bOvffR4I8Z4WoieZzzqk//gfp1+hRdu3JZH/Z+V1179lHR4iUV+OiR9uzcprWrlkuSsma106BPE5+PA4nz9/PT3t27JEnFi5eIMd8b8Cpzc3OTh0fkb7sGDRok2LZBgwa6ePGibt++LXd39xiVWKJKSiXWj7Ozs0qWLKlLly690HW8pCCRAaSy/fv36+rVyLqy7du3jzFBdkIsLCzUvn17zZw5U+fOndOxY8fiLd1kSl9++aV27typQ4cO6cCBA2rXrl2M9Q0bNtSMGTPireGXNWtWLVy4UO3atVNwcLB+++03/fbbb8nqI6kcHR21a9cujRw5UrNmzdLDhw/166+/6tdf474j2WAwqFu3burSpUuK91mgQAFt375drVq1koeHh+bOnau5c+fGatehQ4c4R2xElz9/fk2bNk1dunTRqFGjYq23sLDQ5MmT1bFjxxTH+7KpWTK3apaMv97so+AQjVp0VPO3x38RxcbaUm1qFFabGoXjbRP4OFQT/jqhXzaef6F4X2Ze9+7pz6WL9efS+EeYvdH0TY0dP/GVT8SltSJFi2rqT79o1IjP5Ovjoz27dmrPrp1xtq1Rs5a+/3Fa+gb4EvH389O61X9r3eq/422TO4+zvhg7QdVrxl12YsbUH4xJpeetX7NS69esND6uXLU6iYwXFBoaql07t2vXzu1xri9WvIS+nvS9Sr5WKp0jeznsejq/iCQd/e+wunVul2D7vHnzac2mbXGumzB2dLzbLfwj5rlWy9btSGTE477XPa1evlSrly+Nt039Rk31+ehx8X4/T57wVbzb/rlwXozHzVq2IZHxAgq7FNFPM2Zp2Kcf6/59b/3x+xz98XvMErwGg0F9+3+oXn0o2WZK7/ToLX9/Py1bOE83r7vruwljYrWxd3DQxMk/qUCh+H9nIOn+/XeTccL2VkzyDRO5detWktoVKJCy0r0rVqzQ8uXL5e7uLktLSzk7O6tOnTrq1auXsQrJ81xdXY3LpUolfA4bff358+djJDKS28+lS5d08+ZNPXr0KMFJyF8EiQwglUVN8i0p2ReYO3bsqJkzZxr7McdERpYsWbRjxw5NnTpVf/75p65cuSJra2u99tpreu+99/Thhx/q5s2bCfbRrFkzHT16VJMmTdKOHTvk5eWlnDlzqkyZMurWrZv69u2rGzdupEq8mTNn1rRp0zR06FAtW7ZM27Zt06VLl+Tl5aWIiAg5ODioXLlyatCggbp166bChV/8pLJy5cq6ePGiZsyYoTVr1ujixYsKDAyUo6OjatWqpV69eql169ZJ6qtly5bGyZd27NihO3fuKGfOnKpXr54+++wz1a4d94WwV83Ja/fVb/pe1SjppMpFcylPTlvlymYjK0sL+T56ovO3fLX77B0t2HFF3k9HasTlqyXHtc/1ruqWzqMyBXPKKYetnLJnVnhEhHwCHuvCLT/tPndHy/Zc013f2CM6EOl/X0/SsaNHdPrUSd2+dVO+Pj569OiRbLNkkXMeZ1WoVFmt27Zj0tx0VKt2Ha1et1FrVq3U/n17dPXqFT30fygrK0vlyuWosuXKq/lbrdSwUeN4J6hEwqb+MlsH9u3WmZMndOvmDT14cF9+fn6ysbGRvb2DSrxWSnXrNdAbTZsrs23CJeyQfsaMn6CDB/br3Nkz8va6p8DAwMj3q+RravJmM73Vqo2sra1NHSaQKkaOmahTx4/K9ewpedy+LX/fqO9nW+XO46yy5SvpzZZtVLZ8JVOHimgqV6mqv9eu159LFmvnju3yuH1LISEhcnRyUrVqNfROt+7xTgSO9NV/4CeqW6+h1q5crtMnj+v+fS9lymSjgoUKq069hurwdlfZ2cWeIxIps2F9ZOkcS0tLtWjZysTR4IVYZNzfHwULFkxSu+fLNiVV9GSCJF25ckVXrlwx3iA8f/585ciRI0ab6MmVxBIo0eN//lpeSvqJiIjQrVu3jCWrUpshIqX/kgCANNGwYUPt3r1bDRo00K5du0waS/Z3FibeCOnGc1EPU4eAKJw9mY2gkDBTh4CnMlszEba5CAkLN3UIiObRY45T5sI+K4lJc+EXGGLqEPBUDlv+LsxFlkwZ94L+i7BtNMHUIaRY8K7Yo6/iktzL71mzZlWbNm30xhtvqFSpUrKzs5OXl5d2796tX3/9Vffv35cUWfJp69atMW68+f777zV8eGQJu02bNql58+bx7mfTpk166623JEVO7P3ZZ58Z17Vs2VIbN26UJAUFBSlz5szx9jNixAhNnjxZknT06NE0uzGbERkAAAAAAAAAACRDYhVJUur27dvKmTNnrOebNm2qwYMHq0WLFjpx4oR2796tWbNmaciQIcY2wcHPqlAkVsY5+hxwQUExq06kVj+piUQGAAAAAAAAACD9GZI2t6w5SuncF4mJK4kRJU+ePPr7779VqlQphYSEaPr06TESGdFHTkTNIxOfx48fG5dtnyt/+3w/CY3ISKif1JRxPykAAAAAAAAAALxCihYtqqZNm0qKnDfDw8PDuC5btmdz8QQEBCTYz6NHj4zLdnZ2MdalVj+piUQGAAAAAAAAAAAZRJkyZYzLt2/fNi5HHyUSfcLuuEQvjfX8xOUp6cdgMKTZKBWJRAYAAAAAAAAAABmGwRD35PDRExwXLlxIsI/o60uXLv3C/RQsWFBZs2ZNsO2LIJEBAGZm165dioiI0K5du0wdCgAAAAAAQNoxGDLufybk6upqXM6XL59xuUiRIsbHu3fvTrCPPXv2SJLy588vFxeXGOtef/1143JC/Xh6eurSpUuSpLp16yYt+BQikQEAAAAAAAAAQAbg5uamrVu3SpKKFSum/PnzG9cZDAa1bdtWUuRIiUOHDsXZx6FDh4wjKdq2bRtrhEfJkiWNozSWL1+uwMDAOPuZP3++cbl9+/Ype0FJRCIDAAAAAAAAAAATW79+vUJDQ+Ndf/fuXXXs2FFPnjyRJH300Uex2nzyySeytLSUJA0ePFhBQUEx1gcFBWnw4MGSJCsrK33yySdx7uvzzz+XJD148EDDhw+Ptf7q1av69ttvJUnFixdP80SGVZr2DgAAAAAAAAAAEjV48GCFhISoY8eOql27tlxcXGRraytvb2/t2rVLv/32m7y9vSVFln8aOHBgrD5KliypYcOGadKkSTp69Kjq1q2rESNGqFixYrp69aq+++47nThxQpI0bNgwlShRIs5Y3nvvPc2bN0/79+/XL7/8Ik9PT73//vuyt7fXf//9pwkTJsjf318WFhb6+eefZWWVtqkGQ0RERESa7gEAkGFlf2ehqUNANJ6Lepg6BETh7MlsBIWEmToEPJXZ2tLUIeCpkLBwU4eAaB495jhlLuyzWps6BDzlFxhi6hDwVA5b/i7MRZZMpp1zwVRsm0wydQgpFrRtZKr36eLiouvXryfarmPHjpo7d65y5swZ5/rw8HC9//77mjdvXrx99O3bV7Nnz5aFRfxFm7y9vfXWW2/pyJEjca63sbHRjBkz1K9fv0RjflGMyAAAAAAAAAAAwMQWLFig3bt36+DBg7p27Zq8vb3l7+8vOzs7FSxYUHXq1NF7772n2rVrJ9iPhYWFfv/9d3Xs2FGzZ8/WkSNH5O3tLUdHR1WvXl0ffPCBWrRokWg8jo6OOnDggObMmaOlS5fq/PnzevTokfLly6c33nhDH3/8scqWLZtaLz9BjMgAAMSLERnmhREZZoSzJ7PBiAzzwYgM88GIDPPCiAzzwYgM88GIDPPBiAzzwYiMjCctRmQgfozIAAAAAAAAAACkP8OrmcBB8sVfAAsAAAAAAAAAAMDESGQAAAAAAAAAAACzRSIDAAAAAAAAAACYLebIAAAAAAAAAACkPwP32SNp+KQAAAAAAAAAAACzRSIDAAAAAAAAAACYLUpLAQAAAAAAAADSn8Fg6giQQTAiAwAAAAAAAAAAmC0SGQAAAAAAAAAAwGyRyAAAAAAAAAAAAGaLOTIAAAAAAAAAAOnPwH32SBo+KQAAAAAAAAAAwGyRyAAAAAAAAAAAAGaLRAYAAAAAAAAAADBbzJEBAAAAAAAAAEh/BoOpI0AGwYgMAAAAAAAAAABgtkhkAAAAAAAAAAAAs0VpKQAAAAAAAABA+jNwnz2Shk8KAAAAAAAAAAAwWyQyAAAAAAAAAACA2SKRAQAAAAAAAAAAzBZzZAAAAAAAAAAA0p/BYOoIkEEwIgMAAAAAAAAAAJgtRmQAAOLlsaCHqUNANLnaTTd1CHjq0qIPTB0CnrLPmsnUIeApSwvupjMXfoFhpg4B0ThwnDIbYeERpg4BT2W2tjR1CHjqwaMnpg4BT2XJZGPqEACzxogMAAAAAAAAAABgthiRAQAAAAAAAABIfwbus0fS8EkBAAAAAAAAAABmi0QGAAAAAAAAAAAwW5SWAgAAAAAAAACkP0pLIYn4pAAAAAAAAAAAALNFIgMAAAAAAAAAAJgtEhkAAAAAAAAAAMBsMUcGAAAAAAAAACD9GQymjgAZBCMyAAAAAAAAAACA2SKRAQAAAAAAAAAAzBalpQAAAAAAAAAA6c/AffZIGj4pAAAAAAAAAADAbJHIAAAAAAAAAAAAZotEBgAAAAAAAAAAMFvMkQEAAAAAAAAASH8Gg6kjQAbBiAwAAAAAAAAAAGC2SGQAAAAAAAAAAACzRSIDAAAAAAAAAACYLebIAAAAAAAAAACkPwP32SNp+KQAAAAAAAAAAACzRSIDAAAAAAAAAACYLUpLAQAAAAAAAADSn8Fg6giQQTAiAwAAAAAAAAAAmC0SGQAAAAAAAAAAwGyRyAAAAAAAAAAAAGaLOTIAAAAAAAAAAOnOwBwZSCJGZAAAAAAAAAAAALNFIgMAAAAAAAAAAJgtSksBAAAAAAAAANIdpaWQVIzIAAAAAAAAAAAAZotEBgAAAAAAAAAAMFskMgAAAAAAAAAAgNlijgwAZmHXrl1q1KhRktv/8ccf6tWrV9oFBLPz4P59nT17WufOnpHr2TM6d+6M/Hx9JUmt2rTT+ImTEu0jPDxc7m7XIvs5c0au587o8qWLCgkJkST99vsCVateMy1fRoY2sXcdfdapmvHxmyNXau+Z2zHaFMqdTRf/6J2sfq/f9VepPvPjXd+iuot6NCmjGqWc5ZjDVg8Dn+jaHT+t3n9ZszecUeDj0GTt72XTpHaFJLWrULmapsycF+96zzu3tX7Vch0/ckget28pOChIWbJkUcHCRVS9Vl21at9Z9g65UivslxLHqYzLw+O2li5epL17dsnT01OZrDOpYMGCerN5C739bjfZ2tqaOsSX0m8zpujPRX8YH0+dOU+VqlaPt72nx22tXfWXjv0X7TiVNYsKPT1OtenQheNUGpo25XvNnzfX+HjOvIWqXoPjUUrxnWE+zp87qwP79ujUyeNyu3ZVvj4PZGVlJUen3KpQqbJat+uoSpWrJqvP/w4d0OaN/+j0yWPy9vKWpZWlHBxyqXiJ11StRi21aNVaWbJkTaNXlHG9UStp57UVK1fTlFmxz2s3/7NW30/8Kkl9DPtygpq3apus+JBGmCIDSUQiAwAyCHd3dxUpUkTSq5nIadqo7gv3sWH9Wo37alQqRPPqqVDUUUPaVU6Tvi/d8onzeTtba/0xrJla1Swa43mbHLZyzGGrGqWc1bd5eXWasF4Xb8bdB5Jm66b1mvbdBD1+HBzj+YcP/eV69pRcz57SquVL9OWEyapao7aJojR/HKcypl07d2j0yGEKCAgwPhccFKRz5/x07txZrVq5QjNmzlahwoVNGOXL58qlC1qxdFGS22/ZuF5TJv0v9nHK31/nzpzSuTOntOqvJfpq4mRVq1kntcN95V24cF6LF843dRgvFb4zzMOHfXro5IljsZ4PCQnRzRvXdfPGdW1Yt0ZvtWqrUWPGy9o6U4L9+fv7aeLY0dqza0esdY8CAnTzxnXt3L5F5StWVMnXSqfa6wCAVwGJDABmZ8CAAfroo48SbFOgQIF0igbmyDlvPrkUKaJDB/Yna7sIRRiXraysVbxECYWGhurK5UupHeJLxWCQfhnUWNZWlrrrE6g89lnibetx/5GqfrQ40T6Hda6mdxqVkiQt2X4+zjaLR7ZQs2oukqRjl+9qxpqTunjrgexsM6lFdRcNaF1RxfPn1NrxbVX3kz913z84zn5eFa07dFGbDm/Huz5zPHeUnz11Qt9P/Erh4eGysLBQ0xZtVKd+I+VydNK9u3e0deM6Hdy3Ww/9/TRm+Meas2SV8uXnGJwYjlMZw/nzrhrx+acKDg5WlixZ1Pf9D1S9Rk0FBwfr300btfLv5bru7q5BH/XXsuUrlTWrnalDfimEh4frx2/HKywsVPb2DvLxeZBg+zOnTui7CV8aj1NvvtVGdes3kqNTbt31vKMtG9fpwN5d8vf305fDPta8ZauUL3/B9Hkxr4Dw8HBNGPeVQkND5eCQSw8e3Dd1SC8dvjNMx9v7niTJySm3GjdtpoqVq8o5b16Fh4XrzOmTWrpovrzu3dXGf9YqNDRU//v2+3j7Cnj4UEM+7KcL589Jkho0bqLGTd5U/gIFZWlhqbt3PXXi2BHt3L41XV5bRtamQxe16Zj889rovvvpV+VydIp3vVPuPCmKDYDpkMgAYHZy586tcuXKmToMmJn3P/hIZcqVV9ly5ZUrl6M8bt9S6xZNktVH0aLFNWzkaJUtW14lS5WWjY2Nfps5nR97iRjYppKqveasCzcfaN2Bqxr+dvxlP0LDwuV6PeELUhYWBtUvH3kh3D/widYevBqrTfu6xY1JjG3Hb6jD+HUKCQ03rt975ra2Hr+hdf9rq8J5suvLrjX16a+7U/DqXh457R1UpFiJZG+3bOHvCg+P/LcdOHSk2nZ8x7iuVJlyqt+oqX79+Qf9vWyhHj8O1splCzX48y9SLe6XCcepjGfyt18rODhYVlZW+nXOPFWs9GzkWc1atVWocGFN/fF7XXd318L5f2jAwMEmjPblseqvJbrgelaFXIro9QZvaOmCuQm2X7pgrvE4NfizUWrXKeZxqkHjppr50/dasTTyOLVi6UJ9PGx0mr6GV8nSJQt17uwZFSlSVI3eaKp5c38zdUgvBb4zzENhl6L6cNAnavTGm7K0tIyxrlyFimrRso369+6mG9fdtWXzBrXv9LYqV60WZ18/fve1Lpw/p0yZMmnid1NUv2HjGOtLly2nho2b6JPPRyosLCzNXtPLIKXntdEVKFhYzvnyp1JEAMwBk30DADKEDwcOUf0GjZQrl2OK+yhXvoLe6dpD5StWko2NTSpG9/Iq6GSnMd1rSZIGz9ipJ9GSCSnVuFJB5XOMvKt59b7LCn4S+4dc9ybPhtp/MmtnjCRGlJ0nb2rFnsgf6n2al5O9He9pSrieOSlJyp4jZ4wkRnTd+3zwrP3ZU+kRVobEcSpjOXP6tI4fOypJatehY4wkRpSevfqoaNFikqQlixca684j5e563tG82TMkSZ+O+ErW1taJbnMu2nEqehIjup59P4zWnuNUarlzx0Mzp/8kSRo9ZnyS3i8kDd8Z5uHHn2epyZstYiUxouS0t9eQocONj3ds+zfOdidPHNOmDeskSR8MHBIriRGdwWCQlRX3FQNRDAZDhv0P6YtEBoCXwpMnTzRz5kw1atRITk5OypQpk5ydnfXWW29p8eLFxrv44tKrVy8ZDAa5uLhIku7cuaMRI0aobNmyypYtmwwGg3bt2hVjm7CwMC1YsECtWrVSvnz5ZGNjo1y5cun111/XlClTFBQUlGC8x44dU9++fVWyZEllzZpVmTNnVsGCBVW1alUNHDhQ69atU0TEs6HiBoPBOD+GJPXu3TvWF+i4ceOS/e8GJGbqgEbKliWTFm1z1b6ztxPfIAm6NS5lXF68/UKcbaqUyC1JunLbV1c9/OLta8ux65KkTNaWalmraLztEL+Q0MgLswndsWZnl005ctpHtudCLl4SO3dsMy63bd8xzjYWFhZq1aadpMi5GI78dzg9Qnup/fT91woKDFSzlm1UqUr8I/yiizru5E3icSo0lONUavl24v8UGBio1m3bq1r1GqYOBzCJqtE++7dv3Yyzzd9/LpUUeSzq9Ha3dIkLAF41pIABZHju7u5q0aKFLlyIeUH07t272rRpkzZt2qTffvtNa9eulYODQ4J9HTp0SK1bt5a3t3e8bW7cuKE2bdro1KmYd/s9ePBA+/fv1/79+zVr1ixt2LBBJUuWjLX91KlT9fnnn8dKrty6dUu3bt3S8ePHNXPmTD18+FB2dtTihul0fL2EWtYsovv+QRr1+75U6dPO1lqta0fe3ezu6RdvciRXtsySpHu+gQn2d8/n2frXy+bT4m1xz7eB+BUs5KLLF8/L0yP+RNWjRwHy842cUL1gYZd0igxIWyeOR07uamubRWXKlI23XbXqzy62nzxxXHXqvp7msb2sdm7brIP7dit79hwaMOTzJG8XdZy6k9BxKiDacapQkXjbIen+3bxRe3bvVI4cOTX08+GJbwC8pJ48eWJctrCIPXIjJOSJ9u6OnNy7Rq3axtExYWFh8va6p7DwcOXK5cioGQB4QSQyAGRoAQEBeuONN3Tt2jVJUrt27dSnTx/ly5dPbm5umjFjhnbv3q19+/apdevW2rNnT7zDhgMCAtSxY0cFBwdr9OjRatq0qbJkyaIzZ84ob968kqT79+/r9ddf182bN2VjY6P3339fDRo0kIuLiwICArRlyxb99NNPunLlilq0aKHjx48rR44cxn2cPn3amMQoUqSIBg0apEqVKsnBwUEPHz7UxYsXtXPnTq1duzZGbGfOnJGHh4eaNWsmSZo4caLatm0bo03u3LlT7d8VyJE1k77/oL4k6cs/DqTaRNrt6xZX1syRZSmW7ox7NIYkBQSHyN7OUtmzZkokzmc/CEsXypUqMWZUe3Zs1e7tW3T3jocsLC3k4OCoMuUrqlnLtqpUNf67aFu176ypk/4nfz9frV+1XK07dInVZvG8Z/XQW7WLvR7IiNyuRc7PU6hQoQRLfBQp8my0V9Q2SL6Ah/6aMeU7SVL/QZ8aR08kRZsOXfTjt+Pl7+erdauWq00cx6lF0Y5TbTp0fvGAX3H+/v76ftI3kqSPP/1c9vYJ3wwEvMxOPC1DKEkuRWOPAL586aIeP34sSSpWvKQeBQRo9qzp2rh+rR4+9JckWVtbq1KVaurV7wNVrcbopsTs3rFVu54/r60QeV5bOYHz2ugmTxyjWzfc5efroyxZ7ZS/QEFVqV5LrTt0YaJvM0OJJiQViQwAZufevXs6e/ZsvOtz585tvGg/fvx4YxLjyy+/1IQJE4ztqlatqo4dO6pHjx5asmSJDhw4oNmzZ2vAgAFx9nv//n3Z2dlp3759qlixovH56tHuxBwyZIhu3rypwoULa+fOnTHKPUlSw4YN1blzZ9WrV0/Xrl3T5MmT9fXXXxvX//333woPD1fWrFl18OBB5ckT8wSqXr166tevn/z8/JQlSxbj8+XKlYsxOiN//vxMiI409XWf15XXIasOnPPQ/C3nUq3fbm88m/tiSTxlpSTp4k0f1SqdV6UKOsgxu628/eMu1/Z6uXzG5YJOr/YIputuMS+w3g68odu3bmjrpvWqW7+xhn01QXZ22WJt17xVe509dUJbN63X9B+/0aWLrqrzekM5ODrp3t072rbpH+3fE3mXYdde76tqjVrp8nqAtPT48WP5+ETevZ/b2TnBttlz5JCtbRYFBQXK09MzPcJ7Kf06fYoe3PdWuQqV9VabDsnatkXr9jpz6oS2bFynn77/WpcuuKpOvYbK5eiku553tHXTeu17ejd0997vq2qN2mnxEl4p06Z8L29vL1WqXEXtO3YydTiAyYSHh2vhH3OMj5s0bR6rjdvVZ+dg4RHh6tWts27euB6jTUhIiI4cPqij/x3SgMGfqmfvfmkX9Esg3vPajetVt0FjDY/nvDa6U8ePGJf9/Xzl7+er8+fOaMWyhfrok+Fq3Z6kN5DRkMgAYHZmzZqlWbNmxbt+7NixGjdunB4/fqy5c+dKksqWLRvnHBEGg0EzZ87U5s2bdf/+fc2YMSPeRIYkDR8+PEYSIzp3d3f99ddfkqQZM2bESmJEqVy5sgYOHKjJkydr/vz5MRIZURdASpYsGSuJEV30URxAeqtbNp96v1lWIaFhGvzLjlTrt6CTneqVi6xvftDVQ9fuxD/3xYbD11SrdF5ZWVpobM9aGjxjZ6w2xfLlUI8mZYyP7WwTHr3xssqcObNq12uoytVqqmDhIrK1zSI/Xx+dOnFU/6xeIX8/X+3fs0MPh/tr8s+/ycoq5kStlpaWGjHma9V+vYGWLpirTetWadO6VTHaVKpaXe/2JImBl8ejR4+My9FvHIiPbRZbBQUFKjAw4XJ3iNvpE8e0cd0qWVpaaejIr5J956WlpaVGjf1adV5voCUL5mrD2pXasHZljDaVq9ZQt179SGKkguPHjmr1yhWysrLSl2PGc6csXmnLFi+Q69kzkqSGjZuqVBylCP39n53TLp7/ux4/fqxadV5X/48Gq3iJ1/QoIEA7t2/RzJ+nKiDgoWb+PEUuLkVUv9Eb6fY6Moro57WFCheRbZYs8vXx0ekTR7U+6rx29w6N8ffX5Omxz2slKW/+AqrX8A2VKVdRTnkib1a4c/uW9u7apj07turJ48ea9t0EGQwGtWpHohbISEhkAMiwjh07Jl9fX0mRE3bHVzIqe/bs6tKli2bNmiVXV1fduXPHWCrqed26xT8x24YNGxQWFqYsWbKoRYsWCcZWv359TZ48WR4eHrpx44YKFSokScb9urq66r///lONGqYZVnzr1q0ktcvpFP+kmng5WVtZaMbgxrKwMGjaqpNyvf4g1fp+p1EpWVhEXgxJaDSGJM3ecEYftqqo/I526teivLLYWGvqymO6eMtH2WwzqVk1F33dp66yZrbWk5AwZbK2lK3Nq3la8+e6bbLLlj3W81Vr1Fb7zl016tMBunLpQuQPwFXL1b5L7OPcdfdr2rppvdyuXolzH65nTmvzP6tU2KWIHBmKj5fAk6clQKTIch+JyWQdmSh9HJw6ZfZeJSEhIfpx0nhFRESo07s9VKRYiRT1c93tmrZsWq9rVy7Huf7c2VPauG61CrkUpWTICwgJeaIJ475SRESEuvV4T8VLxJ7vDXhVHD96RDOnT5Uk2Tvk0vDRY+JsFxT0bOTw48ePVaNWHf348yzj79NMDg7q0PkdFS1eQh/1e0/h4eGaOX2q6jVsTKLwOX+tj/u8tlrN2moX7bz21ImjWrdyuTo8N7H66w0bq1nLNrH+XUuVKadGTZvr4L7dGjfyU4WGhmrWtMmqU6+hHHI5pulrApB6LEwdAAA8b+zYsYqIiIj3v6iRF9HLT9WsWTPBPqOvj69slZ2dnYrGUfM0ytGjkbVRAwMDZWVlJYPBEO9/rVq1Mm4XvQzFu+++K2traz1+/Fh169ZV69at9euvv+rs2bOKiIhI8DWkpoIFCybpP7x6hneprlIFHXTjnr++Xno4Vfvu2riUJCn4Saj+3nspwbb+gU/UecJ63X06mXfXxqV05Jdu8l87SLf/7K95n7+pvA5ZNXbhAfk+irwg+TDoSUJdvrTi+rEXxd4hl8Z8M8VY/3/NimWx2pw5eUxD3u+hg/t2y9Ept0aO/UYrNuzU5r3HtGztFg3+/AvZZM6snVs3a2DfbnK/FneyA8hIMkWbcDUkJCTR9k9CIo8vNpkzp1lML6sl8+fohrub8jjn1Xv9PkxRH6dPHNPAft11YO8uOebOrS/GfaOVG3dq6/7jWr5uqz4eNlqZbTJrx9ZN+qhPV7lxnEqxubN/k5vbNeXNm08fDhhk6nAAk7l29bJGfjZYYaGhsrGx0TeTp8rBIe752GxsYo4KHvjx0DhvsqtUuaoaNm4iSXJ3u6YrlxM+H34VJXRe65Arl8Z+m/B5rZ1dtgSTQ7Vfb6AefSK/i4KDg2ONQoZpJHRtxdz/Q/oikQEgw3rw4Nmd4olNdO0crf519O2iy5kzZ4J93Lt3L+nBRRO9DEWpUqW0bNky2dvbKzQ0VP/8848GDBig8uXLK3fu3OrRo4f27t2bov0AL6pkAXsN61JNkjT0190KfByaan1XK5lHpQpGThS64bCb/B4lnnQ4ccVLtQYv1az1p+Tp8yjGuqMXPdV+3Dr9sOKYsj0tKeUb8Diubl55+fIXUJWnpVZu37ohb69nx7InT57o6zEj9CjgoRxyOWr63MVq0ryV7B1yycrKWk65ndW24zuaOusPZcpko/ve9zR5wpemeilAqsmaNatxOSnlooICI++2TUoZKjxzw/2ali6ILAM6+LNRsrVN/r/fkydPNOGr4cbj1Mzfl6hpi9ZyyOUYeZzK46x2nd7RtN/mK5ONjby97mnS+NGp/VJeCW7Xrmre3MhJ00d88aVs+bzjFeVx+5Y+HvC+/P39ZWlpqQnf/qDKVavF2z5LlmffKfb2DnqtVJl429as/bpx+fy5+OeFRNzy5S9gLCH4/HltUrVs19F4AfrUiWOpGh+AtPVq1mAA8NJJjUx4fKWpooSFhUmSHB0dtXNn7Hr98Xl+Lo2OHTuqSZMm+uuvv/Tvv/9q79698vLykre3txYvXqzFixfrvffe07x582RhkTb55ps3b6ZJv8jYBrerJBtrS12746csNlbqXD92+Y+yhZ/didawYgE520de5Nhw2C3BxEe3p6MxJGnJ9vNJjsnTJ1BDf92tob/uVh77LMpmm0n3fAPlHxiZCMmfy85YUio1y2C9bAq7FNV/ByKTpN5e9+ToFJn8PXJov/EHYLtO78Y7tN6laHG90bylNq1bpUsXXHX18kUVK/Fa+gQPpAEbGxvlzJlTvr6+upfIBN7+fn4KCopMdjgnMjE4YlqxbJFCQkKUL38BPQ4O1o4tm2K1iV7S7vjRw3pw31uSVLteA9naZtF/B/cZj1Ptu3SN9zhVpGhxNW3eShvWrtSlC666cumiipfkOJUcixctUEhIiAoUKKjgoGBt3rghVpur0Up7HfnvkO57R75fDRo2IvGBl4LXvXsa/GFfeXndk8Fg0OixExOdyyJPnmffDU4JzIMoSXmifY/4+nDumhKFixTV4TjOa5PK3iGXsufIKT9fnxQlQgCYDokMABmWg4ODcfnu3bsqWTL+Gr7RyztF3y45cuWKvID78OFDlS5dOtHER0Jy5Mih/v37q3///pKk8+fPa+3atZo+fbo8PDy0YMECVa5cWR9//HGK95GQAgUKJKldwOP0K3cF07OxjvxMF82bQwtHJDwPjCR98e6zkm2v9f5DN+49jLOdlaWFOtWP/Pu86xOoLceupyi+uz6BxlJTUSqXePbD5eilhC9GvsriS/becL9mXC7+WukE+yj5WhltUuTw+xvX3UhkIMMrWqy4jh87qhs3big0NNRYquJ5bm7P/k6KFC2WXuG9FEKeluTyuH1LE74anmj7RfN+My4vW71ZtrZZYhynSiZ2nCpVxjgJ+I3rbiQykunJk8j369atmxo5fGii7Wf/OtO4vOHf7cpPIgMZnK+Pj4YM6KvbtyJv+vpsxGi91bptotsVKVbcuBweFp5g2/CnN8dJkqVVyn9Pvtpe/CZGKgKZF0o0IakoLQUgwypXrpxx+fDhhGv5//fff3FulxyVK1eWFDmBW9R8GamldOnSGjlypA4dOmQsd7F8+fIYbfhyR0bVorqLHHPYSpKW776osPDUS5B1eP3ZD8e/98Q9ASwiJ8mN4ujoZFyOnpCN/sM6LqFhz0bcvEgiFzAXlatUlSQFBQXK1fVcvO2OHjliXK5UuUqax4WYLC2fJZjCEjtOhT6b74TjFIDkCHj4UB8PfF9u165Kkj4aMlSd3u6apG3z5ssvZ+e8kqQ7d24nOPfhrVvPRsY7OSU8egNxux4twZ0r2nltUvn6PJCfr2+KtwdgOozIAJBhVa1a1VgWYsGCBRo6dGicpZgePnxoTAqUKVNGefPmTdH+WrdureHDhysiIkLTpk3TsmWxJxd7UQULFlTJkiV14sQJeT8dqh8lc7QJRh8/Zi4ApL7+U7ep/9RtCbYZ3bWmvuwWORLjzZErtffM7UT77fbGs7JSi5NRVioxpQo6qFO9yPJX20/c0BUP31Tr+2Vyx+OWjh85KEnKl7+gHHM/+9HsnC+/cfnMqeOq9XqDePs5feJZAjdvtO2AjKpR4yb6fU7kCIC1q1eqQoWKsdqEh4frn3VrJEnZsmdX9Ro1Y7VB/EaO+Vojx3ydYJv5c2ZqwdxZkqSpM+epUtXqMdZHP06dPnlMtRM4Tp06/qzWOcep5Jvw9SRN+HpSgm1m/TJdv82aIUmaM28hfxN4KQQHBWnokAG6eN5VktSr3wfq2btfsvpo+Mab+nPJAj0KCNCRwwdVo1adONvt2vHsXLsiyfFku+NxS8f/e3peW6CgnHInPxm0Yc3fxmRTxcrxz30CwPwwIgNAhmVjY6N+/SJPMM+ePasJEybEahMREaFBgwYZkwKDBg1K8f5ee+01de7cWZL0559/asqUKQm2d3Nzi5XsWLNmjXyf3v0Rl5s3b+rChQuSYs+tkStXLmXKFDmp8dWrV5MbPmAS9nY2al498rN8xs1bp695J7LFM/lyZY13XQFHO634qpWsrSwV/CRUQ3/d/cKxZkQH9+5SWGj8c5P4PLiv8aOGKiQk8i7lNh3fjrG+SrWaxiTp+lXLde3KpTj7+e/gXu3fvUOS5OiUW8VKlIqzHZCRlK9QQVWeTt66ZtVKnTp5IlabhfPn6drTu3O7de8pa2vrdI0RUtXqNZU5c+SovnUr4z9OHT6wV/t2b5cUeZwqXpLjFIDEhYQ80YjPhuj0yeOSpLe79tCHA5Nf3vedbj1kY2MjSfppymQ9CgiI1WbThnU6fjSyUkDdeg2UxzllN9i9rA7sTfi89sH9+xo3Mtp5bYeY57WeHrd1+WLCN00d3LfbWMbQxiazmrVKvHQYAPPBiAwAGdqYMWO0atUqXbt2TePGjdOZM2fUu3dv5c2bV25ubpoxY4Z27dolSapdu7ZxToqUmjVrlo4ePapr167ps88+09q1a9WzZ0+VLVtWNjY2un//vk6dOqXNmzdrx44dat++vd59913j9tOmTVO3bt3UsmVLNW7cWKVLl1aOHDnk4+Ojo0ePavr06QoKCpIkffjhhzH2bWVlperVq2v//v2aN2+eKleurEqVKhkvqjg4OKR4/o+M4MTxY7p589ncCr4+PsblmzdvaN3aVTHat2nbIc5+nm938eIF4/KB/fvk4fFshEHBgoWNpUeQMp0blDTOvZGcSb4l6eeBjeWUw1Zr9l/RsSv35BfwWI45bNWoUgH1a1FeObLaKCwsXAOn79ClWz6Jd/gSmjFlkkJDQ1WvUROVKVdBznnzK5ONjfx8fXXqxBFtWPO3/Hwj/23KVaysNh3fibG9XbbseqdHX82f84sCAx/p4w96ql2nd1W1Rm3ZZcsunwf3dWDvTm1cu0rh4ZE1n/t99Emco9/AcSojGj5qtHp1f1fBwcH68P0+6tf/Q1WvUVPBwcHavGmjVq74S5JU2MVFPXv1NnG0rya7bNn1bs8++mN25HFq0Ps91L5zV1WrUVvZsmfXgwf3tX/PTm1Ys9J4nOo/kOMUzB/fGebhq5HDdPjgfklStRo11bpdxxiT2j/P2tpahQq7xHreOW8+vT9gkGZM+1FXL19Snx5vq0evvipe4jU9ehSgndu3avXfkd8pWe3s9PFnI9Lk9WRkM36cpGlhoarfsInKlK+gPHnzyybqvPb4Ef3z3Hlt204xz2s973jos4F9VaZ8RdV+vYGKlXhNOe0jfx/fuX1Le3Zu1Z4dW42jMT4YPDRFIzqQBqiijSQikQEgQ8uWLZu2b9+uFi1a6MKFC1q5cqVWrlwZq13dunW1bt26F66X7ODgoP3796tLly7au3ev9uzZoz179sTbPnv27LGeCwwM1IoVK7RixYo4t7GwsND48ePVrl27WOtGjRql1q1b6/79++raNWbN1rFjx2rcuHHJej0ZyZpVK4zlPZ536sRxnTpxPMZz8f3YG//VF/HuY8G8OTEet2rTjh97L6hb48iJWUPDwvXnrovJ2tZgkGqUclaNUs5xrr/vH6RPZu7S33tf7bkx7nvf05oVS7VmxdJ429Rr1ESfjRpnHNUVXbfe/eXv76fVy5coKDBQyxb+rmULf4/VzsrKSn0+HKImzVulavwvE45TGU/p0mX03Q9TNXrkMAUEBOjnabFHWxZ2cdGMmbOVNaudCSKEJPXo84Ee+vtp5V+Rx6mlC+Zq6YK5sdpZWVmp34CP1bRFaxNECSQP3xnmYdeOrcblo/8dVvcu7RJs75w3n9ZsjLsUa/f3+srfz0+L5v+u6+5umjjuy1ht7B1yafKU6XEmQyDd97qn1SuWanUi57WffxH3ea0kuZ45Jdczp+LdPnPmzBrwyXC1atfpheMFkL5IZADI8FxcXHTq1CnNmTNHK1as0NmzZ+Xv7y8HBwdVrlxZ3bp1U9euXVPtzjxnZ2ft2bNHGzZs0LJly3Tw4EF5enoqJCREOXPmVIkSJVS7dm21adNG9evXj7HtsmXL9M8//2jXrl1ydXWVp6envL29lTlzZhUuXFj169fXhx9+qAoVKsS575YtW2r79u366aefdOTIEXl5eRmH1gLmpli+HMYkxPYTN3TXJzBZ2/+w/Kgu3/JR3bL5lN8pm3JlyyzfR4/ldsdP/xy+pj/+Paf7/sFpEXqGMfyriTp94qhcz57SHY9b8vP1VeCjR7LNYiun3M4qW76S3nyrjcqUj137P4rBYNBHnwxXk+attHHdSp09dUL3PO8o+HGwbG2zKH+BgqpQuZpateukAoVc0u/FAemkYaPGWrF6nZYsWqi9e3bp7t27kXfcFiykps2a652u3WVra2vqMF9pBoNBAz8d8fQ4tUpnTh3X3Tsxj1MVq1RT6/adVZDjFAAT+mjIUNVr0FirVvypkyeO6b63lzJlslGhwi56vUEjdXmnm+yyZTN1mGZpxJiJOnXiqFzPxHNeWyHyvLZsPOe1JUuV0ahx38r17CldOn9O9+97y9/XR2FhYbLLll0uRYupcrWaeqtNB9k75ErnVwcgNRgiosZUAQDwnIDHfEWYE6cO000dAp66tOgDU4eAp+yzxn03HtKflSV1AczFg4Anpg4B0XCcMh9h4ZzbmouQsHBTh4CnHj2Of14KpK8C9jamDsEkcnRdZOoQUsxvaQ9Th/BKYUQGAAAAAAAAACDdGQzcDIOkYQY0AAAAAAAAAABgtkhkAAAAAAAAAAAAs0VpKQAAAAAAAABAuqO0FJKKERkAAAAAAAAAAMBskcgAAAAAAAAAAABmi0QGAAAAAAAAAAAwW8yRAQAAAAAAAABId8yRgaRiRAYAAAAAAAAAADBbJDIAAAAAAAAAAIDZIpEBAAAAAAAAAADMFnNkAAAAAAAAAADSHXNkIKkYkQEAAAAAAAAAAMwWiQwAAAAAAAAAAGC2KC0FAAAAAAAAAEh/VJZCEjEiAwAAAAAAAAAAmC0SGQAAAAAAAAAAwGyRyAAAAAAAAAAAAGaLOTIAAAAAAAAAAOnOYGCSDCQNIzIAAAAAAAAAAIDZIpEBAAAAAAAAAADMFqWlAAAAAAAAAADpjtJSSCpGZAAAAAAAAAAAALNFIgMAAAAAAAAAAJgtEhkAAAAAAAAAAMBsMUcGAAAAAAAAACDdMUcGkooRGQAAAAAAAAAAwGyRyAAAAAAAAAAAAGaLRAYAAAAAAAAAADBbzJEBAAAAAAAAAEh/TJGBJGJEBgAAAAAAAAAAMFskMgAAAAAAAAAAgNmitBQAAAAAAAAAIN0ZDNSWQtIwIgMAAAAAAAAAAJgtEhkAAAAAAAAAAMBskcgAAAAAAAAAAABmizkyAADxCguPMHUIiObGXx+ZOgQ8Vaj7PFOHgKfuLOtr6hBgxD1S5sLaivfCnHj6BZs6BDxlZ8MlEHNhZUlNfHPhmM3G1CHgFcccGUgqznABAAAAAAAAAIDZIpEBAAAAAAAAAADMFuMqAQAAAAAAAADpjtJSSCpGZAAAAAAAAAAAALNFIgMAAAAAAAAAAJgtEhkAAAAAAAAAAMBsMUcGAAAAAAAAACDdMUcGkooRGQAAAAAAAAAAwGyRyAAAAAAAAAAAAGaLRAYAAAAAAAAAADBbzJEBAAAAAAAAAEh/TJGBJGJEBgAAAAAAAAAAMFskMgAAAAAAAAAAgNmitBQAAAAAAAAAIN0ZDNSWQtIwIgMAAAAAAAAAAJgtEhkAAAAAAAAAAMBskcgAAAAAAAAAAABmizkyAAAAAAAAAADpjjkykFSMyAAAAAAAAAAAAGaLRAYAAAAAAAAAADBblJYCAAAAAAAAAKQ7SkshqRiRAQAAAAAAAAAAzBaJDAAAAAAAAAAAYLZIZAAAAAAAAAAAALPFHBkAAAAAAAAAgPTHFBlIIkZkAAAAAAAAAAAAs0UiAwAAAAAAAAAAmC0SGQAAAAAAAAAAwGwxRwYAAAAAAAAAIN0ZDEySgaRhRAYAAAAAAAAAADBbJDIAAAAAAAAAAIDZorQUAAAAAAAAACDdUVoKScWIDAAAAAAAAAAAYLZIZGRQ7u7uMhgMMhgMmj9/vqnDAV4pLi4uMhgM6tWrl6lDAQAAAAAAAF56r2xpqV27dqlRo0aSpLFjx2rcuHGJbtOrVy8tWLBAkuTm5iYXF5c0jBCvgmXLlqlr166SpK+++kr/+9//krytn5+fnJ2dFRwcrAoVKujUqVNpFSZgFh48uK9zZ0/L9ewZuZ47K9dzZ+Tn6ytJatm6ncZO+DbJfd2+fUt/LV2k/w4dkOcdD4WHR8jJyUk1atVRp7e7qljxEmn0Kl4OjwICdHD/Hp0/d1YXz5+T17278vXx0ePHwbLLll0uRYupdt16atW2o3LkzJmsvsPDwzWgT3edO/PsmLbv2LlUfgUZ38SeNfRZh0rGx29+uV57z95JdLtGFfLr3YbFVae0s5ztsyg0LEL3fAN19voD7Tx9W0t3Xdaj4NB4t29RrZB6NC6pGiVzyzGHrR4GhejaHT+tPuCm2ZtdFfg4/m1fBefPndX+fXt06sRxuV27Kl+fB7KyspKjU25VqFRZbdp3VKXKVRPsIzgoSAcP7NN/hw7o/LlzunXzhgKDApU1a1YVKuyiWrXrqkPnt5XL0SmdXlXG9OD+fZ09e1rnzp6R69kzOhftO6NVm3YaP3FSon0EBQXp4P69OnTwgM67ntXNG5Hvhd3T96J2ndfVscs7cuS9iFdqfF8cP/qfhnzQO0n7693/I/X9YGAqvoKX2z3PO/r3n9X678Be3bt7R4GBj5Qjp73yOOdTxSrVVf+NN+VSNO5zIs87t7Vh9XKdOHpYd27fUnBQkGyzZFHBwkVUrWYdtWzfWTntc6XzK8o4jH8brvH8bRRJ/G/j9Wplk73ffUc5p4rL+XNndWDfHp06Gff3d+t2iX9/xyc4KEhdO7eVx+1bkiTnvPm0ZuO21Az/lXPu7Bnt3bNbJ04c17WrV+Tz4IGsrKzllDu3KlWuovYdOqpK1WqmDhNAGnhlExmAOWjXrp2yZ88uf39/LVmyJFmJjL///lvBwcGSpJ49e6ZViCYTlTgsXLiw3N3d03x/DRs21O7du9WgQQPt2rUrzfeH5Gve+PVU6Wf138v1w3cTFRISEuP5mzdv6ObNG1q3ZqU+/myEurzTLVX29zJyPXdG474YFuc6X58HOnnsgU4eO6JlC//QVxMmqWadpL93q1csi5HEQGwViuTSkDYVkrVNzqyZNHtIQ7Wu6RJrXY6smVQif061r1NUhy/e02m3+7Ha2GW21h9DG6tVjcIxnrextpRj9syq8Voe9W1WWp2++VcXb/kmK7aXxQd9eujk8WOxng8JCdHNG9d188Z1bVi3Rm+1aqsvxo6XtXWmWG0vX7qo/r26KTAwMNY6fz8/nT19SmdPn9KyJQs16qvxatqsRZq8lpdB00Z1X2j7y5cuqk/Pd+N8L/z8/HTm9CmdOX1KSxYv0Jdj/qc3m7/1Qvt7WaXl9wVezNoVS/XHbz8rOCgoxvPe9+7K+95dnTt9QoGPHunDT4bH2nbb5vWaPnmiHj8OjvF8wEN/nT97SufPntKaFUs1avx3qlKjdpq+jozK9dwZjRudwN+GzwOdPH5EyxY9/duo/eJ/GwULubxwHy+jD/v00MkTSfv+HjUm7u/vhMyeNd2YxMCL692zm44fOxrr+ZCQEN247q4b1921bs0qtW7TTmPHT5B1puS9XzAN5shIuhEjRmjy5MnGxzt37lTDhg0T3GbTpk2aPXu2jhw5Ii8vLzk5Oal69erq37+/WrRI2u+J0NBQzZ07V0uWLNGFCxcUEBCgfPnyqUmTJhoyZIjKlk1+cj0lSGQAJmRra6tOnTpp3rx5unbtmvbv36+6dZP2w3vRokWSJEtLS3XrxgXX9JQeiRUkzDlvXhV2KarDB/cna7stmzfo24ljJUl2dtnUrWcvVateS9aZMunSBVctmv+7bt68oR+/+1r29g5cJExA7jzOqlKthl4rXVa58zgrl5OTIsLDde/uXe3avkV7dm6Tr6+PRg4dpNkL/1SJkqUS7dPr3l399stPMhgMypEjp3x9fdLhlWQsBoP0y4B6sray0F3fQOXJmSXRbbJnsdY/41uqavHIu8bXHnTT6oPXdO2Ov8LCI1TA0U71yuVVu9pF4u1j8bA31KxqIUnSsStemrHujC7e9pWdrbVaVC2kAS3Lqni+HFo7poXqfrZK9x8+Tp0XnIF4e92TJDk55Vbjps1UqUpVOTvnVVh4uM6eOqkli+bL695dbfxnrUJDQzVh0vex+nj0KMB44bxCpSp6vX4DlS5TVjly5pTPAx/t2rFVa1f9rUcBARr7xXBlzZpVdV6vn66vMyNyzptPLkWK6NCBpH9nBAQ8ey8qVq6ievUbqkzZcsqRI6d8fB5o5/atWr1yhR4FBOjLUcOUNaud6tbjvYhLan5fjBo7UaXLlIt3vb2DQ1q8hJfO0vmztXDOL5Kk/AULq0WbjipZuqyyZrWTv7+frl66oAN7dshgEfvC0rnTJzTl6zEKDw+XhYWFmrRordr1GsnB0Ulenp7aummdDu/frYf+fho/8hP9umil8uYvkN4vMUOI9bfh+PRv414cfxsLYv9tLPxzTaL72LRhrZYt+kOS1KJV27R4GRmet3fM7++KlavKOW9ehYeF68zpk1r63Pf3/76N/f0dn4sXXPXX0kWysbGRpZWVAh89SquX8crwuvf0/cqdW2++2VxVqlaLfL/Cw3Xq5EktXDBP9+7e1fp1axQaGqpJ3/9o4oiB1HPy5ElNmTIlye3Dw8PVv39//f777zGev337tm7fvq01a9aoX79++u2332RhEf/sE97e/2fvrsOiStswgN9DpzQiBogdmGAHdmDHrt3dratrfcZaq7vq6q6JserahS12YwM2UiIg3TEM3x8jw4wThJRw/66L6xrmvOfMO3HmzDnP+z5PKDp37oxHjx7J3O/t7Y1t27Zhz5492Lx5M0aNGpW9J5QDDGQQFbAhQ4Zg165dAMTBiawEMnx9fXHz5k0AQLt27WBlZZWnfSQqDEaNmYBqNWuieg17mJmZI/DTJ/Rwbpvl9RMTErB+jTj9lJ6eHra77EeFipUly6vXqIm2HTpjzPCBeP/uLX5fsxJNm7eAnp5+rj+XH109hwY4fu6q0uVt2nfEzWtXMX/WFKSkpGD3tq1Yue7PTLe7fvUKxMfFwbl7L3wK8Mezx48yXae4mdilJhwqW+K1fwROP/DBnD51M11n/eimqF/RAonJQgxaexWuj3xllj/5EIrTD3wwe+c9qCu4aNWzcXlJEOPKswD0Wn4BKUKRZPktj8+4/DQApxd3go2lIX7t54Dp27MXZCwKbGztMH7SNLRq2x7q6uoyy+xr1UanLt0wethA+Pn64NIFV/Tq+zPqfpP2QE2ghrbtO2Lk2Amwq1BR7jEaNWmKxk2bY+6MKUhNTcXvq1egcdPmHMWmwOixE1C9pj1q1Ew/ZgSga6esHzPU1ARo16ETxoybqPC9aNykGZo0a4FZ0yYhNTUVa1Ytx8lmF/lefCO3jxelrEvDjukfv8tT9weSIEbbjl0x7ZfF0NDQlGlT16Eh+gwYKjd7FQD+27cTIpH4GDB++jx07fWzZFmVajXRrFVbbNu0DscP7UNSUiKOH9qLiTPn5+Ez+jHVc2iA464q9o12HXHzutS+sX0rVq6V3Teysi88eyIeuS4QCNC+c5fv63QRZWNrh3GTpqFVG/njd81atdHJuRvGDM84fvfsI3/8ViQ1NRW//W8xUlNTMXLMBJw+eYyBjFxga2eHydOmo227DnLvV63addClWzcMHdQfvj4+OH/uLPr+3A/1HRwLqLdEuSc9KCEUCmFpaYmQr0E9VRYsWCAJYtStWxdz5sxBhQoV8OHDB6xZswZPnz7Fjh07YGFhgZUrVyrcRmpqKnr27CkJYvTq1QujR4+GqakpHjx4gOXLlyMkJARjx45F6dKlszzDI6dY7JuogLVo0UJSb+XIkSNITk7OdJ1///0XaWlpAIpmWikiRcZMmIzmLVrBzMw8R+vfuX0T4eHilDk/DxgsE8RIZ2BggGkz5wIAwsNCcfb0yRz3tyj79qRBkRat2qCcjXiE/wsF0/W/dcPtMm5dvwpjYxNMmDLzu/tYFJU118eiAeIT58l/30ayVDBBmSbVSmJgK/Fnfem/7nJBjG+litLk7hvUOmNfmfbPbZkgRrprLz7hyK0PAIAR7avCxEA7074VNes3bUXbDp2U7h/GJiaYOjMjRYvblYtybWrVqYsVa9YrvHCermWrNnBq0w4AEODvjzevX31nz4umcROnoEXLnB8zateph1VrN6h8L5xatUFryXvhh9evvHL0WEVZXhwvKOdEIhE2r1sOALCrWAXTf1kiF8SQpqkpv8zra/rHEkbGMkEMaQOHj5XcfuX54nu6XGRlad9w+r59w8/nI155vgQA1K3vCCsr62xvozj4feNWtG2v+vg9ZYbq47ci/x3Yh9evPGFjWx6Dh4/Mlb4SsHnLP+jQsbPS98vExBQzZ8+T/H/5UtbeLypYAoHgh/3LLxs3bsSjR49QtWpVjByZ+XfK27dvsW7dOgCAg4MD7ty5g379+sHR0RH9+vXD7du34eAgPrdcu3Yt3r9/r3A7e/bswe3btwEAEyZMwLFjx9CxY0c0aNAAkydPxp07d1CiRAmIRCJMmTIFQmHe1kxkICMPnDx5En379kW5cuWgo6MDY2NjODg4YOnSpYiIUJ4mY9iwYRAIBJKL2p8/f8bcuXNRo0YNGBoaQiAQqMzdf+TIEbRt2xaWlpbQ1dVF1apV8csvvyDya2FDZTw8PLB8+XJ06NABZcqUgba2NgwMDFCpUiUMHToU9+/fz/Jzv3PnDkaNGoUqVaqgRIkS0NLSQpkyZdClSxf89ddfKvvy/v17TJ8+Hfb29jAyMoKuri7s7OwwbNgwuLvL50BU5MyZM+jTp4/keZiZmaFx48ZYtWoVYmNjla63ZMmSLH0JXb9+XdJO2Xvx+PFjjBw5EpUrV4a+vj50dHRQtmxZ1K9fHxMnTsTp06clQQhA/IU9aNAgAEB4eDhcXV0zfZ7paaVKlCiBHj16yC1/8uQJxo0bhypVqsDAwAD6+vqoUqUKxo8fj7dv32a6/fj4eCxbtgy1atWCvr4+zMzM0KxZM+zatQtpaWlZeh0AceR2z5496NKlC6ytrSXvSbNmzbB+/XokfJOTF8h4L/bs2QNAPPsks4NFcnIyzpw5g0mTJsHR0REmJibQ1NSEmZkZGjZsiCVLliA0NFRhH9P3uxs3bgAAbty4IfdY6ftkOltbWwgEAgwbNkzl65jTz6OLi4vksX18fCASibBt2zY0adIEJiYm0NfXR61atbBixQqFubtJsVdeHpLbTZoqT/9Rz6EBtLXFF2HdLvOH7/fQ0xenPUpKVp1mKC42FhvWiEeATJg6M9sFwouLDWObwVBXC/vc3uC2Z+aFvQFgXGdxCpbIuCRsPZezAp/1vqakeh8YhQ+fo5W2u/TUHwCgpakO529qaZBYfccGktsB/v45345DxnY++ft9V5/o+zg0aCi5HRCQ8/e0uMvq8YK+z5OH9yTfGX0HDYe6RvYTNAiF4lkaJUuVVtpG38AQRsYm4vYKZnVQ1n3PvnHe9bTkNtNKfR/p4/enLHzXfw78hO1bNwEA5ixYnO26GvR9HKWPzfydREWAn58fFi5cCAD4+++/oZWF2i9//PGHJKiwadMm6OrqyizX09PDpk3i7ymhUIgNGzYo3E56MMTU1BRr18qn1qtYsSJ++eUXAOLruidOnMjis8oZppbKRREREejTpw/c3Nxk7k9KSsLjx4/x+PFjbNmyBadOnUKjRo1Ubuv+/fvo2rWr0ouv3xo5cqQkPVG6N2/eYNWqVdi7dy+uXr2KqlXl881ev34drVq1krs/OTkZ79+/x/v377F3717MmzcPv/32m9LHT0hIwMiRI3Hw4EG5Zem511xdXfHlyxcsWbJErs26deswf/58uenLHz9+xMePH7F37178+uuvSothJyYmYsCAAXI7THh4OO7fv4/79+9j06ZNcHV1RZ06dZQ+j++1YcMGzJo1SzLdOl1AQAACAgLw5MkTbNmyBTExMTAwMJAsHzJkCJYvF4+O2rdvH3r27Kn0Mdzd3fH69WsAQJ8+fWS+jEQiEWbNmoU//vhDJlgCiKOxb9++xY4dO/DXX39hzJgxCrcfEBCA1q1b4927d5L74uPjcefOHdy5cwcnTpzAlClTMn0t/Pz80K1bNzx/Llu0Nzw8XLKtrVu3wtXVFZUry4+Mz44xY8ZIAh/fPtbDhw/x8OFDbN68GadOncpyDZLvkZufx/j4eLRv3x5Xr8pOPX/58iVevnyJ06dPw83NDfr6TH+UmSipQKqpmZnSdhoaGihRwghfvoTg5YtnEAqF0MjBiX5x5+fzEe/evAEA2Ngqr70AAFs3rUfolxDUqeeAzt2Uf/8VZ72b2sHZ0QZh0Yn4ZXfWBhhoaqhJinO7PfuEpJRUAOKUOdamelBTEyA4IkFyvzJmhjoAgJBI+eCzNOnlzaqXwn63zAPnxY30rMusjMZVJiUlYztq37Ed+n4y76mK3MKkXHaOF/R9bl67BEA8kKphk4xBHTHRUYiOikQJI2MYljBSuY0y5Wzx/s0rBH/+pLRNXFwsor7WuSrDAtM59j37RlpaGi6dPwNAXJexZet2ud6/4kT6u15NLfPj7trfliEhIQGdnLvJDD6g/JEi837x2Ew/vokTJyI2NhZDhw5Fy5Ytce3aNZXt09LScOrUKQBA1apVlV6DbtSoEapUqYI3b97g1KlT2Lx5s8zA4bdv3+LVK/Hs759++gl6eorrMw4bNkwSzDhx4gT69u2b7eeYVbwyk0uSkpLQtm1bPHnyBOrq6hgwYAA6d+6M8uXLIyUlBTdv3sT69esREhKCzp074+nTp7CxUTxaMTY2Fr1790ZiYiIWLFiAdu3aQU9PDy9fvkSpUqXk2m/ZsgWPHj1CgwYNMH36dFSqVAkhISFwcXHB4cOHERgYiA4dOsDDwwOGhoYy6wqFQujr68PZ2RmtW7dG1apVUaJECYSEhMDT0xMbN26Er68vVq1ahcqVK2P48OFyjy8SidC9e3dcvnwZAFCpUiVMmDABDg4O0NPTw+fPn3H37l0cPnxY4fNdu3Yt5swRT9WsVasWxo8fj0qVKsHY2Bhv3rzB5s2bce/ePSxbtgzm5uYKL6IPHTpUctG4du3amDlzJqpVq4bw8HAcOnQILi4uCAwMRJs2bfDixQuULq18BFFOvXjxQhLEKF++PCZNmoQ6derA1NQUMTExePPmDa5duyb5MpFWqVIlNGrUCPfv34erqysiIiJgYmKi8HHSZ2MA8mmlJk+ejC1btgAQp6waNmwY7OzsoKenh+fPn+OPP/6Ap6cnxo4dCysrK3Tr1k1m/ZSUFDg7O0uCGM7Ozhg9ejTKlCmDgIAAbNu2DWfPnsWXL19UvhZhYWFo1qwZ/P39oa2tjdGjR6Nly5awtbVFbGwsLl26hD///BPv379Hp06d8OTJExgZiU+aJkyYgD59+uDXX3/FqVOnYG1tjYsXVY+KFwqFsLOzQ8+ePdGgQQOUK1cOGhoa8PX1xZUrV7Br1y6EhYWhZ8+e8PDwgKWlpWTdFStWYNasWRg+fDjc3d3h4OCA3bt3y2w/K9Fuabn5eRw9ejTu37+PoUOH4qeffoKVlRX8/PywZs0a3Lt3Dw8fPsTy5ctVBhpJTFfqoKtqRkxaWhri4sTLU1JSEODvB9vydnnev6IgMSEBX76E4M7NaziwdxdSU8UjQH7qrzwFnseLZzh17DA0NDQw85eF+dXVH4qRvhbWjmwCAPh174MsF9KuZWsGXW3xTz0P33AY6mpi0QAHDGxVWZL6KSklFbc9P2P10ae45aF4lkdsYgpMDLRRQl/1d6GRXsbyamWNs9TH4ubp44wZpt/zvfJEqn4Mv58K1hP3jPeivF2FAuzJjyUnxwsA2L7lT4QEByM8LBTaOrooZW2NuvUd0aNPP5Szsc2Hnv/YXnuI0zyVLGUNPX19XLt0Dv/t2wkf74x0EunFv7v16a/wN7Bzj774c/X/EB0VCdcTh+Hc8ye5Ngd2b5Pc7twj7y5mFEWJiQn4EvJ139iX9X3jW08fP0RwkPi43qJVO9Z8+04yx2871cfdyxfO4e7tmyhRooRMSirKP+48NlMRcvjwYZw9exampqaS2RGZ+fjxIwIDAwEALVu2VNm2ZcuWePPmDT59+gQfHx+UL58ROE9PKZXZdqysrFC5cmW8ffsWd+7kba1EBjIAhISEwMPDI9N2qtIi/e9//8OTJ09gbGyMK1euoH79+jLLmzVrhoEDB6Jx48b4/Pkz5s+fj3///VfhtsLCwmBgYIDbt2+jdu3akvsdHRUXKHr06BE6d+6MU6dOyYwa7tSpE2rWrIlFixbBz88Py5Ytw5o1a2TWrVOnDgICAmCsII1Hhw4dMGnSJHTp0gWXL1/G0qVLMWTIELkRhJs3b5YEMXr27ImDBw9K0rKkc3Z2xrJly/D5s+xFEi8vLyxYsAAAsHjxYixevFgm+le/fn3069cPQ4cOxf79+7FgwQIMHjxY5iK/q6urJEjSpk0bnDt3TuZHd/v27dG4cWOMGTMG4eHhmDFjBv777z+Fr+X3OHr0KEQiEfT19XHv3j2ULFlSZnnz5s0xatQoREVFKYxiDhkyBPfv30dycjIOHz6MsWPHyrURCoU4dOgQAHF6oxYtMkZSXb58WRLE2LFjh1zOPEdHRwwaNAjOzs5wc3PDlClT0LlzZ5nPzJYtW/DihfgEZ9q0aTJTy+rXr4/u3btj8uTJ2Lx5s8rXYsqUKfD394eNjQ2uXbsm80UIAE5OTujbty+aN28Ob29vrFmzBitWrAAAWFpawtLSUvKZ1NTURM2aNVU+3tKlS2FnZyeXcsrBwQG9e/fGhAkT0KRJE3z58gWbNm3CsmXLJG1Kly6N0qVLS2Y06OvrZ/p4quT25/Hu3bvYt2+fJP0YANSrVw+dOnWCg4MDPDw8sH37dixbtoyzBjJRvnzGj9gnjx+hWvUaCtu9ee0lk7Ir6PNnXihU4dzpE1i59FelywcNG4V2nZwVLhOmpGD18sVIS0tD/8HDUd5OeS764mzF0IYoZaqHu15BcLnyJsvrVZUKJqipCXBnXU9UKm0s00ZbUx1t6pRBq1qlsWjfQ/x+QnYWHQC8CYhAo6pWqFrGGOYldBAanajw8ZrVyBhsUdbCQGGb4kwkEmHvru2S/9u075ij7bx98xp3b90EAFSsVJkn6AXo7ZvXuH1LnJqS70Xmvud4ke7l82eS2ykpKXj3Jhrv3rzG0UP/YuiocRgxZgILrishEokQ4OcDQFzfYusfq3HqyAG5dp/8fbHjr/W4e9MN/1u7CQaGJWSWt3fuAc/nT3Hlwhn8tf43vHvzCo2atYSpmQVCgj/D7eJZ3L0pHinab+ho1HNUnYmAgHNnsrBvdFS9b3zrglRaqY7O3VS0pMyIRCLs3Z1x/G7bTvnxOzo6ChvWiQeYTZgyAyampnneP5IlEomwa0dGMLVDx7wtPEy55Ac+dAcEBGSpXZkyZbK97cjISEydOhUAsHr1apibZ632m5dXRt02Rdl5pEkvf/Xqlcz1u+xu5+3bt/D390dcXFyeZQ3hHCsAW7duhb29faZ/ikbSA+KRvX/99RcAYNmyZXJBjHQ2NjaSnGZHjhxBXFyc0j7NmTNHJoihira2NrZv367wIuaCBQskF2V37twpV0ja3NxcYRAjnZaWliQHmq+vL549eyazXCQSSZaXKVMGe/fulQtipFNTU5Mbef77778jJSUFDg4OckEM6fU2bdoEbW1txMbG4ujRozLL0197TU1N7N69W+HIodGjR6Nt27YAgOPHj8sFVHJDUFAQAKBy5cpyQQxpRkZGCqc39uvXT9J36VkX0i5evIiQkBAAwKBBg2Rer1WrVgEAevfurbTwj46OjiQI4evrKzcd7e+//wYgfi/Tt/etNWvWwNpaeaE4Hx8fyYX5zZs3ywUx0tWtWxcTJ04EIK4J8T0qVKig8qTV3t4eo0aNAiCuYZOXcvvz2KtXL5kgRjptbW1MmjQJgDj4KX2AIcWaNGsuyQV9cJ8LIhXULBKJRNi6+U+Z++LjlX9Xk3KVqlTF9r2HMG7ydKX75/49O/Hxw3uUKl0Gw0aNy+ce/hiaVrfC8LZVkSIUYfLft7K1rqmBjuT2zJ61Uam0MS4+9kOzWSdg1GcHyg7Zi8lbbyEyLglqagIsH9pQkopKmutDcYFwDXU1LB6oeFBFhVIlMLhNRppAA13lxWOLq4P798DTQ1x01alNO6XBVFWSk5OxcukipKaK04GNmzQ1V/tIWZecnIxlS36VvBcTJ08r2A79wLJyvDAzt0Cvn/pjycq12LbnIHbuP4KV6zaiS/fe0NDQgEgkwu5tW7Dtrz8Vrk/ielTp6W99PrzHqSMHYGpmgTmLVuLI+Vs45fYAa//ahao1agEAvF4+w/qVi+W2o66ujlkLl2PB8nUoX7EyLpw5jiVzp2LKqAFYvmAm7t68htr1HLHyj38wbMykfH2ORU2lylWxfc8hjJukfN9QJDExAdfdxAMNLUtaob5jw0zWIFUO7t8Dr/Tjd+t2qKri+L1pwzqEh4XBvlYddO/F2UgFYd9eF3i8FA/ObNO2ParXyPkgRaKsKFu2bJb+cmLOnDkICgpC06ZNs1TgO510cCWzAIp03/y/qeGXk+2kpaVlObiTEwxk5IIbN24gKioKgLhmgSrpI+hTUlLw+PFjpe0GDhyY5cdv37690gvLampqGDp0KABxfv4nT56o3FZSUhL8/Pzg5eUFDw8PeHh4yNRa+LbewbNnzyQf0NGjR8vUfciKM2fEeTt79+6t8seZsbEx7O3tAQD37t2T3C8UCiVFmtu3b6/yy2H06NGSdVQVqM6p9LRfXl5eePjwYbbXNzExQdeuXQGIi6Z//PhRro10gGPw4MGS29HR0ZLnlNlnsFq1apIorvRr+enTJ0ntjb59+yoNSOnq6qrMd+fq6orU1FTo6emhUyfVox/S94fAwED4+eVeEa6IiAh8+PABnp6eks9xesDOy8tLrhZLbsmLz6Oq7wLpoKm3t3e2+ppetyWzv6KkpFUp9OrzMwAgJCQYo4YNwI1rVxEbG4ukpCS8fPEM0yaNxb07t6CpmXERNilJ8ehzEmveqg32/ncSe/87ie17D2HJyrVo0aot3r15jSXzZ+POzesK1/P388XeXeLRUjPmLIC2jo7CdsWZpoYaNo9vDjU1ATadeQkvP/ngmyp6OhkDHHS1NXDlWQB6rbiIx++/IFkoQmh0InZcfIXeyy8gNVV8cet/g+XzOG+74IVPoeJ0a6M6VMPOaa1Q08YUmhpqMDXURn+nSri8ohv0tTWR/LXehq4WZ4hJe+L+CH9tFM9yNDE1w9wFi3K0nXW/LccrL/EsYueuPdC8pXydM8ofq1cug5en+L3o0q0HWji1LuAeFX45PV5Uq1ETx85exoy5v6Jth86oXrMWqlSrjhat2mDeov9hy859MDAQp8/d77ID796+zsdn9eNITMyoY5ScnARtHR2s3rQDrTs4w7BECWhr68C+Tn2s3rQddhWrAADu3nTDa88Xctvy8/HGlfNn4PPhvdwyAHjl8QIXz55A6JfgvHkyRUxzpzbYe+gk9h46ie17DmHJiq/7xtvXWLJgNu7cup6t7d28fhXxXwdNtu/UhTUCvsMT90fYsinj+D1HxfH76WN3nD11HOoaGpizQPEgTcpb7o8eYuOG3wGIayIuWLSkYDtE9B1u3bqFHTt2QENDA3///Xe2vlNiYmIktzO7Tis9c+LbFNy5tZ3cxLNMiFMaKSpA/a1hw4YpLCjs7p6RL1FRDQtl0kfwf8vAwAB2meRdlKYs5VS6Bg0yLkq8fPlSrshLXFwcNm7ciEOHDsHT01MyskyRb4uPP336VHK7efPmWe4zIJ4RkF5r4ZdffpEUhsmM9Ovm7e0tSQHTsKHqkSbSy7OSSiy7+vfvj99++w1JSUlo2rQpOnbsCGdnZzRr1gw1atTI0pfOkCFDcOzYMQDA/v37JTN4AHGw4vRp8RThhg0byhTIfvr0qWSEVf/+/dG/f/8s9Vn6tZR+TZTNKkrn4OCgdFn6/hAfH5+tVEdBQUEoV65cltt/6+XLl9iwYQPOnz+vdN8CxCPuIyIiZOpk5Ja8+Dyqmr5nKjVVWfoAkxVZHREQGa+6CPCPZuqMOQgM8Med2zfh5+uD2dPlRwpWq14T1WvUxLEj4jRuzCmsmqFhCRhKpZ2oVsMebTt0xgXX01ixeD5+mTkZ8xb+T66I99oVS5CclASnNu3QuFmLbzdLAOb0qYuqZU3gFxKDFYeUD35QJilZdv/9dc8DiERpcu3uvgrGqfs+6NXUDtXKmqCmjSk8fMMly6PjU9D3t0s4sbAjShrrYYBTJQxwqiS3nYV7H2Byt1qwNNZFTELeBIx/RN7v32HujMlIFQqhra2N39ZugKmpWba347JzG06dEM9KrV7DHrPnK09DQnlr145/cPL4EQBAjZr2mDc/Z4Gp4ianxwtdXcWFJdNVr1kL0+cuwLKF85CWlobj/x3A3IX/y5Pn8CP7dpZwx669UFZBXRFtbR0MHTsJi2dPBgDcuHpRMksDADyePcHiuVMQFxsDSytrDB0zEfUcG8OwRAlEhIfj/u3r2Lv9L9y4cgEezx5jxYa/YcvUkSqp3DeWSO0bXXuq2EoG6bRSnbp0z/X+FhfeH95h3syM4/fKNcqP38nJyVj1NV3qz/0HoVLlKvncW3r//h2mT5kE4df3a936P2Fmlv3fW0TZ9e0shtyQnJyMMWPGIC0tDdOnT892+vPExIzBmJnVfJUexJyQkCCzLLe2k5sYms8F6al+sks6B7s0VameFMnsgqx0mqPw8HCZZT4+PrC3t8f8+fPx4sULlUEMQP7DKB3YyE4QB8id1036+WT2OlhZWSlcL7dUrVoVBw8ehImJCYRCIc6ePYvx48fD3t4elpaWGDx4MG7dUp0WpFOnTrCwsAAgDmRIO3r0qOT1/7bId268lhFSaXbS+6CMquW5vT9kxc6dO1GvXj3s3r1bZRAjXV59qebF51FRPZV00qOrMtt3SUxLSwu/b9yK+Yv+h8pVqskEGE1NzTB81Fhs270faci42GtYooSiTVEmOjp3Q6u2HSASibBhzQpER0VKlp09dRxP3B9CT18fU2dlLYhd3FQubYTZvesAAGZsv4v4JGG2tyEdTAiJTMDzj2FK215+mvEDvH4l+e/4px9C0Wj6MWx19UBQhOz3tfvbEPRcdh7rjj+H4deUUpGxWStIXtQFfgrAlPGjER0dDXV1dSxbtQ516ysfDKDM8aP/YeumPwCIi3tv2Px3phd3KW8cO3JIMrvGtrwd/vxrG3RVHKspc6qOF1nVpn0n6OuLRwo+feKeSeviSfebgRn1GjRW2rZu/YZQVxcPSHr7ylNyf3JyMlYtmYu42BiYmJnjj2370KZDF5iYmkFDQxMWliXRtdfPWPvXLmhpaSMs9At+X75Q2cNQJnKyb4SGfsHjh/cBiAfn2NiyzltOBH4KwFTp4/dvqo/fLjv+ga/PR5S0ssLo8Uyplt8CAvwxbvQIREdHQV1dHavXrUd9B9UDfqlwEQgEP+xfmTJlsvSXHStXrsTr169Rrlw5LF4sn+YxMzpS2Q6+LTHwraSkjPM2XV3dPNlObuKMjFwgfQHxyZMnMilJVFH2Qf62mHZmvmfK4uDBg/Hx40cIBAIMHz4c/fr1Q7Vq1WBhYQEtLS0IBAKIRCJJn6TTTH0v6ddt0aJFKtMVSVNWMKYwTN3s3bs32rZti//++w8XL17ErVu38OXLF4SGhmL//v3Yv38/hg4dil27dimc4qupqYl+/fph06ZNePv2LR48eCAZuZ+eVkpLSwv9+vWTWU/6tfznn3/QpEmTLPVXumh6bknvi7m5uVwNDlWU1dLIzOvXrzFu3DgIhUJYWlpi9uzZaN26NWxtbWFoaCjZH3ft2iXJKZibn2NlCsPnUZW8GDXwo1BTU0OPXn3Ro1dfxMXFITwsFDo6ujAzN5fsl/5+vpL2dhxFmGPNWraC2+ULSEhIwP27t9G+UxcAwL97dgIA6tRzwPOnimcaRIZnXHS/cvEcAEBHVxfNWhSPVDqTu9WCtqY6vIOioaetgb7N5IsI1yiX8R3uZG8NK2PxxVTXR76ITxIiICxjSu+nMNW1XgJCM5ZblFCc5isoIgEztt/FjO13UdJYF4Z6WgiJjEd0vDhgUtpMH7ra4p+WXv7ZS4NVFH0JCcGksSPx5UsIBAIBfl2yHC1btcn2di6ed8XalcsAAKVKWWPj3ztgnAfHb8rchXNnsWqFeKR/KWtrbPlnV578liqOlB0vskpDQwNlbWzw2ssToTkcVFPUaWlpwcjYBFGR4u9nC0sr5W21tVHC2BgRYaGS9gDw+MEdhH4Rv77de/eHqZnioqO2dhXRuoMzLpw5jndvvOD97g3sKnGEek7I7Bv3bqN9R9X7xqXzZyXnYyzynTNfQkIweVzG8XvB4uVokcnxe5/LDgCAY8PGuH3jusI2iV8H0yUmJODyBfFvWxNTUzg0aKSwPWVNSEgwxo4aji8h4vdr6bKVaNW6bUF3iyjHXr9+jd9++w0AsGnTphwVzTY0NJTczizNk3T95m/TR327HR0V6aBVbSc3MZCRC6Snq1lYWOSoEv33CA5WnXtUerl0KprXr1/j9u3bAID58+dj+fLlCtdXNVo8vdYCAHz+/DnTKvbSpF83TU3NbE+VAmSfT2avg/RIfen1ANmR7SKRSGkeUVUF2tMZGRlhzJgxGDNmDADg1atXOHXqFDZt2oTAwEDs2bMHdevWxdSpigt0DhkyBJs2bQIgDl40bNgQfn5+ktoLzs7Ocv2Xfi319PRy9FpKn4inp/xSRtXy9L7ExMSgWrVq2Q7MZZeLiwuEQiHU1dVx48YNpZ/BvJiF863c+jzmh6x+T0UliPK4JwVLX19f7odBamoq3r4R59cuXaYsLxh+BxMTqX1Cqqh9ytfRHHdv3cDdWzcy3c6S+bMBAFalrItNIENbU3wcsrMqgb2zMr/4Pf/njJSAVcYcgF9ILF5J1dRQV1MdXFVXz1guTM082BscmYDgSNnZbXUrZPwmcH9XvC8kRkZEYPK4kfgUIA4az5y7AJ27Zj+9x83rbli68BeIRCKYW1hg87ZdKFlS+cVHyjs3rrlh0a/zJO/F1u0uKGnF9yK3KDteZEdhH0RSGNiUr4AXT8UzVkQi1bN5RV8vhkv/lvfzyajJVrFKNZXrV6pSDRfE5RDh7/uRgYwcMjHO3r5x8Zw4rZSmpibadnDOs34VVZEREZgyPvvH7/QajGdPncDZUydUP0ZkBBb+MgsAULe+IwMZ3yEiIhxjR41AwNdBevPmL0TX7j0KtlNE32nDhg1ITk6GnZ0d4uPjcejQIbk20unJ3dzcJNeXunbtCn19fZnrPZnVPpUe5PptCvJvtyN9DVjZdtJnqeQVppbKBXXr1pXcvnPnTr4//qNHj7K8XPoCt6dnxjThn3/+Wen60jVAvlWvXj3J7Zs3b6rsx7fs7OxgZGQEIOevm52dnST1zoMHD1S2lS7A/e2Ffukoo3SKpW+9ffs2232sVq0a5s2bh/v370sumB4+fFhpewcHB1SvXh0A8N9//yElJQX//vuvZBbBt2mlAKBOnTqSk7ecvpY1atSQ3FZViB5Q/ZlI3x+SkpJUtstMVk9G0z/HtWvXVhlIy6wvuXHym1ufRypYjx89QFRkJACgXXvVBetJtS8hGQE9pl7Jf35fYuEXIq6fY2NpqLKtnVVGCrXA8MyD9or0apKRvuLobW8VLYu22JgYTJkwGh+9PwAAJk6dgb79BmR7O48e3MOCOTOQKhTCyNgYm7buQJmyOa8lRTn38P49zJs9TfJebPlnF8ryvchV33u8EAqF8PcVz6Y0zyRFanFmXycj6B306ZPSdnFxsZI0RmYWGelS09NNAUBqquqUh0JhxnLp9Sh7vkgVTM8speC7N6/w4Z34fLVR0xYwymbK6uIuNiYGUydmHL8nTJmBPj9n//hN+SMmJgbjx4yC94f3AICp02ei34CBBdwryqmCTg/1PX+5LT1Fk7e3t6QG7rd/6bV1AWDZsmWS+9MHHadfUwTEg9hVkV5erZrsIIWcbKds2bI5mkWSVQxk5IK2bdtKLl5u3LgxX9LWSLt06RI+KxmdIRKJJAXKTUxMZAIP0j8uVc00+Pvvv5Uuq127tiRit2PHjmxVpldXV0fnzp0lz+HVq1dZXjedhoYGWrZsCQC4fPmyykjjjh07JOs4OTnJLJNOa6TqgreiSGhWlS1bVlKg+9ui6d8aPHiwpN2FCxckaaXMzMzg7Cw/ssbCwkJSxP3AgQOZzqhQpEyZMpL+HTlyRCa/nbTExEQcOXJE6Xa6du0q+TL/448/st2PdOlT1pT1I13651jVZ/jz58+SQunf+3iq5NbnkQpOWloatv/9FwBAQ0MT3XtnLeUdKXbtyiXJbbuKGcWhj569jNuPPVX+1amfkdc2/b6jZy/na/8L0piNN6DbY5vKv+VSBcDb/3pGcr9fSMax+OS9jwAAI30ttKpVWunjdW+UcRy8+yrzWkPfqlrGGH2+pr+6+iwA7wOjsr2NoiAxIQHTJ4/Hm1deAIDho8ZiyPBR2d7Oi2dPMXvaJCQnJ8PAwBB/btkusw9R/nn+7AlmTJ0ofi8MDfHX3ztQge9FrlN2vMgqt0sXEBsrDtzWqce86Mo0dcpIt3Ln5lWl7e7ecJOc09asnXH+aFUq4zji8fyJysd6+SzjGFXSWvnxh1TLzr7BIt85l5iQgBlTMo7fw7J5/L7/1CvTP6tS1gDEM4zT79u6Y0+ePJ+iLiEhAZPGj8ErL/GgxtFjxmHEqDEF3CuiwqN8+fKwthZ/56Rnd1EmfVB66dKlYWtrK7OsWbNmktuqthMUFCQZ+N20adOcdDnLGMjIBcbGxpg0SVzQ6e7du5g+fTpEIuXpWIKDgyUXMXNDUlISxo4dq7DY76pVq/Dy5UsAwIgRI2SqyFeqlPFDyMXFReG2t27dilOnTil9bDU1NcyeLU75ERAQgCFDhigtACMSiRAYGChz3y+//AJ1dXWIRCL06dNH5YXf1NRU/Pvvv3JtJk6cCEBceGbkyJGSaZ3Sdu3ahUuXxD8Ce/XqJVeYvEmTJtDQEI8U2rBhg8Jg1Nq1a2VG0X/r5MmTiPw6ilsRf39/SYQys3oQgwYNkqS3+uWXXyRBnn79+imtwfLrr78CAKKjo9GnTx+VfUlKSsJff/2FxMREmfvHjh0LQPxezps3T+G6s2fPlnsfpVWpUkVS7+TQoUNYv3690rYA8PHjRxw8eFDu/vT3KCQkBDExMUrXT/8cv3v3Dnfv3pVbHh8fjwEDBmRa4Dv98by9vb8rGJkbn0fKO5GREUq/o1JTU7H2t2V4/kx8Yj5sxGiULp2/qQJ/FOdOn8g06Pffv3tw7474R1Gp0mVQu259le0pb2w+8xIJXwuFrx7RSFKMW1q/lhXR0l78Q/fcI1+ZehnprE2VjwItY66PI/M7QFNDDYnJQszYnv+zUwuDlJRkzJkxBS++fof8PGAwxk1SnEZSlbevX2HG5PFISEiArq4u1m/aimrVa2S+IuW6N69fYerEcUhIiIeurh7+3PwPqlXnLMrs+N7jRXR0FJ64K//9DQBeHi+wfs0KAOIRnT36Kp9pXtzZVawMx0biixI3rlzAU3f5GcThYaHYs30zAHF6ovbOGRfE6zg0gPbXwT+uJ47g44d3Ch/n0b3buHvTDQBgbmGJCkwrJefcmdz9LZWamorLF10BAEZGxmjSrEXudbaIS0lJxtyZ3xy/J2b/+E35IyU5GdOnTMKzp+L3a+CgIZg0dXoB94oo97i4uCAtLU3ln3QB8GvXrknuTw9ECAQCdO8uPn6/fv0a9+/fV/hY9+/fl1yn7N69u9wMk8qVK0tmaRw+fBjx8fFK+5yuZ8+eOXreWcU5nrnkf//7H27cuIEHDx7gzz//xPXr1zF69GjUqVMH+vr6iIiIgKenJ65cuYLz58/D3t4eo0Zlf4SeIg4ODjhz5gyaNm2K6dOno1KlSggJCcGePXskMwjKlCmDhQsXyqxXt25d1KxZEx4eHvjnn38QERGBwYMHo1SpUggICMD+/ftx9OhRNG3aVGW6ookTJ+LMmTO4fPkyTpw4AXt7e0yYMAEODg7Q09NDUFAQ7t+/j4MHD2LAgAFYsmSJZF17e3usW7cO06dPh5eXF2rWrIkxY8agdevWKFmyJBITE+Hj44N79+7h6NGj+Pz5M16+fCmTb83Z2Rl9+/bFkSNHcOnSJTRq1AgzZsxA1apVERERgUOHDmHXrl0AxLUIFF1Yt7S0RN++fXHw4EFcvHgR3bp1w8SJE1GyZEn4+flh3759OHbsGJo0aaLwYjkgnn0wcOBAODs7o3Xr1qhWrRqMjIwQEREBd3d3bNq0SXIxfdy4cSrf0zJlyqBVq1a4evWqTAowRWml0nXu3BlTp07Fn3/+iZs3b6JatWoYN24cmjVrBjMzM8TFxeH9+/e4desWjh8/joiICAwdOlRmG5MmTcLu3bvh4eGBP/74A+/fv8fo0aNRpkwZBAQEYNu2bXB1dUWDBg0kQR1FU+m2bt0Kd3d3eHt7Y+bMmTh16hSGDBmCGjVqQFtbG2FhYXj+/DkuXLgANzc39OzZE/3795fZRnrBcpFIhHHjxmHy5Mky+fgqVhQXYB48eDA2bdoEkUgEZ2dnzJ49G82aNYOOjg4eP36MDRs24N27d5l+jps0aYLdu3cjJCQEM2bMwKBBgySpzzQ1NWFjY6N0XWm58XkkxZ49fYwAPz/J/5FSxScD/P3k8tF26S5/AH386CHWrlqO9h06oa6DI6ysrJGclIR3797g5LEjePtGHDRs0rQ5ho8em0fP5Me3a9sWbP5jLVq2bodadeqhdJmy0NXTQ3xcHLzfv8Ol82fx8vlTAOL9Z86CJXleL4cU8w+Nw7KD7lg5rBHsbc1wa21P/H78GTx8w2Goq4kejctjdEfxlOGouGTM2XVP4XY2jm8OixI6OHnvIx6//4KouGSYG+mgVa3SGNWhOoz0tZCaKsLELbfw9lPxnI3x67zZeHBPfJxxaNAQ3Xr2xof3ii/yAeJ9o5yNrcx9Af5+mDJhDGJiogEAYydOhYGhocrtmJiawtTUTOny4urpk8fw9/eV/B8plTrU398Pp08dl2nfrXsvmf/9/f0wadwoyXsxYdJUGBgY4P075WlGTU3NYGrG90La9x4v4mJjMWXscFSoVBktnNqgSrXqMDO3gJqaOoKDPuPurRu4eO60ZOBI/8HDULUaA3+qjJ06G688nyM2JgaLZ09Gj58GokHj5tDS1sabVx74b99OhH5N9TVk9ESYW5SUrGtgWAI/DRqBfTu2ID4+DjPGDkG3Pv1Rz7ERDAxLICIiDPdvXcf508clA/uGj5+qtP5gcSazb9SW2jfilewb81X/lnp47w7Cw8IAAG07dIaGhuLBbyRv4TfH7649sn/8pvwzd/ZM3LsrrvXaoGEj9OzdB+9UHJs1NTVha6t6IClRUTRt2jRs27YNqampmDx5Mm7evAldXV3J8oSEBEyePBmAOFPItGnTFG5n1qxZGDlyJMLDwzFnzhxs3rxZZvmHDx8kxckrVqzIQMaPQltbG5cvX8awYcNw/PhxPH/+XDJLQ5ESJUooXZZdEydOxI0bN+Di4oJ+/frJLS9VqhQuXrwouSibTiAQYN++fWjdujUiIiJw+PBhudoN9vb2OHLkiGRKkiJqamo4efIkhg4diqNHj+Lt27dKdwBFpk2bBn19fUybNg1RUVFYu3Yt1q5dq7CtlpaWJAWQtL1790IoFOLEiRN48uQJBg0aJNfG2toarq6uKF1a8dTmDRs2wN3dHe/evcPZs2dx9uxZmeX9+vXDqFGj0LZtW4XrA+LR/0eOHFGaeklNTQ1Lly5Fjx49lG4j3ZAhQ3D1asaU76pVq6JBgwYq19mwYQNMTU2xbNkyBAUFyQSNvqWvry/3Y1hLSwuurq5o3bo1Pnz4oPB1aN++PaZPn45OncS1AxS9H6amprhz5w5++ukn3Lp1Czdv3lRZQ0XR/tC6dWs0atQI9+/fx4EDB3DgwAGZ5emzJhwdHbF06VIsXrwYkZGRWLBggdy2Zs6ciZo1a6oMZPTr1w+//fYbvL298ccff8ikxbKxsYGPj4/Sdb+VG59Hknfq+FG4njmpcNnzZ08kMynSKQpkAOKRhocO7MOhA/vklgkEAnTp3hNz5y+GpqbWd/e5KIuOisKZE0dx5sRRpW0sS1rhl0XL4NiwcT72jL614eQLmBhoY2avOqhSxhjbpjjJtQmOjMfPv13Ch8/RCrchANCgSkk0qFJS4fKw6ERM++c2jt4pvrUxrl/NSH3m/vABBvbtobJ9qVLWOHn+isx9z548RkR4mOT/P9atyvRxR42dgNHjlf/mLK5OHj+Cs6dPKlz2/OkTPH8qe8z4NpDx9Ik7wqXei9/X/pbpY44ZNxFjJ0zOfmeLuNw4Xnx491aS+18RdXV1DB01DsNHj//u/hZ1ZcrZYsnqjVjx6yxEhIfh8P5dOLx/l0wbgUCAfkNGoe/A4XLrDxg2BrHR0Th55F8kJMTjv3078d++nXLtNDQ0MGzsFLTp0CXPnsuPLsv7xsLMf0tJp5Xq6Nwt1/pYHFx3kz1+D/qph8r2VqWscfLcFZVtKO9clUq39vDBffTpqfrzbm1dGucvu+V1t+g75UGpiWKvcuXKmD17NlatWgV3d3c0bdoUc+fORYUKFfDhwwesXr0aT5+KA+azZ8+WydojbejQodi1axfu3LmDv/76C0FBQRg9ejRMTEzw8OFDLFu2DNHR0VBTU8PGjRsl2W7yCgMZucjQ0BDHjh3D7du3sWfPHty6dQuBgYFISEhAiRIlUKFCBTRo0ADOzs5o3759rj727t270b59e2zbtg0vX75EbGwsbGxs0KNHD8ybNw8mJiYK16tTpw6ePXuG3377DefPn0dgYCAMDQ1RsWJF/PTTT5g4caLCC9Xf0tPTw5EjR3Dt2jXs3r0bt2/fRlBQEFJTU1GyZEnUqVMHXbp0kRt1n2706NHo1q0b/vnnH1y6dAlv3rxBZGQktLW1Ubp0adjb26Ndu3bo3bu3zKj8dDo6Ojh+/DjOnDkDFxcX3L9/H6GhodDX10flypXRo0cPTJo0CQYGBkqfQ8mSJfHgwQOsXr0ax48fh5+fH/T19SWzRAYOHIjr168rXf/gwYM4e/Ysrl+/Di8vLwQFBSE0NBQ6OjqwsbFBixYtMG7cONSqVSvT1xMAevfujYkTJ0rqjqTXzVBFIBBg0aJFGDx4MP7++2+4ubnB29sbUVFR0NPTQ9myZVG3bl20b98ePXv2lInGpitXrhyeP3+O33//HUeOHMGHDx+gra2NqlWrYsiQIRg7dqxMvYlvA2TprKyscPPmTbi6uuLgwYO4d+8egoKCkJKSAmNjY1SqVAmNGzdGt27d0KKF/NRnNTU1XLp0CWvWrMGZM2fw4cMHxMXFKUz7tGjRIjg4OODPP//Eo0ePEBcXB0tLSzRo0ADjxo1Du3btlKZPS2dgYIC7d+/it99+w6VLl+Dr66t02lxmcuPzSHmjTr36mDJ9Ntwf3oePz0eEh4VBTU0AcwtL1HdsiK7de6Kmfe2C7maht37zNty9fQMvnz9FgL8fIsLDEBUZBW0dbZiYmKJSlapo0qwlWrfrCB0F3zOU/xbtfwTXR74Y3bE6mla3gpWJHhJTUvH+UxTOPvLFVlcPRMfLp8JLt+7YM7wLjELTalYobW4AM0NtRMYl42NQNM4+9MHuy68RFpPzGkNEVDR97/HC3MICy1avh8eL53jl+RJfvoQgKjICyUlJMDAwRFkbW9R1cETXHn1QinUYsqxm7Xr4Z/9xnDp6EPduXkPQ508QpqTA1Nwcteo6oFuf/qhYuZrCdQUCAcZOnY3WHZxx4cxxeL54ipCgz0hMSoSurh6sS5eFfd366Ny9D8qUs83fJ/YDWb9Jat8IULBvVK6KJs2/7hs6qn9LxcXG4vbNawAA2/J2qFbDPj+eAhERFXIrVqxASEgIdu3ahadPnyoc/D5y5EgsX75c6TbU1dVx8uRJdO7cGY8ePcKxY8dkio0D4sH9mzdvlgx4zkuCtPyuTE1EP7zly5dj4cKF0NDQQExMTJaCXfRjikpQXu+H8l9yKt+PwqLcoF2ZN6J88fngyILuAn2lwfQxhUZCinztPCo4sYnCgu4CfWWgzbGchYWGOodgFxa6WkwBW1joFNOvqIqzzhd0F3Ls/bq8v3j/rSVLlmDp0qUAxDUynJycVLY/d+4ctm3bhkePHiE0NBTm5uZwdHTE2LFjsxx8EAqF2L59Ow4cOIBXr14hLi4O1tbWaNOmDaZOnYoaNfIntScDGUSULWlpaahVqxY8PDzg4OCAR48eFXSXKA8xkFG4MJBReDCQUXgwkFF4MJBReDCQUbgwkFF4MJBReDCQUXgwkFF4MJDx4ymIQEZxVkx3ESJSxsfHB2XKlFGa127RokXw8PAAALli4URERERERERERFklYJEMyiIGMohIhouLC3bv3o0BAwagadOmsLa2RkpKCl69eoU9e/ZI6oRUr14do0ePLtjOEhERERERERERUZHHQAYRyfHz88OqVauULq9atSpcXV2hra2dj70iIiIiIiIiIiKi4oiBDCKSMXLkSBgZGeHSpUt4//49vnz5gvj4eJiamqJ27dro2bMnRowYAS0trYLuKhERERERERER/cCYWYqyioEMIpJRtmxZTJ8+HdOnTy/orhARERERERERERFBraA7QEREREREREREREREpAwDGUREREREREREREREVGgxtRQRERERERERERER5TsBi2RQFnFGBhERERERERERERERFVoMZBARERERERERERERUaHF1FJERERERERERERElO+YWYqyijMyiIiIiIiIiIiIiIio0GIgg4iIiIiIiIiIiIiICi0GMoiIiIiIiIiIiIiIqNBijQwiIiIiIiIiIiIiyndqaiySQVnDGRlERERERERERERERFRoMZBBRERERERERERERESFFgMZRERERERERERERERUaLFGBhERERERERERERHlOwFLZFAWcUYGEREREREREREREREVWgxkEBERERERERERERFRocXUUkRERERERERERESU7wTMLUVZxBkZRERERERERERERERUaDGQQUREREREREREREREhRYDGUREREREREREREREVGixRgYRERERERERERER5TuWyKCs4owMIiIiIiIiIiIiIiIqtBjIICIiIiIiIiIiIiKiQouppYiIiIiIiIiIiIgo3wmYW4qyiDMyiIiIiIiIiIiIiIio0GIgg4iIiIiIiIiIiIiICi0GMoiIiIiIiIiIiIiIqNBijQwiIiIiIiIiIiIiyneskUFZxRkZRERERERERERERERUaDGQQUREREREREREREREhRZTSxERkVIa6pziWZhoa/KwXViEHR5d0F2gr8YcflHQXaCvNveuWdBdoK+0NTherTDR1Ncs6C7QV1rq3DcKiy8xyQXdBfpKV0u9oLtARJQlvCJCRERERERERERERPmOJTIoqzgcgYiIiIiIiIiIiIiICi0GMoiIiIiIiIiIiIiIqNBiaikiIiIiIiIiIiIiyncC5paiLOKMDCIiIiIiIiIiIiIiKrQYyCAiIiIiIiIiIiIiokKLgQwiIiIiIiIiIiIiIiq0WCODiIiIiIiIiIiIiPIdS2RQVnFGBhERERERERERERERFVoMZBARERERERERERERUaHF1FJERERERERERERElO8EzC1FWcQZGUREREREREREREREVGgxkEFERERERERERERERIUWAxlERERERERERERERFRosUYGEREREREREREREeU7lsigrOKMDCIiIiIiIiIiIiIiKrQYyCAiIiIiIiIiIiIiokKLgQwiIiIiIiIiIiIiIiq0WCODiIiIiIiIiIiIiPKdgEUyKIs4I4OIiIiIiIiIiIiIiAotBjKIiIiIiIiIiIiIiKjQYmopIiIiIiIiIiIiIsp3zCxFWcUZGUREREREREREREREVGgxkEFERERERERERERERIUWAxlERERERERERERERFRosUYGEREREREREREREeU7AYtkUBZxRgYRERERERERERERERVaDGQQEREREREREREREVGhxdRSRERERERERERERJTvmFmKsoozMoiIiIiIiIiIiIiIqNBiIIOIiIiIiIiIiIiIiAotBjKIiIiIiIiIiIiIiKjQYiCDiHLVkiVLIBAIIMjDJIfXr1+XPMb169fz7HGIiIiIiIiIiCjvpF/f+RH/KH+x2DdRMXf9+nW0atUKALB48WIsWbIk03WGDRuGPXv2AAA+fvwIW1vbPOwhUfYlJSXh1IljuHrlEt69fYPYmFgYmxijSpVq6NKtOzp0ci7oLhYrgYGfcGD/Pty6eR1BQUHQ0tRC2bJl0b5jJ/zcfyB0dXULuovFQkpKMs6cPoUrly7g3du3iIqKhIaGJixLWqJ27bro2acv6tSpV9DdLLTKm+qidukSqGKhj9JG2jDU0UCqKA0RCUK8+xKH6+/D8fZLXJa2ZaGvhfZVzVGzlAHM9bUgEACR8UJ4fI7B5beh+BSVpHL9qpb6qGShjwpmerAqoQ1DbXXoa6kjOTUNYXHJePO1Pz7hCbnx1H9Irzw9cOf2TTx/+gQfvT8gMiIcGhoaMLewRK06ddGtZ2/UqVtf5TYSExJw7+5tPLx/F688PRHg74f4hHjo6+ujnI0tGjVuil59f4aZuUU+PasfU3hYGDw9XsDT4yW8PD3g5fkSUZGRAADnbj2wZNlvKtcP/PQJ3Tu3zdZjlrK2xunzV3Pa5SKL+8WPicfv/BcS9BkXzh7Hg7u3EBL0GfHxcTAyNoFVKWvUrtcALVq3R/kKlWTWSUxMgPv9O3jy6B7evvJC4Cc/JMQnQE9fH2XK2qB+wybo0vMnmJqZF9CzKjo8PV7i1s0bePr0Cbw/vEdEeDg0NDRhYWmJOnXroWev3qhX36Ggu0lEeYCBDCIiKlJ8PnpjxpSJ8PH5KHN/6JcvCP3yBXdu38Spk8exbsNG6OnpF1Avi4/r19ywYN5sxMbGSu5LTEiAp2cUPD09cPzYEWzesg3lbGwKsJdFX2DgJ0yZOA4f3r+TuT8lJQW+Pj7w9fHB6VMn0G/AIMyZt4Cji77xa7sKqFrSQO5+TXWglKY6SpXQRosKprj1IRw7HgQgVZSmdFutKppiiGNpaKrLToy2KqEOqxLaaFnRFAceB+Ly2zCl25jQtBxM9bXk7tdQB/S0dFHWRBetK5nh8ptQ7HcPhPLeFE1jRwzGsyeP5e5PSUmBv58v/P184Xr6JDp36Y75i5dCU1P+tXz39g3GDBuI+Ph4uWXRUVHwePEcHi+e4+C/e/HLwqVo16FTnjyXoqBD62b5/pg2NuXz/TELO+4XPyYev/PfySMHsOvvP5GYIDsYIDQkGKEhwfB4/hTxcbEYP22uZJn3+7eYPm4IEhTsGzHRUXjl+QKvPF/g+H/7MW3uIji17Zjnz6OoGj5kIJ48dpe7PyUlBX6+PvDz9cHpk8fRtVsPLF66DJpa8t9lRPTjYiCDiHLVkiVLsjSrgygvhIeFYcKYkQgK+gwAaNe+I7p07wELC0t8+RKCs6dO4vKlC7h/9w7mzZ6BjX/9U8A9LtpevfLC3FnTkZiYCD09PYwcPRaODRoiMTERF8+fw7Gjh+Hr44NJE8bg4OFj0NeXv1BM3y8lJUXmIkilylUwaMgw2NqWR3xcHJ4+fYx9e1yQkBCPQwf2w8LCEiNGjSngXhcuJnqaAIDw+BQ89I3Em5A4hMUnQyAQoJK5HjpXs4CpvhaaVzCFupoAW+74KdxOIxtjjGxUFgAQl5yK869C4BkUC2FqGmxMddGluiWsSmhjsGNpRCcK8cAvSuF2klJFeBEYjXdf4hEUk4TIhBQkpIhgrKsBOzM9tK5kBmNdTXSoaoFkYRr+e/Y5b16YQir0SwgAwMLCEq3bdUCdevVhZVUKqSIRPJ4/w7/7XPAlJBjnzp6CUCjEslVr5bYRFxcruVhbq049NGvREtWq14CRsTEiwiNw3e0yTh0/irjYWCyePwf6+vpo0qxFvj7PH5FVqVKwtbXD/Xt3sryOpaUlDh49lWk7l13bcfHcWQDimR4ki/vFj4fH7/z37+5t2LN9MwCgTDkbdOrWG1Wq1YS+vgGioyPx/u1r3LnhBoFAdjBCfFysJIhRo1ZdNGzSApWr1UCJEkaIjIzAnRtXcf70McTHxWLV0l+gp6+PBo2b5/vzKwq+hHz9LrO0RPv2HVGvvgOsSpWCSCTC82fPsHfPLoQEB+PM6ZMQCoVYtfb3Au4xEeUmBjKIiKjI2Pb3X5IgxpjxEzFuwmTJsqrVqqN5CyfY/rUR2//egts3b+DKpQto254jovLKmt9WIDExERoaGvh7+y7UrlNXsqxho8YoZ2ODDb+vha+PD/a67Mb4iZNVbI1y6vq1q5KLILVq18GuPf9CXV1dsrxRk6Zo6dQaQwf1h1CYApddOzBk2AhoaPBnYrrAqCQcfvoZD/2jkPbN9IYPofG4/TECiztURKkSOmhS3gRX34XhTYhsmiktdQEGO1gDABJSUrHs4nsERCVKln8MT8AD30gsbF8R5Ux0MdixNJ4FxiBJKJLrz9wzb6Bs0sezTzG49DoUSztVQklDbXSqbgFXrxDEJqd+34vwA7GxtcP4SdPQqm17mc86ANjXqo1OXbph9LCB8PP1waULrujV92fU/SYFhZpADW3bd8TIsRNgV6Gi3GM0atIUjZs2x9wZU5CamorfV69A46bNORpagVFjJ6B6jZqoXtMeZmbm2U4VpaGpiYqVKqtsk5qaiiePHgIA9PX14dQ6e6moigPuFz8eHr/z11P3+5IgRttOXTHjlyXQ0NCUaVPXoRH6DhiGlJQUmfsFampo2aYDBo0YB5vyFeS27dCwCRwbNcPSX6ZBlJqKLetXwfFwM+4bOWBrZ4fJ06ajbbsOct9ltWrXQZdu3TB0UH/4+vjg/Lmz6PtzP9R3cCyg3lJWcVegrGKxbyIiKhJSU1NxzvUMAHFu7NFjJyhsN2bcRFiVEl9M3L1ze771r7h5+eKFZNp3j169ZYIY6YYMGwE7O/HJ3r/798qdFFLueP7sqeT2iFFj5E76AKB6jZpo0dIJABATE42P3h/yq3s/hN+vf8QDP/kgRrrYpFT8+zhQ8n+DckZybWqXLgEjXfEFkYuvQ2WCGOkSUkSS7RjraqKFnYnCx1ORuUrcn+RUXH8fDgDQUBOgooWe6hWKmPWbtqJth04KP+sAYGxigqkz50j+d7tyUa5NrTp1sWLNeoUXa9O1bNUGTm3aAQAC/P3x5vWr7+x50TR2wmQ0b9kKZnmYF/7h/Xv48nXGQeu2HaCjo5Nnj/Wj4n7x4+HxO/+IRCJsXLsCAGBXqQpm/rJULoghTVNTdlkN+zpYsGytwiBGuiYtWqFpyzYAgMBP/nj/lvtGTmze8g86dOys9LvMxMQUM2fPk/x/+ZL8dxkR/bgYyCCiXLVkyRIIBIJMR5fcvn0bvXv3hpWVFXR0dGBnZ4dx48bh/fv3AAAnJycIBAI4OTll6XEPHz6MNm3awMLCArq6uqhSpQrmzJmD8PBwhe1r1qwJgUCAfv36KVzu4uIieR516tRR2Ob+/fuSNhcuXJBZlpycjDNnzmDSpElwdHSEiYkJNDU1YWZmhoYNG2LJkiUIDQ1VuN3Tp09Ltnvo0KFMn/vMmTMhEAigoaGBwMDATNsXVX6+voiNiQEANGrcVOmPW3V1dTRq3AQA8MrLE58CAvKtj8XJNbcrktvde/ZW2EZNTQ1dvqb/iImOxqOHD/Kja8WOUCpAVKZMWaXtypTNWMagUva9CsqYgVHSUFtuuZ1pRjDheWC08u0ExyL56yyMBjbGOe5PQkrGDAxNNf7k/1Z9xwaS2wH+/jnfjkPGdj75K04pRnnP9WxG6innbt0LsCc/Nu4XhQuP3/nn8cO7+OTvCwD4eeBwqOfRrJY69TP2jc+feA6SVxwbNJTcDuB3EFGRwrMaIsp3q1evRosWLXD8+HEEBwcjKSkJHz9+xD///IN69erh0qVLWd6WSCTC4MGD8fPPP8PNzQ2hoaFITEzE27dvsXbtWjRs2BBBQUFy67Vs2RIAcOPGDYXblb7/xYsXCgMi6W00NDTQrJlsIcsxY8agW7du+Ouvv+Du7o7IyEgIhUKEh4fj4cOHWLp0KapWrYo7d+RzRDs7O6NUqVIAxAEVVYRCIfbv3w8A6NixI6ytrVW2L8qioiIlt01NzVS2NTXLWP70iXyxOPp+T78WFNXV1UP16jWUtnNwzJjq/ezpkzzvV3FkY5tR9DYgQPmFqfSLVgKBAOVsbPO6W0WOhnpGAF+kYOqGgXZGcDU6Qah0O6I0SNJAVTTXg1oOptoLADSUCoIERsvP/ijukpOTJbeVBb6zIiUlYztq37Edyrm4uDjccLsKALC2Lo169ZlCJKe4XxQuPH7nn5tulwGIX8OGTVtK7o+OjsInf19ERyuuWZVd0vuYGgcZ5JkUvs4/nPSBnD/iH+Uv7tFElK8OHz6MefPmIS0tDaampli9ejXu3r2Lu3fvYvXq1dDQ0EC/fv3w+XPWCpMuXLgQ+/fvR48ePXD8+HE8fvwY586dg7OzMwDg/fv3mD59utx66TM9goKC8Pr1a7nl169fl9xOS0vDzZs3lbapV68eDAxkixQLhULY2dlh5syZ+O+//3Dv3j08evQIR48exbhx46ClpYWwsDD07NkTIV8LlqVTV1fHsGHDAACXL19GgIoZA66urpL1R4wYobRdcaCnlzHaOTY2RmXb9JkbAOD9gVPw80J6aoNy5cqpzNVcvryd3DqUuzp27iL5jnLZtQOpqfK1El6/8sKtm9cBAJ2k2lPWVSupL7n9KSpJbnmiVK0LXS3VF/Z0NcU/0TXV1RTO7lBEIABMdDVQp7Qh5rergGolxe/hy88xCvtT3D19nBHEtpX6HsquJ48f5cp2KOeuXr6IxMQEAECnLt14UeE7cL8oXHj8zj+vPV8AAEqWsoaevj7cLrlizKBe6NOxOYb/3BV9OjbHiH5dceSAi0wwIrtePs3Yx8rZct/IK+7uGd9B5e2Up/sioh8Pq0ARkURISAg8PDwybRcZGZmj7SclJWHKlCkAAHNzc9y7dw8VK2bk2G3cuDF69OiBxo0b4+3bt1na5t27d7F8+XIsWLBA5v6OHTuiY8eOuHTpEo4ePYqNGzfCwsJCsjx9RgYgDkhUrVpV8r+fnx98fHwgEAjg7OyMs2fP4vr16+jRo4ekTWpqqmQ2haL0V0uXLoWdnZ3cybSDgwN69+6NCRMmoEmTJvjy5Qs2bdqEZcuWybQbOXIkVq1aBZFIhL1792L+/PkKn/+uXbsAABYWFujatauKV6roK1u2HDQ0NCEUpkhqMygjvTzoc/FNx5VXkpKSEBERAQCwtLJS2baEkRF0dfWQkBCvcPYUfT8TExMsW7kGv8ydiWdPn2BQ/74YMGgIbGxsER8fj+fPnmDfnt1ISUlBtWrVMWPW3ILu8g9HAKBLDUvJ/w98I+XaBErVxKhWUh8+4QkKt2VrqgtdzYxAh5m+Jj5HKw9E7B9UW+myj2Hx+OcuUyp8SyQSYe+ujBpJbdp3zNF23r55jbu3xAMdKlaqzIslBeTcGam0Ul2ZViqnuF8UPjx+5w+RSAR/348AACMjE2zZsAonjxyQaxfg54vtm9fjzg03LF+3GQaGJbL1OB/evcGDe7cAAOUrVGIgI4+IRCLs2rFN8n+Hjp0KsDdElNs4I4OIJLZu3Qp7e/tM/06dOpX5xhQ4efIkgoODAYhraUgHMdJVrlwZixcvzvI269evr/Aiv0AgwIwZMwCIZ0fcu3dPZrmlpSWqVasGQHb2hfT/1atXR9++fRW2efz4MWK+juqXDoqkq1ChgsoRgfb29hg1ahQA8euiaP30AImy9FLBwcE4d+4cAGDQoEFyReeKG109PTg2FOdDfff2DS6cO6uw3YVzZ/H+XUagLC4+TmE7yrm4uIzXVHqmjDK6eroAgPj4+DzrU3Hn1Ko1Dhw6hp69++LN61dYtGAehg7qh/FjRuDvLZuho6OL2XPnY+eef2FmnncFeYuqjtUsUNFcPCPjoV+kwiDF88AYCL9W6e5U1UIm1VQ6AYC+tWWDf7oa2U/LkihMxa4HAVh68T0iVaSxKq4O7t8DT4+XAACnNu1QTUX6O2WSk5OxcukiyQjpcZOm5mofKWuCPgdKRv/XqlMXZcvZFHCPflzcLwonHr/zXlxsLEQi8azJjx/e4eSRAzA1t8Dcxb/h2IXbOHPtIdb9tQvVatQCAHi9fIbfV2b9fBUQ7xsbflsC0dd9Y9jYybn7JEhi314XeLwUz7Bp07Y9qteoWcA9IqLcxBkZRJRvrlwRF/9VU1PDwIEDlbYbNGgQpk2bhjQFOca/NWDAAKUBg/r160tue3t7yy1v2bIlXr16JVcnI/1/JycnSTAhvU6GqampTBt1dXW5+hiKREREIDw8HImJiZLnZWxsDADw8vJCSkqKXCBi1KhRuHbtGt69e4fbt2/LPc7+/fshFIovUGU3rZSqdFXSTCxLZ2u7BW3s+El49OA+hEIhFi34BQH+/nDu1h3m5hYIDf0C19OnsO3vLdDU1JQUQ0xKZMqV3JaclPGaZiXApqWpBQBISmQe/7ySkpKMs2dO4vq1qwq/W8PCQuF69jSsS5eBU6vWBdDDH1dVS338XFdc1ygqIQUuDz4pbBcenwK3t2FoX9UcpvpaWNyhIg4++YxXwbEQpqahnKkuetcqiVrWJZCSKoKmuni8kZaG6jQ58868AQCoCYASuhqoXtIAbSqZYUC9UihVQhuHngQiNfPDabHxxP0R/tq4AQBgYmqGuQsW5Wg7635bjlde4lmszl17oHnLVrnWR8q6865nJN9pzl04GyOnuF8UXjx+573ExIyBNMnJSdDW0cHaTTtQ1iajRkmtug5Ys3kHpo4ZDO93b3DnxlW88nwhCW5k5q/fV+Lta08AQLvO3dC4mVOuPgcSc3/0EBs3/A5AXBNxwaIlBdshyjJmhaSs4owMIpJYvHgx0tLSMv0bOnRojrafnrbKzs5OchFfEVNTU9jZZW2qrXRKKEXbSRcTI18zQVmdjPTZF05OTihXrhzKly8vVycjvU3dunVRooTiacUvX77EiBEjUKpUKZiamqJixYqoWbOmZGbLkiVLAIinv6an4ZHWq1cvmJiYAAB2794ttzz9PkdHR9Ssmb2RJmXLls3S34+mVu06mL9oKTQ0NCAUpmDL5j/h3L41Gtazh3P71tiy+U9oaKhjxux5knX09fVVbJFyQks7I6d/esBIleSvRUG1dXTyrE/FWUJ8PMaOGoFdO7YhOioKw4aPwvFT5/DwyQvcuueOrf/sRN169eHl6YEZUydi3x757xtSrLSRNqa1tIWGmgDJQhE23vJFdJLyGRAHngTi2adoAECpEjqY4VQe23+2x+4BtbC0YyXUsi4B77B43HgfLlknIUWkbHMAgICoRAREJcIvMhEen2Nx+FkQfnF9i+hEITpVs8CsVnY8OfzK+/07zJ0xGalCIbS1tfHb2g0wNTXL9nZcdm7DqRNHAQDVa9hj9vxfc7urlEXnzp4GAGhpaaFdB6YPyQnuF4UXj9/5Q0tLthZVp669ZIIY6bS1dTB8TMZMihtXLmZp+wf37sD5M8cBAFWq1cSkmYpTBtP3ef/+HaZPmQTh1++ydev/hJlZ9r/LiKhwYyCDiPJN+sV66VoVymSlDaA6bY2aWsZXnKLieN/WyQDEMxW8vb0hEAgky9MDHultUlNTcfv2bZll39q5cyfq1auH3bt3Zynvf0KCfBoSHR0dDBo0CIC4SLp0up6HDx/C01M8qqe4F/n+Vo+evbHn3//Qqk076OpmfD40NDTQ0qk1/v3vuMwUY0MlgSjKOengUFbSRSXEiz//WUlDRdn399bNePpEXBdm0dLlmDpjFsrb2UFTUwsGBgZo1KQptu3cA8cGDZGWloY/1q/FmzevM9kqWehrYW6bCjDQ1kCqKA2bb/viTYjqVHVCURp+v/YRO+77wyc8ASKp0bVRCSk4+TIYyy6+F+eY+iouWf74lZnw+BS4PBLPDLG3NoRTBdNM1ij6Aj8FYMr40YiOjoa6ujqWrVqHuvUdsr2d40f/w9ZNfwAQFzHesPlvmWMN5R/Ply/g81E847aFU2sez3OA+0XhxuN3/tDVkx3UVL9BE6Vt6zo0hLq6OLHJ29eZ15Y8e/IIdv+9EQBQ1qY8lv/+F/eNPBAQ4I9xo0cgOjoK6urqWL1uPeo7OBZ0t4goDzCQQUTFlpWVFapUqQIgI0iRnjKqevXqkmBKekAjvc2zZ88QHR0ts0za69evMW7cOAiFQlhaWmLt2rV4/PgxwsLCkJycLJnZsnPnTsk6ytJopdfRiI2NxdGjRyX3p8/G0NXVRf/+/bP93P39/bP096OqVr0Gfv9jE27cfYhzl6/h1LlLuHX/MTZs2oLydnbw8/WVtK2goFYLfR9tbW3JrKuQTAJ50VFRSEgQBzusMikMTtmXlpaGUyeOAQBsbG3RrXtPhe00NDQw4Wsuc5FIhDMnT+RbH39ExroamNfWDqZ6mhClpWH7PX88CYjO0rppAK6/D8ev595izH8emHnyFSYd88SkY144+jwIKaI0WBlmjA79FJWzlGsvP8cgSSiezdGgnHGOtlFUfAkJwaSxI/HlSwgEAgF+XbIcLVu1yfZ2Lp53xdqVywAApUpZY+PfO2D8deYk5T9XqSLfnbt0K8Ce/Ji4XxRuPH7nHy0tLRgZZ3xmLUoq/z2qpa0No6+/cSMVzKiXdu3SOWxetwIAUNLKGqv+/EfmcSh3hIQEY+yo4fgSIv4uW7psJVq1blvQ3SKiPMIaGUSUb9LTJH358iXTtllpkxtatmyJN2/eSAIY0mml0n1bJyO9jZqaGpo3by63TRcXFwiFQqirq+PGjRtK01+Fh4crvF9arVq14OjoiEePHmH37t0YOnQoEhMTcejQIQDi9FNGRkZZfLYZypQpk6V2cck/dmJ1DQ0NWFmVkrv/lZen5HaNmlnLbUvZY1ehIp48doefnx+EQiE0NBT/5Pj4MaN+TXm7CvnVvWIjLCwUUVFRAIAqVaurbCtd2FX6fSFZBtrqmNemAkp+DTbsffQJtz+qvpihTKJQhMTYZJn7BALAxkQXABAck4TYpOzPyACAtDTxbA5tDTWYG2Req6aoioyIwORxI/EpQBycnzl3ATp3zX4thZvX3bB04S8QiUQwt7DA5m27UFLFxS7KW8KUFFy6eA4AYGpqhsZN5X+PkXLcLwo/Hr/zl61dRTx/8ggAJAW5lUmf6a+urq60zb1b17Bm2a8QiUQwNbfA6o3bYWHJfSO3RUSEY+yoEQj4OgBv3vyF6Nq9R8F2inJEWd1Tom9xRgYR5ZsaNcQ/sr29vRXWhEgXHh6usDh3Xvi2ToZ0oe90NjY2sLW1ldTJSG9Tp04dhUGE9JRPtWvXVlnDw93dPUt9TJ+VcfPmTXh7e+P48eOIjIwEwLRSOZGamgq3q5cBAFZWpVC7Tt0C7lHRVLdefQBAQkI8vKQCR99yf/RIcrtO3Xp53q/iJj39AQCkpiqv3QAAQmFGPRMNDeUn58WZrqYa5ra2QxljcT2XQ08CceVtWK4+RvWSBjDUEb9vD3wjc7wddTUBDLXF72NiJnU2iqrYmBhMmTAaH70/AAAmTp2Bvv0GZHs7jx7cw4I5M5AqFMLI2Bibtu5AmbLlcru7lA23b91A1NffQh06OysNlpM87hc/Bh6/85d9nYzfoJ8DA5S2i4uLRXRUJADA3KKkwjZP3e9j+cLZSE0VooSRMVb98Q+sy/x4tQcLu5iYGIwfMwreH94DAKZOn4l+AwYWcK+IKK8xkEFE+aZNG/F0dZFIhAMHDihtt3//fqWplnKbdGqoAwcO4N27dzL1MdKlBzbc3Nxw69Ytmfu+JRSKTzaka1p86/Pnzzh9+nSW+ti/f3/o6+sjLS0NLi4ukrRS5cuXR6tWrbK0Dcpw8vhRBH0OBAD06vuzytFUlHPSU7rTUyN8SyQS4ezpkwDEtUocGzTMj64VK0ZGRjAwMAAAvHj+TPL9pMhj94ygknXprM3aKk601AWY1coO5c3Eua1PvgzGWa/cnz3Yq5Z4xKYwVYRr7zKfuadM/TIloKku/qnvH5mz9FQ/ssSEBEyfPB5vXnkBAIaPGoshw0dlezsvnj3F7GmTkJycDAMDQ/y5ZTvsKlbK7e5SNkmnlXLu2qPgOvKD4X7x4+DxO381c2onuX3nhpvSdnduXJWcp9asLT8Ax/PlMyyeOxUpycnQNzDEyg1/w9aOaWxzW0JCAiaNHyOZZT96zDiMGDWmgHtFRPmBgQwiyjc9e/aEpaUlAGDJkiX48OGDXJt3795h6dKl+dYna2trVKokPvHauFFciE26Pka69MDG3r17JbMhFNXHACDZ3rt373D37l255fHx8RgwYIDCAt+KGBoa4qeffgIA/PPPP3BzE/+4HjZsGKdgKhASHKx02cMH9/H7mt8AiPMNDx46PL+6VezY16qFel8Lhp48fgzPnz2Va7PXZRe8v44IHThoCDQ1i2/6m7yipqaGZs3F31VfQkKwc/vfCttFR0Xhzw2/S/5v0dIpP7r3w1BXE2Bay/KoYikuCHrh1Rccfa66/osiBlrq0FBT/L0tEABDHUtLHuO0Zwi+xCXLtathZYCSBloqH8faSBtDHEtL/r/tnbPUVz+qlJRkzJkxBS+ePQEA/DxgMMZ9zSGfHW9fv8KMyeORkJAAXV1drN+0VSaFCxWMqKhI3Lklnh1bsVJlVKlarYB79GPgfvFj4fE7f9lVrAzHxs0AANevnMdT9/tybcLDQuGybTMAQFNTEx26yKZj+/D2NRbOmojEhATo6Opi+brNqJxJWjDKvpTkZEyfMgnPnoq/ywYOGoJJU6cXcK/oewkEgh/2j/IX5+ASUb7R0dHBH3/8gQEDBiA0NBQNGzbE3LlzJXUmbt68idWrV0MkEqFSpUqS2RF5rWXLlnj37p0kD62imRbp96W3UVNTQ4sWLRRub/Dgwdi0aRNEIhGcnZ0xe/ZsNGvWDDo6Onj8+DE2bNiAd+/eoWnTprhz506W+jhq1Cjs3r0bISEhkscfNmxY9p5oMdG3Z1fUd3BEsxYtUaFiRWhqaiEo6DOuXb2C865nIBKJYGRkhNXr/oC2tnbmG6Qcm/PLAgwb1B+JiYkYN3oERo0ZB8cGDZGYmIgL58/h2JH/AIiDSkOGMaiUV8aMm4jr192QmJCAv7dshpeXJ7p264EyZcoiKSkJL188x7/790pmKjVo2BiNmzQr4F4XLhOblUMta0MAgOfnGFz/EI4yRjpK2wtFIgTFyAchqlkZYKhjadz3icTrkFiExqVAU12Acsa6aFXJDLam4toYzz5F45RHiMJtV7HUx5zWdvAMisXLz9Hwi0hEbFIq1NUAM30t2JcyRLPyJtDSEI9Xuv4+DF7Bsd/7EvxQfp03Gw/uiY+vDg0aolvP3vjw/p3S9pqamihnYytzX4C/H6ZMGIOYGHER97ETp8LA0FDldkxMTWFqavb9T6CIefbkMfz9/ST/R0ZmBNYC/Pxw5pRsceKuSooap7t04RxSUsSpdDgbI+u4X/x4ePzOX+OnzsErj+eIjYnBwlmT0fPnQWjQuBm0tXXw2uslDu3bidAQ8YCpoaMnyqSWCgzwxy/TxyE2JgYAMGzMJOjpG+DjB+X7hrGJKUy4b2Tb3Nkzce/ubQBAg4aN0LN3H7x791Zpe01NTdjals+v7hFRHmMgg4jyVf/+/eHt7Y2FCxciLCwMc+bMkVmup6eHI0eOYNWqVXj37h10dJRfKMotTk5O2LFjh8z/37K1tYWNjQ18fX0BiItwGxsbK9yeo6Mjli5disWLFyMyMhILFiyQazNz5kzUrFkzy4GMJk2aoHr16vDyEqcCaNOmDcqVYx5iRYRCIa5fu4rr164qXF6hYiWsWLUWlasor19CuaNatepYvW4DFsybjdjYWGz8Y71cGxtbW2zesg36+gYF0MPiobydHTb8+Rd+mTsTkRERuHn9Gm5ev6awbYOGjbD29z/yt4M/gAbljCW3a5QyxKouVVS2/xKbjOknXylcZqyriY7VLNCxmoXcMlFaGm5+CIfLw09IFSlPsaiuJkAta0NJcEWRVFEazr/6gv+efVbZ16Lo+tc6SADg/vABBvbtobJ9qVLWOHn+isx9z548RkR4Rv2TP9atyvRxR42dgNHjJ2Wvs8XAyRNH4fo1jeC3nj97gudfZwikyyyQce5rWil1dXV0dO6SK30sDrhf/Hh4/M5fZcrZ4n9rNmHZgpmICA/Df/t24r99O2XaCAQC9B86Gj8Nkq1T6PH8CSIjMtJB/v3n2kwfb9CIcRgyakLudL4YuXrlkuT2wwf30adnN5Xtra1L4/xl5enCiOjHwkAGEeW7BQsWoEWLFli/fj3u3r2LqKgoWFlZoU2bNpg1axaqVauG+fPnA4DCYtq5TTpFlKL6GOmcnJywZ88eyW1VFi1aBAcHB/z555949OgR4uLiYGlpiQYNGmDcuHFo164dXFxcstXPQYMGSV4XFvlWbtHSZbh39w48PV4i9EsI4uPjYWJiikqVq6Bt+w7o3KUbUxjlI6dWrXHkxGn8u28vbt28juDgYPEoz7Ll0K5DR/QbMAi6uroF3c0ir1HjJjhx+hxOHj+GO7dv4sOH94iJjoGGhjrMzMxRo6Y9OnbuAqdWrTlFOg+9CYnDgceBqG5lAOsS2iihq4G0NCAyIQVeQbG4+SECH8LiVW7j/KsvCIxKQrWS+ihnogtjXU2U0NGAQADEJ6ciMCoJr0Nicds7AiGx8rNCiH5kfr4+8Hj5AgDQoFETmJvLBwSJihIev/NXzdr1sP3fEzh55ADu3rqGoMBPEKakwNTcHLXrOqJ7n/6oWIXp7IiICoogLb8q6hIRZVFKSgqMjIyQkJCAX3/9FcuWLSvoLhUKAwcOxIEDB2BiYoLPnz/nS1qkuGQeIgoTdSW59Sn/iVSMlqf8Nebwi4LuAn21uXfNgu4CfaXGC5qFioin3IWGljrLhBYWXxSkYKSCUdKI6XYLC51iOty85YasZaoojG5Mb1rQXShWeBQnokLn5MmTkkLYjRo1KuDeFA6RkZE4cUKcQ3rgwIGs7UBERERERERERMUGAxlElO/ev3+vdJmPjw9mzJgBAChZsiQ6dOiQX90q1DZu3CgJ7owbN66Ae0NERERERERERJR/iumkJSIqSFWrVkXnzp3RpUsX1KhRA/r6+ggJCcG1a9fw999/IzIyEgCwbt06aGgUz68poVAIHx8fJCUl4dq1a1i5ciUAoFu3bqhRo0YB946IiIiIiIiI6Puxxg9lVfG8QkhEBSo1NRVnzpzBmTNnFC5XU1PD8uXLMWjQoHzuWeEREBCASpUqydxnZGSE9evXF1CPiIiIiIiIiIiICgYDGUSU786cOYPz58/j7t27CA4ORlhYGLS1tVG6dGk4OTlh4sSJqFmTBUPTWVpaonHjxlixYgUqVKhQ0N0hIiIiIiIiIiLKVwxkEFG+69KlC7p06VLQ3SjUbG1tkZaWVtDdICIiIiIiIiIiKnAMZBARERERERERERFRvmOJDMoqtYLuABERERERERERERERkTIMZBARERERERERERERUaHFQAYRERERERERERERERVarJFBRERERERERERERPlOwCIZlEWckUFERERERERERERERIUWAxlERERERERERERERFRoMbUUEREREREREREREeU7ZpairOKMDCIiIiIiIiIiIiIiKrQYyCAiIiIiIiIiIiIiokKLgQwiIiIiIiIiIiIiIiq0WCODiIiIiIiIiIiIiPKdGotkUBZxRgYRERERERERERERERVaDGQQEREREREREREREVGhxdRSRERERERERERERJTvmFmKsoozMoiIiIiIiIiIiIiIqNBiIIOIiIiIiIiIiIiIiAotBjKIiIiIiIiIiIiIiKjQYo0MIiIiIiIiIiIiIsp3AhbJoCzijAwiIiIiIiIiIiIiIiq0GMggIiIiIiIiIiIiIqJCi4EMIiIiIiIiIiIiIiIqtFgjg4iIiIiIiIiIiIjynRpLZFAWcUYGEREREREREREREREVWgxkEBERERERERERERFRocXUUkRERERERERERESU7wQC5pairOGMDCIiIiIiIiIiIiIiKrQYyCAiIiIiIiIiIiIiokKLgQwiIiIiIiIiIiIiIiq0WCODiIiIiIiIiIiIiPIdS2RQVjGQQURESqmr8RcFkSJq3DcKjS197Au6C/RV/YWXCroL9NWLlR0LugskJQ1pBd0F+iopRVTQXaCvLEpoFXQXiIjoB8PUUkREREREREREREREVGhxRgYRERERERERERER5TsBONudsoYzMoiIiIiIiIiIiIiIqNBiIIOIiIiIiIiIiIiIiAotBjKIiIiIiIiIiIiIiKjQYo0MIiIiIiIiIiIiIsp3aiyRQVnEGRlERERERERERERERFRoMZBBRERERERERERERESFFgMZRERERERERERERERUaLFGBhERERERERERERHlO4GARTIoazgjg4iIiIiIiIiIiIiICi0GMoiIiIiIiIiIiIiIqNBiaikiIiIiIiIiIiIiynfMLEVZxRkZRERERERERERERERUaDGQQUREREREREREREREhRYDGUREREREREREREREVGixRgYRERERERERERER5Ts1FsmgLOKMDCIiIiIiIiIiIiIiKrQYyCAiIiIiIiIiIiIiokKLgQwiIiIiIiIiIiIiIiq0WCODiIiIiIiIiIiIiPIdS2RQVnFGBhERERERERERERERFVoMZBARERERERERERERUaHF1FJERERERERERERElO8EzC1FWcQZGUREREREREREREREBSw6OhqHDh3CzJkz0bJlS1SsWBFGRkbQ0tKCpaUlnJycsGbNGoSFhWVpe3fv3sWgQYNgY2MDHR0dWFlZoUOHDjh48GC2+nXw4EG0b98eVlZW0NHRgY2NDQYNGoR79+7l5GnmiCAtLS0t3x6NiIh+KInCgu4BEZFqyUJRQXeBvqq/8FJBd4G+erGyY0F3gaSkgafchUVSCo8ZhYW2JsfVFhZqHA1faOgU07w5fXY/Kegu5NjR4fVyfZtXrlxBu3btMm1nbm6O/fv3o0OHDkrbLFmyBMuWLYNIpPj45+zsjKNHj0JHR0fpNhISEtCnTx+cO3dO4XI1NTUsWrQIixcvzrTP34tHDiIiIiIiIiIiIiKiQqBs2bIYMmQI/vzzTxw/fhz37t3DnTt38N9//6Fv375QV1dHaGgounXrhufPnyvcxj///IOlS5dCJBKhQoUK2LlzJx4+fIiTJ0+iVatWAABXV1eMGDFCZV9GjBghCWK0atUKJ0+exMOHD7Fz505UqFABIpEIS5YswbZt23L3RVCAMzKIiEgpzsggosKOMzIKD87IKDw4I6Nw4YyMwoMzMgoPzsgoPDgjo/AorjMy+rr8uDMyjgzL/RkZqampUFdXV9nm5MmT6NmzJwCgZ8+eOH78uMzy8PBw2NnZISoqCuXKlcPjx49hbm4u8xg9e/bEmTNnAADXrl2Dk5OT3OO4ubmhTZs2AICuXbvixIkTMn0LDQ1F/fr14efnB2NjY3h7e8PExCRHzzsreOQgIiIiIiIiIiIiIipgmQUxAKBHjx6oUqUKAODWrVtyy3fs2IGoqCgAwOrVq2WCGOmPsWXLFsljrV27VuHjrFu3DgCgoaEh0z6dubk5Vq9eDQCIjIzEjh07Mu3792Agg4iIiIiIiIiIiIjoB2FoaAgASExMlFt28uRJAECJEiXQq1cvheuXKVMGbdu2BQBcvXoVMTExMstjYmJw9epVAEDbtm1RpkwZhdvp1asXSpQoAQA4ceJE9p9INjCQQURERERERERERET5Tk0g+GH/CsqbN2/w7NkzAEDVqlVlliUnJ+Phw4cAgMaNG0NLS0vpdlq2bAkASEpKgru7u8yyR48eITk5WaadIlpaWmjUqJFknZSUlOw9mWwoptnXiIiIiIiIiIiIiIhyJiAgIEvtlM1myI74+Hh8+vQJZ86cwZo1ayAUiouaTps2Tabd27dvkZqaCkA+yPEt6eWvXr2SFAEHAC8vL4XtlG3n0qVLEAqFePfuHapXr56l55RdnJFBRHlqyZIlEAgEEBSzAmIuLi6S5+3j45MnjzFs2DAIBALY2trmyfaJiIiIiIiIiEixsmXLZukvp6SvLenr66Ny5cqYOXMmgoODAQDz5s3DgAEDZNaRDq5kFkCR7pu/v3+ebCc3cUYGURF0/fp1SRR18eLFWLJkScF2iKgABAZ+woH9+3Dr5nUEBQVBS1MLZcuWRfuOnfBz/4HQ1dUt6C4WG3wvClZYWBg8Xr6Ax8sX8PR4CU+Pl4iMjAQAdOveE8tWrirYDhYh4WFh8PQQv85enh7w8nyJqK+vtXO3Hliy7DeV6wd++oTundtm6zFLWVvj9PmrOe3yD8dAWx0tq1rAvqwR7MsYoaSRNkz1taCtqY6YxBS8D47DjddfcORhACLjVU9rNzPQwpCmNmhR1RzlzPSgpaGGLzFJeOQdgYP3/fHMN1Ll+upqAlSxMkCtcsawL1MCtcoZo6KlPjTUxWPFnFbewKeIhNx66kVWbGwsbt+8AU9P8X4TEhyMiIhwJCYmwbCEIezsKqJZixbo2asPjI1NCrq7RV7dmqpHXKar7+CIHS778rg3RdsrTw/cvX0Tz589wUfvD4iMCIeGhgbMLSxRq05ddO3RG3Xq1le6fmDgJ/Rybpetx7QqZY2T5658b9cJwOfPgTh57Chu3byBz58DER8XBxMTU1iXLg2HBg3RvkNHVKxUuaC7WeTxPIOKqzp16mDbtm1wdHSUWyZd68LAwEDldvT19SW3Y2Nj82Q7uYmBDCIq1pycnHDjxg20bNkS169fL+juUC65fs0NC+bNljmAJiYkwNMzCp6eHjh+7Ag2b9mGcjY2BdjL4oHvRcFr3aJJQXeh2OjQulm+P6aNTfl8f8yCVKucMf4YVEfhMjMDbZgZaKNhBVOMalkeMw++wO23oQrbtq5ugXX9asFQV1Pm/rKmeihrqoce9ayx7fpH/H7+rdK+jG9jh6ntK+X4uZCYx8sXmDdnhsJlEeHheBz+EI/dH2Lv7p1YsWotmjRtns89JMp940YMxrOnj+XuT0lJgb+fL/z9fOF6+iQ6d+mOXxYthaam8vzm2WFjW7yOGXnl4L/7sOmPDUhIiJe5Pzg4CMHBQXj65DHiYmMxe978Auph8cDzjKLjR87fkZezDwCgR48ecHBwAAAkJCTgw4cPOHz4ME6cOIH+/fvjjz/+QJcuXWTWkS7+rao+BgBoa2tLbickyA7Aya3t5CYGMoiI8sCwYcMwbNiwgu5GsfTqlRfmzpqOxMRE6OnpYeTosXBs0BCJiYm4eP4cjh09DF8fH0yaMAYHDx+Dvr7qkQWUc3wvCp9SpaxhW94O9+7eLuiuFHlWpUrB1tYO9+/dyfI6lpaWOHj0VKbtXHZtx8VzZwGIZ3oUN4ERCbj/IRyeAdH4HJWAL9FJEAgEsDLSQcdaJdG+ZkmYGmjhn+H10HvjPbz+HCOzvkN5E2waXBdaGmpISknFvjt+uPbqC+KShKhgqY9hzW1hX9YI41rbISo+GTtu+Cjsh0DqtDsxJRWvAmNgqq8JG3N9he1JOSurUnBo0BDVq9eAlVUpmFtYQCQSITg4CFcuX4TblcuIiIjA1Enjsf/gUVTJJE8zfb++P/fHT/36K12uq6uXj70pekJDQwAAFhaWaN2uA2rXrQ+rUqUgShXh5YtnOLDPBV9CgnHu7CkIhUL877e1ctuwtLDEv0cyP2bs2bUNl867AgA6d+2eu0+kGNr+z1Zs2fQnAMDG1ha9evdF9Zr2MDQ0RGRkJN688oLb1SsQqP3Il2YLP55nUGGRG7UvVDE2NoaxsbHkf0dHR/Tr1w/79u3D0KFD0b17d+zcuVPm+pOOjo7kdnqxbmWSkpIkt7+dwZRb28lNDGQQEVGRsua3FUhMTISGhgb+3r4LtevUlSxr2KgxytnYYMPva+Hr44O9LrsxfuLkAuxt0cb3onAYO34iatS0R82a9jAzN8enTwHo3L5NQXerSBo1dgKq16iJ6jXtYWZmnu1UURqampmmoUhNTcWTRw8BiKdwO7XOXiqqH93992FoufKG0uXnXwShbQ1LbB1WD1oaapjcrgIm7n0m02ZJz+rQ0lCDMFWE0bse4977cMkyz0/ROP8iCNtG1EezyuaY2qESzj4LQlBUIr711DcSC4954oVfJN4ExSJVlIbVP9szkJFNjg0a4sKV60qXd+jYGW5Xr2DG1IlISUnBP1s3Y/2fm/Ovg8WUqakp0+LkIRtbO4ybNA2t2rSHurq6zLKatWqjk3M3jBk+EH6+Prh0wRU9+/yMuvUdZNppaGqiQkXVs8JSU1Px1P0RAEBPXx8tWxWvY0Zue3D/niSI0aVbdyxauhyamrIz+xo2aowhw0ciJUX1RT/6PjzPoOJu8ODBOHv2LA4fPoxJkyahW7duMDU1BQAYGhpK2mWW5ikuLk5y+9v0Ubm1ndzEYt9ERFRkvHzxAk8euwMAevTqLfODNt2QYSNgZ1cBAPDv/r1ISVGdQ51yhu9F4TFh0hS0dGoFM3Pzgu5KkTd2wmQ0b9kKZmZ591o/vH8PX76IR/K2bttBZqRUcSBKy7zNFc8QfAgRn2w5lDeVWVazTAlUKSU+KXN9HiQTxEiXkpqGJSe8AAA6muoY2lxxSorbb0Nx6L4/vAJjkJqVjpFC317EVaR1m7awLS9OifP0iXted4koz/2+cSvatu+k9PNvbGKCKTPmSP53u3IxR4/z6IH0MaN9sTtm5CaRSISVy5YAACpXqYrF/1shF8SQllvpwEgezzOIxLp3F8+yi4uLw4ULFyT3S88SkS7YrYh0aqxvi5Ln1nZyEwMZRMXM9evXIRAIIBAIJDUhDh8+jDZt2sDCwgK6urqoUqUK5syZg/Bw+ZP7bwUEBGDixImws7ODjo4OrK2t0a1bN1y5knkROR8fH0lfXFxcVLa1tbWFQCBQmq4pMjISK1asQOPGjWFiYgJNTU1YWFigevXq6NmzJ7Zu3Yrg4GBJ+2HDhkEgEODGDfGozhs3bkj6kv5na2sr8xjp96cXT3dzc0Pfvn1RtmxZaGpqyrR3cXGRtPfx8ZHrr0gkgpubG2bNmoWmTZvC3NwcmpqaMDY2Rp06dTBr1iz4+fll9hLSN665ZXzuuvfsrbCNmpoaunxNxRITHY1HDx/kR9eKHb4XRHnD9WxGGhHnbkwRokxcUioAQEtD9nTHvoyR5PbN11+Uru8bGg/fUPHIsg72JfOgh5RdenrimS7SqQuIirL6jg0ktz8F5CwH+zmpY0bnLj2+t0vF2r27d+Dn6wsAGDZyFDQ0mOCkoPA8o+j59lrMj/RXkCwsLCS3fb9+PwFA5cqVJYHy169fq9yG9PJq1arJLKtevbrCdqq2o6GhgUqV8q6GHL95iYoxkUiEwYMHY//+/TL3v337FmvXrsWJEydw69YtWFlZKVz/1q1b6NKlC6KjoyX3ff78GWfOnMGZM2ckF/zz2qtXr9C2bVsEBgbK3B8aGorQ0FC8evUKJ0+eRGpqKiZNmpQrj7lgwQKsXLkyx+v/73//w9KlS+Xuj4qKwvPnz/H8+XNs3boV+/fvR8+ePb+nq8XK0yfioom6unqoXr2G0nYOjo6S28+ePkGTpvlfoLeo43tBlPvi4uJww+0qAMDaujTq1XfMZI3iqbyFPqpZi2ddeH+Jk1lmrJcxejY0VnXaj9DYZNiY66OsqR5KGevgc6R8einKHz4fvfH2jfgE2ba8XQH3hih/SOcjV1PLfObSt+Li4nDzmhsAoJR1abnUVJQ9ly+KRzsLBAK0aOkkuT8qKhKRkZEwNjaGkZFxwXSumOF5BpHYp0+fJLel0zlpaWmhQYMGuHfvHu7du4fk5GSlxbrTB/dqa2tLioqnc3R0hJaWFpKTk3Hjxg3MmzdP4TaSk5Nx//59yTqqZqt9LwYyiIqxhQsX4u7du+jRoweGDBkCGxsbBAcH46+//oKrqyvev3+P6dOn4+DBg3Lr+vn5SYIYampqGDNmDPr06QMjIyO8ePECq1atwpIlS+S+CPPC4MGDERgYCE1NTYwePRqdOnWClZUVRCIRAgICcP/+fZw4cUJmnRUrVmDWrFkYPnw43N3d4eDggN27d8u0UfZFf/z4cbx8+RL29vaYPn06atasiYSEBDx79izLfRYKhShVqhR69uyJxo0bS2a0+Pv74+7du9iyZQtiY2MxYMAAPHnyRC4yTop99P4AAChXrpzKUVLlpS6CpK9DuYvvBVHuu3r5IhITEwAAnbp0K/BRYIWJjqYaShrpoHV1S4x2Kg9NdfFMjD23fGTaxSenSm4b6qg+FZJeXrGkAQMZ+SwhIQEhIcG4ef0aXHbtgFAoBAAMHDy0gHtWPFy+dBGXLl7A58BPUFNTg5m5BWrXqYNuPXrCsUGjgu5esfD0cUYaNVu77Afwrl2ROmY485jxvV6+eA4AsC5dGvr6Bjjvega7dmzD+3fvJG3Si3/3GzhY6bkkfT+eZxCJHTlyRHLb3t5eZlmPHj1w7949REdH4/jx4+jXr5/c+gEBAZJsKm3atJGpiQGIa2S0adMG58+fx5UrVxAQEKCwuPnx48clA5zzeiAuAxlExdjdu3exfPlyLFiwQOb+jh07omPHjrh06RKOHj2KjRs3ykxZA4CZM2dKvqj279+P/v37S5Y5ODigb9++aN68Odzd8zaPsbe3Nx4/Fo/IWL9+vdyMiwYNGqBXr15YvXo1IiMjJfeXLl0apUuXhr6+OE2Bvr4+atasmaXHfPnyJdq0aQNXV1doa2tL7m/RokWW+z1q1CgsXrxYLlJdr149dO/eHZMnT0ajRo3w6dMnrFy5Evv27cvytourpKQkREREAAAslcwiSlfCyAi6unpISIhHUFBQfnSvWOF7QZQ3zp2RSivVlWmlejmUxuqf7ZUu/9vNG6effpa5L712BgA0sDPFxZfB364GADDV14KdRUbRbmtj5pXPD6dOHsfiX39RunzEyDHo7Nw1H3tUfHl/eC/zf7yfL/z9fHH29Cm0at0WS1f8JnfBg3KPSCTC3t3bJf+3bdcx29s4d/a05HanLt1ypV/FlUgkgs9HbwCAsbEJ1vy2Agf/lT8/8/XxwYbf18Lt6hVs2vIPDEuUyO+uFnk8zyia1BhnleHi4oJ+/fqprGu0YcMGnDt3DgBQvnx5NG/eXGb5qFGjsHLlSkRFRWHevHlo164dzMzMJMtTU1MxYcIEpKaKB/nMnj1b4ePM7sBSVgABAABJREFUmjUL58+fh1AoxMSJE3H8+HGZ+k6hoaGYO3cuAMDY2BijRo3K2ZPOItbIICrG6tevj/nz58vdLxAIMGPGDADimQP37t2TWR4UFCSZ4dClSxeZIEY6Q0NDbNu2LQ96LUv6B4mqQIJAIICJiUmuPKaamhp27NghE8TILltbW5XT7cqUKSM5kJw+fRppaSwimpm4uIz0IXp6epm219XTBQDEx8fnWZ+KK74XRLkv6HMgnjx+BACoVacuypZTXICaAK9P0ej15138fv6t3DL3jxGIjBena+nlUBo25oq/o6Z3rAQN9YxTJX1tjv8qSFWqVsP+g0cwZfpMjirPYzq6uujQqTMWLlmGXXv/xaGjJ7B1206MGjMOxsbGAMT56adPnsDiuXno4P498PJ4CQBwat0OVVWkz1Ek6HMgnn49ZtjX5jHje8XGxEAkEgEA3r97i4P/7oO5hQVWrFqLG3ce4J77M+xw2Qf72rUBAM+fPcWShQtUbZJyiOcZVBwsWbIEpUuXxpgxY7B3717cuXMHz58/x+3bt7F161Y0a9ZMcs1OS0sL27ZtkwkuAICpqSlWr14NQFw/o2HDhti9ezfc3d1x+vRptGvXDmfOnAEA9O/fH05OTgr70rp1a8lsjvT1Tp8+DXd3d+zevRuNGjWS1HddvXp1rl13U4a/yImKsQEDBig9Gaxfv77ktre3t8yya9euSaK2w4cPV7r9Bg0aoEaNGvD09MyF3ipWqlQpyW0XFxesX78+zx4rXdOmTeUKgX+v6OhohIWFIT4+XhK0SP9hFh0djY8fP8IuB1PKlQkICMhSO3Mr+WmDhVWyVPHPrORk1NIUT/dOSmSqkNzG94Io9513PSM5Pjh34WwMALjsEYyX/lEAxKmlypnpoXNtK7S3t8KGgbWx4vRrXHslW9A7MUWErVe98UvXqjDQ0cC/4xpg7bm3uPH6C+KShLCz1MdoJzt0r2eNZKFIUixcR5Pjv/JDq9ZtUeOEeIZsYmIiAvz9ceniebhdvYxf5szE7Lnz0cKpVQH3smi7dPWGwlHkjZo0Rb8BgzBp/Bi8fuWFx+6PcOS/gxgwaEgB9LJoe+L+CFs2bQAAmJiaYc6CRdnexoVzGceMzpyN8d0SEhIkt5OSkqCjq4vtu/bI1Oyp7+CIbTv3YOjAfnj75jXcrl7GyxfPYV+rdkF0ucjieQYVF+Hh4di+fTu2b9+utE2ZMmWwa9cutG3bVuHysWPHIjAwEMuWLcOHDx8wYsQIuTadO3fGrl27VPZl165diI6Oxrlz53Dt2jVcu3ZNZrmamhoWLlyIMWPGZOGZfR8GMoiKsapVqypdZmpqKrkdExMjs+zly5eS246OqguNNmjQIE8DGelT6G7duoUNGzbg4sWL6N27N5ycnNCoUaMsjdLIrlq1auXKdnx9fbFu3TqcOXMGvr6+KtuGhobmaiCjbNmyWWqXkPLjzATRkpohk5URgskp4hG52iqma1LO8L0gyn3pKUK0tLTQrkOnAu5N4RCTKERMYkaqqJcB0XB9HoTu9ayx5md7bB1WD/OPeOC4+yeZ9Xbd9IGdpT5+blgWJY10sK6//HHdPywep58GYmLbigCAuKRUuTaU+0qUKIESUhfRa9rXQsfOzjh7+iQWLpiHaVMmYPH/VqB7j14F2MuiTVUqHDNzc6xd/yd6du0MoTAFhw78y0BGLvP+8A7zZk5GqlAIbW1trFyzAaamZpmv+I3zruJRtlpaWmjLY8Z30/pmJn7PXn1kghjpdHR0MGnKNEyZOA4AcPHCOQYychnPM6g4uHjxIlxdXXHnzh28f/8ewcHBCAsLg66uLiwtLVGnTh106dIFP/30U6bXvJYuXYoOHTrgr7/+wq1btxAcHAxjY2PUrl0bw4cPV5hh5Vu6urpwdXXFgQMH4OLigufPnyMyMhIlS5ZE8+bNMWnSJDRu3Di3nr5KDGQQFWOqvvDU1DJGHqbPvkgXHh4uuW1paanyMUqWLJnD3mXdwYMH0bdvX9y7dw9eXl7w8vLCsmXLoKmpiUaNGmHAgAEYNmyYyvyC2ZEbU+XOnz+PPn36ZHmKq/QoIFIsvd4JkLWpwwnx4tc0L4JdxR3fC6Lc5fnyhSQ3dwun1sy5nYlTTwLRqpoFnOuUwqIe1XDVMwRRCbIXO3496onbb0IxrIUtapc1kqSRik5Iwakngdhw4R36N8oI+n+7PuWvLt164OaN67h08TxWrVgGp1atYWRkXNDdKpbKlC2LRo2b4PatG/D380VISDAsLfP+935xEPgpAFPHj0Z0dDTU1dWx7Ld1qFvfIdvb8fR4Ad+vx4zmLVvB0JDHjO8l/dsWABo3aaq0bYNGjaGhoQGhUAgvD4+87lqxw/OMoolpI2VVqVIFVapUkaSP+l5NmjRBkyZNvns7AwYMwIABA3KhRznHQAYRfZfCcMApXbo07t69i6tXr+L48eO4ceMGvLy8kJKSglu3buHWrVtYt24dzp07h8qVK3/3432bezC7QkNDMWDAAMTHx8PAwACzZs1Chw4dUKFCBRgZGUFLSzz91c3NDW3atAGAXK+R4e/vn6vbKwy0tbVhbGyMyMhIhGRSzC06KgoJCeIfvlaZFImj7ON7QZS7XKWKfDNFSNZc9QyBc51S0NfWQIsq5jjz7LNcmwsvg3HhZTB0NNVgYagNoSgNwVGJEH095ErXz3gXFCu3PuUvp9ZtcOnieSQkxOPO7Vss+l2A7CpUwO1bNwAAX4JDGMjIBV9CQjB53Eh8+RICgUCABYuXo0WrNjna1nmZIt9MRZgbtLS0YGJqioivA/pKWpVS2lb8O9gEoaFfEBERrrQd5QzPM4iKNwYyiCjbpGckBAcHq0xTFBwcrHSZ9KyP9OJpykgX9VKmTZs2kgv/YWFhuHLlCrZt2wY3Nzd8+PABP//8M54+fZrpdvLa0aNHERkZCQA4ceKE0nyG0jNfcluZMlmrfZEozLMu5Am7ChXx5LE7/Pz8IBQKoaGh+DD38WNG3ZfydhXyq3vFCt8LotwhTEnBpYvnAACmpmZo3LR5AffoxxAelyy5bW2iq7JtYooI/uHyMx9rljH6ujwVbz7HyC2n/GVikpH29HNgYAH2hArDQKaiJDIiAlPGj8SnAPFAo5lzF6Bz15wFIIQpKbj89ZhhYmqGRk2a5Vo/i7sKFSrCPfwhAEAkUp1uMPXrcnV1XnLLCzzPICq+WLWOiLLN3t5ecvvRo0cq26pabmhoKLkdERGhtF14eDjCwsKy0UPAzMwMP//8M65evYpu3cSjV589e4Z3797JtCuIE7H0miGmpqZKgxgA4O7unl9dKjLq1hMXqU9IiIeXl/LaLO5Sn8s6devleb+KI74XRLnj9q0biPoa/O7Q2VnpyTrJKmmUkU4yPjn7UflyZnqoXlqcjuWyRzCEoh+nZlRRFRKSMTiGKUIKlveH95LbFpmkmSXVYmNiMHXiaHz0/gAAmDBlBvr8nPO0HXduSx0zOvGYkZvqSaX5CghQPrs9NjYWkV/PbTNLw0w5w/OMokcg+HH/KH8xkEFE2daqVStJeqU9e/Yobffo0SN4qMgLamJiAmNjYwCqL9ofOnTou1Irpc/SAMRpnaSl181ISkrK8fazSygUX1BJTExUOhMlPj4e+/bty7c+FRWtWmcEhk6dOKawjUgkwtnTJwGIC1o6NmiYH10rdvheEOUO6bRSzl17FFxHfjCdamWkunnzOftpoaZ1qCi5vf+uX670ib7P5YsXJLcrVvr+VKGUM58CAnD/3l0AQNmy5WCZD/XwiqrEhATMmDIeb155AQCGjRqLIcNHfdc2z0mllcrprA5SrE27DpLb165cUdrO7eplyblrTmqcUOZ4nkFUfDGQQUTZVqpUKXTvLv5hfPr0aRw+fFiuTWxsLMaOHZvptlq0aAEAOHXqFD58+CC3/M2bN1i4cKHS9Z89e4Znz54pXZ6WloYrX39oCgQC2NrayiwvVUqc39Tb2zvX61AoU6lSJQDiYIWi1y41NRWjRo1CINMmZJt9rVqS0VInjx/D82fyqcT2uuyC99dRbwMHDYGmpma+9rG44HtB9P2ioiJx52se+oqVKqNK1WoF3KOC18uhNLQ0VJ/CDGtuA6dq4lGw/mHxcP8om6pRX1sdelrK612NcSqPrnWtAQDH3T/hiU/k93WaVDr1f/buO6ypsw0D+B32cLBlqAzFhago7oVbUdzSuvfAWa1d7tHhp1VbtVqrdaBW6164J86qDAXcyEbZoOwA+f5AAkgIQ0gC3L/r4rpOct68eZLDOSfJc973OXm8yAtK9rnuEddkMKtdO9+V0VR2bt64Jr7gRpKY6GgsnD8XQqEQADDiy5GyCq3SEQrT8d3Xc/HE2xMA8MWosZgxa95n9ZmQEI+7H/eTetYN0KAhzxllqUHDhujYOfu764Xzbvjv/r0CbaKjo7B10+8AAFVVVQwaPFSmMVYV/J5BVHVxnCERlcr69etx+fJlfPjwAaNGjcLNmzcxfPhw1KhRA0+ePMGaNWvw8uVL2NvbSx1tMXPmTJw+fRopKSlwcHDAihUrYGdnh8TERFy9ehW///47DA0NoaysjKioqAKP9/b2xsSJE9G6dWs4OTmhZcuWMDY2hlAoREBAAHbv3o3Lly8DAAYOHChOXOTo0KEDdu/ejcjISCxYsABjxoxBzZrZc2KrqqrC3Ny8DN+1bM7Ozli0aBHS0tIwceJEeHt7o1evXqhZsyb8/PywefNmeHh4oGPHjrhz506ZP39l9+0PizFhzEikpqZixtRJmDJtBlq3aYvU1FRcOH8Ox478CwAwt7DAuAkT5Rxt5cZtoRg8PR4hJDj3ivL4+Nyp/IKDg3DqxPF87QcN4Zfu0vL29EBIiOT3OjQ4GGdOncjX3mnQEKn9XbpwTvyDIUdjZJvTqz6+H9AQF30i4BEYh+CYZCSnZUJbXRkNTarDyc4U9pbZtbzSM7Kw5JgfPp0VytJQG3umtsYFn3e4+yoGITHJUFYSwMqoGobZm6FNvexaDD4hCVh96lmhsWipKaNvs/zFQ+vq50551LdZLcQlCcW3n4W/x7Nw1tr41J9bt2DDuv+hR6/esLNrhdp16kBLSxvJyYl49fIlzrmdgbdX9o+9qqqqWLp8tXhkMJWt//38IzIyMtCjZ280a9ECpqZm0NDQQFxcHDwePsDRI/+Kp8yxa9kKX4wcLeeIK66l33+D/+5lf863b9MWToOHwf/1q0Lbq6qqoq65hdQ+L188Lz5nOLLId7n45rsf8OSxNz68f495s2Zg1Jhx6NSlK9TV1eHn64NdO/5CRER2AeqZc+ZxxFI54vcMoqqJiQwiKhULCwucPn0aAwcOxIcPH7B161Zs3bo1X5tly5ZBIBBITWT06dMHc+fOxaZNmxAaGoopU/IPp65bty5Onz6Nfv36SY3n4cOHUutxdOjQAX///XeB+7/88kv88ssvePPmDX777Tf89ttv4nXm5uYIDAyU+rylUbt2bWzbtg1TpkxBamoq/ve//+F///tfvjZffPEFpk6dKrWGBknWuHET/O/XjVj8/TdITEzEpt82FGhjbmGBLVv/grZ2NTlEWHVwWyiGE8eO4vQnP6Dn8PbyFP9AmIOJjNI7eeIo3D5OY/Cpx96eeOyd/70uKpFx7uO0UsrKyujbf0CZxFgZ6Gqr4ct2dfBluzqFtnkbn4IfDvvi7ivJNbZqaqnii7Z18EVbyX1c9YvEt/8+QWJq4Ven62qr4X9f2Ba6/vsBjfLd3nTpNRMZhUhIiMfxo4dx/GjBkao5atUyxorVP6Nd+w4yjKzqiYqMxKF/9uPQP/sLbdOjV28sX/kj1NTUZBhZ5XLj2mXx8qMH/2GM82Cp7Y1NTHHyXOHTGQHA+bN5zhmOPGeUB3MLS/y+ZRu+mT8PMTHR2P33Duz+e0e+NgKBAJOnzcCESZ83TRhJx+8ZlYs8apdSxVSmiQxXV9ey7E5s3Lhx5dIvEX0eBwcH+Pn54ZdffsG5c+fw9u1b6Orqwt7eHnPmzEGfPn2wYsWKIvv5/fff0a5dO/z555/w9vaGUChE3bp1MWTIECxcuBD6+vqFPnbkyJGoVasWLl++jIcPHyIsLAwRERHIyMiAkZERWrZsiS+++AJffvkllJQKTkVRrVo13L17F7/88gsuXbqEoKAgJCcnf87bUiwTJ05Ew4YNsW7dOty5cwfx8fEwMDBA8+bNMXHiRDg7O+PGjRvlHkdl5dCtO46cOI0D+1xxy/0GIiIisq9kq1MXvfr0xZejxkBTU1PeYVYJ3BZEpRMcFAhfnycAgDbtOsDAwFDOESmGSTsfwaGxIVpZ6KCuvhYMqqtDR0sVacIsxCSm4Vn4B1x/FoVzj98iVSi5DlVAVBJWnHiKDvX10MC4Ogyqq0NJCYj+kA6PwDic9gzH7ZeSEyBU9rZt34lb7jfh7eWJkOAgxMTEICEhHurq6tDT00fDRo3RuasDevfpx/NFOVv10xp4PHqIJ4+9ERYagvi4OCQlJUFTSwvGtYzRrIUdnAYNRvMWdvIOlT4RHBQIv4/njNZt20Of54xyY9eyFY6eOoNDB/bj+rWrCA8LhVAohIGhIezt2+DL0WPQqHETeYdZJfB7BlHVIxCV4aTwSkpKZZ5FEwgEUufpJCKi8iPlQlQiIoWQniH5x2qSvVZLL8k7BProyc995R0C5SGCbOqwUdHSCklwkuypq7Jkq6JQ4tXwCkOjis6bM+6fJ/IOodRcRzWTdwhVSpnvIrIqlktERERERERERERERJVfmSYyAgICyrI7IiIiIiIiIiIiIqqklDgoiIqpTBMZ5ubmZdkdERERERERERERERFVcZyUkIiIiIiIiIiIiIiIFFYVLSNDRERERERERERERPIkYMF5KiaOyCAiIiIiIiIiIiIiIoUl8xEZ/v7+OH36NB4/fozo6GikpKRAJBIV2l4gEODq1asyjJCIiIiIiIiIiIiIiBSFzBIZycnJmDVrFvbt21cgcSESiQoMI8ppw+FFRERERERERERERERVl0wSGSKRCEOGDMGVK1cgEolgYGCA2rVrw9vbGwKBAJ07d0ZsbCxevHiBjIwMCAQCNGzYEMbGxrIIj4iIiIiIiIiIiIhkjJewU3HJpEbGkSNHcPnyZQDA8uXL8e7dO7i6uorX37x5Ez4+PoiLi8OGDRugra2N2NhYrF69GtevX5dFiEREREREREREREREpIBkksj4559/AADt27fH8uXLoaSkJHHKKG1tbXz11Ve4evUqPnz4gKFDhyI8PFwWIRIRERERERERERERkQKSSSLj0aNHEAgEmDp1arHat27dGi4uLoiOjsamTZvKOToiIiIiIiIiIiIikjUlgaDC/pFsySSRER0dDQCwsrIS36eqqipeTklJKfCY/v37AwDOnj1bztEREREREREREREREZGikkkiQ0Ulu6Z49erVxfflXX737l2Bx9SsWRMAEBISUs7RERERERERERERERGRopJJIsPU1BQAEBUVJb7P2NgYmpqaAABPT88Cj3n16hUAICMjQwYREhERERERERERERGRIpJJIqN58+YAAB8fH/F9AoEAbdu2BQBs3bo1X3uhUIgNGzYAAKytrWURIhERERERERERERHJkEBQcf9ItmSSyOjevTtEIhEuXLiQ7/5JkyZBJBLhxo0bcHBwwB9//IG1a9eiTZs24gLhzs7OsgiRiIiIiIiIiIiIiIgUkEAkEonK+0nevXsHMzMzKCkp4cWLF/mKfjs6OuLChQsQfJLGEolEsLOzw507d6ChoVHeIRIRkQSpnN2PiBRcekaWvEOgj1otvSTvEOijJz/3lXcIlIcI5f6Vm4opTchzhqJQV5XJdbVUDEq8rFxhaKjIOwL5mHrYV94hlNoO56byDqFKkcmZw9jYGEKhEKmpqfmSGABw4sQJLF68GLVq1YJIJIJIJELNmjUxa9YsXL9+nUkMIiIiIiIiIiIiIqIqTGa5PiUlyTkTdXV1rF69GqtXr0ZsbCwyMjJgaGhYYIQGEREREREREREREVUe/A2YikuhBi3p6enJOwQiIiIiIiIiIiIiIlIgnJSQiIiIiIiIiIiIiIgUlkKNyCAiIiIiIiIiIiKiqoEzS1FxySSR0b1791I/ViAQ4OrVq2UYDRERERERERERERERVRQySWTcuHEDAoEAIpGo0DafFnbJacuCL0REREREREREREREVZdMEhldunQpMiGRlJSE169fIz4+HgKBAA0aNICJiYkswiMiIiIiIiIiIiIiIgUlsxEZxXXu3DnMnTsXsbGx+Pvvv9GxY8fyC4yIiIiIiIiIiIiI5EKJs/FQMSnJO4BPOTo64vbt21BRUcGQIUMQFhYm75CIiIiIiIiIiIiIiEhOFC6RAQDGxsaYP38+oqOjsXbtWnmHQ0REREREREREREREcqKQiQwA6NSpEwDAzc1NzpEQERERERERERERUVkTCCruH8mWwiYy1NTUAADh4eFyjoSIiIiIiIiIiIiIiORFYRMZt2/fBgBoaWnJORIiIiIiIiIiIiIiIpIXhUxk3Lt3D6tWrYJAIECbNm3kHQ4REREREREREREREcmJiiyeZNWqVUW2ycrKQlxcHB49eoT//vsPWVlZEAgEmD9/vgwiJCIiIiIiIiIiIiJZErDYBBWTTBIZK1asKNE/pUgkgoqKCtauXYtevXqVY2RERERERERERERERKTIZJLIALKTE9IIBAJUr14dlpaW6Nq1K6ZNm4YmTZrIKDoiIiIiIiIiIiIiIlJEMklkZGVlyeJpiIiIiIiIiIiIiIiokpHZiAwiIqp4hJlMRCsSJc4dqjC4LRSHihK3haLw+aWvvEOgj/68FyDvECiPaW0t5B0CfaShqizvEOijVGGmvEOgj7hfkLwpyTsAqjD4v0JERERERERERERERApLJokMJSUlqKio4OnTp8V+jL+/v/hxRERERERERERERERUNSlMse+yfhwRERERERERERERKS4Bp+2lYlL4qaX4z0xEREREREREREREVHUpbCIjOjoaAKCtrS3nSIiIiIiIiIiIiIiISF5kmsgo7uiKpKQkbN68GQBQr1698gyJiIiIiIiIiIiIiIgUWLnUyLCyspJ4f+/evaGqqir1sWlpaYiMjERWVhYEAgGcnJzKI0QiIiIiIiIiIiIikiMlVhWgYiqXREZgYGCB+0QiEcLCwkrUT7t27fDtt9+WUVRERERERERERERERFTRlEsiY/z48flu7927FwKBAAMHDoSOjk6hjxMIBNDQ0ICJiQk6dOiA7t27s9g3EREREREREREREVEVVi6JjN27d+e7vXfvXgDATz/9hCZNmpTHUxIRERERERERERERUSVULomMTy1fvhwAYGRkJIunIyIiIiIiIiIiIiIFxxoZVFwyTWQQERERERERERERERGVhJK8AyAiIiIiIiIiIiIiIiqMTBIZd+/ehbKyMjQ1NREWFlZk+7CwMGhoaEBFRQUeHh4yiJCIiIiIiIiIiIiIZEkgEFTYP5ItmSQyDh06BJFIhAEDBsDMzKzI9mZmZnByckJWVhb++ecfGURIRERERERERERERESKSCaJjNu3b0MgEKBfv37Ffkz//v0BAO7u7uUVFhERERERERERERERKTiZJDL8/f0BAE2aNCn2Yxo1agQAeP36dbnEREREREREREREREREik9FFk+SmpoKANDQ0Cj2Y9TV1QEASUlJ5RITEREREREREREREcmPEktNUDHJZESGnp4eACA4OLjYjwkNDQUA6OjolEdIRERERERERERERERUAcgkkZEzpdTp06eL/ZiTJ08CABo2bFgeIRERERERERERERERUQUgk0SGo6MjRCIRXF1dcevWrSLbu7u7Y9++fRAIBBgwYIAMIiQiIiIiIiIiIiIiWRIIKu4fyZZMEhnTp0+HgYEBMjMz4ejoiC1btojrZuSVmpqKTZs2oX///sjIyICuri5cXFxkESIRERERERERERERESkgmRT7rlatGv755x84OjoiOTkZ8+bNw6JFi9CqVSuYmJgAAN6+fYtHjx4hOTkZIpEIKioqOHjwIGrUqCGLEImIiIiIiIiIiIiISAHJJJEBAD179sTFixcxduxYhIeHIzExEe7u7vnaiEQiAICZmRn27dsHBwcHWYVHREREREREREREREQKSGaJDADo1q0b/P394erqirNnz8LLywvR0dEAAAMDA7Rs2RJOTk4YM2YM1NXVZRkaEREREREREREREcmQEotNUDHJNJEBAOrq6pg6dSqmTp1aZFsvLy+4urpi48aNMoiMiIiIiIiIiIiIiIgUjUyKfZfE27dvsW7dOjRr1gz29vbYtGmTvEMiIiIiIiIiIiIiIiI5kfmIDElSUlJw/PhxuLq64tq1a8jKygKQXTNDwOFFRERERERERERERERVllwTGdevX4erqyuOHz+OxMREALkFv01MTDBkyBAMGzZMniESERERERERERERUTlQuOmCSGHJPJHx/PlzuLq64sCBAwgNDQWQm7yoXbs2hg0bhuHDh6NDhw4cjUFEREREREREREREVMXJJJERExODgwcPwtXVFR4eHgBykxc6OjqIj4+HQCDAr7/+CmdnZ1mEREREREREREREREREFUC5JTKEQiHOnDkDV1dXXLhwAUKhUJy8UFNTg6OjI8aMGYP+/ftDU1OzvMIgIiIiIiIiIiIiIgXECXmouMo8kXH//n24urri8OHDiIuLA5BbtLtjx44YM2YMnJ2doaurW9ZPTURERERERERERERElUyZJzJyalvkjL5o2LAhxowZg9GjR8PCwqKsn46I5OzGjRvo1q2bxHWampowNDSEnZ0dnJ2d4ezsDBUVmZfmoUoiNiYGfr5P4Ofrg6e+vvDz80FCfDwAYMDAwVjx4y+l7js1JQVfDB2IsLDs2k0mpqY4c+FqWYRdKcXGxMDX9wn8fHyyt4efD+I/bgungYOx8qc1Jervzi13HD96GH6+PoiLi4Wurh5smtpi6HBndOzcpRxeAeX4bcM67Nm1U3x7xy5XtG7TVo4RVW5TJo6Fx6OHJXrMjl17Yd+a26Q8xMTEwNfnCXx9ss8tfr65x7KBg4Zg9c8lO5ZVRZGBLxHs8xBvX/kh7m0wUj4kQElZGdo6+jCu1wSNO/eBiXVTqX0I01IR4vsIIU+9EBX0EgmRb5GRlgJVDS3o1DJDHZtWsHHoD62aesWKSZiWCt9rZ+Dv4Y73kW+RmSFENT1DmNu2gW3PQaiuX6ssXnqlxeNUxcDzd9l65ueLu7fd8djbEwFv/BEfFwsVFRUYGBqhWQs7OA0ehhZ2raT2EfDGH48e3MczP1+8fv0ScbGxSIiPg5KSMvT09dHYpin69O2Pzg7dWQ/2MyQmJuK2+034+fngqZ8vIiMiEBcXi9TUNFSvUR1WVvXRqUsXDBk6HDo6vICaqLIpt18Uq1evjk2bNmH8+PHl9RREpOBSUlIQHByM4OBgnDp1Cr/99htOnz4NY2NjeYeGPXv2YOLEiQCAgIAAJlorgN7dOpVb33/+sVmcxKCi9XToWCb9ZGVl4ceVy3Dy+NF890dGRiDyWgSuX7uCIcNGYPGylVBSUiqT56Rcz58/w37XPfIOg6RQUlJC3boW8g6j0urepYO8Q6jQTv5vId6+8i1wf1aGEAkRYUiICMOLu5fRoH1POIyfB2UV1QJtY0Le4MSaryFMSymwLi3pAyLePEfEm+d4cuUEuo6dh/ptukqNKSEiHG6bliIhIizf/fHvQhH/LhTPbl9AjynfwaI5f/AtKzxOyR7P32VrxqSx8PbyKHC/UChESHAQQoKD4Hb6JBwHDMIPy1ZCVVVNYj97/t6Oi+fOSlwXHhaK8LBQXL10AXatWmPNr7+jpo5OWb6MKsPX5wm+/3aBxHVxsbHwiH0Aj0cP4Lr7b/y0Zh06dOws4wiJqDyVSyJDJBIhMTERkyZNwu+//44xY8Zg5MiRMDExKY+nIyIF4eLigpkzZ4pvJyYm4tGjR1i/fj0CAwPx8OFDDBo0CPfv3+dVKPRZjE1MYGFphft373x2X8+fPcXBA65QV1eHiooKkpKSyiDCqsPYxBQWlpal2hZ/bNooTmI0atwE4ydORu06dREaEoy9u//G82dPceLYEejo6mLOPMlfWKh0srKysHrFUmRkZEBPTx+xsTHyDqlKWLn6F6SkJEtt88bfH999Mx8A0KZtOxjV4tXjsmBiYgoLSyvcu3tb3qFUGEnx2ccNbR19WNl3hol1U1TXM0RWVhYi/J/h8eXjSIqLxst7V5CVmYFe074v0Ed6arI4iWFcvwnMm7WFkYU11KvVQOqHBLzxvINn7heQnpKMKzv/B1VNLZjbtpYYT3pqcr4kRuMu/VC/dVeoqKkh7PkTeJ37F+kpybi8/WcM+X4DDOrWK6d3pmLjcUqx8fxd9qKjIwEAhoZG6N6rD5rbtYKxiQmyMrPg88Qb/+zbg6jICJw7ewoZGRlY9cs6if0oK6vAxrYZmjW3Qz3rBtDXN4Curh7ev09AUGAATh47DP/Xr+Dl8RAL583E9t37eaFOKRkbm8C+TVs0aWIDY2MTGBh+PPdEvMOVyxdx7cplxMXFYd5sF+w/eBQNGzWSd8hUBCX+PkTFVOaJjBs3bmDPnj04duwYPnz4AG9vbzx+/BjfffcdHBwcMHbsWAwdOhTVqlUr66cmIjkzMjJC06b5pw9o164dRo8ejTZt2uD169d48OABzp49CycnJzlFSRXV1Okz0aRpUzRpagt9fQOEh4VhYL+en9VnZmYmflq5DJmZmZg6YyZOnTjGREYxTJ0xEzZNbWFjYwt9AwOEh4ViQN+SbYugwADs27sbANDEpil27tkPDQ0NAIBNU1t0ceiOqRPH4qmfL/bt2YVBQ4ahbl3zMn8tVdU/B1zh5+sDS0srdOvRC7t2bpd3SFWCWe3aRbZxO3NavDzAaXA5RkPTXWbBpqktmjbNPpaFhYXCsXcPeYdVYeia1EHboRNh1aojlJSU860zrtcYDdr3wIk1C5AQEYbXD27AxqE/TBvY5msnECihnn0X2A8cDT3Tgsf4OjatULdpa1zYugqirCzc/mcr6v68S+IFMd4XjoqTGO2GT4Zd3xF54mkCs4bNcGrdN8hIT8OdQ39i0LeSf4ys6nicUmw8f5c9cwsrzJj9Fbr16A1l5fzHsqbNmqNf/4GYNnE0goMCcemCG4YM/wJ2rewL9LNo2apCp1Fu064Dho74Eou/XYAb1y7D54k3brvfQBeH7uXymiqz1m3a4sKVG4Wu79PXEdeuXsGCebMgFAqxfdsWbPh9i+wCJKJyVebp3y5dumDXrl2IiIjAgQMH0KdPHygpKSEzMxPXrl3DxIkTYWxsjJEjR+LcuXPIzMws6xCISMHo6urihx9+EN++cOGCHKOhimr6rDno3LUb9PUNyqzPgwf24dlTP5hbWGL8pCll1m9l5zJrLrp07QZ9g9Jvi3/2uyIjIwMA8O0PS8RJjByampr49oclAICMjAwccN1b+oApn7dvw7F18+8AgMXLVkJVteB0LyQfWVlZOOd2BgCgpaWF7j17yTmiym3m7Lno6vB5x7KqzHHuKtRv3aVAEiOHZvWa6OA8TXz7zaNbBdoY12+C3jMWSUxi5LC0aw+rltlTGr6Peovo4NcF2mRmZMDn6ikAgK5JXbToPUziczXq1AcAEP7SB5EBL6S8OioMj1Pyw/N3+Vi/aRt69u5XIImRQ0dXF3MXfCu+fe3KRYntiqoFqaysjDHjJ4lvP5YwnRUVrbDtlFf3Hj1hYWkJAPDyfFTeIRGRDJXbODYNDQ2MHDkS58+fR0hICNauXQtbW1uIRCIkJyfj8OHDcHJy4nRTRFVEmzZtxMtBQUEAgKSkJPz777+YMmUKWrRogZo1a0JVVRWGhobo2rUrfv31VyQmJhbZ94kTJzB48GDUrl0b6urqqF69OqysrNC5c2csXboUDx48ELe9ceMGBAKBuD4GAFhaWkIgEOT7u3Hjhni9g4MDBAIBHBwcpMaxYsUK8eMlyVm3YsUKAMC1a9cwYsQI1KlTB6qqqhLrdLx79w6LFy+Gvb099PT0oK6ujjp16sDZ2RlXrlwp8r2hwr0ND8P2PzYDAH5YuqLQ+W6p7IlEIty4nl1M3cLSCs2at5DYrlnzFrCwyP4ScvP6VYhEIlmFWKn98uMqJCcnw2nQENi3blP0A0hmHty/h8jICABAz159oKmpKeeIiD6PWaPm4uWEqLel76dhM/Hyewn9hL94jPSU7BGVDTv0hKCQ6Voadsj90f2N191Sx1OV8TglPzx/y0+rPO93WGhIqfvR0tYSL6elpX1WTCSdlpY2AL7PFYVAUHH/SLbKrdh3XsbGxli4cCEWLlyIx48fY+/evTh48CAiIiIQHR0t/tFvwYIFuHPnDoYPH47OnVmQh6gyyXvFUM5IrP79++PmzZsF2kZHR8Pd3R3u7u7YunUrzp07h0YS5rXMzMzEyJEjceTIkXz3p6enIzExEQEBAbh9+zbOnz+PR48U60qMxYsX4+eff5ba5sCBA5g+fXqBqY5CQ0Nx5MgRHDlyBJMnT8aff/5Z5BVAVNCan1YhJSUZjgMG8sugjIWFhiIqMns+4lb2kuc6z9HSvjUCAwMQGRmB8LCwYk15QYW7eOEc3G9eR82aOliw8NuiH0AydfbMKfHygIGD5BgJUdnIFArFy4UlF4rVT0aefgQF+3n7yk+8bPLJ9FV5GVk0gIqaOjLS0/Du9dNSx1OV8TglHzx/y1d6erp4ubBRaMVx+cJ58bKFpdVnxUSFCwx4g5cvngPg+0xU2cj8l6/mzZtjw4YNWLduHS5evAhXV1ecPn0aqampCA8Px5YtW7BlyxYYGRlhyJAhGDZsGHr04Fy1RBWdj4+PeNnU1BRA9nQxtra2GDhwIOzt7WFqagqRSISgoCCcOHEChw8fRkBAAAYPHgxvb+8CU89s27ZNnMTo1KkTpkyZgnr16kFbWxsxMTF48uQJLly4gISEBPFjWrduDR8fH5w6dQpLlmRPW3Px4kVxTDksPw5FLQ/Hjx+Hj48PbG1tMX/+fDRt2hQpKSnw9vYWtzl8+DDGjh0LkUgEKysrzJ49G02aNIGhoSECAwPx999/49y5c/j7779Ro0YNbNiwodzirYwunnfDnVvuqFGjJuYv/E7e4VQ5b97kTgtS1JeLvOsD3vgzkfEZ3r9/j3VrshOo8+YvhK6unpwjorySk5Nw7Wr2SDsTU1PYt24r54iIPl/4yyfiZV2Tup/RT+7nSEn9xL0NzrO+TqH9KCkro6aRKWJCA/I9hoqHxyn54Plb/rw8ci+Ks7Aq2Q/j8XFxCAkOwukTR3H29AkAgI6OLvo4DijTGKu6lJQUREZGwP3GdezZtVM8he3osePlHBkRlSW5XcKrrKwMR0dHODo64v379/j333+xb98+3LlzByKRCBEREdi+fTv++usv8QGIiCqmjIwMrF+/Xnw7Z4qm3bt3w9raukD7tm3bwtnZGZMnT0afPn3w4sULHDhwAJMnT87X7vDhw+L2169fLzAqoWfPnliwYAFiY2PF92lra6Np06b5Rmg0aNBA4rRO5cXHxwc9evSAm5sb1NXVxfd36dIFQPaIlGnTpkEkEmHSpEnYvn17vtfWsmVLDB06VDyq4/fff8f06dPRsGFDmb2Giuz9+wRsWLsGADD7qwXQ1eOXQVmLjIgQL9eqVUtqW2NjY/Hyu3eln5aEgN82rEN0dBRa2LXEkGHD5R0OfeLK5UtISUkGAPTvP7DQaQqJKgpRVha8zh8W365v36VU/USHvEHQk+xpQvXMLKBrWjCRkRgXDQBQUdeAulY1qf1V0zNETGgAUj8kIFOYDmVOLVlsPE7JB8/f8pWVlQXX3TvEt3v26lvkY1ymjIeXx0OJ63R0dLFmwyZUr16jzGKsqk6dPI7lS34odP2kydPg2N9JhhERUXkrtxoZJVGjRg1MnToV7u7u8Pf3x/Lly1GvXj2IRCLOh01UgSUlJeHmzZvo1asX7t+/DwAwNzeHs7MzAEhMYuTVs2dPDBw4EABw8uTJAuvfvXsHAOjQoYPUqZX0FOyHaiUlJezcuTNfEiOvbdu2ISEhAWZmZti6dWuhr23lypUwMzPL/nDt6lqeIVcqv69fh5iYaDRr3gJDho2QdzhVUt7p0jQ/zl9bGE3N3LmEc348oZLz9HiEE8eOQEVFBUuWreSPTwrIjdO1UCXz+PIJcUFty5YdYWgh/XOfJJnCdNzYuxGirCwAQNshEyS2E6Zmnx9U1Yuu16CiljvCV5iWWuKYqjIep2SP52/5O7h/L576Zo8Kc+jeC42a2JS6L+eRY3Do+Fm0sGtVVuGRBA0bNcb+g0cwd/7X3GcqCCVBxf0j2VK4SdUtLCywfPlyLF++HHfu3MG+ffvkHRIRFdPKlSuxcuXKQtcbGRnh5MmThf6AHxUVhfj4+HwFuQwNDQEAjx8/LtDexMQEr169wpkzZ7Bo0SIYGBh85iuQjY4dO0odAXL69GkAwIABAwp9rwBARUUF7du3x9GjR3Hv3r0SxRAaGlqsdjUNTYtuVIF4PnqI0yePQ1lFBT8sXcEPtnKSnmcfz1s/RxJVtdwrZdNSWayvNITCdKxesRQikQijx45HfesG8g6JPhHx7h0ePcy+4ty2WXOYW5Tf9IZEshD+4gn+O74LAKBZXQddxswpVT+3/tmKqMBXALKLeFu0aCexXU4tDuVi1AxTznPeyUjneaW4eJySPZ6/5c/z0UNs3bwRAKCrp49vFy8r1uOWrvwJKSkpEIlESPzwHs+e+uH4kUM4+u8/CAsLxaJlq6CvXzG+uyqybt17wuZEUwBAamoqQkNCcOnieVy7ehk/fPs1vvluEbo4dJNzlERUlhQukZFXx44d0bFjR3mHQUSfydLSEsOHD8fChQthZGSUb92dO3ewadMmXLlyJd8UUJ+Kjo4ucN/48ePh7u6O169fo379+hg6dCh69eqFzp07o7YCz6PfrFmzQtdlZmaKa2Vs374d27dvL1afOaNTiqtOncLnj87rfWpmifpVZOnp6fhp1XKIRCKMHDUW1g04FZe8qOVJ0AnzFIKVRJinuKK6RuGJPSrczr+2IyDgDUxMTDHDZba8wyEJ3M6eRtbHK86dBg2RczREnyc2LBAX/liFrMxMKKuqobfLYmjV0ClxP57nDuHZrQsAsot0dx5d+PErJzmRWYwpifMWIFdR43mluHickj2ev+Xrjf8rfP/1HGRmZEBdXR0/r90IPT39Yj3W1Cz/d9EWLe0xdMSXWPTtfNxxv4FJY5yxY88/MKplXEgPVBw1atRAjRq5U3Q1tW2Gvo79cfb0SSxd/D2+mjsTy1f9hEGDh8oxSiIqSwoxtRQRVQ4uLi7w8fGBj48PfH198fr1a8THx+PNmzdYu3ZtgSTGihUr0KlTJxw+fFhqEgPILt71qUmTJmHRokVQUVFBQkICdu/ejVGjRqFOnTqoX78+vv76a7x586ZMX2NZ0NXVLXRdbGxsqeoCJSdzyp2i7NrxJ4ICA1DL2ATTZ/LLoDxpa+dOJ5WSnCSlZf7ppPJOM0XFE/DGH7t2ZidEv1u0BJpafA8VkdvZ7JF4ampq6NOnn5yjISq991HvcHbjYqQlJ0KgpIRe036AaQPbEvfjd9MN/x3fAwDQMa4Dx3mroaquUWh7VY3sY5swreDnxU9lpOdOJyWtT8qPxynZ4vlbvsLDQjHPZSrev38PZWVlrP7lV9i1sv+sPtXV1bF0xU/Q0NBExLt32PLb+qIfRKUyYOBg9OrdF1lZWVjz02okJMTLOyQiKiMKPSKDiCoWIyMjNG3atFhtr169Kp6GysrKCgsXLkSnTp1Qt25daGtri+tCLFu2DKtXry60n59++gnTpk3DgQMHcPXqVdy/fx/Jycnw9/fHhg0bsHnzZmzatAkzZsz4/BdYRpSVlQtdl5mZOwJiypQpmDdvXrH6VFMrWaHKkJCQErWvDPbu2gkAaNOuPdxvXpfYJidhlpKSgovn3QAAenr6aN1W8lQWVDpGeQp8R+Qp/C1J3tFGxsYm5RZTZbV/314IhULUrl0HqSmpuHDOrUAb/9evxMsPH9xHzMcRcF0duvGHExnw8/PBG//XAIDOXR1Qo2ZNOUdEVDpJ8TE4s+EHJMXHAAIBuk1YAEu79iXu59V/13Fr/x8AgOr6RnBa8DM0q0vfL6rpGiASQEZaKtKSE6UW/E6MjQIAaFSvyULfxcTjlOzx/C0/UZGRmDNjMqKiIiEQCLB4+Y/o0q1HmfSto6uLZi3s8OD+XbjfuIYMoRAqRUyzSqXj0L0HLl08j5SUZNy5fYtFvxWcEqd8pmJiIoOI5GLHjh0Askcn3L9/X1wL41NFjdQAsguIL1q0CIsWLYJQKMTDhw9x+PBhbN++HampqZg5cybatm0LOzu7UsWqpJQ9eC1nOH1h8hYwLq28hclFIlGxE0MlVdyptz6kSX/NFUnOFEZnTh7HmZPHpbaNj4vD4u8WAgBa2rdmIqOMWVnVFy8HBkgfNZV3vaVVvXKLqbJK/zg1V2hoCL7/dkGR7f/6c6t42e3iVZjxh5Byd/Z0bvFcp4GD5RcI0WdI+ZCAMxt+wPuotwCAziNd0LBDzxL3E+B9D9d2/QqRKAtaNfXg9PUaVNOT/BkxL12TuuLluLchMK7XWGK7rMxMJHyMMe9jSDoep2SP52/5iI+Lw1yXyQgLzb7o6+vvFsPRqWwL2+t8HJ2fmpqC+Ph4GBTyPZg+j65u7vfqt+HhcoyEiMoSp5YiIrnw8/MDAHTr1q3QJAYAPHr0qET9qqqqokOHDvjtt9/wzz//AMhOCBw9ejRfu5IUea5evToAIC4uTmq7ly9flihWSdTU1GBjYwMgu34IUWVkVrs2DD9ONefx6KHUtp4e2ccAI6NaMDUzK/fYiGRJKBTi4oVzAABdPT107NRFzhERlVxachLOblyMuPBgAEC7YZPQtPvAEvcT+swLl//8GVmZmdCoVgNOC35GTSPTYj3WxNpGvPz2pU+h7SIDXyIjLXtqKeP6TUocY1XE4xRVFYkfPmDerKkIeOMPAJg5dwGGfzGqzJ8nKjJSvMyRM+UnMjJ31LcW32eiSoMjMohILnLqQEgbxeDl5YX//vuv1M/Ro0fuEOBPi4VraOTOiZyWlia1H0tLSwDZiYoPHz6IExt5RUdH4/Lly6WONa+BAwfCz88Pz58/x8WLF9GnT58y6beqe/TkWZFtnPr2wNvwcJiYmuLMhasyiKpqEggEcOjWA0f+PYjAgDd48tgbzZq3KNDuyWNv8YiMrt16lCgBSdlW/7QGq39aI7XNtj82Y/u2LQCAHbtc0bpNW1mERgDu3L6FuI8jD/s5DhBPq0hUUQjTUnFu01JEB2dPO9Sy/5ew6+dc4n7evX6K81tWIjNDCDVNbQz46ifomVkU+/GmDZtBTVMb6SlJeHH3Clr0HSHxnPHibu5nNSu7DiWOsyricUo+eP6WrdSUFCyY64IXz54CACZMmY5xE6eU+fNERryD7xNvAICxiWm+unFUti5fvCBerm/dQI6RUHHwax4VF0dkEJFcWFtbAwBu376N169fF1gfFRWFsWPHSu1j//79UgtjX7p0Sbyck4zIYWKSO9e+v7+/1Ofp2rUrgOwh3ps3by6wXigUYsqUKRILkpfGvHnzUK1a9tzOEydOFI9eKYybmxuePHlSJs9NJCujxowT14tZ+8uPSE1Nzbc+NTUVa3/5EQCgoqKC0WPHyTxGovJ29sxJ8fKAMp66gqi8ZWYIcXHrKrx7nf3Dn23PwWg7ZEKJ+4kO9ofbpmXISEuFiroGHOeugqGFdYn6UFZRhW2P7H0o7m0wvC8eLdDmnf9TPL99EQBg2sAWRpYNSxxrVcTjFFV2QmE6vvt6Lp54ewIAvhg1FjNmFa9OYY7goEA8enBfapvEDx+w7IdvxNPdOg7g/lQap04eL/JCxH2ue3D71k0A2SPBW35moXYiUhy8nIKI5GLcuHE4c+YMkpKS0LVrV3z//fdo1aoVAODu3bvYsGED3r17h/bt2+PevXsS+xg7diwWLlyIoUOHokOHDqhXrx40NDQQERGBy5cvY9u2bQCAatWqYfTo0fkea2dnBw0NDaSmpmLp0qVQVVWFubm5uB6GmZkZNDU1AQD9+/eHubk5goKCsHTpUkRHR2Po0KHQ0NCAn58fNm3aBC8vL7Rr1w7370v/AFsctWrVwt69ezF8+HC8ffsW9vb2mDBhAvr164fatWtDKBQiNDQUDx48wNGjR/HmzRucOXMGzZo1++znVmTenh4ICQkW347PM9VXSEgwzpw6ka+906AhMoutqvHy9EBIcJD4dnx8/m1x+pMaJAMHDy3Qh7mFJcZNmITdf+/AUz9fTBo3CuMnTUGdOnUQEhKCvbt24vnHq+LGTpiEuuYW5fNiiOTkfUICbt28AQCoX98ajZvYSH8AlTlPj0cICc5zXslzLAsODsKpE/mPZYOGFDyWVWWX/1qDEL/sH/7MGrVA4059EBMWWGh7ZWUV6Bjnr9GVEBmOsxsXIz05EQDQZvB4qGlpS+1Hs7oOtGroFLi/Rd/heP3wJhIiwnD/6N94HxmO+m0coKyqhvAXj+Hp9i+yMjOhoqaOjl/OKPHrrYp4nKKqYOn33+C/e9lT+tq3aQunwcPyFVL/lKqqaoHPpVFRkZg9fRKsGzREl2490KixDfQNDKCsrIyY6Gg8eeyFMyePiQuy16tvXS4jPqqCP7duwYZ1/0OPXr1hZ9cKtevUgZaWNpKTE/Hq5UucczsDb6/sc5OqqiqWLl8tvniKiCo+JjKISC6GDx+OiRMnYvfu3QgPD8fcuXPzrVdWVsbGjRsRFxdXaCIDACIiIrBt2zZx0uJTNWvWxKFDh1CnTp1891evXh1z587F2rVr4enpid69e+dbf/36dTg4OADIrluxf/9+9O3bF0lJSdi4cSM2btyYL9bffvsNsbGxZZLIAIChQ4fi1KlTmDBhAmJjY/Hnn3/izz//lNhWSUmpSgxLPnn8KM6ePilx3WMvTzz++IE1BxMZ5efksSM4U8i28PbyFH95yCEpkQEAs+bOR2xsLE6dOIbnz57ih28KFrMcPHQ4Zs356nNDJlI4Fy+eFxdzHcDiuXJx4thRnP4kCZ5D0rGMiYz8Ajxza3mFPffG4RUuUttX1zfCmP+55rvv7StfpHyIF9++++/2Ip/X3mk0Wg8qOGpXTUML/eeuhtumpUiICMNT9/N46n4+fxtNLfSY8h0M6tYr8nmIxymqGm5cy51y7tGD/zDGebDU9sYmpjh57orEda9evsCrly+kPr5j565YsuInaHy8aI5KLiEhHsePHsbxo4cLbVOrljFWrP4Z7dpzGkGiyoSJDCKSm127dqF79+7466+/4O3tjfT0dBgbG6NLly6YPXs22rRpgxUrVhT6eF9fX7i5ueH27dvw9/dHREQE4uPjUb16dTRq1Ah9+vSBi4sLatWqJfHxa9asgbW1NVxdXeHn54eEhARkZmZKbNupUyd4eHjgp59+wtWrVxEVFQUDAwN06NABCxYsQIcOHaTGWhpOTk4ICAjAjh07cO7cOfj5+SE2NhYqKiowNjaGjY0NunfvjuHDhxdI1BBVBEpKSli+6if06Nkbx48ehp+fD+Lj4qCjqwsbG1sMG/EFOnZmUVGqnNzOnAKQnQzv13+AnKMhqhxq1jLFiGV/wPfaGfh7uCMh8i2yMoSopmeIurat0aznYFTXl/y5kAricYqoeJo3t8PvW3fgwX/38PypHyIj3iE2NgapqanQ1taGqWltNG3WHL36OqJ5i5byDrdC27Z9J26534S3lydCgoMQExODhIR4qKurQ09PHw0bNUbnrg7o3aefeIYFUnxKrJFBxSQQiUQieQdBRESK6UNalrxDoDyUWAVNYXBbKA5+lFUcSvwWqjD+vBcg7xAoj2ltLeQdAn0kqQg8yUeqUPIFZCR7GqqceklRaKrKOwL5+OlqwbqpFcXiHvXlHUKVwmLfRERERERERERERESksDi1FBERERERERERERHJnAAcLUfFwxEZRERERERERERERESksJjIICIiIiIiIiIiIiIihcVEBhERERERERERERERKSzWyCAiIiIiIiIiIiIimVNiiQwqJo7IICIiIiIiIiIiIiIihcVEBhERERERERERERERKSwmMoiIiIiIiIiIiIiISGGxRgYRERERERERERERyRxrZFBxcUQGEREREREREREREREpLCYyiIiIiIiIiIiIiIhIYXFqKSIiIiIiIiIiIiKSOYGAc0tR8XBEBhERERERERERERERKSwmMoiIiIiIiIiIiIiISGExkUFERERERERERERERAqLNTKIiIiIiIiIiIiISOaUWCKDiokjMoiIiIiIiIiIiIiISGExkUFERERERERERERERAqLU0sRERERERERERERkcwJOLUUFRNHZBARERERERERERERkcJiIoOIiIiIiIiIiIiIiBQWExlERERERERERERERKSwWCODiIiIiIiIiIiIiGROiUUyqJg4IoOIiIiIiIiIiIiIiBQWExlERERERERERERERKSwmMggIiIiIiIiIiIiIiKFxRoZRERERERERERERCRzSiyRQcXEERlERERERERERERERKSwmMggIiIiIiIiIiIiIiKFxamliIiIiIiIiIiIiEjmBJxaioqJIzKIiIiIiIiIiIiIiEhhMZFBREREREREREREREQKi4kMIiIiIiIiIiIiIiJSWKyRQUREREREREREREQypwQWyaDi4YgMIiIiIiIiIiIiIiJSWByRQUREhcrMEsk7BMpDVZXXHyiKlPRMeYdApHBUlXmMUhQT7c3lHQLlMWTnA3mHQB8dm9xG3iHQR2oqPGcoCgEvhieiCoJnDiIiIiIiIiIiIiIiUlgckUFEREREREREREREMsdRQVRcHJFBREREREREREREREQKi4kMIiIiIiIiIiIiIiJSWJxaioiIiIiIiIiIiIhkTolTS1ExcUQGEREREREREREREREpLCYyiIiIiIiIiIiIiIhIYTGRQURERERERERERERECos1MoiIiIiIiIiIiIhI5pQELJJBxcMRGUREREREREREREREpLCYyCAiIiIiIiIiIiIiIoXFqaWIiIiIiIiIiIiISOY4sxQVF0dkEBEREREREREREREpgEePHmHVqlXo3bs3ateuDXV1dVSrVg0NGjTAxIkTcfv27RL1d/78eQwZMkTcV+3atTFkyBCcP3++2H1kZGTgzz//ROfOnWFoaAhNTU3Uq1cP06dPh5+fX0lfYqkIRCKRSCbPREREFU58Sqa8Q6A8NFSV5R0CfZSSzn2D6FOqyrxGSlFkZvErniJx3v1Q3iHQR8cmt5F3CPQRr8BWHMpK3BiKQqOKzpuz478geYdQalPbmpd5n126dMGtW7eKbDdu3Djs2LEDampqhbbJysrCtGnT8PfffxfaZsqUKdi+fTuUlAr/LB8dHQ1HR0c8fCj5M426ujq2bNmCKVOmFBn35+C3DSIiIiIiIiIiIiIiOQsPDwcAmJqaYt68eTh69CgePHiAe/fuYcOGDTAzMwMAuLq6YsKECVL7Wrx4sTiJYWdnh4MHD+LBgwc4ePAg7OzsAAA7d+7EkiVLCu0jMzMTQ4YMEScxhg4divPnz+O///7Dpk2bYGRkhLS0NEyfPr1EIzxKgyMyiIioUByRoVg4IkNxcEQGUUEckaE4OCJDsXBEhuLgiAzFwREZioMjMhRHVR2R8feDYHmHUGqT29Qt8z4HDBiAcePGYdiwYVBWLvgbQHR0NDp27IiXL18CAG7evIkuXboUaPfy5UvY2NggIyMD9vb2cHd3h6ampnh9cnIyunbtikePHkFFRQXPnj1D/fr1C/Sza9cuTJ48GQAwc+ZM/PHHH/nWv379Gq1atcL79+9Rv359PHv2DCoq5fPPzG8bRERERERERERERERydvbsWTg7O0tMYgCAgYEB1q9fL7599OhRie1+++03ZGRkAAA2b96cL4kBAFpaWti8eTOA7PoXGzdulNjPr7/+CgDQ09PDunXrCqyvX78+fvjhBwDZSY0TJ05Ie3mfhYkMIiIiIiIiIiIiIqIKoFu3buJlf3//AutFIhFOnToFAGjUqBHatWsnsZ927dqhYcOGAIBTp07h04mbXr58iWfPngEAnJ2doaWlJbGfvFNcMZFBRERERERERERERFTFpaWliZcljdwICAgQ19ro2rWr1L5y1oeFhSEwMDDfutu3bxdoJ4mxsTEaNGgAALhz54704D9DFZ19jYiIiIiIiIiIiIjkqSLXzAkNDS1Wu9q1a5fp8968eVO83Lhx4wLrnz59Kl5u1KiR1L7yrn/27BksLS1L3c/Lly8REhKCpKQkaGtrS21fGkxkEBERERERERERERGVQJ06dYrV7tMpmz5HVlYW1qxZI77t7OxcoE3eBEtRSZS8ryEkJOSz+xGJRAgNDRVPWVWWOLUUEREREREREREREZGC27hxIx48eAAAGDp0KFq1alWgzYcPH8TL1apVk9pf3pETiYmJ5dJPWeGIDCIiIiIiIiIiIiKSuYp8lf2nIxjK282bN/H9998DAIyMjLBt2zaJ7VJTU8XLampqUvtUV1cXL6ekpJRLP2WFiQwiIiIiIiIiIiIiohIo69oX0vj5+WHIkCHIyMiAhoYGjhw5AiMjI4ltNTQ0xMvp6elS+81bOFxTU1NqP3lvl6SfslKRk15ERERERERERERERJVWQEAAevfujbi4OCgrK+PQoUPo0qVLoe2rV68uXi5qmqekpCTx8qfTR5VVP2WFiQwiIiIiIiIiIiIiIgUTHh6Onj17Ijw8HAKBALt27cKgQYOkPibvSJG8BbslyTs91qfFy0vTj0AgKLeRKkxkEBEREREREREREZHMCQSCCvtX3qKjo9GrVy+8efMGALB582aMGzeuyMc1adJEvPz8+XOpbfOub9y48Wf3U6dOnXyFv8sSExlERERERERERERERAoiISEBffr0wdOnTwEAa9aswaxZs4r1WEtLS5iamgLILhAujbu7OwDAzMwMFhYW+dZ16tRJvCytn3fv3uHly5cAgI4dOxYrxtJgIoOIiIiIiIiIiIiISAEkJyejf//+8PT0BAAsXrwY3333XbEfLxAIxNNPPX/+HPfv35fY7v79++KRFIMGDSowyqRBgwbiURqHDx9GcnKyxH727NkjXh4yZEix4ywpJjKIiIiIiIiIiIiISOYEFfivPKSnp2PIkCG4c+cOAGDevHn48ccfS9zPV199BWVlZQDAnDlzkJKSkm99SkoK5syZAwBQUVHBV199JbGfhQsXAgBiY2Px7bffFljv7++PX375BQBQv379ck1kqJRbz0REREREREREREREVCwjR47EpUuXAADdu3fH5MmT4evrW2h7NTU1NGjQoMD9DRo0wDfffIM1a9bg0aNH6NixI7777jvUq1cP/v7++N///gcvLy8AwDfffANra2uJ/Y8fPx67du3CnTt38Mcff+Ddu3eYOnUqdHV18eDBA6xevRrv37+HkpISNm3aBBWV8ks3CEQikajceiciogotPiVT3iFQHhqqyvIOgT5KSee+QfQpVWUO9lYUmVn8iqdInHc/lHcI9NGxyW3kHQJ9JIMauVRMykrcGIpCo4pebu76KETeIZTaOPs6Zd5nSYuIm5ubIzAwUOK6rKwsTJ06Fbt27Sr08ZMnT8Zff/0FJaXCP8tHR0fD0dERDx9K/kyjrq6OLVu2YMqUKSWKvaT4bYOIiIiIiIiIiIiIqBJRUlLC33//DTc3NwwaNAimpqZQU1ODqakpBg0ahHPnzmHnzp1SkxgAYGBggLt372Lr1q3o1KkT9PX1oaGhASsrK0ydOhUeHh7lnsQAOCKDFMCNGzfQrVs3ies0NTVhaGgIOzs7ODs7w9nZuVyHKFHllZ6ejmPHjuH8+fN48OABoqKi8P79e9SsWRPm5uZo06YNhg0bhu7duxd5AK9KFGlExjM/X9y57Y7HXp4IeOOP+LhYqKiowMDQCM1a2GHgkGFoYddKah9ZWVkIDHiDp74+2X9+vnj96gWEQiEAYOuOPWjVWnGv1KtIIzL8fH1wy/0mvLw88cb/NeJiY6GiogpDIyO0sGuJIUOHoWUre3mHWWqKNCLjmZ8v7t52x2NvyfuG0+Ci941PPbh/FxfOncUTbw9ER0VDWUUZenr6qG/dEPZt2qHfACdoaWmX0yuquD53W4SHh2Fo/14lek5jE1OcPHflc0MvE4o0IiM2Jga+vk/g9/F47+fng4T4eADAgIGDsfLHNUX2kXPO8PV9Aj8fHzz188Grl7nnjO1/74V967bl+TJKTZFGZDz188Xd2zfF5++4uOzzgYGhIZq3aFms83ded2+748Sxw3jm54u4uFjo6uqhsU1TDBnmjA6dupTjKyk9WYzI0FJVRmvzmmhoVA3WhtowqKaGmhoqUFNRQlJaJoLiUvAwKB4XnkXhQ1qGxD6UlQSwM6uBVnVrolGtajCrqQFtNWWkZmTh7fs0eIcm4KxfJN69T5May9fdrdC7kWGx4h63zwsRH9JL/HpLS5FGZJTFcSolJQX37tzC/Xt38eypL0KCg5Gckoxq2tqoa26B9h06YZjzlzAwKN72kCVFGpEh3hY+Ptnbw88H8R+3hdPAwVj5U9HbIq87t9xx/Ohh+Pn6iI9TNk1tMXS4Mzp2VrzjVEUbkRETEwNfnyfw9cnef/x8c7fXwEFDsPrnkm0vRVJVR2Ts9wiVdwilNqZVbXmHUKVU0V2EKoqUlBQEBwcjODgYp06dwm+//YbTp0/D2NhY3qGRHORNel2/fh0ODg7Fetzx48fx9ddfSxxqFxMTg5iYGHh6euLPP/9EgwYNsGHDBvTv378MI6fPNX3SWHh7ehS4XygUIiQ4CCHBQXA7fRKOAwZh0fKVUFVVk9jP+bOnsWrZovIOt8qbOG40PD0eFbhfKBQiOCgQwUGBOH3yOJwGDsbylauhqiZ5e1HRZkwaC2+v4u0bPywrfN/I8f59An5cvhjuN64VWJeUmIiQ4CBcv3oJts2bo0HDxmX2OiqDst4WxWVuYVkm/VQ2vbp1/Ow+3M6cwoqlP5RBNFXXtEljijx/nz19Ao4DBmHx8lVS94usrCz8vHoZTp84lu/+yMgIREZG4Ob1qxg0ZDh+WLqySl6U0rCWNhb1ljy3tY6WEnS0VNHcrAaG25lg7RV/eIQk5GtTU0MFO0Y2Q01N1QKPr6asBGtDFVgbamNQM2P8fS8YJ59ElMvrqEo+9zj16uULTBo3EsnJyQXWJSQkwOfJY/g8eYwD+/diybJV6N3X8bOerzLr6fD55wwg+zj148plOHn8aL77IyMjEHktAtevXcGQYSOweFnVPE6Vle5dOsg7BCKSEyYySKG4uLhg5syZ4tuJiYl49OgR1q9fj8DAQDx8+BCDBg3C/fv3SzxnHFVNq1evxrJly8S3e/XqhYEDB6JJkybQ0dFBbGwsXrx4gTNnzuDy5ct4+fIlFi9ezESGgomOigQAGBoaoXuvPmjRshWMjU2QmZUF38feOLBvD6IiI3Du7ClkZGRg9Zp1EvvJOwhRRUUF9awbIDMjA69fvZTJ66gqoiI/bi8jI/Tu3RctW9nD2MQEWVlZeOztDde9uxAZEYEzp08iIyMDa9atl3PEFVd0dP59o7ldq+z3OjMLPk+88c8n+8aqXyTvGwCQ+OED5s6YgufP/AAAXbv3RPeevWFWuw6UlZQREfEOXh4Pcf3qZZm8toqmLLaFkaERDhw5VeRz7d31Fy6ddwMAODoNKtsXUgkZm5jCwtIS9+/eKdHjRMh7zlBFfWtrZPCcUSLRUVEAsveLHr36oEVL+4/n70z4PM7eLyLz7Bc/rvm10L62bflNnMRo2Kgxxk6YjNq16yI0NBj79vyNF8+f4dSJo9DV1cPMufNl8voUTeSHNDwOe49XUUmISkxHbLIQSgLAQFsNnevpoaOVHnQ0VbHCsQHmHfXDm5jcH8BVlZXESYzXUUm4FxiH5xGJiE8WQltNBfbmNTHI1hjqKkpw6WSBtIwsnH8aJTWe6MR0LDr7XHqbJOHnv/BKoDTHqcTERHESo7ldS3Tu4oAmNk1Rs6YO4uJicf3qZZw4dgRJiYlY8sM30NauppCjARRNac8ZAPDHpo3iJEajxk0wfuJk1K5TF6Ehwdi7+288f/YUJ44dgY6uLubMW1DWoVdJJiamsLC0wr27t+UdChHJABMZpFCMjIzQtGnTfPe1a9cOo0ePRps2bfD69Ws8ePAAZ8+ehZOTk5yipIpi9+7d4iSGkZERDh8+jK5duxZo17NnT8yaNQu+vr6YP38+oqKkfykj2TO3sILL7K/QrWdvKCvnn17Jtllz9BswEFMnjEZwUCAuXXDD0BFfwE7CtEWWVvXw9XeL0NjGFg0aNoK6ujp2bNvCH6XKmIWVFeZ8NR89e/UpsL2aNW+BAQMHYvyYkQgKDMT5c2cx4osv0cq+tZyirdjMLawwY/ZX6Naj4L7RtFlz9Os/ENMm5u4bQ4ZL3jcAYP3/fsLzZ35QU1PDj//bgC4O3fOtb2zTFA7de+Krhd8jM1NxptZSFGWxLVRUVVGvvuQrqnNkZmbC61H2NDVa2tro2q1n2b6QSmLq9Jlo0tQWNk1toa9vgPCwUDj1K9l7ZWVVH998vxg2NrZo0Kgx1NXVsX3rZp4zSsDcwhIus79Cd4nn7xZwHDAIUyaMynf+btmq4PkgKCgA+113AwAaN2mK7bv2QUNDAwDQpKktunTtjumTx+HZU1/sc90Fp8FDUaeuefm/QAXyOOw9xu7zLnS9u38s2lvqYkW/BlBTVsLo1mZYfeGVeL0IIniEJMD1QSieRyQW7D/8PW77x2LtoMbQUFXGlPZ1ceNVDFKEWYU+Z0aWCEGxKZ/1uiqzzz1OKSkJ0KtPP0ybMQtW9eoXWN++Qyd06NQFC7+ajczMTKxd8yNOdrrICwIlmDpjJmya2sLGxhb6BtnbYkDfkp0zggIDsG9v9nGqiU1T7NyzX3ycsmlqiy4O3TF14lg89fPFvj27MGjIMNStYsepsjLdZRZsmtqiadPs7RUWFgrH3j3kHRYRyQDHslGFoKurix9+yB3af+HCBTlGQxVBWFgYZs+eDQDQ1tbGzZs3JSYx8mratCkuXryIhQsXyiJEKoENm7ehZ59+BX4EyaGjq4t5X38rvn3tykWJ7Wxsm8F55BjYNmsOdXX1comVgC1bt6NPX8dCt5eurh6+/uZ78e3LlyRvLyra+k3b0LO39H1j7oKi9w1vLw+cdzsNAJg+a26BJEZeAoGA9aokKKttUZSH/91D1MdRat179hb/SEL5zZg1F126doO+vkGp+2hq2wxfjhoL2+YteM4opY2b/0SvEp2/L0lsd2i/KzIzsus6LPx+cYH/ew1NTSz8fjEAIDMjAwf37y2L8CuU4pRFuRcQh5C47MRCU5Pq+dbFJAmx6MxziUmMHC8ik3DWL/v4U01dBS3r1Cx9wPTZx6nmLVpizbqNEpMYORy69UD3Htm1l0JDgvH82dNSPVdl55KzLQxKf874Z78rMj4ep779YUmB45Smpia+/WEJACAjIwMHXKvecaqszJw9F10dPm97kWIRVOA/ki0mMqjCaNMmtzBbUFBQvnWZmZnYu3cvBgwYAFNTU6irq0NfXx+dOnXChg0bkJJS+JVADg4OEAgE4noLr169wuzZs2FtbQ0tLS0IBAJxbYVP275+/RozZsyAlZUVNDU1YWFhgcmTJxeIz9fXFxMnToSVlRU0NDRQp04duLi4IPLj9CuFuX//PpYsWQIHBwcYGxtDTU0NNWrUQJMmTeDi4oKnT6V/EJ0wYQIEAgEsLCwAAPHx8Vi2bBlsbGygra0NHR0ddOnSBQcOHJDaT46EhAT88ssv6NixIwwNDaGmpgYTExM4OTnh6NGj+abt+ZRAIIBAIMCKFSsAAA8fPsTIkSNRu3ZtqKurw8zMDGPHjsWzZ88KPDYwMBACgSBfUfhu3bqJ+8z527Nnj3j9xo0bxUOtV61ahUaNGhXrNSopKWHMmDESnz/vcxw/fhyOjo4wNTWFioqKxHodZ86cwfDhw8WvUV9fH+3bt8eaNWuQmFj4l8Q9e/aIny8wMBBpaWn49ddf0bJlS9SsWRM1atRA27ZtsXXrVl4VnUfeIt2hISFyjISKo3Wb3AK5oSHBcoyk8su7b4SFSt43jh76BwBQrVp1DP9itEziqoqKsy2Kcu5s7tRTjgMGf25IRHKXt2B6mITzgUgkEtftsbC0gm2zFhL7sW3WQlwzxv3GNamfS6uyZGH2Z0c15dL9FPA47L142aQGE6kVgX3ez1ylPPeQdCKRCDeuXwWQfZxq1ryFxHbNmreAxcfj1M3rV3mcIiIqIV5ORxWGqmpu4bm8P94GBwdj4MCBePz4cb72sbGxuHPnDu7cuYNt27bBzc0NDRo0kPocp06dwujRo5GUlFRkPFeuXMHQoUPx4cMH8X1BQUHYtWsXzp49i5s3b6JRo0Y4ePAgJkyYgPT0dHG70NBQ/Pnnnzh//jzu3r0LU1PTAv3v2bMHEydOLHC/UCjEs2fP8OzZM+zYsQObNm3KV1ekMC9evEDfvn0LFLy+desWbt26hXv37mHLli2FPv7q1av44osvEBMTk+/+d+/e4ezZszh79iwcHR3x77//olq1alJj2bp1K+bNmye+YgUAwsPDsX//fhw/fhznz59Hly6ln79VJBJh797sK1y0tbUxderUUvclqe9x48Zh3759hbZJTU3FqFGjcOLEiXz3x8bG4v79+7h//z42b94MNzc3tGjRQurzxcXFYfjw4fDwyF8o88GDB3jw4AH+/fdfuLm5FfmeVwV597HCrvwkxSHMs71Y7LB8ped7rwvuG0JhOm7dzP6RsE279uIrzzMzMxEdFYnMrCzo6xvwivQyUNS2KEpSUhLcr2dvKxNTs0KnCSOqSPLtFxLO3+FhoeJRSHYSpp3Kq2Wr1ggKDEBkZATCw8NgZla7bIOt4GrraKCevhYAICS+dFM+qSrnXn+axR9hK4R8n5H5matchIWGimvEFTVdakv71gjMOU6FhcGsNo9TRETFxUQGVRg+Pj7i5Zwf/mNiYtCpUyeEhIRAXV0dU6dORdeuXWFhYYHExERcunQJv//+O16/fo1+/frB09MTNWtKHgIdHByMMWPGQEtLC0uXLkXnzp2hrKyMhw8fFviRODw8HM7OztDR0cHPP/+MNm3aID09HceOHcPvv/+OyMhITJkyBRs3bsS4ceNgbW2Nr7/+Gs2aNUNSUhJ27dqFffv2ISgoCAsWLMChQ4cKxJORkQFdXV0MGjQIXbp0gbW1NbS1tREeHg5PT09s2rQJ0dHRmD17Nho1aoTu3QufBiQ5ORlOTk6IiYnBkiVL0LNnT1SrVg1eXl5YuXIlQkND8ccff8DJyQl9+vQp8Pg7d+6gX79+EAqFqFWrFubMmYPmzZvD1NQU4eHh+Pfff7F//36cO3cO48ePx7FjxwqN5eLFi3jw4AFsbW0xb9482NraIiUlBSdOnMDvv/+O5ORkjB07Fq9evYKamhoAwMzMDD4+Pnj48CEmTZoEANi1axdat87/IbH2xw+Bfn5+iI6OBgB07twZ1avnHzr/OX777Tc8efIEnTt3houLCxo0aID4+Ph8CaLx48eLkxjNmzfH119/jcaNGyM2NhaHDh3Cnj17EB4ejh49euDJkycwMzMr9PmmT58ODw8PfPHFFxg/fjyMjIzw8uVLbNy4EQ8fPoS7uzvGjh1bIGlSFXl5PBIvW1hayTESKo5HH+f4B7Jrl1D5ybdvWBXcN169fIG0tDQAQL36DZCUmIi/tm3GuTOn8OFD9pW3qqqqaNHSHhOmTEcr+zYF+qDiKWpbFOX6lYtITc3+8bFf/4Gc55wqBU+P3POBpPP3mzevc9d/vJK5MOZ51ge+8WciA4C6ihL0tVXRzkIXI+xMoPJxJMaJx+9K1Z+taQ3xcnCc9GRIDQ0VrBvUGBb6mtBQVcaH1AwExCTjv8B4XHwehbSMwutrUNnx5GeucpfvOFXE95C86wPe+DORQQSAH2mpuJjIoAohIyMD69evF9/OmcZn7ty5CAkJgbm5Oa5fvw5Ly/xfbhwcHDBixAh07twZb968wdq1a/HTTz9JfI6AgACYmpri3r17qFu3rvj+tm3bFmj76tUrWFtb486dOzA0NBTf36lTJ6ioqODXX3/FnTt30L9/f7Rp0waXL1+GlpZWvrhSU1Nx5MgRHDt2DFFRUfn6AYB+/fph1KhR+R4HAHZ2dujfvz/mzp2LLl264MmTJ1i+fLnUREZUVBTS09Nx79492NjYiO9v1aoVHBwcYGtri9TUVGzdurVAIkMoFGLMmDEQCoXo27cvjh07li+mli1bYsCAAejSpQumTZuG48eP4/Lly+jVq5fEWO7fvw9HR0ecOHFCnKgAshMO+vr6WLJkCYKDg+Hm5oYhQ4YAyP4BrWnTpuLkBABYWloWKAyfI+/onFatWhX6vpTGkydPMG7cOPH0T59yc3PD4cOHAQA9evTAuXPn8r3O3r17o3379pg2bRpiY2OxYMEC/Pvvv4U+38OHD/Hzzz/nqxHTqlUrjBgxAgMGDMDFixdx8uRJnDt3Do6OjmX4SiuWrKwsuO7aIb7do3dfOUZDRcnKysKunX+Jb/fp20+O0VRuWVlZcN2du2/07FVw3wjw989tL8rChNEjEBKcf4pEoVCIh//dw6MH9+EyZz7GTZxSfkFXUsXZFkU5d/a0eLnfgIFlEheRPGWfv3eKb/fsXfB8EBkRIV42qmUstb9axibi5YiI0v1QXxn0amiAhT0K/8H6kGc4rr+KKXR9YfS0VNG7UfZ3lvhkYb5ppiTRUlNGM7PcxIe+thr0tdVgX1cHzi1N8fOlV3j6rvDpVunzvXzxHLdv3QQA1LduwERGOcl7nKpVq5bUtsbGucexd+/elltMRESVEccVkkJLSkrCzZs30atXL9y/fx8AYG5uDmdnZwQGBop/AN6yZUuBJEYOOzs7zJo1CwDy1VCQZM2aNfmSGNJs2rSpQPIBQL5pnqKjo7Fz584CyQgAcHFxAZCdpLl3716B9WZmZhIfl6NmzZpYtWoVAOD27dsFpnz61OrVq/MlMXLUr18fgwcPFvfzqUOHDiEwMBAaGhpwdXUtNKapU6eK65hIe581NDSwe/fufD/u55g7d674/lu3bkl9PdLkfS+MjIxK3Y8kOjo62LJlS6FXwf7xxx8AspMvhb3OqVOnomfPngCya228fVv4B9hmzZrh+++/L3C/iooKdu7cKZ5ybevWrSV+LZXJwf174eebPWrLoUcvNG5S8H+dFMc+1z3w9XkCAOjRszea2EhOStLnO7h/L57m7Bvde6GRhH3j/fsE8fL+PX8jJDgI7Tp0wq79/8L9P2+cv3ob3y5ahmrVqkMkEmHrpg1w/zgPNBVfcbaFNO/ehsPr45Xrts3tUKeueZnHSCRr2efv7PNBt0LO38l5pnyV9tkYyC6mmyPlY600yvU6Kglzjvpi9/3S1UmY29US2mrZ038d8AiDMLOQqaVEwNN3H7D7fggWn3mOmYd98NUxP/x24424mLhhNTX87NQI9Qykb1MqvfT0dKxesUQ8LfOsOV/JN6BKLO/U1Jpa2lLbamrm/s+npPA4RURUEkxkkEJZuXJlvuLN1apVg4ODA27cuAEg+0fpkydPQl1dHW5ubsjMzISWlhb69ZN+NW9OvYXw8HAEB0suKqumpoYRI0YUK04dHR2JUzAB2SMFcqYyatasGRo3biyxXfPmzcXLb968KfI5k5KSEBgYCD8/P/j6+sLX1zdf3ZBPa4TkJRAIMGrUqELX54xaiI2NRXx8fL51p09nX/3ZtWtXiYmbvHLeZ0mJmRy9evUqNLlQvXp1WFtbAyjee1KYvHVLtLWlf5AsKScnp0KnqsrIyMDNm9lXPPXu3Rt16tQptJ+cuh0ZGRni/29Jxo8fX2jSpHbt2ujduzcA4MaNGyUq/B0aGlqsv4rA89FD/LFpIwBAV08f3y1eJueISJpHDx9g08bsEXZ6+vpYvGyFfAOqxDwfPcTWzbn7xreF7BspKblTg6SlpaFNuw5Yv2kbmtjYQk1NDbp6ehg64kv8ummruJ7J1s0bWaCyBIq7LaS5cO6M+D135GgMqgQ8Hz3Alk0bAAB6evr4bvFyie3S09PEyyp5PvtKoprnApLUtNQyiLJiuhsQh2mHnmDaoSeYc9QXP196hdtvYlHfUBs/9KqPtuY6Je5zZCtTtLfUBQB4hybgjE9EoW3/vBOE+cef4pBnOB6FJMA/OhnPIhJx/mkU5h3zw0GPMACApqoy5jtwOtDy8r+fV+Opny8AYMDAwejiUPgIfvo86Wm5xynVEhyn0lLTpLQkIqJPcWopqhAsLS0xfPhwLFy4UPwj+KNH2fNMJycnQ0Wl+P/K7969kzjqwtraGhoaGsXqw9raWuq81Do6Ovjw4YPU4uI6Ojri5bw/vOcVHR2NDRs24NixY3j16pXUH43yTrv0KQMDA+jr6xe6Xk9PL18seWPLeZ8vXrxY7Lm4370rfCh/o0aNpD42J5bC3pPiyJtoKE7h9pJo1qxZoevevHmD5I9X/0makiyvvOt9fX0LbfdpHZBPtWnTBm5ubkhKSsKbN2/EiaCiSEuy5BWXnFF0Izl68/oVvlswB5kZGVBXV8cv6zZCT6/w/3WSr9evX2H+3NnI+Li9ft3wu9RjE5XeG/9X+P7r3H3j57WF7xvq6vlHjs2atwDKEgrutrBrBYfuPXHtyiUEBrzB61cvYd2gYbnEX5mUZFtIc97tDIDsCy969uF0bFSx+b9+hW8XzM3dL6Scv9XU1MXLGUKh1H6FeYoaa6gX73N9ZZSUnomk2Nwk9cvIJNx8HYseDQywsIcVlvdrgI3X3+Dyi8K/P+TVzVof49pkz+P/NiEVay77Q1oqOyld+sU1e/4LRSOjarCrUxPWRtpoYlyNU0yVsV07t+Pk8SMAAJumtvh+ES/0KU9q6rnHKWEJjlPqGupSWhJVHaz7RsXFRAYpFBcXF/HUTAKBABoaGjAwMJBYoDsyMrJUz5FcyDBzXV3dYvdR1LD2nCtWpbXLaQNA4pX0Hh4e6NOnT5FTRuXIe0Xtp4obr6RYSvM+l0UsJRld8Km8P4xGRBR+tVhpSPs/iY2NFS8XNaVV3rlR8z7uU0X1k3cOVmn9VEbhYaGY6zIV79+/h7KyMlav+RV2rezlHRYVIjQ0BDOmTsL79wlQVlbG/37dgFb20hN1VDrhYaGYl3ff+EX6vqGVZwoEXV09NGzUpNC2bdt3wrUrlwAAz/x8mcgoQkm3RWH8fJ8gKCB7pGLnrt1QvXqNIh5BpLjCwkIx12WK+Hzw45r1aNmq8POBVp7RtYV9js+R9zOoZhGfOauiqy+j0dZCB13r62NWFwvcD4zDhzTpn7nbmOvg6+5WUBIIEJOUjh/OPEdcivQfaovD7Wkk7Opkf8drZlqDiYwydOzIIfFoZQtLK/z+x1/cH8pZ3lkAUpKlX0iXdzqpvNNMERFR0ZjIIIViZGRUaAHnT+X80G1gYIDr168X+zkKq6Uh6epTeUlPT4ezszNiYmKgqqqKOXPmYNCgQWjQoAF0dXWh/vGKjzdv3qBeveyCbeU1xUfO+9yvXz+sXbu2XJ6jrOWdtsvT07NM+y7u/0lZXVFQXlcmhISUbm5kRREVGYnZ0ycjKioSAoEAS1b8iK7desg7LCpEZGQEpk+ZiKjI7O21cvXP6Na9p7zDqpSiIiMxZ0buvrF4+Y/oUsS+UStP8VzDIgpU1sqThI2Pq1rJ05IqzbYozPl8Rb4HlVWIRDKXff6eVKLzt1Ge41JkEQW8I/IUzq1VRGHwqupeQBy61teHpqoy7OvqSC363cy0Opb0sYaqshI+pGZg8dnnePu+bKbCCc4zYkRfu2BNOSqdC+fOYs1P2XUUTUxNsXX7rhJdsEelk/c4VdSFdHlnLzA2Nim3mIiIKiMmMqjCyrnq/sOHD2jcuLFCJSI+17Vr18Q1IrZu3YopU6ZIbCeLK/D19fURHh6O9PT0YieZ5M3GxgYGBgaIjo7GrVu38P79e9SoUf5Xr+adoqskH2DzPu5TERERUqcoy/s80vr5VO3atYvVLj6l9CNjykt8XBzmzJiMsNDsZMzX3y2GoxN/2FNUcXGxmD5lEkI/Js++X7QUToMGyzeoSio+Lg5zXUq+b1jWqy9ezsrMkto2K89oOWWVynPeLWul3RaSZAiFuHzxHIDs+hrtOnQqsziJZCk+Lg6zZ0wS7xcLv1uM/k6Di3yclVXuMSowMEBq26A86y2s6pUu0EouIc9oCqPqhU9r09BIGysdG0JdRQnJ6ZlYfPY5AmIKH3VdUqyyVPZuXr+GZUu+R1ZWFgwMDbFtx558FyBQ+cl3nAqQXusx73pLHqeIALCAMxUf/1eowrKzswOQXZg0p45DZeHn5yde/uKLLwptJ4vXnfM+P3r0COl55vOUh+KOThAIBBg/fjyA7BoZO3fuLM+wxKysrMRTZ/33339S2z548EC8LC1B9PDhQ6n95KzX0tKClVXlL5aY+OED5s6cioA3/gCy5/If8WXhhexJvj58+ACXaVPwxv81AGDe/K/x5ajRco6qckr88AHzZuXuGzPnLsDwL4q3b5iYmomvCHz7NkzqCL/Q0NzRXIaG0kdvVFWfsy0kuXP7JhLi4wEAffr1L1FdMCJFkX3+nvLJ+bt45wNTs9owNMyeatPLQ/rnIi/P7M/GRka1YGpq9hkRV155Rz+kCCVfsGKpr4mfBjSClpoy0jKysPzcC7yILNu6c3V1NcXLscny/Y5RGTy4fw/ff/MVMjMyUFNHB1u370KdOgXrQlL5MKtdG4YfpwT2eCT9OOXpkec4ZcbjFBFRSTCRQRWWk5OT+Ift3377Tb7BlLGMjNwCy4UVq87KysKOHTvKPZaBAwcCABISErB79+5yfz5p8hZjT0uTPqx9/vz54qTCsmXL8Pz582I9R1ZWFg4cOFCq+FRUVNC1a1cAwOXLlxEaGlpo25zkioqKChwcHAptt2/fvkJ/VAwLC8OlS9lz1Ts4OFSqUUmSpKakYP4cF7x49hQAMHHKdIybKHm0EslfSkoKZrtMw7On2YnZqdNmYNKUaXKOqnJKTUnBgrm5+8aEUuwbDj16AwCSEhPx8L97hba7ce2KeLm5XctSRFu5lcW2+NS5PNNKcfQZVUTZ5+8ZeJ7n/D1+4tRiP14gEKCLQ3cA2Vcy+zzxltjO54m3+ErnLg7dWTi0EJ3r547gDYwpWHPErKYGfnZqhOoaKhBmZmH1hZd4Ev6hzOPob5NbB+5J2Psy778qeeztiQXzZiE9PR3VqlfHH3/uRL361vIOq0oRCARw+DhNXmDAGzx57C2x3ZPHuceprt168DhFRFRCTGRQhdWwYUOMGDECAHDo0CFs2LBBavuAgAAcPHhQFqF9Nmvr3A+ee/bskdjmhx9+KPP6D5KMHz8ederUAQAsXLgQ7u7uUtvfvn0bN2/eLJdYTExy5xD19/eX2tbMzAxbtmwBkJ0M6tq1a5FxPX36FH379sW6detKHeOsWbMAZNc5mTx5MoTCgsUQd+3aJU5ADB06NN/r+pS3t7fEeDIyMjB16lTxKBkXF5dSx1wRCIXp+HbBXDzxzv6f/2LUWMyYPU/OUVFhhOnpmD93Nry9srfX6DHjMHvefDlHVTkJhen47utP9o1ZJd83vhw9Vlx/6fcNa5GUWLDo6nm30/B8lD2arGPnrqjFeZ3zKattkVdCQjzu3so+d9WzboAGDRt/dpxEsiQUpuObBXPw+ON+8eWosXCZ/VWJ+/lyzDjxBRu/rvkJqamp+danpqbi1zU/AQCUVVTw5ehxnxd4BdSroQFUlaX/KDqkmTHammfXS3ibkArft/kTFIbV1LBmYCPoaakhM0uENZf98TA4oURxNKpVDXpaqlLbjG9TGy0/Fvr2j06CHwt9l9qL588wb9YMpKQkQ1NTC79v2Y7GTSrGdMCVzag8x6m1v/wo8Ti19pcfAWRfzDZ6bNU7ThERfS6OTacKbdu2bXj06BHevHmDr7/+GqdOncK4ceNgY2MDdXV1xMTE4PHjx7hw4QKuXbuGIUOGYOTIkfIOu0h9+vSBkZERIiMjsWTJEgQGBmLIkCEwMDDA69evsWPHDly9ehUdO3bEnTt3yjUWdXV1HD58GA4ODkhMTET37t3x5ZdfYvDgwbC0tERWVhbevn0LDw8PnDhxAj4+Pti8ebN4ZEJZqlu3LmrXro3Q0FD8+uuvqF27Nho2bCj+wFirVi1Ur15d3H7ixIkIDQ3FsmXLEBkZCQcHB/Tu3RuDBg1C48aNoaOjg9jYWLx8+RJubm64cOECMjMz8xULL6n+/ftjxIgROHLkCC5duoR27dphwYIFaNSoEeLi4nDo0CHs2rULQHZNi6IScPb29vjuu+/g7e2NcePGwcjICK9evcKGDRvE01M5OTlhwIABpY65Iljy/Tf47172/7p9m7YYOGQY/F+/KrS9qqoq6ppbSFx39tSJfLdfvsgdrXP/7m28DQ8T365dty5a2LX6jMirpu+++Rr37t4GALRp2w5Dhg3Hq1cvC22vqqoKCwtLWYVXqSz9ZN9wGly6fcPYxBRTXWZjy2/r4f/qJSaN/QJjJ0xGfeuGSEpKxPWrl3Hi6L8AAO1q1TDv6+/K5fVUZGW1LfK6fPG8OCHuyCLfJeLl6YGQkCDx7fi4OPFySEgwTp86nq/9wEFDJfbzabsXec4Zd+/cRniec0adOuawa8lzRl5Lvl+YZ79oh4FDhsP/deHnAxVVVZibFzwfmJtbYsz4Sdi7aweePfXF1AmjMG7iFJjVrouw0GC47t6JF8+fAQDGjptU5L5VGY1pXRvTOtbFbf84+L37gPCEVKQKs6CpqgRLfS10a2CApibZn5PTM7Pw+80AZOUZ9FtdXQVrBjYS18045v0WIfEpMNfTlPR0AIDEtAzEJOW/aMe+bk18YWeKR8Hx8Ax9j+DYFCSmZ0BVWQmW+pro08gIjY2rAQBShZn47Yb0uieV2ecep0JCgjF7xhR8+JA9omXm7HmoVq0aXkv5zKWnpw+9j7UmKZeXpwdCgvNsi/hPtsXJT7bF4ILnDHMLS4ybMAm7/96Bp36+mDRuFMZPmoI6deogJCQEe3ftFI9MGzuhah6nyoqnxyOEBAeLb+fdXsHBQTh1Iv/2GjRE8jmeFAdHJ1FxMZFBFZqenh7u3LkDZ2dn3Lp1C+7u7lJHDMii4HNZ0NbWhqurKwYPHozU1FRs374d27dvz9fGwcEBW7ZskUkB7nbt2uHGjRtwdnZGSEgIDhw4IHX6pfJ8nxctWoSZM2ciICAAgwbl/1Fn9+7dmDBhQr77li5dChsbG3z99dcIDAzEpUuXxKMhJLGxscHatWs/K0ZXV1dkZGTgxIkT8PT0xJgxYwq0MTU1hZubG8yKmBf1r7/+wuTJk3Hw4EGJI4o6duxY6qmwKpIbVy+Llx89+A+jRwyW2t7ExBQnz1+RuG718sWFPs51d/56Kv2dBjORUQpXr+TuYw/+u4/hQwZKbW9qaobzl6+Vd1iV0o1r+feNMc6DpbY3NjHFyXOS940x4yfjfUIC9u35G0GBAfhxxZICbXT19LF2w2Z++ZagLLdFjvNnTwEAlJWV0dexciesy9rJ40dw9vRJiesee3nisVf+Ua2FJTJWLl1U6HPs3ZV/is8BAwczkfGJ6/nO3/cxaoT0hJyJiSlOnb8qcZ3L7K8QGxuDMyeP48XzZ1j83dcF2gwcMqxKj9isoaEKRxsjOOaZtulTUYlp2HDtDbxC80/nZKmvido6uUkL55amcG5pKvX5Lj2PwvprBQsbq6kooYOVHjpY6Ul4VLaID2lYc/k1XpZx7Y2K5HOPU16ejxAbGyO+vX7dL0U+57QZszB95pySB1vJnTx2BGcK2RbeXp7iUcY5JCUyAGDW3PmIjY3FqRPH8PzZU/zwzYICbQYPHY5Zc7763JCrtBPHjuL0Jxen5ZC0vZjIIKo8mMigCs/Y2Bju7u5wc3PDwYMHce/ePbx79w5CoRA6OjqwtrZG+/btMXDgQHTp0kXe4RZbnz598OjRI6xZswbXrl1DVFQUdHR00KRJE4wePRqTJ09GcJ6rEMpbu3bt8OrVK+zZswdnzpyBl5cXoqOjoaSkBENDQzRu3Bhdu3bFsGHD0LBhw3KLw8XFBbVq1cL27dvh7e2N2NjYfDVFJBk6dCgGDBiAo0eP4vz583j48CEiIyPx4cMH1KhRAxYWFmjXrh2GDx8OBweHz74aQENDA8ePH8eZM2ewZ88e3L9/H9HR0dDW1kaDBg0wePBgzJ49G9WqVSuyL11dXdy9exe//fYb/v33X/j7+0MkEqFx48YYN24cXFxcKn1tDCKSnZlzF6Bz1+44fuQQvL08EBMdBTU1ddQ1t0Cnrt3g/OVoVMsz8o3KT3BQIPx8ngAAWrdtD30DQzlHRCRfSkpKWLriJ3Tv0Rsnjh3BMz8fxMfHQUdHF41tbDF0uDM6dKo4n/XL2uKzz9HGXAc2xtVhWlMdOlqqqKGugrRMERJShPCPTsZ/gXFw949FWkZWucVx6VkU4pOFaGxcDZb6WtDRVEV1DRVkZYmQkJqB11FJuB8Yj+uvoiHMlFwHjqiiUlJSwvJVP6FHz944fvQw/Px8EB8XBx1dXdjY2GLYiC/QsXPVPU4REX0ugaiwKrJERCQXe/bswcSJEwFk13axsLCQWyzxKZlye24qSEOVSStFkZLOfYPoU6rKLL+nKDKz+BVPkTjvfijvEOijY5PbyDsE+ogzySgOZSVuDEWhUUUvNz/sHS7vEErNuYX00YtUtqroLkJERERERERERERE8sRUGhUXL5siIiIiIiIiIiIiIiKFxUQGEREREREREREREREpLE4tRUREREREREREREQyJ2DRHComjsggIiIiIiIiIiIiIiKFxUQGEZGCmTBhAkQiEUQiESwsLOQdDhERERERERERkVwxkUFERERERERERERERAqLNTKIiIiIiIiIiIiISOZ4lT0VF/9XiIiIiIiIiIiIiIhIYTGRQURERERERERERERECouJDCIiIiIiIiIiIiIiUliskUFEREREREREREREMicQCOQdAlUQHJFBREREREREREREREQKi4kMIiIiIiIiIiIiIiJSWJxaioiIiIiIiIiIiIhkjhNLUXFxRAYRERERERERERERESksJjKIiIiIiIiIiIiIiEhhMZFBREREREREREREREQKizUyiIiIiIiIiIiIiEjmBCySQcXEERlERERERERERERERKSwmMggIiIiIiIiIiIiIiKFxamliIiIiIiIiIiIiEjmlMC5pah4OCKDiIiIiIiIiIiIiIgUFhMZRERERERERERERESksJjIICIiIiIiIiIiIiIihcUaGUREREREREREREQkcwKWyKBi4ogMIiIiIiIiIiIiIiJSWExkEBERERERERERERGRwmIig4iIiIiIiIiIiIiIFBZrZBARERERERERERGRzAnAIhlUPByRQURERERERERERERECouJDCIiIiIiIiIiIiIiUlicWoqIiIiIiIiIiIiIZE7AmaWomDgig4iIiIiIiIiIiIiIFBYTGUREREREREREREREpLCYyCAiIiIiIiIiIiIiIoXFGhlERFQoDVVleYdARCSVmgqvy1EUKemZ8g6BPqqmwa95iuTopNbyDoE+Muy+RN4h0EcxN36UdwhEpCCUwCIZVDz85kdERERERERERERERAqLiQwiIiIiIiIiIiIiIlJYHHNMRERERERERERERDIn4MxSVEwckUFERERERERERERERAqLiQwiIiIiIiIiIiIiIlJYTGQQEREREREREREREZHCYo0MIiIiIiIiIiIiIpI51sig4uKIDCIiIiIiIiIiIiIiUlhMZBARERERERERERERkcJiIoOIiIiIiIiIiIiIiBQWa2QQERERERERERERkcwJwCIZVDwckUFERERERERERERERAqLiQwiIiIiIiIiIiIiIlJYnFqKiIiIiIiIiIiIiGROiTNLUTFxRAYRERERERERERERESksJjKIiIiIiIiIiIiIiEhhMZFBREREREREREREREQKizUyiIiIiIiIiIiIiEjmBGCRDCoejsggIiIiIiIiIiIiIiKFxUQGEREREREREREREREpLE4tRUREREREREREREQyJ+DMUlRMHJFBREREREREREREREQKi4kMIiIiIiIiIiIiIiJSWExkEBERERERERERERGRwmKNDCIiIiIiIiIiIiKSOQFYJIOKhyMyiIiIiIiIiIiIiIhIYTGRQURERERERERERERECouJDCIiIiIiIiIiIiIiUliskUFEREREREREREREMqfEEhlUTByRQURERERERERERERECouJDCIiIiIiIiIiIiIiUlicWoqIiIiIiIiIiIiIZE4Azi1FxcMRGUREREREREREREREpLCYyCCiz2JhYQGBQIAJEyaUuo/AwEAIBAIIBALs2bOnzGKTt5zXtGLFinLp/8aNG+LnuHHjRrk8BxERERERERERkbxxaikiObpx4wa6desmcZ2mpib09fXRvHlzDB06FKNHj4a6urqMIySquMLDw/DP/n245X4D7969g5qqGurUqYPeffvhi5GjoampKe8QK7WYmBj4+jyBr88T+Pn6wM/XB/Hx8QCAgYOGYPXPa+QbYCXyzM8Xd2+747G3JwLe+CM+LhYqKiowMDRCsxZ2cBo8DC3sWpWq79SUFIwaMQjhYaEAAGMTU5w8d6Usw69UYmNi4Ov7BH4+2f/zT/1y/++dBg7Gyp9K9n9/55Y7jh89DD9fH8TFxUJXVw82TW0xdLgzOnbuUg6voHJISkzEvTvueObni+fP/BAVGYH4uDikpaWiWvUasLSqh/YdO2PAoGGoqaMjsY8MoRCPHt7Hf/fu4KmvD0KCApGYmAhNTU2YmtVGqzbtMGT4FzCrXUe2L66S8vP1wS33m/Dy8sQb/9eIi42FiooqDI2M0MKuJYYMHYaWrezlHWaFFxsTAz/fJx+PT77w8/NBwsdj1ICBg7Fi9S8l7vO/+3dx3u0MvL08ER0VBWUVZejr6aN+g4Zo07YdHAcMhJaWdhm/EsWXcuenYrVz93yDPnP+LnC/QCBAQ3MD2DepDfvGdWDf2AxN6xlDXS37J5zes3filldAsePRVFeFy7B2GNq9KSzN9KCuqoLQyARcuPsCW4/cQ3BEfLH7qqrsmjYqVrtW9q2xc8++co6GcvA7H1HVwkQGkYJKSUlBaGgoQkND4ebmhg0bNuDs2bOwsLCQd2hVmoWFBYKCgjB+/PhKNXqksrlx/RoWf/8NEhMTxfelpqTAzy8Bfn6+OH7sCLZs/Qt1zc3lGGXl1r1LB3mHUCXMmDQW3l4eBe4XCoUICQ5CSHAQ3E6fhOOAQfhh2UqoqqqVqP+/tm0WJzGoaD0dOpZJP1lZWfhx5TKcPH403/2RkRGIvBaB69euYMiwEVi8bCWUlDjA+lNP/XywfNE3EtfFx8XCyyMWXh4P8Y/rbixbvQZtO3TK1yYuLhajhzkhISG+wOMTEz/g5YtnePniGY4e2o+Zc7+G86ix5fEyqoyJ40bD0+NRgfuFQiGCgwIRHBSI0yePw2ngYCxfuRqqaiU7jlGu3t07Fd2omN6/T8DKZYtx8/rVAuuSEhMRHByEa1cuwbZZCzRs1LjMnreqGNW3BXYuGV4mfVmZ6eHkr+NhXdcg3/0NzQ3R0NwQE5zsMXHlYZy/+6JMno9IVvidr/IQsEQGFRMTGUQKwsXFBTNnzhTfjoyMhK+vL9atW4fQ0FD4+flh4MCB8PLygrKyshwjzS8wMFDeISgskUgk7xCqpGfPnuK7hfORmpoKLS0tTJ46Ha3btEVqaiounj+HY0cPIygwELNnTsPBw8egrV1N3iFXeiYmprCwtMK9u7flHUqlEx0dCQAwNDRC91590NyuFYxNTJCVmQWfJ974Z98eREVG4NzZU8jIyMCqX9YVu+8Xz5/i33/2QV1dHcoqKkhOSiqvl1EpGZuYwsLSEvfv3inxY//YtFGcxGjUuAnGT5yM2nXqIjQkGHt3/43nz57ixLEj0NHVxZx5C8o69EqhVi1j2Nm3QaPGNjCqZQx9Q0OIsrIQGRGBG1cv4eb1K4iPj8N3C2Zjh+shWDfIvdJWmJ4uTmJYN2yEzl27o0nTZtDT00di4gfcv3sLR//9B+lpafh9/Rqoa6hj0FBnOb3Sii8q8uNxzMgIvXv3RctW9tnHsawsPPb2huveXYiMiMCZ0yeRkZGBNevWyzniysHYxAQWFla4f6/kx6jEDx8wa/pkPHvqBwDo1r0nevTqg9q160BJWRkR797C0+Mhrl25XNZhVzjbj/+Hv47fL3R9UqpQ4v15f9RLF2bA1z8CqirKsK1vXKLnr6alhhO/jhMnMf4+9RBHrjxBapoQXVpZ4ZuxXVGzmgb2rfoS3V3+wpNXb0vUf1U04ouRcP5yZKHrNTW1ZBhN1cXvfERVExMZRArCyMgITZs2zXdf9+7dMXHiRDRr1gyBgYHw8fHBiRMnMHx42VydQ1QZrf3lJ6SmpkJFRQV/7tiF5i3sxOvatmuPuubm2Lh+HYICA+G6ZzdcZs2RY7SV13SXWbBpaoumTW2hb2CAsLBQOPbuIe+wKh1zCyvMmP0VuvXoXSDJ3bRZc/TrPxDTJo5GcFAgLl1ww5DhX8CuGNOzZGZm4pdVy5GZmYnJ02bi9MljTGQUw9QZM2HT1BY2Ntn/9+FhoRjQt2eJ+ggKDMC+vbsBAE1smmLnnv3Q0NAAANg0tUUXh+6YOnEsnvr5Yt+eXRg0ZBjq1uWVhnm1tG+D4+cKXiWeo0fvvnC/fhU/LJwLoVCIXX9twy+//i5eLxAI0LptB0xxmY2mts0LPL5V67Zw6N4bc6ZPRFpaKrb+vgE9+/SHtnbVmz6nLFhYWWHOV/PRs1efAsexZs1bYMDAgRg/ZiSCAgNx/txZjPjiS7Syby2naCu2qdNnoolNUzRpagt9fQOEh4VhoGPJjlEAsHbNj3j21A9qamr4Zd1GdHXonm99E5um6NajFxZ88wMyMzPLKvwKKSouEU8DIkv8uOcBUViw8Qw8noXh8au3SEvPwOJJ3UucyJg/qjMa1DUEACz64zw2/pN7Ucl/fiG45RmAS39MgbamGtbNdZQ4zRXlp6enh/rWDeQdRpXH73xEVRPHohMpuOrVq2PJkiXi21eucG5yosL4PHkinp5i8NBh+T7Q5hg3YRKsrOoBAA7sd4VQKPlKOPo8M2fPRVeHbtA3MCi6MZXa+k3b0LN3v0JH6uno6mLugm/Ft69duVisfv/9Zx+eP/ODuYUlxk6cXCaxVgUus+aiS9fP+7//Z78rMjIyAADf/rBEnMTIoampiW9/yP5ckJGRgQOue0sfcCVVnJGrXbr1QF1zSwDAk0+mZzM0qoXftu6QmMTIYWPbDENHfAkge7qph//d/YyIq7YtW7ejT1/HQrebrq4evv7me/Hty5eKdxyjgqbPnIPOXbtBX7/0xyhvTw+cO3saAOAye16BJEZeAoEAKiq8drI0Hj0Lxbaj9/HALwRp6Rml6kNFWQkzh7cHADwLiMRvBwuOvrnvG4w9Z7OPgV1aWqFVI7PSB00kI/zOV/kIKvAfyRYTGUQVgK2trXg5JCSk0HbXr1/H+PHjYWVlBS0tLdSoUQO2trb45ptvEB4eLvU5wsPD8f3336Nly5aoWbMmVFVVUatWLdja2mLkyJHYs2cP3r9/X+BxFhYWEAgEmDBhQqF9Z2ZmYuvWrWjbti1q1KiBmjVromXLlvj111+RlpZW9BuQx8mTJzFixAjUrVsXGhoa0NHRgb29PVauXIm4uLhCHzdhwgQIBAJxjZH4+HgsW7YMNjY20NbWho6ODrp06YIDBw5IfLyDgwMEAgGCgoIAAHv37oVAIMj35+DgkO8xOfevWLFCYp9v3rzB+vXr4eTkBAsLC2hqakJTUxPm5ub44osvcOHChRK9NwRcv5ab6Bs0ZJjENkpKShgwcDAA4MP793j44D9ZhEYkN61atxEvh4UWfg7J8TY8DDu2bQYAfLt4eYnralDpiUQi3Pg437yFpRWaNW8hsV2z5i1gYZH9I/zN61c5lWEpaWlnT/+Rll6yzyI5WtqXbN+i0mvdpq14OTQkWI6R0L+Hsj8rV6teHc5fjpZzNCRN11ZW0KmeXej4wHnPQs8V+895ipcHdm0ik9iIPge/8xFVXbw8gqgCUMtT1FBVVbXA+tTUVEycOBGHDh0qsM7X1xe+vr7Ytm0bDh48CCcnpwJtbt26hQEDBhRIVERGRoprdRw6dAgGBgYYMGBAiWJPTEyEo6Mjbt26le9+Ly8veHl54eDBg9i5c2eR/cTFxWH48OG4du1avvvT0tLg4eEBDw8PbN26FadOnUK7du2k9vXixQv07du3QH2PW7du4datW7h37x62bNlSvBdYSgEBAahXr57EdcHBwQgODsbhw4cxZswY7N69m1ezFZOXZ/YVZZqaWmjSxKbQdvatc6ek8PbyRIeOZVf8kkjRpKeni5eVlIq+Un3dL6uRkpKCfv0HolWeH2qp/IWFhorrBRQ1dU5L+9YIDAxAZGQEwsPCYFa7tixCrDSCAgPw6kV2YVvzj0mhkkoX5u5byiy6Xq6E+Y5jfK/lRShMh/uN7M/ibdt1gLq6OoDsi5aioiKRlZkFfQMD8f0kXx2a5U47eMs7sNB2Hs/DkJSSDm1NNbS35VSFpPj4nY+o6uIvY0QVwLNnz8TLOSMKcohEIgwfPhxubm4AACcnJzg7O8PKygpKSkp48OAB1q9fj+DgYAwfPhx37tyBvX3u/OhpaWn48ssv8f79e1SvXh0uLi7o1q0bjIyMkJ6ejoCAANy9excnTpwoVexjxowRJzHatGmD+fPnw9raGhEREdizZw+OHDmC6dOnS+0jLS0NPXv2hKenJ5SVlTFq1Cg4OjrC0tISQqEQ7u7u2LBhAyIjI+Ho6AgvLy+Ym0v+EJ6cnAwnJyfExMRgyZIl6NmzJ6pVqwYvLy+sXLkSoaGh+OOPP+Dk5IQ+ffqIH7d7924kJSWhT58+CA8Px6BBg/Djjz/m67skc2NnZmZCTU0Nffr0Qa9evdCkSRPo6ekhNjYWL1++xB9//AE/Pz/s378fVlZWWLlyZbH7rsoC3vgDAOrWrSs1+WNpaVXgMUSVldfHofdA9lz00ly+cA53b7ujRo0a+aakItl48+a1eNnCUvq2svjkOMZERtFSU1IQFRWJO+7XccB1FzIzs6drcR45rlT9eefZt8wtJV+cQGXj0aOH4mVLK77X8vLyxQvxaOr69a2RmJiI7Vs34ezpU/jwIfuCKFVVVdi1ssekKTNg35rJ8KHdm2JYd1uYm+ggM0uEiJgPuO8bjH3nPOHuGVCuz93Ywki8/CIoqtB2mZlZ8A+NQTNrEzS0MCzXmCqDy5cu4tLFC3gbHgYlJSXoGxiieYsWGDh4CFq3kX5BHZUNfucjqrqYyCBScJmZmVi3bp349qeFvnfu3Ak3Nzeoqqri9OnT6Nu3b7717dq1w9ixY9G5c2f4+fnhq6++wu3buUXe7ty5I5526p9//ikw4qJdu3YYOXIkNm7ciOTk5BLF7ubmhlOnTgEAHB0dcerUqXwfNBwdHbFq1SosX75caj+rVq2Cp6cndHR0cOXKFbRq1Srf+k6dOmH06NFo37493r59i0WLFhU6RVRUVBTS09Nx79492NjkXr3RqlUrODg4wNbWFqmpqdi6dWu+RIalZfbVmjkjYnR0dAoUZy8JExMTBAYGwsTEpMC6Hj16YMaMGZg0aRL27NmD9evXY8GCBahZs2apn68qSEtLE08vZmQsvRBijZo1oamphZSUZLx7904W4RHJRVZWFlx37xDf7tmrb6Ft379PwMZffwEAzJy7ALp6euUeH+UXGREhXq5Vq5bUtsZ5jnPv3r0tt5gqOrfTJ/DzyiWFrh8zYQp69+tf4n6jo6Lgdib7Ig8dXb1800xR2crKysKunX+Jb/fp20+O0VRteX8IzBKJMG7kcAQHB+VrIxQK8eD+PTz87z5mzZ2PCZOmyjpMhdLEMv+xvLqWOurXMcCYfi1x+uZTTP3pKN4nlW56u6KYGdUAACQmpyEhMVVq29DIBDSzNoGRbjWoqSojXVi1i7RL88b/db7bycFBCAkOwtnTp9Cte0+s/OkXVK9eXU7RVX78zlc5KQlYbYKKh+NyiRRUVFQUrl27hq5du8LLywtAdhKjU6fc4ZAikQj/+9//AABz584tkMTIoaurK06G3LlzB69evRKvy3tC79KlS6HxqKiooEaNGiV6DVu3bgUAqKurY8eOHRKvlliyZInUhEBiYiL++OMPAMDq1asLJDFymJubY+nSpQCAI0eOICkpqdA+V69enS+JkaN+/foYPHgwAORL9pQHbW1tiUmMHAKBAOvXr4eysjKSkpJY5L0Y8m5zLS2tIttramXPGVzSBB1RRXJw/1489fUBADh074VGUobfb974K2JjYmDbrAUGDR0hqxApj7zHMU0t6aP8NDVzj3MpKTyOlZR1w0bY6XoILnPmQ1DCL88ikQhrf16B5I/ba8KUGZxKpxztc90DX58nAIAePXujiU3pLyShz5OQEC9edt29E8HBQejQsTP2HjiMuw8f4/L1O/h+8XJUq14dIpEIW37fIK77U9UkpaTj8OXHcFlzAj1c/kLbCVvQ/6tdWLPnOqLjs48dA7s2wZE1Y6CiXD4/y1TTyj4uJaakF9ESSE7NbVNNk7WxJNHQ1ESffo5YumI1drkewKGjJ7Dtr78xZdoM6OjoAMiu3TB/zkwWli5H/M5HVLVxRAaRgli5cmWh0wdpaWlhxowZWLNmTb77nz59Cn//7CujPh2p8am8SYp79+7B2toaAPL9mL57927MmzevVPF/KjMzEzdu3AAA9O7dG6amphLbKSkpYfz48fjmm28krr958yYSEhIAFP81CoVCeHh4SEzMCAQCjBo1qtA+WrVqhUOHDiE2Nhbx8fHiD6XlTSgUIiLi/+zdd1gUVxfH8d9SVWyAomLD3nsvsZfYNfrGEmM3lhhNYkxvpvfEaIwaNfYkRmPvvXexocaKgg2VJk3q+we6QmgLArvA95OHJ8POnbtnGWdnd87cc+/owYMHiop6cgeUs7OzfHx8dPLkSfXunfhEZmnh7e1tUrtCRbNOqZLwOBPHJzaXzH/ZPZrA+GFY8neoAVnV8aNHNH3qj5IkRydnvfneh0m2dT92VGtX/SNrGxu9+d5Hqb6wi/SRmvcx2zjzZz0My5i7ebODFq3bqkrV2AvfDx+G6Ya3l7Zt2aTdO7bqo3cnacLEt9WsRatU9blg7izt271TUuyE372f75/OUeOxo0cO6+cfv5ckOTk7670PPzZvQDlcaGiocfnhw4dq1Lipfpz6q6ytY+dfsnNyUp/n+6l8+Qp6afggRUdH65eff1TLVm1y3HmlXM+vEx0Fsf3IZf267IBWfj9EdSq5qkXdsnqpVyNNX3Yg3WPIZRd7uSfChNEVD8OftMltbyspNOnGOdTmbbuUL5Eb+xo3baZ+AwZq3JiXdP7cWR07ekR///WHBgxMW9lCJI/vfEDOxogMIAuoXbu2xo8fn+BEffTok9rMTZo0kcFgSPInb968xrZxR2E0b95cZR/VTH/11VfVsGFDffnll9q3b1+8CWJT6/Lly8a7Hho0SH7C0oYNky7HEPc1FitWLNnXGHdkR1JDRwsVKiRnZ+ckn88pTimVBw8eJBv304qIiNAvv/yixo0bK2/evCpZsqSqVq2qGjVqGH98Hk36eu/evXR97pIlS5r0k5XYxbkb1pS7oB5P0mqfK1eGxQSYy5XLF/X2xFcUFRkpe3t7ffHNj3JySvy9Lzw8XF999pFiYmLUt/9AVahYKZOjxWOpeR+LO/mxfS5GAyQlX778Klu+gsqWr6Aq1WqoXcfO+vK7Kfrgky9184a33p74itatNn0esE3r1+q3X6dKklyLl9DHn3/D5NMZ5NKli3pt/DhFPnof++6HKcl+hkPG++/Io1denWhMYsRVu249tW7bXlJsOapLFy9kSnyWJLlSTj5+wRrw/hKFR8TO0zOmT8bMqxAWHtu/rW3CffRf9nZP2oQ+ZDRBYhJLYjzmXKiQvv1himxsYr+v/7kk8TLHeHp85wNyNj51AxZizJgxOn36tE6fPi13d3etWbNGgwcPlpWVlfbv369WrVrp7t34k7Q9vsidWnGHVdra2mrNmjWqUqWKJOnIkSN699131bx5cxUsWFDPPvuslixZEm+UgCl8fX2Nyy4uLsm0TL4OeHq8xrhSGn4a92JEal9zavj6+qpJkyYaN26cDh06lGLSKO4dcEhc3MnWTRk6HBoS+zc1ZUgykJXcvOGtCWNGKjAwUNbW1vr0y+9Up179JNvPmz1T1zyvqkjRoho5ZlwmRor/ivs+FhqSdIlEKX45qbhlpmCaZ7t0V+t2HRUdHa0fv/lcgXFK5iRl/55d+mLye4qJiZGzcyH9+Mtvci7ExLgZwdvbS6NHDlNgYICsra319Xc/qF795G+MQcbLE6fknaOjkypXqZpk2yZNmxmXPR6VOMQTnjf9tO1I7Mj68iULqVih9J9TISgk9s51U0pF5cn1pI0ppaiQUImSJdW4SVNJktf1a/LxuZPCFkgLvvNlT4Ys/IPMRWkpwEK4uLjEG1FQu3Ztde3aVa1bt9aQIUPk6empESNGGCfPluJfaF+zZo3c3NxMfq64qlatqtOnT2vNmjVas2aNdu/erUuXLik0NFSbNm3Spk2b9MMPP2j9+vUpJiUS8zRDyeO+xuPHj5s0fFSSSpSw7JJIEyZM0LFjxyRJPXv21LBhw1SzZk25uLgoV65cxr9ZqVKl5OXlpZiYmHR9fi8vr3TtzxLY29urYMGC8vf3l08Kk7kFBgQYLwIWTWGSOCAruevjo1dGD9fduz4yGAx676PP1KJ122S3WThvtiSpQaMm2rtrZ6Jtwh4lU8NCQ7Vl43pJkqOTk+o3zJi7SHMqlziJ/Tt3kr8AEnfkYdGiSc+5hKQ907K1tm/ZqNDQUB3cv1cdOnVNsu3xo4f13luvKTIyUvny59cPv8xSiZKlMjHanMPH545GjRiquz6x72OTP/1Crdu0M3dYkFQkzmcml2RuRJKkIkWevC/5P5qYF/Gd9/RRp6axoyBdC+XXrXvpOxr8hk+gVC12rowCeXMlO0qkhEsBSZKPXxATfT+FsuXKae+eXZKku3d85OKS/HGC1OM7H5CzkcgALNzgwYO1Zs0aLV++XKtXr9b27dvVpk0bSYo3vL5gwYLJTpqdEmtra/Xs2dM42fWtW7e0ceNG/fLLLzp27JiOHTumUaNGacUK08ovODo6GpdTuhiT3Pq4r7Fw4cIWn6AwRWBgoP766y9J0gsvvKBFixYl2dYvg774mfp3DIvMkKfPMGXLldfxY0d1/fp1RUZGJjrBvCRdvXrFuFymbLnMCg/IUP5+fho/ZrhueMcmKie+9Z46d+uR4naPh+WvXbVCa1cl/x7v7++nD955Q5JUp14DEhnprGzZ8sZlzzjvU4nx5H3sqRV0fFJO8vatW0m2O3vmlN587WWFP3yo3Hny6PufZ6p8BUqwZQQ/P1+NGjFM3o9uuHj73Q/UrUdP8wYFo3LlnrxHRUdHJ9s2KvrJxXBrm5RLG+VE6X2j0n+d8/RRr0fLlUoX1mGPxG9ksra2Utnise+H/3reTbQNTJPT5oIxF77zATkXpaWALOCLL74w1p999913jY/XqVPHuLxv3750fc5ixYpp6NChOnDggOrWrStJWrt2rckljsqVK6fcuXNLii1XlZzk1mfka0yL9PhwevHiReOFw759+ybZ7vz58woKCnrq58tJ6tStJym25MrZsx5Jtjsa599c7Tp1MzwuIKMFPXigCS+P1NUrsWUqxo5/XX36DjBzVEit4iVKqPCjkY/HjiZ/7jx+LHYOKReXInItXjzDY8uO7sYp+5E7iZITly7+q9dfGaXQkBDZ2dvrmx9/UbUaNTMrxBzlwYMHGvPSCF25fEmSNOG1ieo34AUzR4W4irkWV9FisSMtbt68keyFeO84o38Lc1d6oiq7PRnpnt6jMSRp/6lrxuVnarsl2a5e5eLKmyd23oEDp68l2Q4pe/z+Jcl4Pkf64ztfNmTu+lDUlsoySGQAWUDFihX1/PPPS5IOHTqkLVu2SJLq1q1rvLN+1qxZCgtLerhwWtna2qply5aSpMjISPn7+5u0nY2NjVq1aiVJ2rx5s24lcadjdHS05s+fn2Q/7dq1M9az/PnnnzP8zqWU5Ho0SdjDhw/T3Edk5JNhDsHBSddAnzFjRpqfI6eKW3pi1YrlibaJjo7W2tUrJcVO2tegYaPMCA3IMGGhoXp9/Bj9e+6sJGnIiFEaNHSEydsfdD+b4k/RYq6SpKLFXI2P/To76fdupI3BYFCrR6XAPK9e0amTJxJtd+rkCeOIjJat23IHaBrt2LrZuFyufIUE669f89RrL4/Ug8BA2djY6PNvflLd+g0zM8QcIzQ0VOPGvKRzjy5IjXxptIaNeMnMUSExbdp2kCQFBwXp8KEDSbbbsW2LcZkLiAmVLuaotg1iR7hc9r6vm/cC0/05dh+/Kv8HsTehvdAp6X0wsPOTdat3nU33OHKKG97eOnhgvySpZMlSKZZfQ9rxnQ/IuUhkAFnEu+++a7xQ8dlnn0mKnZj68QiNK1euaNCgQcleYA8MDNS0adPiPbZnzx5dunQpiS2k8PBw7doVW+czb968KlzY9Ektx4wZIyn2ov+oUaMSnTz7yy+/1OnTSU8AWLBgQY0bFzv57P79+/Xaa68lO5T9zp07mj17tskxplaxR3ehXb58Oc19lC9f3rgv58+fn2hyZs2aNQn2FVJWo2ZN1X00qfHKf5br5An3BG0WzJurK4/uWn9h4CCT510BLFFERLjemjhep04clyT1HfCiRr88wcxR4WkMGDjIOArzmy8/S3CTQlhYmL75MvZzgI2NjV54cVCmx2jp1q1ekeINB38unq8D+3ZLklyLl1CtOvXirb9966ZeHTNcvvfvy9raWh9//o2aNm+RYTHnZBHh4Xpt/DidcI99H3th4CCNm/CamaNCUgYMHCR7+9i793/87utERw+vX7tax44eliQ1f6ZljpvHp3OzyrK2TvpSi4ujg/74fIDs7WLL4cz651CGxBERGaXpy2KTTVXKuOi1Ac0TtGlUraSGdI19/9t9/IqOnb+RIbFkdbt2bo93M9p/3b93T2+8Nt446v5//fpnVmg5Et/5gJyLOTKALKJ69erq3r27Vq1apd27d2vv3r1q3ry5Ro8erS1btmjFihX6+++/dfz4cY0aNUoNGzZUgQIFFBgYqPPnz2vnzp1avXq1cuXKZUwMSNK2bdv06aef6plnnlGXLl1Us2ZNFS5cWKGhobpw4YJmzJih48djv1gOHz48yfqTienWrZu6detmnES8WbNmeu2111ShQgX5+Pho3rx5+uuvv1S/fn0dPXo0yX4++eQT7dq1S4cOHdKUKVO0c+dOjRw5UrVr15aDg4P8/Pzk4eGhrVu3asOGDapRo4ZGjDD9buTUaNq0qXbs2KEjR47oq6++UqdOneTg4CBJyp07t4qbUN7D2dlZnTt31rp167Rx40Z16NBBY8aMUenSpeXj46Ply5dr3rx5Klu2rPz9/XX3LrVqU+PNd97TkIH9FRYWptEjh2nES6PVoGEjhYWFaeOG9Vr+d+z8JKXd3DRoyFAzR5t9HT92VF7Xrxt/9/d/Mt/L9evXtGrFP/Ha9+j1XKbFlp188PYkHToQW3avfsNG6tazty5fuphke1tbW5Uq7ZZJ0eU87sePyev6k7Iccf/de3ld1+qV8f/dd++Z8N99abcyGjRkmH6f85vOepzRsEEDNHjYCJUsWVJeXl6aP3e2zj8affPikGHsz0TMnTVd0376Vq3atFfN2nVVvERJ5c6TRyHBwbpy6aI2b1irUydjL3rY2trqzfc+NiaPJCnA31+vjh2hO3diJxHtN3CwSruV1ZVkjq18+fNTPieN3po0UQf275UkNWzUWL1699HFixeSbG9rays3tzKZFV62cuL4MXl5JX5u9rp+XWv+M09Stx699F9Fi7lq1NhX9POP3+nSxQsa/MLzGjx0hCpUrKTgoCBt37ZFy//+U5LkkDevXp/0dga9Gsv1w2tdZWtjrZU7z+jQGS9du+Wn0PBIORfIoxZ1ymh4j4Yq7Bj7/WHfSU/N+Odgov0M7Fwn3u+1KjxJCHVoVEGlixU0/n7F2zdeKanHflyyR33a1lDFUoX1xcudVLa4s/7edkphDyPVom4ZvTmolWxtrBUSFq5JP69Ph1efPX39xWeKjIxU23YdVLN2bbm6FleuXLnk5+enY0cOa9nffxknta9Tt5769qcsXkbjOx+QMxlizF2nBcjBdu7cqdatW0uSPvroI3388cfJtj9y5IgaNowtadChQwdt2rRJUuxErRMmTNCMGTNSLL1UpkwZXbnyZNKrjz/+WJMnT04x1h49euiPP/4wznvxmJubm65du6bBgwdr3rx5CbZ78OCBOnXqlOT8FnXq1NHs2bNVr17snUC///67hgwZkmg/Q4YM0T///JNg3X+1bt1a27dvj/fYkCFDNH/+fJUuXVqenp5Jbjtv3jwNHRr7Qefq1atyc3OLt/7GjRuqWbOmfH19E2zbsmVL7dy50/j741EXie1bLy8vNW/eXNfjXOiNq1SpUtqwYYM6d+6c5N837r+fHTt2GEt5paesNtn3Yzt3bNd7b09Kco6R0m5umjZ9lkqVLp3JkeUcH7z7tlanMHF0XCc9/s3AaNJfaHjCEWbm0LhO1VS1L1rMVSvXb0318/Ts3E63b91M8/YZyc7GcgYYf/Te21rzqIyBKY6fPp/o49HR0fr04w+SLJcgST2f66P3P/pEVlaW8/ot5bjo3bW9bt+6mWI7lyJF9c6Hn6ph46bxHj9+9LBeGZW6ix6duvbQ+5O/SNU2GSlvrqxzv1qtaqmbON3Vtbg2bNmeckMLEhGZ/MTYmeXjD94xlloxxdGT55JcN23KD5r/++wkv3s4OTnru5+mqmatOomuNxeXth9k+HOcX/aGShdzTLHdih1nNOarFQoISrw8cOi+z01+zoXrj+ulzxM/Z5Qt7qSV3w1WhVKFEl0fEBSmoZOXasP+zP0sdn/nZ5n6fE+jc4c2unUz5fNK2/Yd9NHkz5Qvf/5MiCr9WGXREpXZ8TtfFjp9p6tDlwPMHUKaNSpXwNwh5Cg59BABsqYGDRqoffv22rJlizZv3qwjR46oQYMGsrW11fTp0zVmzBj99ttv2rlzp65fv66goCDlzZtXZcqUUb169dSpUyd17do1Xp9vvPGGatasqa1bt8rd3V03b96Uj4+PJKlo0aJq2LChBg0apC5duqQp5nz58mnnzp2aMWOGFixYoHPnzslgMKhcuXLq27evXn31Vd2+fdukfpYvX669e/dq/vz52rNnj27evKnQ0FDlz59f5cqVU8OGDdWlSxd16NAhTbGaonjx4jp8+LC+/PJL7dq1S97e3mmam6RkyZI6fvy4vv76a61atUrXrl1Trly55Obmpp49e2rChAlydEz5CxAS16p1G/29YrUWL1ygPbt36s6dO7F3opcspfYdn1W/AQMTJOUAwFJYWVnpo08+V9t2HfTPsqXy8Dgtfz8/FXR0VLVqNdT7f33V7BnKHCXlh2mzdGDvLp066a4bXtfl63tfAf4Bss9lL0dHJ1WoVFlNm7dU2/bPKhfnAiBNxk14XS1atdaypX/qxPFjunfvruzs7VWqlJtatGqtfv0HKm++fOYO0yxGfLZMz9Qpo0bVSqlMcUc5F3BQfgd7BYWEy9snQAfPXNfi9cd1yMMr5c7SwZUbvmo8dJpGP9dYz7WprrLFnWVnay1vnwBtOvCvfll6QNfv+GdKLFnVJ59/pWNHj+jUyRO64e0lfz8/BQcHK3eePCpapKhq1q6jbj16qlZty0rcZXd85wNyHkZkAACSlFVHZAAZzVLuPIdljcjI6TguLEdWGpGRE1jKiAxkzogMmCYrjcjI7rLqiIzsKKeevhmRAVPxzQ8AAAAAAAAAAFisHJrrAwAAAAAAAACYE4OCYCpGZAAAAAAAAAAAAItFIgMAAAAAAAAAAFgsSksBAAAAAAAAADIdlaVgKkZkAAAAAAAAAAAAi0UiAwAAAAAAAAAAWCwSGQAAAAAAAAAAwGIxRwYAAAAAAAAAIPMxSQZMxIgMAAAAAAAAAABgsUhkAAAAAAAAAAAAi0VpKQAAAAAAAABApjNQWwomYkQGAAAAAAAAAACwWCQyAAAAAAAAAACAxSKRAQAAAAAAAAAALBZzZAAAAAAAAAAAMp2BKTJgIkZkAAAAAAAAAAAAi0UiAwAAAAAAAAAAWCwSGQAAAAAAAAAAwGIxRwYAAAAAAAAAINMxRQZMxYgMAAAAAAAAAABgsUhkAAAAAAAAAAAAi0VpKQAAAAAAAABA5qO2FEzEiAwAAAAAAAAAAGCxSGQAAAAAAAAAAACLRSIDAAAAAAAAAABYLObIAAAAAAAAAABkOgOTZMBEjMgAAAAAAAAAAAAWi0QGAAAAAAAAAACwWJSWAgAAAAAAAABkOgOVpWAiRmQAAAAAAAAAAACLRSIDAAAAAAAAAABYLBIZAAAAAAAAAADAYjFHBgAAAAAAAAAg0zFFBkzFiAwAAAAAAAAAAGCxSGQAAAAAAAAAAACLZYiJiYkxdxAAAMv04GG0uUNAHAYG3QIJhEVEmTsEPJLL1trcIeCRiCjO35bE3ob7By0GH6UsxgcbL5g7BDzyftvy5g4BjzjmyZmfpU5ef2DuENKsVql85g4hR2GODAAAAAAAAABA5iPJDBNxawgAAAAAAAAAABbAx8dHa9eu1YcffqhOnTqpUKFCMhgMMhgMGjJkSKr727Bhg3r16qUSJUrI3t5eJUqUUK9evbRhwwaT+4iMjNSMGTP0zDPPqHDhwsqdO7fKlSunUaNGycPDI9UxpQUjMgAAAAAAAAAAsABFihRJl36io6P10ksvac6cOfEev3Hjhm7cuKGVK1dqxIgRmjlzpqyskh7vcO/ePXXu3FlHjhyJ9/iVK1c0a9YszZ8/X9OmTdOIESPSJe6kMCIDAAAAAAAAAJDpDFn4v8xQqlQpdejQIU3bvvfee8YkRp06dfTHH3/o8OHD+uOPP1SnTh1J0uzZs/X+++8n2UdUVJR69eplTGI899xz2rBhgw4dOqSff/5ZLi4uevjwoUaNGpWqER5pwWTfAIAkMdm3ZWGybyAhJvu2HEz2bTmY7NuyMNm3BeGjlMVgsm/LwWTfliOnTvZ9yivI3CGkWc2SeTOk348++kgNGjRQgwYNVKRIEXl6eqpMmTKSpMGDB2vevHkp9nHhwgVVq1ZNkZGRql+/vnbv3q3cuXMb14eEhKhly5Y6evSobGxsdO7cOZUvn/D9YO7cuRo+fLgkaezYsfrll1/irb906ZLq1aunwMBAlS9fXufOnZONTcYUgeITFQAAAAAAAAAAFmDy5Mnq2rXrU5WY+umnnxQZGSlJmjp1arwkhiTlyZNHU6dOlRQ7/8WPP/6YaD/fffedJMnJyUnffvttgvXly5fXO++8Iyk2qbFixYo0x5wSEhkAAAAAAAAAAGQDMTExWrVqlSSpcuXKaty4caLtGjdurEqVKkmSVq1apf8Wbrpw4YLOnTsnSXr++eeVJ0+eRPuJOwE5iQwAAAAAAAAAQLZiMGTdH0t19epV3bx5U5LUsmXLZNs+Xn/jxg15enrGW7d3794E7RJTtGhRVaxYUZK0b9++tIRskowpWAUAAAAAAAAAQDbl7e1tUrsSJUpkcCTxnT171rhcuXLlZNvGXX/u3DnjXBxp6efChQvy8vJScHCwHBwcUht2ikhkAAAAAAAAAACQCiVLljSp3X9LNmW0uAmWlJIocV+Dl5fXU/cTExMjb29vY8mq9EQiAwAAAAAAAACQ6Sy4QlOW9eDBA+Ny3rx5k20bd+REUFBQhvSTXkhkAAAAAAAAAACQCv8dwWApwsLCjMt2dnbJtrW3tzcuh4aGZkg/6YVEBgAAAAAAAAAAqZDZc1+YKleuXMbl8PDwZNs+fPjQuJw7d+5k+4n7e2r6SS9WGdIrAAAAAAAAAADIVPny5TMup1TmKTg42Lj83/JR6dVPeiGRAQAAAAAAAADIfIYs/GOh4o4UiTthd2Lilsf67+TlaenHYDBk2EgVEhkAAAAAAAAAAGQDVatWNS6fP38+2bZx11epUuWp+ylZsmS8ib/TE4kMAAAAAAAAAACygTJlysjV1VWStGvXrmTb7t69W5JUvHhxubm5xVvXvHlz43Jy/dy+fVsXLlyQJDVr1iwtIZuERAYAAAAAAAAAANmAwWBQjx49JMWOlDh48GCi7Q4ePGgcSdGjRw8ZDPHrZVWsWNE4SmPp0qUKCQlJtJ958+YZl3v16vW04SeJRAYAAAAAAAAAINMZsvB/luzVV1+VtbW1JOmVV15RaGhovPWhoaF65ZVXJEk2NjZ69dVXE+3njTfekCT5+vrqzTffTLD+8uXL+vLLLyVJ5cuXz9BEhk2G9QwAAAAAAAAAAEy2d+9eXbp0yfj7vXv3jMuXLl2KNwJCkoYMGZKgj4oVK2rSpEn66quvdPToUTVr1kxvvfWWypUrp8uXL+vrr7+Wu7u7JGnSpEmqUKFCorEMHjxYc+fO1b59+/TLL7/o9u3bGjlypBwdHXX48GF9+umnCgwMlJWVlX7++WfZ2GRcusEQExMTk2G9AwCytAcPo80dAuKw9Ds+AHMIi4gydwh4JJettblDwCMRUZy/LYm9DYUQLAYfpSzGBxsvmDsEPPJ+2/LmDgGPOObJmZ+lPG4EmzuENKtWPGMmtR4yZIjmz59vcvukLu9HR0dr5MiRmjt3bpLbDh8+XLNmzZKVVdKfV+7du6fOnTvryJEjia63t7fXtGnTNGLECJNjTgs+UQEAAAAAAAAAMp3BkHV/LJ2VlZXmzJmjdevWqUePHnJ1dZWdnZ1cXV3Vo0cPrV+/XrNnz042iSFJhQoV0v79+zV9+nQ1b95czs7OypUrl8qWLauRI0fq2LFjGZ7EkBiRAQBIBiMyLAsjMoCEGJFhORiRYTkYkWFZGJFhQfgoZTEYkWE5GJFhOXLqiIyzN7PuiIyqrhkzIgOJ4xMVAAAAAAAAAACwWCQyAAAAAAAAAACAxcq4acQBAAAAAAAAAEgCVf9gKkZkAAAAAAAAAAAAi0UiAwAAAAAAAAAAWCxKSwEAAAAAAAAAMh+1pWAiRmQAAAAAAAAAAACLRSIDAAAAAAAAAABYLBIZAAAAAAAAAADAYpHIAABJnp6eMhgMMhgMmjdvnrnDAQAAAAAAyPYMWfg/ZC4m+waQpe3cuVOtW7dOdF3u3Lnl7OysWrVq6bnnntMLL7wge3v7TI4Q6cX3/n15nDkljzOndfbMGXl4nFaAv78kqWv3nvr4sy/T3HdYaKj6PtddN254S5KKubpqzcZt6RF2tuR7/77OGPfF6QT7YvJnX6XYR2hoqA7s26ODB/br3Nkz8rp+XSGhIcrr4KBSpd3UpGlz9X6+nwoVKpzBrybrY39YhuCgIB3Yt1vnPM7o/DkP3fW5I38/Pz18GKa8+fKrTNlyatLsGXXt0VsFChZMtI/bt27q8IF9OutxWpcu/ivf+/fl7+enGMWoYEFHVahUWW3adVS7Dp1kY2ubuS8wi+G4sBznPM5o/97dOnniuK5euSx/P1/Z2NioUGEX1axdR9169lbtOvXS1HdYaKgG/K+Hbj46fxct5qqV67emZ/g5VkREuNasXqWtmzfq4oULCgjwl42NrVyKuKhWrTrq1ed/ql27rrnDzPbqVK9sUrt69Rto9ryFGRxN1lSigL2quORVGefcKpLXTnntrBUVIwWGReqqb6gOewXoqm9osn00KJFf/eoUM+n5/nS/pSPegQke71DRWR0rFUpV7Jv+vafNF+6napusLj3OGVevXNbRwwd1zuOMLl26ID9fXwX4+8nKylpOzs6qUq26Oj7bRc+0aiODgQvRQFZCIgNAthUaGipvb295e3tr3bp1+uGHH7R27Vq5ubmZOzSkQYfWzTOs7xm/TDUmMZCy9q2bPdX2Fy/8q2GD+iskJCTBuoCAAJ0+dVKnT53U4kXz9f6Hn6jDs52f6vmyO/aHZTjrcVofvTsp0XX+fr5yP+Yr92NHtGTB7/rw06/UqGnC97TVK/7W/DmzEu3D585t+dy5rX27d2rJgt/11Q9T5Vq8RLq+huyE48IyjB72ok64H0vweEREhLyuX5PX9Wtat3qlOnftoXc+nCxbW7tU9T/r16nGJAbSz82bNzT+5dG6fOlivMcjIiJ0zdNT1zw9tXrVCvUbMFBvvv0eFwJhscY2LalyznkSPG4jqXBeOxXOa6eGpQroiFeA/j55W1ExmR9jcu4GhZs7hEyVXueMeXNmatP6tYmuu3nDWzdveGvb5o2qU6+BvvpuSpI3mACwPCQyAGQbY8aM0dixY42/+/j46MyZM/r222/l7e0tDw8Pde/eXe7u7rK2to63rZubm2JiLOyTK5JUtFgxuZUpq4P79z11X+fPndUfixfI3t5eNjY2Cg4OTocIc46ixVzlVqZMqvZFUFCQ8eJgrTp19UyLVqparboKFCgoPz9f7di2RSuW/63goCC9/84kOTjkVbNnWmTUS8hW2B/mVaRIUdWp31CVq1STS5Gici5cWDHR0fK5c0c7t23Wrh1b5e/vp7deH6ffFvypChXj32lrMFipfMVKqlm7ripUrKxChQrL0dlZIcHBuuHtpfVrVuj0yRO6fOmCJowZoQV//aPcuRNeoEF8HBfmc++ejySpcGEXtWnfUbXq1FPRYsUUHRWt06dOaMnCebrrc0fr165SZGSkPvnyW5P7/vf8Wf21ZKHs7e1lbWOjEM7f6SIiIiJeEqNCxUoaOGiI3NzKKCQ4WO7ux7Rw/jyFhobozyWLVLiwi4aNeMnMUWd//+vbX8/365/kes4FiSuQK/aSV0BohE7eeqArvqHyD4mUwSC5OeVWy7KOKpjbVg1KFpC1waDF7rdS7HPmQS8FhkUmud4/NPF1+z39derWg2T7NhgMerlpSeW2tVZoRJRO3w5KMZ7sJL3OGdbWNqpWo6Zq1qqjchUqytm5kBwdnRQYGKBrnle1cvlSXb50Ue7HjuiNCWM18/dFsrKi8j6QFZDIAJBtuLi4qHr16vEea9OmjYYOHaqaNWvK09NTp0+f1ooVK9SnTx8zRYm0GjlqrKpWr66q1WvI2bmQbt64oe6d2j1Vn1FRUfp88oeKiorSyNFjtWrFchIZJojdFzVUzbgvvNUtFfvCysqg9h076aXRL6tsufIJ1jdp2lxNm7fQG6+OU1RUlL756jOtbL6JOz6TwP6wDHXrN9Q/65MuSde2w7PavWOb3nljvCIiIjR31q/68rsp8doMHTlGI8e8kuj29Ro0UvdeffTTd1/q7z8W6eYNL61d+Y/+139gur6O7ILjwjKUdiur0eNeVeu2HRLcRFK9Zi116tJdLw19QdeveWrzxnXq1aev6tSrn2K/UVFR+vKTjxQVFaXhL43V6pXLSWSkk507thmTGDVr1dbc+Yvj7bvGTZupZas2GjywvyIjIzRv7mwNGjJMNjZcWshITk5OKl+hornDyHJ8gsK1/tw9nbr1QP+9Ze26f5iOeQdqXLNScslrp7ol8uvANX9dSaHM1N2gcPklkaxITlB4lILCo5JtU9nFQbltY4+3UzcfKDI6Z91ol17njHc//CTJ96SGjZvquf/103tvvq6d27fo9KkT2rt7p1q0apMhrwmm4eMTTEXKEUC2ly9fPr3//vvG37dupXZyVjTq5Vf0TMvWcnZOXW3Z5PyxeKHOnfVQabcyGjxsRLr1m92Nfnm8WjzFvqhVu66++vbHRC8OPtaqdVu1adtekuTtdV3nz51N03PlBOwPy/DfL9yJadG6rUqVLiNJOpVI6QRTLgS+OOTJe9XJRPpALI4Ly/D9z7+qXYdOSR4fBR0dNf71N42/b9+6yaR+/1qyUOfPxZ6/Xxw6PF1iRayTJ9yNy8NGvJTovqtarbpatGwlSXrwIFBXr1zOrPCAVJlz+IZOJpLEeCw4PEqrPXyMv9csli9zAktC/RL5jctHE5lnI7tLr3NGSp+nrK2tNXDwMOPvfJ4Csg4SGQByhBo1ahiXvby8Eqz39PSUwWCQwWDQvHnzjI+HhIQoX758MhgMeuGFF1J8ngMHDhj7mT59eqJtbt++rffee0/169eXk5OT7O3tVbJkST3//PPJJlkSi/Gff/5R586d5erqKhsbG7Vq1SrFGBHr1s0bmvnLVEnSOx98nOq63Mh49Rs2Mi57eyc8bpG52B/pI49DbPmPh+EP07i9g3E5PI19IP1wXDy9eg0aGpdvmPA3vHXzhn77Nfb8/eZ7H3H+TmeRERHG5RIlSibZrkTJJ+si4mwDZDWX7z+ZC8nZwdZscdjbWKla0bySpPvB4SmODMmpUnvOSMrjz2OS9PAhn6eArIJEBoAcwc7uyZdcW1vTP6DmyZNHPXv2lCStWrUqxbJDixcvlhR7F8jzzz+f6Pry5cvriy++0LFjx+Tn56fw8HB5e3vr77//Vvv27TVixAhFRiY/XDkmJkaDBg1S7969tWHDBt26dUtRUckPVUZ8X33+iUJDQ9S5a3fVj/OBGJYjPPzJBIfW1K01O/bH07vmeVUX//1XklTarUya+ti6aYNxuZRb2XSJC2nHcfH04v4NraxSHtn07ZefKjQ0VJ26dFe9+py/01vc96bkknPej24MMhgMKlXaLaPDAjKMtdWTmjbmnDKxVrF8srOOPY8cy4GjMUyV2nNGUrZsfPJ5yq0Mn6fMzZCFf5C5KGQJIEc4d+6ccdnNzS1V277wwgtatGiRgoODtWrVKg0YMCDRdpGRkfr7778lSR07dlShQvHLWSxdulQvvviiYmJiVLZsWY0bN05Vq1ZV4cKF5enpqTlz5mj9+vWaM2eO8ufPrx9++CHJmH766SedOnVKzzzzjMaMGaOKFSvK399fnp6eqXptOdWmDeu0b89u5c9fQK+98Za5w0ESjh89YlwuU7acGSOBxP5Iq7DQUN2966N9u3do8YK5ioqKTVQ/33+QyX0EBgbo9s2b2rh+tZb/tURSbFK+V5++GRIzTMdx8fTcjx01LruVTf5i0paN67V/727lz58/XnkRpJ9nO3fV9GlTFBQUpHlzZ6v5My0TlHk5f+6s9uzeKUnq1Lmr8ubNa4ZIc5Ytmzdp86aNunXzhqysrORcqLBq1a6t7j17qUHDxuYOL0sr5/zkzvw7QSnfmd+vdjEVzmsnBztrPYyM0r3gCF24G6L91/yTnQQ8JfVL5uyyUqZKzTnjv/z9/OR1/ZpWr1imtatXSJIKFnRUx85d0zVGABmHRAaAbC8qKkrffvut8ffUTvTdrl07ubi4yMfHR0uWLEkykbF161b5+MTWWP1vGap79+7ppZdeUkxMjIYNG6aZM2fGq91Zt25dPffcc3rvvff0xRdfaMqUKRo1apQqVaqU6HOdOnVKgwYN0rx585hYNJUCAwP0wzdfSZLGvfq6HJ2czBwREnPh3/Pau2eXJKl8hYpcIDQz9kfqrFu9Ql9Mfj/J9QOHjFCHTl2S7eOzj97VhrWrEl2XK1duffDplyqeTNkXZDyOi6cXHR2tBb//Zvy9Xftnk2wbGBigH7/7UpI0djzn74zi6OioT7/4Ru+8NVEn3I9rYP//acDAQSpd2k0hISE6eeK4Fs7/XREREapSpape54aQTHHl8qV4v4dcvyav69e0dvUqtW7TTpM//1L58pl3foesyCCpTfkn7yUnbz5IcZvyhZ4kPmzsbORgZ6PSjrnVqpyjVnr46OC1gFTH4ZjbRmWcckuSrvqG6H4I5doSk5pzxmNjRgyW+7Ejia4rWNBRX/3ws/Lly5/oegCWh0QGgGzr7t27On36tD788EO5u8dOXNinTx81b948Vf3Y2Niob9++mjp1qjZv3qz79+/L2dk5QbvHZaXy5s2rHj16xFv366+/KiAgQMWLF9f06dOTnIBs8uTJmj9/vm7cuKEFCxbo888/T7RdwYIFNW3aNJIYaTDl+291//491axVW716/8/c4SAR4eHh+vTj943l0l5+5VXzBpTDsT/ST4VKlfXWex+rSrUaKTdOQruOnTR2/EQVKVosHSNDanFcpI8/Fs3X2TOnJUmt2rRX5arVkmw79cfv5Hv/vmrUrK0ez3H+zkitWrfRkj+Xa+GC37Xyn2X68L234613di6kseMmqFfv/yl37txmijJnyJU7t1q2aq2GjZqoTNmyypMnj/x8fXXs6BEtW/qn/P39tWP7VgW+EqBff5ubqhK6kFqUdVRpx9h/w6duPZB3QNIjMu4Fh+v0rSBd8wuVf2jsyAtnB1vVKJZXNYvlk621lf5Xs6gUIx28nrpkRr0SBWT16HvdES9GYyQlNeeMlDzff6CGjRyjgo6O6RUegExAIgNAtjF58mRNnjw50XV58uTR6NGj9dVXX6Wp7xdeeEFTp05VRESEli5dqjFjxsRbHxoaqpUrV0qSevbsqTx58sRbv3r1aklS165dZW9vn+Tz2NjYqEmTJlq2bJkOHDiQZLtu3bo91V1X3t7eJrUrUNg1zc9hiY4fPaLVK/+RtY2N3vngYxJBFurrLz7VWY8zkqSu3XuqRas2Zo4oZ2N/pF6L1m1VpWp1SdLDh2G64e2lbVs2afeOrfro3UmaMPFtNWvRKtk+Rr08QQNeHCpJCg4O0qWLF7RmxTJt3bRBPnfu6N2PPlPJUqUz+qUgCRwXT+/40SOaPvVHSZKjk7PefO/DJNu6Hzuqtatiz99vvvcR5+8MFhERrrVrVmrnjm2KSWTSgPv372nd2tVyLV5CrVrzbz8jbd62S/nyJ7xbvHHTZuo3YKDGjXlJ58+d1bGjR/T3X39owEDTyxbmdGWdc6tLlcKSpAcPI7X81J0k256+HaQjiZR78goI04mbD1TFJVBDGhSXjZVB3au5yONOkB48NH3+wnolYvdxRFS0SaNCcqLUnDPi+mDy5woNDVVMTIyCHgTq3FkP/fP3n1r21xLduOGtdz/8RM7OhVLuCBmL0zpMxIx0AHKE2rVra/z48Wm+S6lRo0YqVy62ZMTjkRdxrV69WkFBQZISlpWKiorSiRMnJEkzZ86UwWBI9mfZsmWSpNu3bycZT82aNdP0Oh4rWbKkST/ZSXh4uD7/5CPFxMSo/4AXVaFi4mW7YF5zZ8/Uyn9i55qpVr2G3n7XtC8pyBjsj7TJly+/ypavoLLlK6hKtRpq17Gzvvxuij745EvdvOGttye+onWPajMnpbBLEWMfNWrVUa8+ffXbgj/Vo/fzOnXiuEYO7qeLF85n0itCXBwXT+/K5Yt6e+IrioqMlL29vb745kc5OSUc7SrFnr+/+iz2/N23/0DO3xksNCREo0YM09zZsxQYEKAhQ0fon1Xrdfj4Ke05cFS/zpyjOnXr6azHGb0+4WUtnP+7uUPO1hJLYjzmXKiQvv1himxsYr/f/Lkk4XcUJK5IXjsNrV9c1lYGRURFa8HRmwoKTzrxEBYZnWx/53yCteXCPUmSvY2VGpYqYHIspQrmkkteO0nSmdtBKT5XTpSac8Z/uRYvoXLlK6h8hYqqXbe++g8crEVLV6pJ8xbat3unhg18Xj53kv7eDcCykMgAkG2MGTNGp0+f1unTp+Xu7q41a9Zo8ODBsrKy0v79+9WqVSvdvXs3zf0/TlDs378/waTaj5MbLi4uateuXbx1vr6+ioxM/cRvISEhSa5zZAhsqs39bYaueV5VkaLFNGrsOHOHg0Qs//tP/fJz7J1WbmXKasovs5T7P6ObkHnYH+nv2S7d1bpdR0VHR+vHbz5XYIB/qra3trbWa5PeUZEiRfUgMFDffflpxgSKJHFcPL2bN7w1YcxIBQYGytraWp9++Z3q1KufZPt5s2c+On8X1cgxnL8z2oxfp8n9eOxkuh9O/kwTXn9DZcqWla2tnfLmzavGTZtp1pz5atCwkWJiYvTTD9/q339JqppLiZIl1bhJU0mS1/Vr8vFJelQBYjnlttWoxiWUx85aUdExWnjslq74hj51vwevBSj60QimuBOIp4RJvpOX2nOGKezt7fXBx58rV67cunP7tqb99H06RQsgo1FaCkC24eLiourVqxt/r127trp27arWrVtryJAh8vT01IgRI7RqVeKTp6bkhRde0CeffKKYmBj98ccfeueddyTFJio2bdokSerbt2+C+S8e18+WpBEjRmjChAkmPZ+dnV2S66ytrVMbfjxeXl5PtX1WNH/ubElSw8ZNtHvXjkTbhIaGGv+/acM6SZKTk7MaNGqcOUHmYBvXr9VXn38iSSrm6qrpM+eSsDMj9kfGeaZla23fslGhoaE6uH+vOnTqmqrtbW3t1Khpc61esUxnTp3QXZ87KuxSJIOiRVwcF0/vro+PXhk9XHfv+shgMOi9jz5Ti9Ztk91m4bzY83eDRk20d9fORNuEPTp/h4WGasvG9ZIkRycn1W/I+Ts1YmJitGrFcklSaTc3de/RK9F2NjY2GjtugoYOGqDo6GitWblCld56JzNDRRxly5XT3j27JEl37/jIhXNCkvLbW2tUkxIqkNtW0TEx+uvkbXncCUqXvoPCoxQSHqW89jYqkMu0S23WBqm2a2wiIzAsUv/6BKdLLNlFWs4Zpiro6Kiatevo8MH92r1zuyIjImTDHDOAxSORASDbGzx4sNasWaPly5dr9erV2r59u9q0SX0934oVK6p+/fo6evSolixZYkxkLFu2TOHh4ZISlpWSJCcnJ+NyTExMvGSLuZQoUcKkdg8eZp+hzREREZKkNSv/0ZqV/yTb1t/PT++99YYkqW79BiQyMtiuHdv14ftvKzo6WoUKF9avv81TkaJFzR1WjsX+yFgFHZ+cE27fuvXUfdy5fYtERibguHh6/n5+Gj9muG54x95MMfGt99S5W48Ut3t8/l67aoXWrkq+JJu/v58+eCf2/F2nXgMSGal0//49BQTETlJcqXLVZNtWiTPJ7tWrVzI0LiSPOWNM42BnrVFNSqqQQ+zNYivP+OhYOo+ASDijTPKqFskrB7vYG9SO3whM9fbZWVrPGanxeKLvsLBQ+fv7q1DhwunaP0xnYJIMmIjSUgByhC+++MI4iuHdd99Ncz+PExVnzpzRqVOnJD0pK1WuXDk1atQowTZ2dnaqVi32y96+ffvS/NxAdnT44AG9PelVRUVGqkDBgpo+c65Klixl7rByLPZHxrsbp+xHWksSxesjN2WNMhrHxdMLevBAE14eqatXLkuSxo5/XX36DjBzVPgva+sn9zlGRSVfFjUyMsK4bGPzdCOF8XSuXL5kXC7s4mLGSCxXLhsrjWxUQkXz2UuS1p69q32e/un6HA521sakRGCYaWWFKSuVuMw6Z9z18TEuUyYSyBoYkQEgR6hYsaKef/55/fHHHzp06JC2bNmi9u3bp7qffv366Y033lBUVJQWL14sJycn7dmzR1LiozEe6969uzw8PHT+/Hlt2rRJHTt2TPNrQdocPXUuxTbdnm2rWzdvqpirq9Zs3JYJUeVsJ08c1+sTXlZ4eLjy5sunX2bMVrnyFcwdVo7F/sgcO7ZuNi6n5e8bGhqig/v3SpLs7XOpBBfUMxTHxdMLCw3V6+PH6N9zZyVJQ0aM0qChI0ze/qD72RTb9OzcTrdv3VTRYq5auX5rmmPN6QoUKKC8efMqKChIp06eUGRkZIKSqY8dO3rEuOxa3LSRvkh/N7y9dfDAfklSyZKl5FKEEXr/ZWtt0IhGxVWyYC5J0pYL97Xjsm+6P0/j0gVk9Wh0zOX7Sc91+FgeWytVdskrSboREKZbgQ/TPaas6GnPGabyuXNbZ06dkCQVLeYqBweHdH8OAOmPERkAcox3333XOPT6s88+S1MfRYsWNZal+uOPP7RkyRLFPJrULblExoQJE5Q3b+wH1aFDh8rDwyPZ51m3bp1xxAeQHf17/pwmvDxaoaEhyp07j6ZMm6kqVc1fdi2nYn88vXWrV+jhw+QvQvy5eL4O7NstKfbCX6069Yzr/P38tGPb5qQ2lSQ9fPhQX0z+QH6+9yVJrdt1kH2uXE8ZOZLCcfH0IiLC9dbE8Tp14rgkqe+AFzX6ZdPmCkPms7KyUvNnWkqKvVN5zm8zEm0XGBCgKT8+mRy3RctWmRFejrNr53ZFRiZ9Z//9e/f0xmvjjeXX/tevf2aFlmVYG6Sh9YurjFPs3fa7r/hq47/3UtWHY24bFc9vn2ybKi4O6lDBWZIUHhWtw14pj66oUzy/bKxiv5syGiNWepwzrl/z1NHDB5NtE/TggT58Z5Lx2OncNX1LViH1DIas+4PMxYgMADlG9erV1b17d61atUq7d+/W3r171bx581T388ILL2jLli3y8vLSl19+KUmqX7++KlasmOQ2RYoU0fz589WnTx/dunVL9evX15AhQ9SpUyeVKFFCERER8vb21uHDh7Vs2TJduXJFa9asUc2aNdP8erObE8ePycvruvF3fz8/47KX13Wt+U/d7G5JTFCJp+d+/Ji8vK4Zf//vvli9Kv4cJN17PBfvdy+v6xo3eoQePIj90jZ2XGyi79LFC0k+p5OTs5ycndMj/GyH/WEZ5s6armk/fatWbdqrZu26Kl6ipHLnyaOQ4GBduXRRmzes1amT7pIkW1tbvfnex8aSh1LsSIv333xNJUqWUqs27VWleg0VLlxEtna2CvD311mP01q78h/dvBFbK7qwSxGNeeV1s7zWrIDjwjJ88PYkHToQW1azfsNG6tazty5fuphke1tbW5Uq7ZZJ0SExL41+WTt3bldYaKhmTJ+ms2c91K17T5UoUVIPHz7U6VMntXjRAt2+dVOS1LBREzVpmvrP00jZ1198psjISLVt10E1a9eWq2tx5cqVS35+fjp25LCW/f2X8b2tTt166ts/6ZuqcqqB9VxVySX2TvuLd4N16HqAiuazS7J9ZHSM7gVHxHvMKY+txjYtJU/fUHncCdLNwIcKehibYHLOY6earnlVs1g+42iMNR53TSotVb9EbFmpqOgYHSeRISl9zhl37/po3KhhqlCxklq0bqvKVarJuVAhWVtb6/69ezp10l1rVi7X/XuxCa1y5StkyIgPABmDRAaAHOW9997TqlWrJEmffvqpNm3alOo+nnvuOY0ZM0ahobGTgknJj8aIu92qVas0ZMgQ+fr6asaMGZoxI/E73aysrBje+h8r/1mmtatXJrrupPtxnXQ/Hu8xEhkZZ+U/f6dqX/z3AqH78aPyfXRHuSR9/+2XKT7nS6Nf1qixr6Q+2ByA/WE5AgMCtHrFMq1esSzJNi5FiuqdDz9Vg0ZNEl3v7XVdi+bPSfZ5qtesrQ8//YpJKZPBcWEZdm7fYlw+eviQBj7fM9n2lIYyvzJly+rHKb/onbcmyt/PT7t37tDunTsSbduwUWN9+/1PmRtgDnPXx0d/LlmkP5csSrJN2/Yd9NHkz2Rnl/QF+pyqZrF8xuUKhR00qVWZZNv7hkTo822JT17v5pRbbk65k9z2YWS0Vnv46OD1gBTjcslrp1KOsX39ezdYQeFRKW6TE6TnOePihX918cK/yW7f7JmWev/jz5Urd9L7FYBlIZEBIEdp0KCB2rdvry1btmjz5s06cuSIGjRokKo+8uXLp27dumnp0qWSJGtra/Xr18+kbbt166arV6/qt99+0/r16+Xh4SFfX1/Z2NioaNGiqlatmtq0aaM+ffqoZMmSqX59AADz+GHaLB3Yu0unTrrrhtd1+freV4B/gOxz2cvR0UkVKlVW0+Yt1bb9s4l+YS5StJimz16gwwf365zHad2+dUu+vvcUEhyiPHnyqEjRoqpctbpat+ugRk2aG0slAkB6a9ykqVasXq+V/yzXvr27dfnyJT0IfCAbG2s5OxdSteo19GznrmrVug3vRRnok8+/0rGjR3Tq5And8PaSv5+fgoODlTtPHhUtUlQ1a9dRtx49Vat2HXOHmq15+4dp8fGbKu2YWyUL5lI+exs52FnL2iCFRETrzoOHungvRIeuB5ickKhX4skk38cYjZGuatWqoynTf9PhQwd0/qyHfO7clq/vfYWFhcnBwUGuriVUvWYttX+2s2rVrmvucAGkkiHmcXF3AAD+48HDaHOHgDgM4mIB8F9hEdzFaCly2Vqn3AiZIiKK87clsbdhakqLwUcpi/HBxqRL9SFzvd+2vLlDwCOOeXLmZ6nLPqHmDiHNyrkwoicz8YkKAAAAAAAAAABYLBIZAAAAAAAAAADAYjFHBgAAAAAAAAAg81H2DyZiRAYAAAAAAAAAALBYJDIAAAAAAAAAAIDFIpEBAAAAAAAAAAAsFnNkAAAAAAAAAAAynYFJMmAiRmQAAAAAAAAAAACLRSIDAAAAAAAAAABYLBIZAAAAAAAAAADAYjFHBgAAAAAAAAAg0xmYIgMmYkQGAAAAAAAAAACwWCQyAAAAAAAAAACAxaK0FAAAAAAAAAAg01FZCqZiRAYAAAAAAAAAALBYJDIAAAAAAAAAAIDFIpEBAAAAAAAAAAAsFnNkAAAAAAAAAAAyH5NkwESMyAAAAAAAAAAAABaLRAYAAAAAAAAAALBYlJYCAAAAAAAAAGQ6A7WlYCJGZAAAAAAAAAAAAItFIgMAAAAAAAAAAFgsEhkAAAAAAAAAAMBiMUcGAAAAAAAAACDTGZgiAyZiRAYAAAAAAAAAALBYJDIAAAAAAAAAAIDFIpEBAAAAAAAAAAAsFnNkAAAAAAAAAAAyHVNkwFSMyAAAAAAAAAAAABaLRAYAAAAAAAAAALBYlJYCAAAAAAAAAGQ6A7WlYCJGZAAAAAAAAAAAAItFIgMAAAAAAAAAAFgsEhkAAAAAAAAAAMBiMUcGAAAAAAAAAMAMmCQDpjHExMTEmDsIAIBl8guJMncIiMM/JMLcIeAR57x25g4Bj9hY88XHUlgxU6PFCA3n/G1JcttZmzsEwOKER0abOwQ8UqTJeHOHgEdC3aeZOwSz8PYLN3cIaVbCke+FmYnSUgAAAAAAAAAAwGJRWgoAAAAAAAAAkOkY1AtTMSIDAAAAAAAAAABYLBIZAAAAAAAAAADAYpHIAAAAAAAAAAAAFos5MgAAAAAAAAAAmY4pMmAqRmQAAAAAAAAAAACLRSIDAAAAAAAAAABYLBIZAAAAAAAAAADAYjFHBgAAAAAAAAAg0xmYJAMmYkQGAAAAAAAAAACwWCQyAAAAAAAAAACAxaK0FAAAAAAAAAAg0xlEbSmYhhEZAAAAAAAAAADAYpHIAAAAAAAAAAAAFotEBgAAAAAAAAAAsFjMkQEAAAAAAAAAyHxMkQETMSIDAAAAAAAAAABYLBIZAAAAAAAAAADAYpHIAAAAAAAAAAAAFos5MgAAAAAAAAAAmY4pMmAqRmQAAAAAAAAAAACLRSIDAAAAAAAAAABYLEpLAQAAAAAAAAAynYHaUjARIzIAAAAAAAAAAIDFIpEBAAAAAAAAAAAsFokMAAAAAAAAAABgsZgjAwAAAAAAAACQ6QxikgyYhhEZAAAAAAAAAADAYpHIAAAAAAAAAAAAFovSUgAAAAAAAACAzEdlKZiIERkAAAAAAAAAAMBikcgAgAwyb948GQwGGQwGeXp6mjscAAAAAAAAIEuitBSARAUHB2vhwoVavXq1Tp48qfv37ysmJkb58+eXm5ubatSooSZNmujZZ59VyZIlzR1uooYMGaL58+dLkq5evSo3NzfzBoSncs7jjPbv3a2TJ47r6pXL8vfzlY2NjQoVdlHN2nXUrWdv1a5TL8ntb968oee6tE/VcxYt5qqV67c+bejZns/tW9q0doUO798jnzu3FBISrAIFHVWkqKtq1W2gFm07yK1sBWP727duaEifzql6DpeirlqwfEN6h54t+N6/L48zp+Rx5rTOepzRWY/TCvD3lyR16d5TH3/6ZbLb37xxQz06t0vVcxZzddXqDdvSGjLiuHXrplYuX6Y9u3fp1q2bCgkOlqOjk1yLF1f9ho3UoeOzKl+hornDzJbu37+vM6dP6czp2OPH48xp+T86drr36KVPv/jKvAFmE5y/sxaPM6e1Z/cuubsf15XLl+Tn6ysbG1sVdnFR7Tp11eu53qpbr765w8xxbt68oSWLFmrP7p26ffu27GztVLJkSXV4tpP69n9BuXPnNneIWdrTfpaK6+aNG1q+9A8dPnRA3t5eCg0NlUMeB5UuU0ZNmjZX7//1k5Ozcwa9EssW6j7NpHa7j15Ux5FTEjxeqUwRtW5YSfWqlVa18q5yccor54J5FRUdLZ/7D3TM45r+2nhUa3eeTrZ/F6d86tyyulo1qKSalYqrZFEn2dla675/sE5fuKFV209qybrDCnsYkabXCSDjkMgAkMCBAwfUr18/Xb9+PcG6e/fu6d69ezp69Kh+//13FSlSRLdv3zZDlMhJRg97USfcjyV4PCIiQl7Xr8nr+jWtW71Snbv20DsfTpatrV26PG9ptzLp0k92turvJfp95s8KCw2N9/g9nzu653NHHqfcFRIcrNGvvvlUz1OiVOmn2j4769imeaY/Z+nSHBvp4Y/FCzX1px8VGhoS7/E7d27rzp3bcj9+TMFBQZr09rtmijB7a9OiqblDyPY4f2ctQwe9oOPHjiZ4PCIiQteveer6NU+tXvmPunXvqY8mfypbu/TZX0jezh3b9d7bkxQUFGR8LCw0VB4eAfLwOKN/lv+tadNnqVRpPiulVXp9llq/ZpW++OxjPQwLi/d4YGCATp88odMnT+ivJQv1+dffq1GTZunynDnJW8M7qn+XhomuK1PCXmVKFFKfjvW0++hF9X9jtnwDghO0G9qrqX5+t69sbKwTrCtWuICKFS6gDs2q6tVBbTVg0myduXgz3V8HEmKKDJiKRAaAeC5cuKCOHTvqwYMHkqTu3burT58+qlixouzs7HTv3j2dPHlSW7Zs0Y4dO8wcLXKKe/d8JEmFC7uoTfuOqlWnnooWK6boqGidPnVCSxbO012fO1q/dpUiIyP1yZffJujDpbCLFv+9KsXnmj93ljZvWCdJ6tytR/q+kGxmybxZWvDbL5Kk4iVLq1P33qpYpZocHPIqMDBAly+c1/7d22Wwiv/RtFBhF81YuCzF/v9aMFc7tqyXJLXv1D39X0A2VLRYMbm5ldXBA/tM3sbFxUV/LEv52Jg39zdtWr9WUuzdiXg6v838VdOnxt5tWNrNTc/1/p+qVq+hfPnyyd/fX/+eO6vt27YmOH6QMYoVc5VbmbI6sH+vuUPJVjh/Zy13fR7tLxcXdejwrOrWqx+7v6KjdfLECS2YP1c+d+5ozeqVioyM1Ffffm/miLO/c+fO6q03XlNYWJjy5Mmj4SNHqUHDRgoLC9OmDeu1fNlSXfP01LixL+mPpcvl4JDX3CFneWn5LCVJJ92Pa/KH7yo6OlpWVlbq0q2nWrZuo0KFXXTn9i2tXb1Se3btUEBAgCa+Ok5/Ll+tEiUss7JBRpu5dLdmLd2T5Prg0PBEH4+MitbhU1d14OQVnbl4U3fuB+qeX5AK5s+jSm5FNLx3c1Wv4KoW9Sto+ZRRajP0R8XExMTrw8U5n2xsrPUwPEIb9nho64FzOn/1toKCH6psyUIa2quZ2jetogqlXbRuxitq2v9r3fDxT8+XD+ApkMgAEM97771nTGL8/vvvGjJkSII27du31xtvvKG7d+9q6dKlmRwhcqLSbmU1etyrat22g6yt4989U71mLXXq0l0vDX1B1695avPGderVp6/q/KfsgY2trcqVr6DkREVFyf3oEUlSHgcHtWydunI7OYn70UPGJEa7Z7vp1Xc+ko2Nbbw2deo3Up8BgxUREX9Yto2NbbxSU4mJiorSKfdH+yKPg5q2bJOO0WcvI0aNVdVq1VW1eg05OxdKdakoG1vbFEsXRUVF6fiRw5IkBwcHtWrDsfE0Dh08YExidO3eQx9O/ky2tvGPn0aNm2jQ0OGKiEj8yzye3qgxL6ta9RqqXr2GnAsV0o0b3urcoa25w8pWOH9nLW5ly+qVV19Tu/YdE+yvmrVqq2v37ho8sL+ueXpqw/q1+l/ffqpXv4GZos0Zvvnyc4WFhcnGxkYzfpurWrXrGNc1atxEpUqX1o/ff6trnp5aMO93jXn5FTNGm3U97WcpSZo3Z5aio6MlSW+8/Z7+13eAcV216jXUpl0H/fjd11qycJ4ehoVpyYJ5evPdD9L1dWQVd32DdPbyrVRvN+aTJYqKik503Y5D/2rW33u0+Jvh6tm2thrXKqvOLapr3a74ZaZCQsP13e+bNWXhdt3zC4q37uS/3lqx9YS+er2XJrzYVi5O+fTBmC4aPXlxqmMFkDGY7BuAUVRUlNati72TrX79+okmMeIqXLiwXn755UyIDDnd9z//qnYdOiX4Uv1YQUdHjX/9Semi7Vs3pel5jhw6oLt3Y+9GbNOug3LlypWmfrK76OhoTfvuM0lS2fKV9No7HydIYsT13wu0pnA/elD3792VJDVv3U729uyLpIwa+4qeadlazs6FMuw5Dh+Me2x05Nh4CtHR0fri048lSRUrVdZHn3ye7DGSXqV2kNDYcePVslVrORfKuGMnp+P8nbVMmz5THZ/tnOT+cnR00sRJbxt/37I5bfsLpjl96pSx1FfP53rHS2I8NmjIMJUtW06StHjRggQ3j8A06fFZ6tTJE5KkAgULxktixDVy1Fjj8ulTJ9L8XDlVUkmMx6KjY/Tj/CfzIzWrUy5Bm6mLd+iDn1cnSGLE9cHPq3XrboAkqUfbWjIYGB0LWAoSGQCM7t69q9BHde7Lly//1P2FhYVp2rRpatu2rYoWLSo7Ozu5uLioXbt2mjNnjiIjI5PcNjw8XGvWrNG4cePUoEEDOTo6ytbWVs7OzmrUqJE+/vhj3bt376ljTM6VK1f0/fffq1u3bnJzc1Pu3LmVO3dulS5dWn379tXGjRuf+jmuXbumihUrymAwKF++fNq2LeHkucePH9fo0aNVqVIl5c2bVw4ODqpUqZLGjBmjCxcuPHUM2UW9Bk/qpd7w9kpTH+vXPild0blrz6cNKds6fviAbnjFzqHzv4FDZW2T/gM8t21Ya1xuR1kps1sX59jo0p2SLU/jwP59un7tmiRpyPARssmA4wfISjh/Zy0NGjYyLnt7JZxPD+lnx/YnF2R79OqdaBsrKyt1fVTu8UFgoI4cPpQZoSERj5NIrsVLJNkmb758KujoGK890teD4Cfzk+SyT/3NVJIUERmlAycuS5IK5ssj54IO6RIbkmYwZN0fZC6+OQEwsoszYd+5c+eeqq+TJ0+qR48euvboYs1jd+/e1bZt27Rt2zbNnDlTa9asUZEiRRJs/9JLL2n+/PkJHvf19dXhw4d1+PBhTZs2TatWrVKzZuk/UdrVq1dVrlzCOzgk6fr167p+/bqWLl2qgQMH6vfff0/Thahz586pQ4cO8vb2lrOzs9avX6+GDZ98mY+OjtYbb7yhn376KUFtzwsXLujChQuaPXu2fvnlF7300kupfv7sJjz8SfkVK6vE7yRMTnBwsHbv2C5JKuZaPEFpCzyxe8dmSZLBYFCjpi2Mjz8IDFBggL/yFyiofPkLpLn/kOBg7d8Tuy+KFHNVjdr1ni5gPJXg4GDt2h6bZHV1La669Sgj8jS2bIpNghsMBrVo2cr4eECAv/z9/VWwYEEVKFDQPMEBZsD5O2uJiLe/uC8yI7kfPyZJyp07j6pWrZZku/oNnpyXT7gfV9Nm6TNxNVKntJubzp87q5s3vJNsExQUJH8/v0fty2RWaDnK/5598r3h36t30tyPnd2TJEhKI0EAZB4SGQCMnJycVLp0aV27dk0nT57U119/rUmTJqX6S8qlS5fUsmVLBQQEKH/+/Hr55ZfVsGFDlSxZUvfv39fq1as1c+ZMHTlyRD169NCePXsSlNWIjIxU2bJl1atXLzVs2FClSpWSjY2Nrl27pq1bt2ru3Lm6f/++evXqpTNnzsjFxSU9/xSKioqSnZ2dOnbsqPbt26tq1apycnKSr6+vLly4oF9++UUeHh5atGiRypYtq8mTJ6eq/yNHjqhTp066f/++XF1dtWXLFlWtWjVem1deeUXTp0+XJLVo0UJDhgxR2bJllSdPHp08eVI//fSTPDw8NGrUKBUtWlTdu+fsu9bdHw29l2JrPKfWjq2bFBYWOyKpU5fuDCFOxvkzpyTFJhnyODhox+b1+mvhHHleuWRs83jy7+59+sdLkppi784tehgWezdV245d2Rdmtm1LnGOjK8fG0zp96qQkybV4cTk45NWGdWs0d/YsXbp40djm8eTf/V54MdXHD5DVcP7OWo4+motEksqUTfymH6SPq1di7wh//D0oKWXKPDluHm+DzPfc//rqi08+UoC/v5Yv/VO9n++XoM2cWb/Ga59TPde+jnp3qKPSxZwVFR2tO/cDdfDkVS1cfVC7j15MuYP/cC7ooPKlXDSkVxMN6t5YknTX74H+XH8khS0TZ2NjpUY13SRJt+8Fyi8wJE39AEh/JDIAxPPKK6/ojTfekCS9/fbbmjFjhrp3766mTZuqYcOGKlMm5TtHBg8erICAANWpU0ebN29Wof/Unu7QoYO6du2qLl266NChQ5o3b55GjhwZr83kyZNVtmzZBF9G69evr969e2vs2LFq2rSp7t69q6lTp+rTTz99ylceX7FixeTp6alixYolWNe2bVuNHj1aw4YN07x58/T999/r9ddfV4ECpt2Bvn37dvXo0UNBQUEqX768tmzZIjc3t3httmzZYkxizJ49W8OHD4+3vkGDBho4cKC6dOmi7du3a/z48ercuXOOLVESHR2tBb//Zvy9XftnU93H+rWrjcuduubspFByoqOj5X3dU5KUv0BB/frT11r195IE7W54XdPsX37Q/t3b9cm3U5U3X36Tn2PrhjXG5Xaduj11zHg669fEKSvVjbJSTyM6OlqeV69IkgoWdNQ3X36uPxYvTNDumqenfvz+W23ftlVTp89UvvymHz9AVsL5O2uJjo7W3NmzjL93fLaTGaPJ3h4+fCi/R3fuuxQtmmzb/AUKKHfuPAoNDdHt27czIzwkonvP3jrpflzr1qzSN19+qnPnPNSiZRsVKlxYt2/d1Ia1q7VzR+wI12EjR6lR46Zmjth8qpaL/x07n0MulS/looHdGmn19pMa+dFCBQaFJbF1rE2/TVCL+hUSXXfX74H6vf6bAoJC0xTf8Oeaq7BjPknSiq3uaeoDqWMQNyHANIwFBRDPa6+9pmHDhhl/9/T01M8//6x+/fqpbNmyKlq0qPr166c1a9YkKHckSXv27NH+/fslSfPnz0+QxHjs2WefVZ8+fSRJ8+bNS7C+XLlyyd5RV6NGDY0YMUKStHLlSlNfnskcHBwSTWI8ZjAY9P3338va2lrBwcHaunVrkm3jWrlypTp37qygoCDVrFlTe/bsSZDEkKSvvvpKktS7d+8ESYzHcuXKpWnTpkmKnWtjx44dJsWQHf2xaL7OnjktSWrVpr0qJzP8PjG3b92U+7HYO3Zq1KqjkqVKp3uM2UVwUJCio2OHV3tevqRVfy+Rk3NhvfnhF/p7wx6t2n5I3/4yV5Wr1ZQknT19Qj988ZHJ/fvcvqXTJ2JLKVStUVuuJUql/4uAyW7fuqnjj46NmrU5Np5W0IMHxuPn0sUL+mPxQhUqXFiff/Wtdu07pANHT2j2vIWqUauWJOnkCXd9/MF75gwZyFCcv7OWhQvm6czp2FGZbdt1UNVq1c0cUfYVHBxsXM6TJ0+K7XPnyS1JCgnhznFzsba21seffaWvvvtJFSpW0qp/lmnihLEaPOB/emviBO3csU31GzTStJlzNGbcq+YO1yyCQx9q6cajGvPJYrUd+oMa9f1SXUZP01e/bTROvt29TS39/eMo2dik7XLlL0t2qM5zn2n/iStp2t6tuLM+HtdVUux8G9/O3ZymfgBkjJx56y6AJFlZWWnOnDnq16+ffvjhB23dujXepNx37tzRX3/9pb/++kv169fXn3/+GW8uidWrY++Kq1SpkmrUqJHsc7Vo0UJLly7VkSNHFBkZmexoAj8/P/n6+iosLMyYQClYsKAk6ezZs4qIiEhQnio9RURE6M6dO3rw4IGioqKMjzs7O8vHx0cnT55U796JT8L32Lx58zRixAhFRUWpadOmWrdunfE1xBUYGKidO3dKkjHZk5QqVaqoUKFCunfvng4cOKD27dub9Hq8vZOu3RqXg1PSyRxLcfzoEU2f+qMkydHJWW++92Gq+9i4/klirjN3cybrcfkOSQoPfyj7XLn09dTZKlnazfh4jdr19PXU3/TaS4N05dK/2r97u857nDImN5KzfdM6475o+2zXdI8fqbNh3ZNjo0tXRmM8rdDQJ8fPw4cPlSt3bv02d77c4pQFqVe/gWbNma/BL/TThX/Pa/u2LTp96qRq1KxljpCBDMP5O2s5euSwfv7xe0mSk7Oz3vvwY/MGlM2FP3xoXDblO46dbWwZwselOWEeV69c1ro1q3TpUuLlkU6fOqHVK5arTJlycklknsjsrlyH9xMdJbH90Hn9+ucurZw2VnWqlFSL+hX00v+e0fQ/diXZ10sfLZJDbjsZDAYVzJdbdauW0sj/PaPRfVvKrXghjf1kiXx8H6Qqvty5bPXn9yNVMF9s8vD1r//WrbsBqXuRADIUiQwAiWrfvr3at2+vwMBA7du3T0eOHNHRo0e1e/duBQTEnsyPHj2qZ555RseOHTOOXjh6NLbO8b///mtyjeKIiAj5+vommOfi9OnT+vHHH7Vhw4Zkh0lHR0fLz88v3efJiIiI0KxZs7Rw4UK5u7vHm4zyv+7du5dsXz/99JN+/vlnxcTEqGPHjvrnn3+SvLvK3d3deMdu//791b9/f5PiTc1Q8pIlS5rUzjc4MuVGZnTl8kW9PfEVRUVGyt7eXl9886OcnJxT3c+GdbGljOzs7NSuI2USkvPfev3PdnsuXhLjMXv7XBo8apw+mvSKJGnXtk0mJTK2bVorSbK1s1PLth2fPmA8lcclW+zs7NSeY+Op2dnbx/u913N94iUxHsuVK5fGjX9V418eLUnatHE9iQxkK5y/s5ZLly7qtfHjFPlof333wxQ5O6d+f8F0cc8XERERKbYPj4j9nmKfK1eGxYTkuR8/qtfHj1XQgwcq5uqq0S9PUKMmTVUgfwHd972v3Tu3a+YvP2vzxvVyP3ZUU2fMVrnyiZdGyq6SK/Xk4/tAAybN1skVH8jO1kZj+rVMNpFx7eb9eL/vc7+sWX/v0eJvhqtLyxrau2iSWg/5QTd8/E2KzdraSou/Ga5alUpIkmYu3a1Faw6ZtC2AzENpKQDJyp8/vzp16qQPP/xQq1ev1p07dzR37lw5OjpKkm7duqUPPvjA2N7HxydNz/PfYdBz5sxR3bp19fvvv5t0gT7uXa7pwdfXV02aNNG4ceN06NChZJMYpjz/lClTFBMTo8KFC2v58uXJDhFPr79hdnfzhrcmjBmpwMBAWVtb69Mvv1OdevVT3Y/HmVO69qhm/TMtWytfKuZyyIly53GI93vdhk2SbFunXiNZW8feM3HhnEeKff979rS8rl2VJDVu3ipV82og/XmcPmWcz6FFqzbM05AOHBziHz9NmjZLsm3Dxk2MIxXPnjmToXEBmYnzd9bi7e2l0SOHKTAwQNbW1vr6ux9Ur34Dc4eV7cU9X5jyGT80JPa7iCllqJD+wsPD9f5bbyjowQM5FyqkuQv/VOeu3eXsXEg2trYqUqSo/td3gGbOXSh7e3vdveujyR+8Y+6wLY7njfvadvC8JKl8KRcVK2zaHJSPPQyP1KiPFyk49KFKFnPS56/2NHnb3yYPVKdnYsvlLdt0TK999XeqnhtPx2DIuj/IXIzIAJAq9vb2Gjp0qFxdXfXss7ETMv7zzz+aNWuWrKysjGWXatWqpUWLFpncb/HixY3L58+f1+jRoxUZGSkXFxdNmjRJbdq0kZubm/Lly2ccXj137lzj/BGJzdfxNCZMmKBjx2Lr9Pfs2VPDhg1TzZo15eLioly5chlHm5QqVUpeXl4pPn/v3r21fPly3b17Vy+++KKWLl2aZCmtuKWrZs6cqaZNTZsI7nFyyRReXl4mt7VEd3189Mro4bp710cGg0HvffSZWrRum6a+NsSbJJTSOSmxs7NTgYKOCvCPnYCysEvSE1Da2dsrf8GC8rt/z9g+OfEm+aaslNmtizPJNyVb0oednZ0cnZzk5+srSSpSNOnyffb29ipY0FH37t2Vn59vZoUIZCjO31mLj88djRoxVHd9YvfX5E+/UOs27cwdVo4Qew4oKH9/f/mkcFNXYECAQkNjkx1FU5gYHBnjwL498vG5I0nq23+gChUqnGi7cuUr6Nku3bTqn2U6d9ZDF/49r4qVKmdmqBbv/JXbxoSCa+ECqS7tdN8/WAdOXFG7JlXUtVUN2dhYKTIyOtltfnrnefXv0lCStHGvh4a+Pz/dry8ASB8kMgCkSceOHVWyZEl5eXnJz89P9+/fV+HChY3DzIOCglS9etomAJw3b54iIyNlbW2tXbt2qXLlxD/c+fpmzIWdwMBA/fXXX5KkF154IdmEjJ9fyhdnJem7775T0aJF9csvv2jFihXq37+//vjjj0STGXGH6ufJkyfNf8fklChRwqR2fiFRKTfKZP5+fho/ZrhueMcmYya+9Z46d0vbBYzIiAht2bReUmx97sZNm6dbnNlZ6TLldMo9toxcdHTy/0aiHyXmrK2tk20XGRmhXds2SZIKOjqpfqOk71RHxouMiNDmR8eGk5OzmjR7xswRZR/lypXXUd/DklI+fqKiHx8/fGRH1sf5O2vx8/PVqBHD5P3o5pe33/1A3Xr0NG9QOUzZcuV1/NhRXb9+Pdn5BK9efTKpcZmy5RJtg4x19cqTfVCpStVk21apUk2rtEyS5Hn1ComM/0iPBMLjicMdcturUMG8un0vMMm2n43voVHPt5Ak7Tl2Uf3fmJ1i4gOA+VBaCkCaubq6Gpcfj1CoU6eOJOnKlSupmrMhLg+P2BI0tWrVSjKJIT2ZjyO9Xbx40ViLtm/fvkm2O3/+vIKCgkzud+rUqRo1apQkadmyZRo4cGC80ReP1a5d2/j33LdvX2pCz/aCHjzQhJdH6uqVy5KkseNfV5++A9Lc3769uxTg7y9J6tipS7ITzuOJGrXrGZdv37iRZLvg4CAFBvhLkpwLJz+HzeH9e4xtW7fvLGv2hVnt3RPn2OjMsZGe6sYpoePtnfTouKCgIPk/Span9xxQQGbj/J21PHjwQGNeGqErly9Jkia8NlH9Brxg5qhynjp1Yz9vhYaG6OzZpEt0Hj1yxLhcu07dDI8LCVnbPLlhJyoy+TkGI+Os570rocpln4xWTetE264uBY3LQSEPk2z31oiOmji0vSTp6BlPPTd+hsIepjwnDQDzIZEBIE1CQkJ09uxZSbHzaDweRdC9e2z5kZiYGE2ZMiVNfT/+cBccHJxkm1u3bmn16tVJrn8acT9cJhfDjBkzUtWvwWDQr7/+qhEjRkiS/vrrLw0aNMg4sfdjhQsXVuPGjSVJS5Ys0d27d1P1PNlVWGioXh8/Rv+ei/13N2TEKA0aOuKp+lwfpyxFWu8KzYmatXpSVmLf7m1Jttu/a7vxrqrqtZL/Yh2vrFSnbk8ZIZ5W3LJSXbr1NF8g2VDb9k8msd+xdWuS7bZv22I8ftIyfwBgKTh/Zy2hoaEaN+YlnXt04XzkS6M1bMRLZo4qZ4pbxmvViuWJtomOjtba1SslSfny51eDho0yIzT8R/HiT0a7nzh+LNm2x489STy5ximvDKm0q7PaNq4kSbp8/a5upiGRUdyloBrVdJMUOyF4UomMl/u30scvx37nOH3hhrq/PD3ZpAcAy0AiA4BRUFCQGjVqpLVr1ya4uB5XdHS0XnnlFT148EBSbPLi8QiCDh06qGHD2PqS3377rZYuXZrsc54+fVpr1qyJ91iFChUkxY6M2L9/f4JtQkJCNGDAgHSf4Pux8uXLG1/P/PmJ18dcs2aNpk2bluq+DQaDZs2apaFDh0qKTVQMGTIkwd/7/ffflxRb5qpPnz7yf3TXYWIePnyoX375RWFhYamOJ6uIiAjXWxPH69SJ45KkvgNe1OiXJzxVnwEB/tq/Z5ckqVyFiqpYqcpTx5lTlC1fUQ0ax5bx2LV1o9yPHkrQxvf+Pc3/LfYYsbW1VYcuSV9oehAYoCMH9kiS3MpVULmKDLE3p4AAf+17dGyUr1BRlSpzbKSnipUqqdkzsSUMNm5Yp0MHDyRoc+/eXU3/OfZmAFtbW/Xo+VymxgikF87fWUtEeLheGz9OJ9xj99cLAwdp3ITXzBxVzlWjZk3jKL6V/yzXyRPuCdosmDdXVx6NdHph4CDjXILIXA0aNlauXLklScv//lOXLl5ItN2+vbu1c3vsTQwuLkVy1PtX5xbVZW2d9CVIF6d8+uO7EbK3i/03POvvPfHWly/lopYNKib7HPnz5tK8L4cY+1i89nCi7V7s3ljfvBH72eqC5x11HTNNfoEhJr8WAObDODYA8Rw+fFjdunVT8eLF1bNnTzVp0kSlS5dWvnz55O/vL3d3d82dO1enT5+WJBUoUECffvppvD6WLFmihg0bytfXV3379tWiRYvUt29fVahQQdbW1vLx8ZG7u7vWrFmjgwcPauLEierW7ckd2C+++KKmTp2q6OhodenSRZMmTVLz5s2VK1cuHTt2TD/++KMuXryoZs2amVx6admyZSpUqFCybezs7DRgwAA5Ozurc+fOWrdunTZu3KgOHTpozJgxKl26tHx8fLR8+XLNmzdPZcuWlb+/f6pHTBgMBs2ePVtRUVFasGCBFi5cKBsbG82ZM8eYQOncubMmTJigKVOmaPfu3apSpYpGjx6t5s2by9nZWcHBwbp06ZL27Nmjf/75R35+fho8eHCq4shKPnh7kg4diN3X9Rs2UreevXX50sUk29va2qpUabdk+9yyaYOxhFhnJglNtVETJumcx0kFPXigjya9op7Pv6CGTZ6Rnb29/j13Rn8tnKN7jyY9HDTyZRUqXCTJvnZu3WjcF+0ZjZFqJ44fk5fXdePv/nEmVve+fl1rVq2I175bj17J9rd543rj/mA0RsaY9NY7OnXyhB4EBmrCy6M1YOAgNW/RUvb29vI4c1pzf5ulO3diyzOOfWWCXIokffwg7Y4fOyqv64kfO9evX9OqFf/Ea9+jFwml1OL8nbW8NWmiDuzfK0lq2KixevXuo4tJXJCVYveXm1uZzAovR3rznfc0ZGB/hYWFafTIYRrx0mg1aNhIYWFh2rhhvZb/HTuvX2k3Nw0aMtTM0WZdT/tZKl/+/Bo8bIRmTp+q4OBgDR/UX8/3H6hGjZsqX/788r1/X7t2btPKf5YZb2B7ecLrsrLKOfcW//DW/2RrY62V207o0KmrunbTV6Fh4XJ2zKsW9SpoeJ9mKuyYT5K07/glzfhrd7ztixUuoI2zxuvkv95as+OU3M9d1517gYqMilaRQvnVpFZZDe7ZRMUKF5Aknbl4U9/9vjlBHN1a1dT0D/rLyspKAQ9C9ca3y1TIMa8KOeZNMnbPG/cVEhaejn8NAGlliEmPmXQAZAthYWEqU6aMyXNbVKhQQX/88Yfq1auXYN2FCxfUu3dvnTlzJsV+Jk+erA8//DDeY5988ok++uijJLeZOHGiqlevbhzZcPXqVbm5ucVrM2TIEM2fP9+EVxKrQIECxpEPXl5eat68ua7HucARV6lSpbRhwwZ17txZ165d0+DBgzVv3rx4bebNm5dsfNHR0Ro0aJAWL14sSRoxYoRmzZplTGbExMTo008/1aeffhqv3FViHBwcdPfuXeXOndvk12sKS5nsu3Gd5CfN+6+ixVy1cn3SJVskafigfvI4fUrW1tZavXG7nAsVfpoQM4V/iGXVbD1z8rg+f/8N+fneT3S9wWBQv0EjNPilccn28+rIgTp/9rSsrK21aMVmOTknn3S0BM557cwdgtHHH7yjdY/KSpjiyMlzya4fOrCvzjw6NtZu3qFCFn5s2FgbzB1CmrgfP6ZJr03Q/fv3El1vMBg0/KXRevmVp7t7PTNZGbLWvvjg3be1+j8Xp5Jz0uPfDIwmfYWGc/62JLntrFNuZAFqVauUqvaursW1Ycv2DIoGj+3csV3vvT0pybn5Sru5adr0WSpVunQmR/Z0wi1oQuX0+CwVExOjH7/7Sn8uXpjshNU2NrYaO/5VvTh4WFpCzRBFmozP8Oc4v26ySrs6p9huxVZ3jZm8RAFB8asvPFOvgjbPNu0z0frdZzTq40XGSb/jmjV5oF7s3ti0oB/pMGKK9hxLOgmfnkLdU1/1ITvwD7WMzy1pUTB31jjHZxeMyABglCtXLt24cUMHDx7U1q1bdfDgQf3777+6c+eOwsLC5ODgIFdXV9WqVUs9evRQ7969ZWeX+MW8ihUr6sSJE1q6dKmWL1+uI0eO6O7du4qKipKzs7MqVaqk5s2bq1evXqpbN2Ht/A8//FD169fXlClTdOTIEQUHB8vFxUUNGzbU6NGj1b59+wSJg/RUsmRJHT9+XF9//bVWrVqla9euKVeuXHJzc1PPnj01YcIEOTo6PtVzWFlZaf78+YqKitKff/6p2bNny9raWr/++qsMBoMMBoM+/PBDvfjii5oxY4a2b9+uK1euKCAgQHny5FHJkiVVp04ddejQQb169Ur3JEZ2dv2apzxOn5IkNWjUJEtcBLFE1WvV1cxF/2jVsj90YPcO3b51Q5EREXIqVEg169RX9z79Vb5i8kPmb3hd0/mzsSO86jZonCWSGNnZ9WueOvPo2GjYuKnFJzGysjp162nZqjX6c/Ei7di+TTdveCsiIkKFChdW/foN1e+FgapcJXUXgYHsjvM3cqJWrdvo7xWrtXjhAu3ZvVN37tyJHb1UspTad3xW/QYM5HuABTAYDHp90jvq1KW7Vv3zt064H9ftWzcVFham3HnyqGTJUqpTr4Ge6/O8SufAkUwjPlyoZ+qVV6OaZVSmeCE5F8yr/A65FBT6UN63/XTw1FUtXnNIh05dTXT7Aycvq+uYaWrTqJLqVi2l4kUc5eKUT3ly2SkwOEyeN+/ryKmrWrrxmA6cvJLJrw5AZmFEBgAgSZYyIgOxLG1ERk5mSSMycrqsOiIjO8pqIzKyM0sZkYFYWWVEBpCZLGlERk6XGSMyYBpGZGQ9jMjIXDmnIB8AAAAAAAAAAMhyKC0FAAAAAAAAAMh0BjGqF6ZhRAYAAAAAAAAAALBYJDIAAAAAAAAAAIDForQUAAAAAAAAACDTGagsBRMxIgMAAAAAAAAAAFgsEhkAAAAAAAAAAMBikcgAAAAAAAAAAAAWizkyAAAAAAAAAACZjikyYCpGZAAAAAAAAAAAAItFIgMAAAAAAAAAAFgsSksBAAAAAAAAADIftaVgIkZkAAAAAAAAAAAAi0UiAwAAAAAAAAAAWCwSGQAAAAAAAAAAwGIxRwYAAAAAAAAAINMZmCQDJmJEBgAAAAAAAAAAsFgkMgAAAAAAAAAAgMUikQEAAAAAAAAAACwWc2QAAAAAAAAAADKdgSkyYCJGZAAAAAAAAAAAAItFIgMAAAAAAAAAAFgsSksBAAAAAAAAADIdlaVgKkZkAAAAAAAAAAAAi0UiAwAAAAAAAAAAWCwSGQAAAAAAAAAAwGIxRwYAAAAAAAAAIPMxSQZMxIgMAAAAAAAAAABgsUhkAAAAAAAAAABgYa5du6aJEyeqcuXKcnBwkJOTkxo0aKBvv/1WISEh5g4vU1FaCgAAAAAAAACQ6QzUlkrSmjVrNHDgQAUGBhofCwkJ0dGjR3X06FHNnj1b69atU/ny5c0YZeZhRAYAAAAAAAAAABbC3d1dffv2VWBgoPLmzavPP/9c+/fv17Zt2zRy5EhJ0oULF9SlSxc9ePDAzNFmDkZkAAAAAAAAAABgISZMmKDQ0FDZ2Nho8+bNatKkiXFdmzZtVKFCBb355pu6cOGCvv/+e3388cfmCzaTMCIDAAAAAAAAAAALcPjwYe3Zs0eSNHz48HhJjMcmTpyoKlWqSJKmTJmiiIiITI3RHEhkAAAAAAAAAAAyncGQdX8yysqVK43LQ4cOTbSNlZWVBg0aJEny9/fXjh07Mi4gC0EiAwAAAAAAAAAAC7B3715JkoODg+rVq5dku5YtWxqX9+3bl+FxmRtzZAAAAAAAAAAAkAre3t4mtStRokSq+j137pwkqXz58rKxSfryfeXKlRNsk52RyAAAAAAAAAAAIBVKlixpUruYmBiT+wwLC9O9e/ckpZwAcXR0lIODg4KDg+Xl5WXyc2RVJDIAAElyzGNt7hCeire3t/GDhZeXV6rvgrA0WXl/ZLd9kZWxLywL+8NyZKd9kcsm654vpOy1L7I69oXlyG77IpdN1q50np32R6j7NHOH8FSy077IqXJxdTqeBw8eGJfz5s2bYvvHiYygoKCMDMsi8E8FAAAAAAAAAIBUyIhREGFhYcZlOzu7FNvb29tLkkJDQ9M9FktDIgMAAAAAAAAAgFTIiBFAuXLlMi6Hh4en2P7hw4eSpNy5c6d7LJYma4/lAwAAAAAAAAAgG8iXL59x2ZRyUcHBwZJMK0OV1ZHIAAAAAAAAAADAzHLlyiVnZ2dJsXPAJMfPz8+YyDB14vGsjEQGAAAAAAAAAAAWoGrVqpKkS5cuKTIyMsl258+fNy5XqVIlw+MyNxIZAAAAAAAAAABYgObNm0uKLRt17NixJNvt2rXLuNysWbMMj8vcSGQAAAAAAAAAAGABevbsaVz+/fffE20THR2tBQsWSJIKFiyo1q1bZ0ZoZkUiAwAAAAAAAAAAC9CwYUM988wzkqQ5c+bowIEDCdp8//33OnfunCRpwoQJsrW1zdQYzcHG3AEAAAAAAAAAAIBYU6ZMUbNmzRQaGqoOHTro3fJQbOEAAKZ2SURBVHffVevWrRUaGqo///xTs2bNkiRVrFhREydONHO0mcMQExMTY+4gAAAAAAAAAABArDVr1mjgwIEKDAxMdH3FihW1bt06lS9fPpMjMw8SGQAAAAAAAAAAWJhr165pypQpWrdunby9vWVnZ6fy5cvrf//7n8aNG6c8efKYO8RMQyIDAAAAAAAAAABYLCb7BgAAAAAAAAAAFotEBgAAAAAAAAAAsFgkMgAAAAAAAAAAgMUikQEAAAAAAAAAACwWiQwAAAAAAAAAAGCxSGQAAAAAAAAAAACLRSIDAAAAAAAAAABYLBIZAAAAAAAAAADAYpHIAAAAAAAAAAAAFotEBgAAAAAAAAAAsFg25g4AAID0EhUVpVWrVmnr1q06ffq0fH19JUlOTk6qXr262rVrpx49esjGhtMfAAAAAABAVmGIiYmJMXcQAAA8rdWrV2vcuHG6ceOG8bHHpziDwWB8rFixYpo2bZp69uyZ2SHmKJ988okkaezYsSpUqJBJ2/j5+Wnq1KmSpA8//DDDYgPM5fbt2ypatKi5wwAAAKnQpk0bSdKLL76ooUOHmjkaPBYdHa0dO3bowIEDun37tkJCQvT555+rWLFixjbh4eGKjIyUtbW17O3tzRgtgPRAIgMAkOVNmTJFr7/+uqTY5IXBYJCbm5uKFCkiSbpz5448PT3jJTa+//57vfrqq+YKOduzsrKSwWDQ6dOnVbVqVZO2uXz5sipUqCCDwaCoqKgMjhDIfHZ2durUqZOGDRumrl27ytra2twhARbn4sWLWrBggfHCVGhoqDZt2qTy5csb25w5c0bXr1+Xg4ODWrZsacZoc4aYmBhduXIl3kjXsmXLxrtRBMjObG1tFR0dra1bt6p169bmDgeS1q5dq/Hjx+vatWvxHv/vd4/p06frlVdeUd68eXXz5k05ODhkdqgA0hGJDABAlnbo0CE1a9ZM0dHRyp8/v9577z0NHTo0wSiAe/fu6ffff9cXX3yhgIAAWVtba+/evWrUqJGZIs/eSGRYpujoaJ09e1ZXrlzRgwcPTPo7Dxo0KBMiyxkeHxeSVLhwYeOdnaYeI0B2Fh0drTfffFNTpkxRdHR0vJsP/nsuWb9+vbp27SobGxtdvXpVxYsXN1fY2drGjRs1ffp07dy5U8HBwfHW5cmTR61atdLYsWPVqVMnM0WYPezevTtD+m3RokWG9JsTFS9eXLdv39bRo0dVp04dc4eT4/32228aPXq08TxRqFAh3bt3L9HzRXh4uIoWLaqAgADNnz9fAwcONFfYANIBiQwAQJbWt29f/f333ypQoID27duX4gXBc+fOqWnTpgoMDFSfPn30119/ZVKkOUtaEhnnz59X1apVZWdnp7CwsAyOMGcJDQ3VZ599pt9++0337983eTuDwaDIyMgMjCxnmThxohYvXiwfHx9JT8reNWjQQMOHD1e/fv2UL18+c4aY7ZQtWzbd+zQYDLp8+XK695vTjRw5UnPnzlVMTIyKFy+uJk2aaNmyZUmeS8qVKydPT0/98MMPmjBhgpmizp5CQkL04osvauXKlZKelOr8r8fvYd27d9eiRYu40zmN4ia50wvn7/TVuXNnbdq0SUuWLFHfvn3NHU6OdvHiRVWrVk1RUVFq3bq1pk2bpsqVKyf73WPkyJGaM2eOBg4cqAULFpgpcgDpgUQGACBLc3V11Z07d/T555/r7bffNmmbr776Su+++66KFCmiW7duZXCEOVNaEhl//vmnBgwYoOLFi8vLyyuDI8w5QkND1aZNGx0+fDjJi1FJYXRM+ouKitK6des0d+5crV+/XpGRkcYLWLlz51bv3r01dOhQtWrVyryBZhNWVlbp3ifHRfrbtm2b2rdvL4PBoHfeeUeTJ0+WtbV1sueSt99+W9988426deumVatWmSny7Cc6Olpt2rTRnj17FBMTI1tbW3Xo0EENGzaMV7LzyJEj2rx5s8LDw2UwGNS8eXPt3LmTclNpwPuU5fvnn3/Up08ftWzZUjt27DB3ODna2LFjNWPGDFWvXl1Hjx6VnZ2dpOS/eyxYsEBDhgxRtWrVdPr0aXOEDSCd2Jg7AAAAnoafn58kpape7eO2/v7+GRFSjpTU3U2rVq3S0aNHk9324cOHunz5subOnSuDwaAGDRpkRIg51o8//qhDhw5JkqpXr65x48apXr16cnJyypCLJ0ietbW1unfvru7du8vHx0cLFy7UvHnz5OHhoZCQEC1atEiLFi1SmTJlNHToUA0ePFglSpQwd9hZ1uDBg80dAkwwa9YsSbF3PX/22WcmbdOwYUNJkoeHR4bFlRPNnDlTu3fvlsFgUMeOHTV79uwkS3fduHFDI0eO1MaNG7V3717NmDFDY8aMyeSIsz4ujFu+5557TgMHDtSiRYs0bNgwTZ06lRFIZrJ9+3YZDAa9+uqrxiRGSh7PscSNUkDWx4gMAECWVrZsWV27dk379+83eb6LQ4cOqUmTJnJzc9OVK1cyOMKc4b9lEeLWNjdVTEyMrKystG3bNiZvTUe1atXS6dOn1bRpU23fvt3kL33IXEeOHNHcuXP1119/GZOsBoNBVlZWatu2rYYPH66ePXvK1tbWvIECGaBUqVK6ceOGli9frp49exofT+4O28OHD6tx48bKkyePgoKCMjni7Ktx48Y6fPiwGjZsqP3796eY8I6KilKzZs2M2xw8eDCTIgUyz4IFCxQTE6Mff/xRp0+fVsGCBdWtWzfVrFlTjo6Osra2TnZ75htLP3nz5lVoaKgOHz6sevXqGR9P7nxx8uRJ1alTRzY2NgoPD8/skAGkI0ZkAACytHbt2mnOnDnatWuXyYmMnTt3SpLatGmTgZHlPIndG2Hq/RJ2dnZq0KCB3nnnHZIY6ezy5csyGAx68803SWJYsAYNGqhBgwb66aeftHz5cs2bN0/bt29XVFSUtmzZoi1btsjR0VEDBw7UqFGjVKVKFXOHDKSbx/PGuLm5mbzN46Qe8wCkr3PnzslgMOi1114zadSetbW1Xn/9dfXr10/nzp3LhAiBzDdkyJB4N+f4+flp4cKFJm1rMBhIZKSjx/shJCTE5G0ezw9XoECBDIkJQOahngAAIEubOHGicufOra+++koXLlxIsf2FCxf09ddfy8HBQZMmTcqECHOGq1evGn8ej3IxGAzavHlzvHX//fH09NTt27cVHBysPXv2qHPnzmZ+JdnP4+RFqVKlzBwJTGFvb6+mTZuqSZMmKlSokAwGg2JiYhQTEyNfX19NnTpV1atX13PPPaerV6+aO1wgXTwu0XL37l2Tt/H29pYkOTk5ZUhMOdXji4QVK1Y0eZsKFSrE2xbIjh6fix/fpBP395R+kH4el7pLzaj6vXv3SoodyQ8ga2NEBgAgS6tUqZKWLVumAQMGqHHjxvrwww81aNCgBBc2/Pz8tGDBAn366aeSpKVLl6pSpUrmCDlbKl26dKKPu7q6JrkOmaNy5co6dOiQbt++be5QkIzQ0FAtW7ZMv//+u3bv3h3v4kfVqlU1cOBAnTlzRitWrFBoaKhWrVqlXbt2ae/evYzOQJZXtmxZHT9+XGfPnlX79u1N2mbDhg2SpGrVqmVkaDlOuXLldOLECeMoGVM8bluuXLmMCgswK24csBytWrXShQsXNH/+fJPmwQoICNCMGTNkMBgYjQ9kAyQyAABZ2uMPpIULF9bFixc1ceJEvfHGGypTpoxcXFxkMBh0584dXb161XhRsHz58vr222/17bffJtqnwWDQtm3bMu01ZEfR0dHmDgGPDBkyRAcPHtTff/+tZ5991tzh4D/279+v33//XUuXLjXW+Y+JiZGDg4Oef/55jRgxQk2aNDG2DwgI0JQpU/Tll1/K399f77//vpYvX26u8LMVT09P3bt3T6GhoSneQduiRYtMiipn6NChg44dO6ZffvlFr7zySooljc6ePat58+bJYDAwki+d9e/fX+7u7lqwYIE6duxo0jYLFiyQwWBQ3759Mzi6nOvBgwfaunWrTp48adL7lMFg0Jw5czIxwuyNm3Isx6hRo/Tbb79p165dmjdvnoYMGZJk2/v376tPnz66ffu2bG1tNXr06MwLFECGYLJvAECWFneSaVNPaUm1f1zCxWAwKCoqKn0DBcwkJiZG7du3165du7RgwQL179/f3CHleDdv3tSCBQs0b948Xbx4UdKT96MGDRpoxIgR6t+/v/LmzZtkH9OmTdP48eNVpEgR3bp1K1Pizo7+/fdfffHFF1q9erUCAwNN2sZgMDAvQzq7c+eOypcvr5CQEA0fPlzTp0+XjY1NopO3btmyRUOHDtXNmzfl7Oysq1evJnusIHXCw8PVtGlTubu768svv9Sbb76ZbPtvv/1Wb731lurWrav9+/czF1M6i46O1qeffqrvv/9ewcHBJm3DZ1lkd6+//rp++uknGQwG9enTR71791a/fv1kMBg0c+ZM5cmTR/v27dOSJUuM5/bJkyfr/fffN3PkAJ4WiQwAQJbWqlWrDKnJvGPHjnTvM6d5PAlfnjx5El0/depULV26VPfu3VOZMmU0ZswYdevWLTNDzBGuX7+u4OBgjRw5UgcOHFDv3r01YMAAVa5cOcl9Exdza6SfpUuXat68edqyZYuio6ONyYvHk3iPGDFCNWrUMKmvs2fPqnr16lysegorV67UCy+8oLCwsFTVMOdvnjEWL15snBC3RIkS6tKli7EcyIgRIxQTE6N9+/bp/PnziomJkZWVlVatWqUuXbqYOfLs5fr16/L19dWoUaN09OhR1axZU4MHD1aDBg3ijXQ9cuSIFi5cqBMnTqh+/fqaNWuWHB0dk+yXc0naDBo0SIsXL1ZMTIysra3l7OwsHx8fGQwGlShRQn5+fsbRfAaDQYUKFTKe2ymHhOwqJiZG48aN06+//prs98DH5/ZXX31VP/zwQ2aFByADkcgAAADpbs2aNerZs6fy5s0rb29v5cuXL976YcOGaf78+ZKe3DkoSZ999pneeeedTI83O/vvqKXUJP648zx9Pd4Xj/dDq1atNGLECD333HOyt7dPVV+XL19WhQoVuKieRl5eXqpSpYpCQkJUvHhxTZo0SXny5NFLL70kg8GgrVu3ytfXV0ePHtXChQt18+ZNNW/eXB9//LGsra3VsmVLc7+EbGnp0qUaNWqUAgICEn2vevzVNW/evJo/f7569eqV2SFme3HPGemFc0nabNq0SZ06dZLBYNDgwYP1/fff68aNG6pZs2a89/5///1Xv/76q3755ReVK1dOK1euVOXKlc0cffZ18eJFLViwQAcOHNDt27cVGhqqTZs2qXz58sY2Z86c0fXr1+Xg4MD5IgNt2bJFX331lXbt2pWgrK3BYFDjxo31/vvvq1OnTmaKEEB6I5EBAADS3bhx4zR9+nS98MILWrhwYbx1e/fuVYsWLWQwGJQnTx5VrFhR58+fV2hoqKytreXu7q7q1aubKfLsJ6Va88nhInn6srKyUrFixTRkyBANHz5cZcuWTXNfUVFR8vb2lkTt7rSYNGmSvv/+e+XLl0/nzp2Tq6urPDw8VKNGjQT/7kNDQzV8+HD99ddf6tevnxYvXmzGyLO/+/fva/r06VqzZo1OnDgR7wJ4tWrV1L17d02YMEEuLi5mjDL7eppzRlI4l6RNv379tHTpUlWvXl2nTp2SpCTfp6TYm0iee+45lSxZUu7u7ipQoIA5ws62oqOj9eabb2rKlCnxRlX+t/ydJK1fv15du3aVjY2Nrl69quLFi5sr7BzhwYMHcnd3l4+Pj6KiouTs7KzatWurUKFC5g4NQDpjsm8AAJDuDh48KIPBoNatWydYN2vWLEmSq6urDhw4oBIlSsjLy0vNmzeXt7e3Zs6cqalTp2Z2yNnW77//bu4Q8MjjMjjpcaHQ2tqaBMZT2Lp1qwwGg8aOHStXV9dk2+bOnVuLFi3ShQsX9Oeff+q5555T7969MynSnMfZ2VkffPCBPvjgA0VHR8vX11dRUVFycnKSra2tucPL9jhnWI7Hn6Vefvllk9p369ZNgwcP1u+//66ff/5ZH3zwQQZHmLOMGjVKc+fOVUxMjIoXL64mTZpo2bJlibbt3LmzypQpI09PTy1btkwTJkzI5Gizrx07diT4fpEvXz61aNEixW3Hjh2r6dOnZ1RoADIBIzIAANlOTEyMrly5Il9fX0mSk5OTypYtmyFzaSBxpUqV0o0bN7R79241a9Ys3joXFxfdv38/wSSi3333nd588814dx4CQEZwdHRUYGCgVq5caZybJ+68Iw8fPpSNTfx7vhYsWKAhQ4aoU6dOWrdunTnCzrYej056/fXXNW7cODNHA1iGPHny6OHDh9q6davxwu358+dVtWpVGQwGhYSEJChLuHHjRnXu3Fm1a9fW8ePHzRF2trRt2za1b99eBoNB77zzjiZPnixra2tjKbb/jsiQpLffflvffPONunXrplWrVpkp8uynQIEC2r59u+rVq5eq7V566SXNmTOH0WFAFpf+40YBADCTTZs2qVu3bsqfP78qVqyoxo0bq3HjxqpYsaLy58+v7t27a/PmzeYOM0e4e/euJCWYG8PDw0P37t2TJPXo0SPeuvr160uSrl27lgkRAsjJgoODJUklS5Y0PvZ4glxJCggISLBNtWrVJEknT57M4OhyHm9vb127dk21a9c2dyiAxXFycjIux/1c5ePjk6Dt45Jrnp6eGR5XTvJ4NHHnzp312WefydraOsVtGjZsKCn2sy/Sz4MHD9S5c2f9+++/Jm8zYsQIzZ49OwOjApBZKC0FAMjywsPDNWTIEP3111+SnkwGGldwcLDWrVundevWqW/fvpo3b57s7OwyO9Qc4/EXvMejYh7bu3evJKlw4cKqVKlSvHWOjo6SpLCwsEyIELAMUVFR8vPzU+j/2bvzsBrT/w/g7/u0qBQVIgZZwhAmsovsxj5JkyTbGDNj+w6zmrHOPmOGmbGviQzZydi1EFLWypZSjaUiIVmq0/P7w+8cUhHOOU+d835dV9fkeZ77XG+a0znn+dz35374sNDfXc+qUaOGjlLpv/Lly+P27dv5ft9UqFBB/X18fHy+PwNPixuqYixpTpUqVXDt2jWYm5vLHYWoxKhcuTKSk5PzvZeqXLkyTE1NkZOTg7Nnz+YrxgJPJ4PwvZRmHT16FEIIjBo1qthj3nrrLQBASkqKtmIZpLp16+Ly5cvo1q0bwsPDCzwHnjd8+HD1fn1eXl66iEhEWsRCBhERlXre3t7YsmULJEmCsbExunXrhlatWqFKlSoAnnyAOH78OPbt24ecnBysX78eubm5CAwMlDm5/qpWrRouX76M06dPw83NTX18586dEELA1dW1wBjVTUJuzKddqampCAkJQUxMTL72a05OTnBzc0PlypVlTqj/bt26hb///htbt27FuXPnkJeX99IxQoh8mx7Tm6lfvz6OHj2KhIQEtG7dGsCTmc41a9ZEcnIy9u7dq55Nq7Jv3z4AgLW1ta7j6r1WrVph8+bNiI2NfeV2IaRdfM2QT+PGjZGcnIxz586pW0sZGxvD2dkZx48fx8qVK9G7d+98YxYuXAgA3ENJw1SrXxwcHIo9RrWnD1+7NWvfvn3qffW6deuGsLAw9UqkZ0mSBF9fXwQEBAAAfHx84Ofnp+O0RKRxEhERUSkWFBQkCSEkhUIhde7cWUpMTCzy2qSkJKlLly7q63fu3KnDpIZl1KhRkhBCqlOnjnTz5k1JkiTp+PHjkomJiaRQKKSlS5cWGLNo0SJJCCE1a9ZM13ENwvXr1yUvLy/J1NRUUigUhX6ZmppKgwcPlq5fvy53XL0VHh4uVa5cWVIoFJIQothfCoVC7uh65bPPPpMUCoU0fvz4fMfHjRsnCSGkcuXKSQcPHlQfX79+vWRubi4pFArJ3d1d13H13oEDByQhhPTOO+9I2dnZcschia8ZJcHvv/8uCSGkAQMG5Ds+b9489euCr6+vFBQUJK1fv17q1auX+viXX34pU2r9ZGtrKykUCmnv3r35jqv+vWNjYwuM2b59uySEkOzt7XUV02CcO3dOqlixoqRQKCRnZ2fp7t27+c4rlUpp8ODB6vdQw4cPl/Ly8mRKS0SaxEIGERGVah4eHpIQQnJ2di7WzY/s7GzJ2dlZUigUkoeHhw4SGqYTJ05IRkZGkkKhkKysrKTmzZtL5ubmkhBCqlChgnTv3r0CYzw9PSWFQiH5+PjIkFi/nT59Wv2Brzg3zCtVqiSdPXtW7th659atW1LFihUlIYRkZWUlffrpp9LMmTPV/+4rVqyQZs+eLXl5eUkWFhaSQqGQXF1dJT8/P8nPz0/u+Hrl4MGDkhBCqlatmpSbm6s+npSUJJUtW1Z9o7ZixYqSpaWl+rljbGwsHT16VMbk+mvKlCmSEELq3r27lJycLHccg8bXjJIhISFBEkJIZmZmUkpKivp4Tk6O1Lx5c/W//7NfQgjJwcFBun37tozJ9Y+Li4ukUCikuXPn5jv+okLGxx9/LAkhpK5du+oqpkE5fvy4ZGVlpX6v9PDhQ0mSJCk3N1fy9PRU/44aNWoUixhEekRI0kua8RIREZVg1atXx/Xr1+Hv748hQ4YUa8zatWvh4+ODatWq4b///tNyQsM1Z84cfP755/na5piYmGDdunV477338l179+5dVKtWDQ8fPsSSJUteqQcxvVhWVhbq16+P69evAwC6du2K0aNHF9p+bdmyZdi7dy+AJ72dL1y4kG8DZHozM2fOxMyZM1GmTBlERUWhUaNGiI2NRePGjSGEgFKpVF9748YNeHt7IywsDJ999hl++eUXGZPrH0mSMGvWLOTm5mL06NH59h/ZtWsXhgwZgjt37uQbU6ZMGSxcuBDDhw/XbVgDMGvWLADApk2bEB0dDSMjI7Rr1w5NmjSBjY3NSzfWnTZtmi5iGgS+ZpQsiYmJUCqVqFq1ar49ZDIyMjBhwgQEBgYiJycHwJMWhL169cLChQvV+zOQZnzzzTf46aefULduXVy4cAEKhQIAoFAoIIRAdHQ0GjZsqL7+3LlzcHFxwePHjzF79mx8+umnckXXawcPHkTv3r2RnZ2Nnj17YuPGjfDx8cGWLVsAAKNHj8bixYtlTklEmsRCBhERlWpmZmbIyclBVFQUnJ2dizXm5MmTcHFxQZkyZfDw4UMtJzRs0dHR2LhxI1JSUmBvb4/BgwcX2OQbALZt24a5c+cCANatW8ee2xr0yy+/4Ouvv4ZCocDixYtfWiRasWIFRo8eDQD4+eef8fnnn+sipkFo3bo1IiMj8dFHH2H+/PkAUGQhAwAePnyIpk2bIj4+Hvv27UPnzp3liG2Q0tPTsXHjRsTGxiI3NxeOjo7w9PREtWrV5I6ml1Q3A1UkScr355d5/rlDr4+vGaVLZmYm4uLikJubi7p168LW1lbuSHopNTUVdevWxYMHDzBq1CgsWLAAxsbGhRYy9u3bhxEjRuD69euoUKECrly5AktLS5n/Bvpr69atGDRoEPLy8lCpUiXcvHkTkiTho48+woIFC+SOR0QaxkIGERGVahUqVMCdO3ewZ88edO3atVhjDhw4gG7dusHGxgbp6elaTkgkr7Zt2yIiIgIjRozAsmXLijXmgw8+wIoVK9C6dWscOXJEywkNR8WKFZGRkYGNGzeqVyWdO3cOTk5OEEIgOzu7wMzzhQsXYuzYsfDw8EBgYKAcsYm0TjW7+XU9u/KP3gxfM4gKFxAQAF9fXwBPViD17t0bixYtghACH3zwASRJQnh4OC5cuABJkqBQKLBt27YCG7KT5vn5+WHUqFFQ3d4cO3Ys/v77b5lTEZE2GMsdgIiI6E3Ur18fERERWL9+fbELGevXr1ePJdJ3ly5dAgB4eXkVe8zgwYOxYsUK9VjSjHv37gEAatasqT5mZmam/j4zMxPW1tb5xri4uAAAIiIitB+QSCYsRJQcfM0gKtyQIUNgYmKCMWPG4L///sPixYvVK8dURT/VjXRLS0usWrWKRYw3kJycXOxrO3fujAkTJuDPP/+Eh4cHPv/88yLHP9tKkohKHxYyiIioVOvXrx+OHTuGlStXol27di/tXb569WqsWLECQggMGDBAJxnpiatXryIlJQUPHjxAixYt8vV6Ju25f/8+ALxSuwkbGxsAT3qlk+ZYWlri7t27yM3NVR979ueSmJiId955J9+YR48eAQDS0tJ0kpGIDBtfM4iK5unpiS5dumDBggXYsWMHTp8+ne81vVGjRujXrx8mTpwIOzs7GZOWfrVq1XrlMUIIbNq0CZs2bSry/LM/LyIqfVjIICKiUm38+PH4+++/kZKSglGjRmHjxo0YOXIkWrVqBTs7OwghkJqaioiICKxYsQK7du2CJEmoVq0axo0bJ3d8vZeZmYlff/0Vfn5+6o1DARTYFHHdunXYvHkzypcvj6VLl8oRVW9VqlQJ169fx/nz59GsWbNijblw4QKAJ62QSHPq1q2LEydOIDk5GS1btgQAWFtbo0qVKkhNTUVwcHCBQsbhw4cBAGXLltV1XL3w7IzMZ2dhvspMz8JwRifpK75m6J5qs3sg/8b1zx5/Hc8+FmlOhQoVMHXqVEydOhV5eXm4ffs2lEolbG1tYWJiInc8vcEu+ERUGO6RQUREpd6pU6fQtWtXZGRkvHRzUEmSYGNjg4MHD6Jp06Y6SmiY4uLi0KtXLyQkJOT7MPL8pojAk5nodevWhSRJCA0NRfv27eWIrJcGDRqETZs2wdnZGRERETA2fvE8ltzcXLRu3RqnTp2Cu7s7NmzYoKOk+m/8+PFYsGABPvvsM/zyyy/q4yNHjoSfnx8qV66MsLAwODo6AgCOHTuGXr164e7du+jevTt27dolV/RSS7XnyPOzMJ/fi+RVcEYn6TO+Zujes5vdP7tx/bPHX8ezj0VU2qxatUorjzts2DCtPC4R6QYLGUREpBeuX7+OiRMnYuvWrUV+cDMyMsJ7772HOXPmoFq1ajpOaFgePXqEJk2a4PLlyyhbtizGjh2LDh06oE+fPoUWMgCgW7duOHjwICZPnoxff/1VpuT6Z8eOHejfvz+EEOjatStWrlyJqlWrFnrt9evXMWrUKOzZswdCCGzfvp39nTUoKCgI/fr1Q506dRAXF6c+HhMTg2bNmkGpVMLIyAhNmzZFVlYW4uLioFQqIYTAzp070bNnTxnTl06qTaSFEAVuEL6u5x+LSJ/wNUP3nv199Ox+MW/ye+r5x6I3s2jRInh6er5SyzUiItI8FjKIiEiv3LhxAyEhIYiJicHt27cBPOnz7OTkBDc3N9jb28uc0DDMmTMHkydPRtmyZXHo0CF1uxzV7MLCChl//PEHPvvsM7Rr1w6HDh2SIbX+cnd3x9atWyGEgImJCbp3715o+7V9+/YhOzsbkiTB3d0dGzdulDu6XsnJycHo0aOhVCoxa9asfP2fly9fjo8//rjQmf4zZ87E1KlTdRlVbzw7o/PZWZhvOtOTMzo1iytkSha+ZhDlp1AoYGJigh49emDIkCHo378/zMzM5I5FRGRwWMggIqJSzd/fHwBQv359tGrVSuY0pOLq6oojR47g66+/xvfff68+/qJCxoEDB9CtWzfY2dkhJSVF15H12uPHj+Hr66tu+VFUqwrV28JBgwbB398fZcqU0VlGAi5evAg/Pz/ExsYiNzcXjo6OGDp0KFxcXOSORqRVXCFTsvA1gyi/Z1f3AYClpSUGDBiAIUOGoGvXrm+8eoaIiIqHhQwiIirVVDfG//nnH3h6esodh/5fxYoVkZGRgeDgYHTo0EF9/EWFjNOnT6NZs2YwNTXFo0ePdB3ZIOzcuRMLFixAaGgoHjx4kO+chYUFOnbsiLFjx6JXr14yJSQiQzRz5syXXpOVlYVLly5h3759ePToEVq3bo3u3bsDAKZPn67tiAaJrxlETxw7dgwBAQHYsGED0tLSADwtatjZ2cHLywve3t5o0aKFnDGJiPQeCxlERFSq2djY4N69e4iKioKzs7Pccej/mZmZIScnB5GRkWjWrJn6+IsKGREREWjTpg3Kli2LzMxMXUc2KEqlEgkJCfnar9WuXfuN2rsQlSa1a9cGAEyaNAnjxo2TOQ29ivT0dIwaNQpBQUH4888/MXbsWLkj6T2+Zsijc+fOEEJgxYoVqFmzZrHGXL9+HT4+PhBC4MCBA1pOaHiUSiX279+PgIAAbN26Fffv3wfwtKhRp04d+Pj4wNvbG3Xr1pUzqkHIzMzE/v37cebMGdy6dQsPHz7Ei25xCiGwfPlyHSYkIk1jIYOIiEq1Zs2a4cyZM9i3bx86d+4sdxz6f9WqVUNKSgo2bNgAd3d39fEXFTJWrFiBDz74oMBGyET6Yu/evWjfvj0sLCzkjmLwTE1NoVQqERoaivbt28sdh15Rbm4uWrVqhejoaBw6dIitJTVI9V5q6NChGDFihMxpDNuL3jMVJT4+Ho6Ojmy5pgOPHj3C9u3bERAQgD179iA7OxvA06KGi4sLfHx88P7778POzk7OqHonLy8P3333HX7//XdkZWUVa4wkSXxeEOkBNvIjIqJS7b333oMkSdixY4fcUegZqlUYYWFhxR7j7+8PIQTatGmjrVhEsurZsydsbGzQpk0bfP3119i9e7d6NifpVpUqVQAA5ubmMieh12FsbIwJEyYgNzcXf/zxh9xx9MqhQ4cQGhoKBwcHuaMQlWhmZmbw9PTEtm3bcOPGDSxevFjdTlWSJERGRuJ///sfqlevLnNS/TN8+HDMmjUL9+/fh0KhQKVKldQrMd566y2ULVsWkiSpj1WsWBE1a9ZEjRo15IxNRBrAQgYREZVqEydORM2aNbFw4UIuoS9BPDw8IEkSlixZguTk5JdeP3fuXHXRY/DgwdqORySbnJwcRERE4Ndff0Xv3r1ha2uLVq1a4csvv8S///7Ltmo6oprBHxsbK3MSel1OTk4AgPDwcJmT6BfVzHFra2t5g9BrUc1ONzMzkzmJYbGxscHo0aMREhKC5ORk/PLLL7C2toYkScjNzZU7nl7Zs2cP1qxZA+BJQSMtLQ379+9Xn09KSsK9e/dw/vx5TJgwAQqFAjY2Nti1axeuXLkiV2wi0hC2liIiolLv8uXL8PDwQGxsLEaMGAFvb280adIENjY26uXdpFt5eXlo1qwZzp49CwcHB8yfPx89e/aEkZERhBCIiYlBgwYNEBUVhblz52LdunUAAFdXV4SEhMgbvpQaOXIkgIL9f1XHXwd7CWtWREQEQkNDERISgvDw8HxFC9XvKoVCgXfeeQdubm7o2LEjOnTogHLlyskVWW8dPHgQXbt2RdOmTXH8+HGYmJjIHYleUXh4OFxdXWFqaopHjx7JHUdv9OrVC3v27MHatWvx/vvvyx3HoL1Oa6lffvkFX3/9NRwdHXHx4kUtJ6TnxcTEICAgAP/88w/+++8/tjPSAi8vLwQGBsLJyQlnz54F8GRSQuPGjQv9t96xYwfc3d1RvXp1nDp1CuXLl5cjNhFpCAsZRERUqj270aTqw0JxCSE4S0qLkpOT0b59e1y9ehVCCFhYWODBgwcAnizxzszMxOPHjwE8+dnVqVMH4eHh7CP8mlQ3PADk+xD37PFXwQ/f2pWXl4cTJ06oCxuHDx/GvXv31OefLWw0adIEnTp1wuzZs+WKq5e++eYb/PTTT+jWrRuWLVvG9h+lzOTJkzFnzhxUq1YN//33n9xx9MbmzZvh4eGBjh07Ijg4WO44BuX5iQd+fn4QQqB///4vXSHz+PFjxMfHIzIyEgAwatQoLFmyRFtR6RnJycn4559/sHbtWsTExACAuqWRubk5+vbtq56wQ2/OwcEB//33HxYsWIAxY8YAeHEhAwA++OADrFy5EjNmzMDUqVN1HZmINIiFDCIiKtUUitfvksibtNp3+/ZtjB8/HoGBgUX+WwshMGjQICxcuBA2NjY6Tqg/HBwc1De/n106/+zx18Fl+LqRl5eH06dPIyQkBKGhoTh06BDu3LmjPs/fV5o1a9YsAMCmTZsQHR0NIyMjtGvXTr2a79kieWGmTZumi5hUiKysLPz999/49ttvIUkShg4dCj8/P7lj6RVfX1+sWbMGw4cPx99//42yZcvKHckgPD/xQHWrpriv4arrbW1tERkZiVq1amk+JAEAMjIyEBgYiICAABw5ciTffgxGRkbo3LkzhgwZAnd3d1haWsqcVr9YWFjg8ePH2L9/Pzp16gQAuHDhAho2bAghBB48eIAyZcrkG7N792706tUL77zzDk6ePClHbCLSEBYyiIioVJs5c+YbjZ8+fbqGktCLJCUlYefOnYiKikJaWhqUSiUqVKgAZ2dn9O3bF/Xq1ZM7IlGJcOfOHYSFheHAgQPw9/fHvXv3uDpGCwq7YfgqBT/+LDSrc+fOL70mLy8PGRkZuHTpErKzsyFJEiwtLXHixAk4OjrqIKVh8Pf3hyRJmDNnDqKjo2FtbY2+ffsWu8jn6+uro6T65/mJB0lJSRBCwN7e/oXt74QQMDMzg729Pdq2bYuPP/4YVatW1UVkg/Lw4UNs27YNa9euxd69e5GTkwPgaQHJxcUFQ4YMgZeXFypXrixnVL2mKmScPHkSTZs2BQBcu3YN1atXhxACiYmJBVZYnjx5Ei4uLrC2tsbt27fliE1EGsJCBhEREWmcauNue3t73mAiegFV4SIkJAQhISE4e/as+qaI6r81a9aEm5sbVq5cKWdUvfImq/mAJzfVSXNUhaVX+Whas2ZNrFmzBu3atdNiMsPzJkU+tuzUrNfZI4O0Y+jQodi2bZt6M3XV76o6depgyJAhGDJkCN/v6kitWrWQnJycb0VGbm4uLC0tkZOTg+3bt6N37975xmzZsgUDBw6EmZmZus0tEZVOxnIHICIiIv3j5uam3iiaH+yInipO4cLBwUG92bebmxtq1qwpZ2S9xEJEydKhQ4eX3ixXKBSwsrJCrVq10LFjR/Tu3ZubtGvJ8wUlzn2Uh+p5wdZe8gsICFB/b2dnh/fffx9DhgxBy5YtZUxlmBo3bozk5GScO3dOXcgwNjaGs7Mzjh8/jpUrVxYoZCxcuBAA+H6KSA+wkEFEREQaZ2lpiaysLDRu3FjuKAavVq1aUCgU2LNnD+rWrVusMcnJyepiVHx8vJYTGo5mzZqpCxfP3hisVatWvsJFjRo1ZExJpHshISFyR6D/x32RSg4+L0qOsmXL4r333sOQIUPQtWvXl7ZYI+1xc3NDUFAQ9u/fj7Fjx6qP+/j4ICIiAlu2bMGwYcPg6emJrKwsrFq1Cvv374cQAv3795cxORFpAltLERFRqXLt2jVs2rQJANCkSRO4ubkVe2xwcDCio6MBAJ6enqhSpYo2IhIAJycnnD9/HiEhIXB1dZU7jkF7ndYU8fHxcHR05L4MGqZqZySEQJ8+fTBo0CB07NixQC9nIiIiKjkePnwIc3NzuWMQnhRb69SpgzJlyiAxMVG9H0lubi5at26NkydPFljhJ0kSatasiZMnT8LGxkaO2ESkIVyRQUREpcrkyZOxYcMG2NnZ4cSJE680tn79+vD29kZaWhpOnjwJPz8/7YQk9O7dG+fPn8f+/ftZyCB6hurD9c6dO3HlyhVERkbCzc0NHTp0QIUKFWROZzi4UqlkUe2r1KJFi2LfLHz06BGOHz8O4EkLHiJDcO/ePWRmZhZrkgFX92kOixglR61atZCQkAClUoly5cqpjxsbG2Pfvn2YMGECAgMD1ZuxCyHQu3dvLFy4kEUMIj3AFRlERFRqJCYmok6dOgCAVatWwcfH55UfY+3atfDx8YFCocCVK1c4E1pLUlJS0LhxY2RnZyM8PBxOTk5yRzJYr7Mi4+TJk3BxcUHZsmWRmZmp5YSGY9WqVQgNDUVISAgSExMBPC1sCCHQsGFDuLm5qdtMsbChPVypVLIoFAooFAqcPXv2lX8eCoWCG0yTXtu3bx8WLFiAw4cP4/bt28Uaw43XyZBlZmYiLi4Oubm5qFu3LmxtbeWOREQawhUZRERUagQEBECSJNSrV++1ihgA4O3tje+//x4XL15EQEAAvvrqKw2nJACoUqUKgoKCMHDgQLRr1w5ffvklvL294eDgIHc0KoY1a9YA4KaImjZs2DAMGzYMAPDff/8hJCREXdhISEhATEwMYmNjMX/+fBY2yOC87vw6zst7M7t27cI333wDAPjss8/g7e1d7LFr167F7NmzAQC//vorunbtqpWMhmzChAmYP38+AP6/rgv+/v7q7319fQs9/jqefSzSPisrKzRr1kzuGESkBVyRQUREpUbPnj2xb98+fPnll/jxxx9f+3GmTp2KH374AT169MCuXbs0mJBUateuDQC4f/8+bt26pZ51bmlpCWtr6xduksi2LW+mc+fO+f4cEhICIYR6hcWLPH78GAkJCUhLSwMATJw4EX/88YfWstJT165dQ2hoKIKDgxEWFoa4uDgAT1dsKBQKdZsEenNcqVSyvM7PIy4uDvXr14exsTGys7O1nFA/SZKEt99+G3FxcejatSv27NnzyuN79OiB/fv3o3Hjxjhz5oyWkhom1SpiADAzM8OAAQPQvHlz2NraqvdcehFV4ZyKT/W76PkVLarjr4OrY+T3zz//YOzYsRBCID09Xe44RPQGuCKDiIhKjZiYGABAu3bt3uhxWrdune/xSPNUbXNUVPMmMjMzX3oD8HU/KNITqsLFs3NVJElCZGTkKz1O7dq18fXXX2s6HhWhWrVq8Pb2hre3Ny5evIi1a9fir7/+wr179yBJEvLy8uSOaPC4UqlkSUpKAgCUL19e5iSl18GDB3Hp0iUYGRlhzpw5rzxeCIG5c+eiadOmiImJQWhoKDp27KiFpIZp8eLFAIDq1avj4MGD6vaqpF1FzfXlHODSKzs7G3fu3OFnDCI9wEIGERGVGqq+wFWqVHmjx1GNL26fYXp1nAUonw4dOuT7oBYaGgohBJo3b/7CFRlCCJiZmcHe3h5t27aFl5fXS1dwkGZcunQJISEh6lZTKSkp6nO8caIZz69UUhkxYsQrrVQSQqB79+7aiGhQkpOTCz1+48YNWFpavnDs48ePER8fj6lTp0IIgUaNGmkjokHYtGkTAKBbt27FXgnzvIYNG6pXuG7cuJGFDA06e/YshBCYPn06ixg6cuXKlVc6TkREusVCBhERlRqqZfRv2l5FNZ6zcrRn5cqVckcwWCEhIfn+rHre+Pn5vfaNKtKs4hYuHB0d0bFjR/UeGfT6uFKpZKlVq1aBY5IkvVaRiL3nX9/x48chhEDfvn3f6HH69OmDf//9F8eOHdNQMgKevl91dnaWOYnhKGrFHVfiERGVDCxkEBFRqVGpUiUkJyfj6tWrb/Q4qvGVKlXSRCyiEs3X1xdCCNjY2MgdxeB5e3u/sHBRv379fIULe3t7OWLqJa5UKlk00brFzMwMEyZMwMiRIzUVy+Co2nPVr1//jR6nXr16AAq2laQ34+DggPPnz+P+/ftyR6E3cOLECTRv3lzuGEREeoGFDCIiKjUcHR2RnJyM4OBgeHh4vPbjHDx4EMDTD95E+szPz0/uCPT/1q1bl+/Pb7/9dr7CReXKlWVKpv+4UqlkeX7V3ogRIyCEwHfffYdq1aoVOe7ZwpKzs/NL21DRi929excAYGtr+0aPoxp/7969N85ET7m7u+OHH37AgQMH4OrqKnccekVHjhzBd999h3379nGzbyIiDWEhg4iISo1u3bph//79CAgIwMyZM1GxYsVXfoxbt24hICAAQgh07dpVCympKKmpqYiJiVHvTWJrawsnJyfevCWD0bBhQ7i5uakLF1wVJh+uVJLX8/sojRgxAgAwYMAAFpZ0qFy5csjIyMCdO3fe6HFU462srN48FKlNnjwZq1evxty5c+Hl5YUGDRrIHYmK4cCBA/j+++8RFhYmdxQiIr3DQgYREZUaXl5emDZtGjIzM/HBBx9g8+bN6lm1xSFJEkaNGoXMzEyUKVMGgwcP1mJaAp78my9ZsgTz5s3DuXPnCr2mYcOGGD9+PEaPHs19S3REqVQiIyMDDx8+fGkrlxo1augolf6LiYmROwL9P65UKlmCg4MBFL53BmlPpUqVkJGRgXPnzsHNze21H+f8+fMAADs7Ow0lIwAoX7489uzZg759+6Jt27b4/vvvMXjwYBZgdUSSJGzZsgX79+/Hf//9BxMTEzg4OMDDwwNt27YtcH1ISAimTJmCiIgI9XgAr7X3DxERFU5Ir9KIlIiISGaffvop/vzzTwgh0LNnTyxfvhxVqlR56bgbN25g1KhR2L17N4QQmDhxIv744w8dJDZcGRkZ6NevH44cOQKg6N7nquJF27ZtsWPHDlhbW+sqokG5desW/v77b2zduhXnzp1DXl7eS8cIIdgOgQxafHw8bt26BQcHB64eI70zbNgwrF69Gj169MCuXbte+3F69uyJffv2wcfHB6tWrdJgQsNWu3ZtAMCDBw+QlpYGIQSEEKhYsSIsLCxeOFYIgfj4eF3E1EtJSUno378/oqOjCz0/aNAgBAQEwMjICOnp6fjggw+wfft2AE/e7woh0K9fP3zzzTdwcXHRZXQqxKpVq9QtDJVKpdxxiOgNsJBBRESlyuPHj+Hm5oaIiAh1r+xBgwahd+/eaN68Oezs7FC2bFlkZWUhNTUVJ0+exM6dO7FhwwY8evQIkiShdevWCAkJgampqdx/Hb0lSRI6duyIw4cPAwAqVKgAT09PtGrVSl14SklJwfHjxxEYGIhbt25BCIH27dsjNDRUzuh66ciRI3B3d8fNmzdfaTNdfuAjfZWWloaNGzcCAIYMGYLy5cvnO3/58mW8//77OH36NIAnz4X+/ftj2bJlnA0tkzNnzmDjxo24desWatWqhSFDhrxwPw16uXXr1sHb2xtCCISGhqJ9+/av/BhhYWFwc3ODEAIBAQHw8vLSQlLD9Cqrjp/H1+/Xl52djebNmyM2NrbIa4QQmDx5MsaPH4+OHTsiKSkJkiTByMgInp6emDJlCho1aqTD1PopOTlZI4+zYcMGfP7553xeEOkBFjKIiKjUSU9Px6BBg9SbtxanHZHq5a5Tp04IDAxEhQoVtBnR4AUEBGDo0KEQQsDb2xsLFiwosnf2/fv3MXbsWKxevRpCCKxZs4ZtvzQoPT0dDRo0QHp6OiwtLfHBBx/A2toaM2bMgBACy5Ytw+3btxEVFYXt27fj0aNHaNeuHUaNGgWgYC970oz09HQcPXoUCQkJyMzMLNYH62nTpukgmWFYtGgRPvnkEzg6OuLixYv5zj1+/BhOTk5ISEjIV/gTQqBdu3bse64FkZGRGDt2LIyNjfHvv/8WWJm3ePFijB07Nt/Pw9LSEhs3bkS3bt10nFZ/5OTkoH79+khMTETlypURFhYGR0fHYo+/dOkSOnTogJs3b8LBwQEXL16EsTG7V2uKau+Y17Vy5UoNJTEsK1euxKhRoyCEQM2aNfHtt9+icePGMDU1xfnz5/Hbb7/h1KlTKFu2LN555x2Eh4cDAAYOHIgff/zxlZ5D9GIKhUJjbWdVK2VYyCAq5SQiIqJSKC8vT/rjjz+katWqSUKIl35Vq1ZNmjNnjpSXlyd3dIPQq1cvSQghderUqdhj3NzcJCGE1KtXLy0mMzwzZsyQhBCSmZmZFBMTI0mSJMXExEhCCEmhUOS79vr165Kbm5ukUCikL774Qo64ei81NVXy9vaWTE1NJYVC8UpfpDnvvfeepFAopC+//LLAuUWLFqmfH/3795f++usvqV+/fupj69atkyGxfps6daokhJB69OhR4FxCQoJkampa6Gu7jY2NlJaWJkNi/bFp0yb1/9tWVlbS3Llzpfv3779wTGZmpjRnzhzJyspKPXbLli26CUykZX369JGEEFKNGjWkzMzMAueVSqXUrl079e8hY2NjadWqVTIk1X/F+Yz3Kl98L0VU+nG6BBERlUpCCHz66acYN24c9uzZg9DQUJw5cwbp6enIzMyElZUVKlSogKZNm6Jjx47o0aMHTExM5I5tME6ePAkhBMaNG1fsMePHj0doaChOnTqlxWSGZ9euXRBCYOTIkS9tc2Bvb49///0XTZs2xezZs9GjRw907txZR0n1X0ZGBtq3b4/4+PhXavFFmqdahdG6desC59auXQsA6Ny5M7Zu3Qrgye+n7t27Y//+/Vi3bh3ef/99nWU1BCEhIeq9r543f/585OTkwNzcHAEBAejSpQv27NmDYcOG4e7du1i0aBGmTp0qQ2r94O7ujpkzZ2L69OnIysrCpEmTMHXqVLi6uhbZsvPQoUPIyspS/x6bOXMmBgwYIO9fhEhDzpw5AyEEPv/8c1haWhY4r1AoMGvWLHTt2hVCCAwdOhS+vr4yJNV/XBVMRM9jIYOIiEo1ExMT9OnTB3369JE7Cj3j9u3bAIBatWoVe4zqWtVY0ozLly8DALp27ao+9uwyfaVSCSMjI/Wfzc3N8emnn2Ls2LFYtGgRCxka9PPPP6t/Ht27d8ekSZPQvHlz2Nraaqx1AhXPzZs3AQBvvfVWvuMPHz7EsWPHIITAhx9+mO/cyJEjsX//fpw8eVJnOQ3FtWvXAABNmjQpcG7btm0QQmDMmDHqm+UeHh44evQo5syZg927d7OQ8YamTp2Kt956C+PHj8eDBw9w//597N69G7t37y70elUBw8LCAvPmzcPw4cN1mJZIu9LT0wEATk5ORV7z7O8qDw8PrWcyVGyPRkTPe/3do4iIiIiKoNo49/r168Uec+PGDQBAuXLltJLJUN27dw8AULNmTfUxMzMz9feZmZkFxri4uAAAIiIitJzOsKhuyPbp0we7d+9G9+7dUaFCBRYxZHDnzh0ABTfTPXbsGHJyciCEyFf8A54WW9PS0nSS0ZCoCkvP71917do1xMfHAwA8PT3znevevTsA4MKFCzpIqP9GjBiBS5cuYdKkSahYsSIkSSryq2LFipg8eTIuXbrEIoYOPXz4EIcPH8bGjRvh7++vfn0nzXr48CEAwM7OrshrKlasqP7++YI4ERFpD1dkEBERkcY5OTkhNDQUK1euRO/evYs1RjXr6kUz4OjVWVpa4u7du8jNzVUfs7W1VX+fmJiId955J9+YR48eAeANW01LTk4GAIwdO1bmJKR6XqSkpOQ7HhISAgBo2LAhbGxs8p1TtSfkZsaal52dDQDIysrKd/zQoUMAnsz8b9GiRb5zlStXBlB4MZZeT9WqVTF79mzMnj0bsbGxRbbsfFmbQtKs//77D1OmTMGGDRuQk5OjPu7i4oKGDRuq/7x8+XIsXrwY5cuXx969e1kk1xG+JhAR6Q5XZBAREZHGeXh4QJIkbNmyBTNmzHjpfgDfffcdNm3aBCEEBg0apKOUhqFu3boAnt5EBwBra2tUqVIFABAcHFxgzOHDhwEAZcuW1UFCw6Hqta26AUvyadCgAQAUaJ2j+j3UsWPHAmNURQ/+/DSvUqVKAKBefaGyb98+AE/2Mnm2BR7wtOBqbW2t/YAGqFGjRvD29sb48eMxZcoUjB8/Ht7e3ixi6FhERAScnZ2xdu1aZGdnq1fFFKZv3744e/YsDh48iL179+o4KRERkfaxdExERKXC8zcwNEEIkW+WOmnO6NGj8ffff+PixYv47rvvsHnzZgwfPhytWrWCnZ0dhBBITU1FREQEVq1ahZiYGABPbi6OHj1a5vT6pVWrVjhx4gQiIyPz9XHu2bMn/Pz88Ouvv6JPnz5wdHQE8KS1zm+//QYhRIEZ0PRmGjdujJCQECQlJRVYBUO61bt3bxw7dgxLlizB22+/DVdXV/j5+eHcuXMQQsDd3b3AGNXeGNWqVdN1XL3n4uKCbdu2Yfny5RgyZAgUCgXS09OxefNmCCHQpUuXAmNURQ8WljTL398fADBgwIBit3q8f/8+Nm/eDADc9FiD7ty5g/79++P27duwt7dXb8LeuHHjQq+3s7PDu+++i+3bt2Pnzp3o0aOHjhPrlwULFrywvdSrXDdt2jRNxaJCKJVKZGRk4OHDhy+dPFWjRg0dpSIibRDSy57lREREJcDzfcw1QQgBpVKp8celJxITE9GlSxdcuXLlpe0NJElC7dq1cfDgQX7A0LCgoCD069cPderUQVxcnPp4TEwMmjVrpt7su2nTpsjKykJcXByUSiWEENi5cyd69uwpY3r9EhgYCC8vL7i7u2Pjxo1yxzFod+/eRcOGDXHjxo18v58kSULbtm3Vq5Ke1apVK0RFReHTTz/F7NmzdRlX723ZsgUDBw6EEAKtWrVC27ZtsWPHDsTFxcHExASXL19G9erV840ZO3YsFi5ciH79+mHr1q3yBNdDCoUCQghER0fna1v0IvHx8XB0dIRCoeAEEQ2aNWsWZsyYgYoVKyIqKkr9/uhFP6P58+dj/PjxaNmyJY4dOyZH7FJP9e+rSfy8oXm3bt3C33//ja1bt+LcuXPIy8t76RhOYiMq/bgig4iISoXp06fLHYFekYODA86ePYsZM2Zg+fLl6s11n2dtbY0PPvgA06ZNU7feIc3p0aMHfH19oVQqceXKFfWGxU5OTli4cCE+/vhj5Obm4sSJE/nGzZgxg0UMDfP09MSOHTuwdu1a/Pzzz/jqq6/kjmSwypcvj/3792Po0KHqlRYA4Orqin/++afA9WfOnEFkZCSEEOjWrZsuoxqE9957Dx4eHti4cSOOHTuGiIgI9azaL774okARQ6lUqldrtG/fXo7IVAjOkdSsHTt2QAiBSZMmFXuSh6r11/Nt2ujVaPL/Ze5VonlHjhyBu7s7bt68yd87RAaGKzKIiIhI67Kzs3HixAnExMTg9u3bAJ5sOO3k5ITmzZvD1NRU5oSG6+LFi/Dz80NsbCxyc3Ph6OiIoUOHwsXFRe5opVZYWFiR55RKJaZOnYqjR4+iefPm8Pb2RoMGDWBhYfHSx+3QoYMmY9L/u3LlClJSUmBvbw8HB4dCrzlz5gxOnz4NAPD29lZv/E2ak5eXhwULFmDDhg3qn8ewYcMwYsSIAtcGBARg6NChAIDY2Fi8/fbbuo6rt15nRcalS5fQoEEDmJiY4PHjx1pOaDhsbGxw7949HDp0CG3btlUff9HP6MyZM3B2dubP4g2EhoZq/DEL23eJXk96ejoaNGiA9PR0WFpa4oMPPoC1tTVmzJgBIQSWLVuG27dvIyoqCtu3b8ejR4/Qrl07jBo1CgAwbNgwmf8GRPQmWMggIiKiN/I6/bSJ9Jk22lKwHQIR6cLrFDJ27NiB/v37o3Llyrhx44aWExoOc3NzZGdn49ixY/n2rHrRz+jIkSNo3749ypUrV+RKWKLSbObMmZg5cybKlCmDqKgoNGrUCLGxsWjcuHGBtsE3btyAt7c3wsLC8Nlnn+GXX36RMTkRaQJbSxEREdEbGT58OIQQcHFxKfSmx82bN7Fw4UIIITB16lQZEhLpHucKEVFpUNQKssjISNy6deuFYx8/foz4+HjMnj0bQgi88847WkhouOzs7HD16lVcuXIlXyHjRVQrx6pWrarFZETy2bVrF4QQGDlypLqVWlHs7e3x77//omnTppg9ezZ69OiBzp076ygpEWkDCxlERESkVWlpaerl3ixk6J5CoYBCocDZs2e5cauOBAcHyx2BiKhY3NzcCqwgkyQJI0eOLPZjSJIEIQTGjBmj6XgGrVWrVrh69Sp27doFT0/Pl14vSRKWLl0KIQRcXV11kJBI9y5fvgwA6Nq1q/rYs7/DlEoljIyM1H82NzfHp59+irFjx2LRokUsZBCVcixkEBGRXsnIyMCZM2dw69YtPHz48KWzon19fXWUjEg+r7s6gKsKXg97YZd8r3KT9nlCCCxfvlyDaYjkVdjv+lf5/f/WW29hypQpGDBggAZT0ZAhQ7Bx40YEBARg4sSJL13xMnnyZJw5cwZCCO4DQHrr3r17AICaNWuqj5mZmam/z8zMhLW1db4xqn3fIiIitB+QiLSKhQwiItILISEhmD59Og4fPlzsMUIIFjKIXkDT+zwQlRR+fn6v9f+3auY5CxnakZ2djYCAAGzdujXfpIQX4f4xb+bZFWSSJKFz587q/8dr1apV5DghBMzMzGBvb4/q1avrIqrB6d+/Pzp16oTg4GB06dIF33//PQYOHKg+n5ubi+vXryM8PBx//fUXjhw5AiEE3N3d820OTqRPLC0tcffu3Xy/921tbdXfJyYmFij6PXr0CMCTVeJEVLqxkEFERKXewoULMX78eEiSxBnkRBqg6otetmxZmZMQaUeNGjVeWsjIyspCenq6unhRsWJFWFhY6Cih4bl06RIGDBiAixcv8rVch4paQdayZctityMk7dm0aRO6dOmCU6dOYdy4cRg3bpz6d5ezs3O+ayVJQuvWreHn5ydDUiLdqFu3Lk6cOIHk5GS0bNkSAGBtbY0qVaogNTUVwcHBBQoZqolufF9LVPop5A5ARET0Js6fP48JEyZAkiQ0btwYW7duxc6dOwE8mS0YHx+PyMhILFy4EM2aNQMAtG/fHrGxsUhISJAzOpFOFXf2eVZWFv7++28AQJ06dbQZiUg2iYmJuHLlygu/0tLScOvWLcybNw82NjawtrbG7t27ceXKFbnj652srCy8++67uHDhAoQQGDBgAEaPHg0A6v2Vxo4di1atWqmPtW3bFtOnT8e0adPkjK53rly5goSEBNSrV0/uKIQnN2iPHj2Kr7/+GuXKlVNP2nn+y9zcHF988QVCQkJ4s5b0mup1IDIyMt/xnj17QpIk/Prrr4iLi1MfP3bsGH777TcIIdCiRQudZiUizRMSp7sQEVEp9sknn2DRokWoVKkSLl++DCsrK8TGxqJx48YQQkCpVKqvlSQJX331FX777Td07twZ+/fvlzG5/lAoFBBCIDo6utDZm0X9PEg7ateune/PiYmJEEKgatWqMDExeeHYx48fIy0tDXl5eQCAb7/9FjNnztRaVkPzOhtMqtq3lC9fHo6OjmjdujV69OgBhYLzkXTp4sWLaN26NWxsbHDixAnY2NjIHUmv/P777/j8889hZGSEPXv2oHPnzkW+dpw6dQpDhw7FhQsXMHfuXIwbN07G5ES6k5WVhdDQUERFRSEtLQ1KpRIVKlSAs7MzunbtivLly8sdkUjrgoKC0K9fP9SpUydfwSImJgbNmjVTb/bdtGlTZGVlIS4uDkqlEkII7Ny5Ez179pQxPRG9KRYyiIioVGvUqBEuXLiAWbNm4ZtvvgHw8hvnXbt2RXBwMJYuXfpGG77SEyxklCyausHdunVr7Nu3jzM7NUj1XFG1KnqW6i15cY5XrlwZv//+OwYPHqzlxPSs6dOn47vvvsOUKVPw/fffyx1Hr7i5ueHQoUPw8vJCQEAAgBe/dty8eRNNmzbFrVu3cPToUTRv3lyO2EREpGM5OTkYPXo0lEolZs2alW8vn+XLl+Pjjz8udN+kmTNnYurUqbqMSkRawEIGERGVauXLl8f9+/cRFBSEd999FwBw7tw5ODk5QQiBR48eFZiFHhgYCC8vL7i5ueHgwYNyxNYrqpuzH3/8Mezs7AqcT0tLw4IFCyCEwPTp04v1mGwV8vpGjBiR78+rVq2CEAL9+vWDtbV1keOe3bi1bdu26g1fSXPc3NwghMCNGzdw6dIlAE/+3WvXro1KlSoBeHKDNiEhQV3scHR0ROXKlXHv3j1cunRJvfGxEAI//fQTvvjiC9n+Pobm0KFD6NixIxo0aIBz587JHUev2NnZIT09HevXr4eHhweA/IWMnJycAkXa2bNn44svvsCwYcOwcuVKOWLrtdzcXOzcuROHDh1CQkICMjMzXzoZQQiBAwcO6CghEVFBFy9ehJ+fH2JjY5GbmwtHR0cMHToULi4uckcjIg1gIYOIiEq1MmXKIDc3FydPnkTTpk0BAElJSahVq5b6huHzN9dPnjwJFxcX2NnZISUlRY7YekVVyNAkrtzQnJetmCHd2rdvH7y8vNSFPR8fnwJtijIyMrB69WrMmjULkiQhICAAPXv2RG5uLrZs2YLJkyfj6tWrMDIywpkzZ/hz1ZFTp06hefPmsLCwwP379+WOo1dMTU2hVCpx7NgxdQ/zy5cvo169ehBC4M6dO7Cysso35ujRo2jXrh0cHBy455WGHT58GEOHDkVycrL62ItuGzy70oyv30RERKQtxnIHICIiehO2trZIS0tDVlaW+lilSpXUN9YvXbpUoJBx69YtAMCdO3d0llPfaXJeBFcBaJZqFUxhq2VIt+Lj4+Hh4QETExMcPXoUjo6OhV5nY2ODCRMm4N1330WbNm3g6emJqKgo1KtXD4MGDUKLFi3QrFkz3L17FwsWLMC8efN0/DcxTKdOnQKAl+41Q6/OwsICmZmZ+X7/P7uCLDk5GY0aNSp0LCckaNaFCxfQs2dPPHz4EJIkwdTUFI6OjrC1teXePFri7++vlcf19fXVyuMSERHJhYUMIiIq1Ro0aIC0tDTExcWhbdu2AJ7cEHF0dERcXBy2b9+O9u3b5xuzZcsWAFC3cqE3ExwcLHcEeoHitvMi7Zs9ezYyMzPx66+/FlnEeJajoyO++OILfPXVV5g9ezaWLFkCAHBwcMCYMWPwyy+/8PmnI1euXMGMGTMghMA777wjdxy9U6tWLZw9exbXr19XH6tYsSJsbW2RkZGB8PDwAoWMEydOAHiymoM058cff8SDBw9gZGSEmTNnYsKECbC0tJQ7ll4bPny4xidxCCFYyCCDkZeXh9u3b+PBgweoVq0ajIyM5I5ERFrCQgYREZVq7du3R2hoKA4dOoRhw4apj7u7u+Pnn3/GX3/9hbfffhuenp7IysqCn58fli1bBiEEOnfuLGNy/dGxY0e5I9Arunr1KlJSUvDgwQO0aNEC5ubmckcyCHv37oUQAq6ursUeo3p+7d+/P9/xzp0745dffsG1a9c0mtFQFGcGdF5eHjIyMhAVFYVt27bhwYMHEELgo48+0kFCw+Li4oKzZ88iKioK/fr1Ux/v0qULNmzYgN9++w0eHh6wtbUFACQkJODnn39mYUkLDh48CCEEJk6ciClTpsgdx2Cw4zfRq1EqlfDz84Ofnx8iIyORk5MDIQTOnj2br+VmUFAQwsLCUL58eXzzzTcyJiYiTeAeGUREVKpFRESgTZs2sLW1xdWrV2FmZgYASE9PR/369ZGRkVFgjCRJMDc3R1RUFN5++21dRyaShWolgJ+fX75Zz8/vnbFu3Tps3rwZ5cuXx9KlS+WIqrfMzc2RnZ2NI0eOoFWrVsUao/odZ2ZmhgcPHqiPnzlzBs7OzihTpox6A3Aqvlfd20f1kWnixImYM2eOtmIZrMDAQHh5eaFJkyY4ffq0+nh4eDhcXV0hhICNjQ06deqErKwsHD58GPfv34cQAqtXr4a3t7d84fWMmZkZcnJyEBYWhnbt2skdxyAkJSUVeS4jIwNjxoxBZGQknJycMGzYMLRs2RKVK1cGAKSmpiIyMhKrVq1CdHQ0WrRogcWLF8PGxgY1a9bU1V+BSKfS0tIwYMAARERE5CsCFrYnXExMDJo0aQIhBE6cOMHiN1EpxxUZRERUqrVq1QorV65Ebm4uMjIyYG9vDwCoUKEC9uzZA09PT1y5ciXfGDs7O/j7+7OIQQYjLi4OvXr1QkJCQoEPfM9r3bo1fHx8IEkShg0bVqA1G70+a2trpKWl4fDhw8UuZBw6dAgAUL58+XzHVfsCVahQQbMhDUhx53NZW1ujQ4cO+OSTT9C9e3ctpzJMffr0QYcOHaBUKhEfH486deoAANq1a4dp06Zh1qxZuH37NjZv3gzg6c9uxIgRLGJoWKVKlXD9+nWu1NOhogoO2dnZGDhwIE6dOoVZs2bhm2++KfC6Xa9ePbi6uuLTTz/Fjz/+iKlTp2L06NEIDw/XRXQinVMqlejbty8iIyOhUCgwaNAgdOjQAePGjSv0eicnJ7Rq1QrHjx/Hli1bWMggKuVYyCAiolLv2ZZSz2revDkuXLiAgwcPIjY2Frm5uXB0dESPHj1gYWGh45RE8nj06BF69+6N+Ph4lC1bFmPHjkWHDh3Qp0+fQq93cHBAp06dcPDgwUL3mKHX165dO2zevBk///wz3N3dUatWrRden5CQgF9++QVCCPUeQCqxsbEAoJ6VS6/m+QJ3YRQKBaysrPJtOk3aYWFhgZCQkELPzZgxA66urli2bFm+13JfX18MHDhQt0ENQPv27REYGIiYmBg0a9ZM7jgG7e+//8bJkyfh6emJb7/99oXXCiHwzTffIDo6Ghs2bMCff/6Jzz//XEdJiXRn1apViIyMhImJCbZv344ePXoAQJGFDADo168fIiIicPjwYV3FJCItYSGDiIj0momJCXr06KF+k0tkaBYuXIjLly+jbNmyOHToULFmor377rs4cOAAjh49qv2ABuR///sftmzZgtu3b6N169aYOXMmvL29Ua5cuXzX3b17F2vXrsWMGTOQnp4OhUKBSZMm5bsmKCio0AIHFQ9brpQMO3fuxO7du5GUlASlUomqVavCzc0Nnp6eMDExUV/XpUsXdOnSRcakhmPSpEnYtGkT/vzzT3h7e8PYmLcM5LJ27VoIITB8+PBijxkxYgQCAwOxbt06FjJIL/3zzz8QQmDMmDHF/nzn7OwMALh48aI2oxGRDvBdCREREZEe27x5s3rj1uIup2/atCmAJy2pSHPat2+PH3/8EV9//TVu3bqFsWPHYvz48ahduzYqVaoEALh58yYSEhKQl5enbp/z3Xff5etVHx8fj507d0KSJLz77ruy/F2I3kRqaioGDBiA48ePFzi3YsUKTJs2DVu3bkXjxo1lSGfYWrRogblz52LChAlwd3fHihUrULFiRbljGaT4+HgAr7byzs7OLt9YIn1z9uxZAE9WWRSX6nmRnp6ulUxEpDssZBARERHpsfPnzwPAK/X2V+27cOfOHW1EMmhffvklatWqhYkTJyI1NRVKpRJxcXG4fPkygPz7NtjZ2WHu3Lnw8vLK9xh16tRBbm6uTnMTaYpSqUS/fv0QGRlZ5DVXrlxBjx49cPbsWd5E17FZs2YBAFq2bImgoCDUrFkT3bp1Q4MGDYrVlnPatGnajmgwVK8HcXFx6hnlL6OagFDcPYCIShvVe9NX2SNMqVQCAIyMjLQRiYh0iIUMIiIq1fz9/d9ovK+vr4aSEJVM9+/fBwBYWloWe8zjx48BIF9rF9IcT09PDBgwAFu3bsX+/fsRExODjIwMAICNjQ0aNWqELl264L333kOZMmVkTkukWYGBgYiMjIQQAnXq1MHXX3+Nli1bwsTEBNHR0fj9999x7NgxpKam4vfff8dPP/0kd2SDMmPGDPWG0kIIPHz4EDt27MCOHTuKNZ6FDM15++23ERkZiblz58LDwwMKheKF1+fl5WHOnDnqsUT6yNbWFmlpafjvv/9eucCnWv1KRKUXCxlERFSqDR8+XP2B+1UJIVjIIL1XoUIFpKSkIDExsdgbt6o2kq5SpYo2oxk0U1NTeHp6wtPTU+4oek0bsy+FEFwR8wYCAwMBAA4ODjh+/Hi+zdTr1auHAQMGoGvXrggNDcWGDRtYyJDB87P5ObtfHr6+vjh+/DgiIiIwYMAALFmypMjX5dTUVIwZMwYRERF8f0t6rVGjRkhLS0NkZGSx20utX78eQgi0aNFCy+mISNteXNInIiIqBSRJeu0vIn2nKl6EhYUVe4y/vz+EEGjTpo22YhHpxJu8PvC1QztOnToFIQQmT56cr4ihYmRkhJkzZwJ40mIqMzNTxwkNW15e3ht9keZ89NFHaN++PSRJws6dO1G7dm0MGDAAP/zwA5YuXYply5bhhx9+wIABA1CrVi31qpl27drho48+kjk9kXYMGDAAkiRh3rx56tWsL7Jx40b1c2PgwIHajkdEWiYkvhMnIqJSLCkp6aXXZGVl4dKlS1i7di02btyIdu3aYcmSJbCwsEDNmjV1kJJIPqtWrcKIESNgZmaGCxcuoEaNGgAAhUIBIQSio6PRsGFD9fVz587FpEmTIIRAUFAQN5OmUk11Q7woO3fuRFRUFIAnszxbtmyp3lg3NTUVkZGRiImJgRACLi4u6NWrFwBg+vTp2g2ux8qWLYtHjx7h6NGjaNmyZaHXPHjwAJaWlhBC4PLly6hVq5aOUxKVDFlZWRgyZAi2b98OAEWuQlbd1unbty8CAgJeqZ0kUWny+PFj1K9fH//99x+aNWuGVatWoWHDhgXe16alpeHPP//Eb7/9BqVSCScnJ5w+ffq1V/ITUcnAQgYRERmUwMBAeHt7w83NDfv27eObWdJ7eXl5aNasGc6ePQsHBwfMnz8fPXv2hJGREYQQiImJQYMGDRAVFYW5c+di3bp1AABXV1eEhITIG55Ii2bNmoUZM2agadOmWLJkSZEtJyIjIzFmzBicOXMG06dP5x4Ab6ioImpR18XExLDfPxm8nTt3YuHChQgJCcGDBw/ynTM3N4ebmxs+/vhj9OnTR6aERLpz5swZuLm54e7duxBCoH79+rhw4QKEEGjatCnu37+PhIQE9SrKChUq4OjRo6hbt67c0YnoDbGQQUREBmfUqFHw8/PD/PnzufSeDEJycjLat2+Pq1evQggBCwsL9Y2QihUrIjMzU73BtyRJqFOnDsLDw2FnZydn7FJLtS/D83spvMl+DdyXQbMOHDiAbt26oV69ejhx4gTKli37wuuzsrLQrFkzXL58GXv27EHXrl11lFT/vGoh42XXERmSvLw8xMfH4/bt2wAAGxsb1KlTRyv7ARGVZJcvX8awYcNw9OhR9THVBLVnb3O2bNkSa9euRe3atXWekYg0j3tkEBGRwfH09IQkSfDz85M7CpFO1KhRA6dPn8bgwYOhUCiQlZWlnqV28+ZNPHr0SP2hz9PTE8ePH2cR4w0UtZcC92UoOf766y8IIfDVV1+9tIgBPGmH9NVXX0GSJPz99986SEgkvwMHDmDo0KGoW7cuLC0tYWxsjHPnzuW7JiwsDAsWLMCaNWtkSmlYFAoFHB0d0apVK7Rq1Qr16tVjEYMMUt26dREeHo6wsDB89tlncHNzw9tvv4169eqhbdu2GDt2LPbs2YNjx46xiEGkR4zlDkBERKRrqv7nFy9elDkJke7Y2toiICAAP/74o3pfgLS0NCiVSlSoUAHOzs7o27cv6tWrJ3fUUq+o/RO4r0LJodoXo0mTJsUe07RpUwBPWk3Rm1uwYEGxCqbFuY7tvjTrwYMHGDZsGDZv3gzg6ezmwtpxGhkZYdy4cRBCoFWrVnB0dNRpViIybO3bt0f79u3ljkFEOsLWUkREZHC2b9+OAQMGwMLCAvfv35c7DhER6Zi5uTmys7Oxf/9+dOrUqVhjQkJC0LlzZ5QpUwYPHz7UckL9pWoZpUlKpVKjj2fo+vTpg127dkGSJLRs2RIdOnTA7Nmzi2z11aRJE8TGxuKHH37AV199JVNqIqIn0tPTIYSAra2t3FGISMO4IoOIiAxKTk4Ofv31VwDghm9ERAaqatWqSExMxKZNm4pdyNi4cSMAwN7eXpvRDIIm59Jpuihi6DZt2oR///0XQggsWbIEH3zwAQBg9uzZRY5xd3dHTEwMQkNDWch4DSNHjgTw5P/l5cuXFzj+Op5/LCJ9l5qaiqlTp2Lz5s3IyMgAAJQrVw79+/fHrFmzUKNGDZkTEpEmsJBBRESlWnJy8kuvycvLQ0ZGBqKiojBv3jzExMRACAEvLy8dJCQiopKmZ8+eWLhwIRYvXowOHTrA09Pzhddv3LgRixcvhhACvXr10lFK/RQcHCx3BHqBVatWAQB8fHzURYyXad68OQDg/PnzWsulz/z8/NQFuWeLD88efxWSJLGQQXrh6tWraNmyJQBg6tSp+Pjjjwu9LiEhAR06dMCNGzfyFcrv3r2L1atXY8eOHThw4ADeeecdXcQmIi1iaykiIirVXmeDQ0mS0KZNGxw8eBBlypTRQiqikiMsLOyVxwghYGZmhvLly8PBwQGmpqZaSKbfilNkfVWcTag5165dQ6NGjZCZmQkA6Nu3L4YPH44WLVrAzs4OQgikpqYiMjISq1atwvbt2yFJEsqVK4fY2FhUq1ZN5r8BkXZUrVoVqamp2LFjR76inaolWGGtpaKiotCyZUuYm5sjKytL15FLPQcHB3XB4sqVK4Uefx3PPhZRabRs2TJ8+OGHMDU1xbVr11ChQoVCr2vZsqV67ysAqF69OqpWrYpz586pX+fr16+P6OhoGBtzPjdRacZnMBERlWqvWo+3tbXFmDFj8O2337KIQQbBzc3tjW6EGBsb45133sHw4cPxwQcfwMTERIPp9Neb3oB6nhACubm5Gns8Q1etWjXs2LEDffv2xb1797Bjxw7s2LGjyOslSYKVlRW2bdvGIgbptfT0dABPChrFpVAoADxZAUuvLjEx8ZWOExmKo0ePAgA6depUZBEjKCgIUVFREELAxsYGa9euRffu3QEADx8+xLhx47By5UpcunQJmzZtwvvvv6+z/ESkeSxkEBFRqbZy5cqXXqNQKGBlZYVatWrBycnptVZxEJVmb7IANycnB5GRkYiKisLChQsRFBTElQHFxIXPJZurqyuio6MxadIkbN26tcgNo42MjNC/f3/8/vvvqFmzpo5TEulW+fLlkZ6ejuvXrxe7DYtq5n/FihW1mIyIDE10dDSEEOjWrVuR1wQEBKi///3339VFDAAwNzfHsmXLEBUVhZiYGGzbto2FDKJSjoUMIiIq1YYNGyZ3BKISLTg4GDk5OZg6dSoiIiJQtWpVDBo0CC4uLqhUqRIA4ObNm4iKisKGDRtw/fp1tGrVCjNnzsTDhw8RExOD9evXIyYmBjExMejVqxdOnz7Npfkv8bLfTXfu3MG2bdsghICvr6+OUtHzqlevjg0bNiA1NRXBwcGIjo7G7du3AQA2NjZo3LgxOnXqhCpVqsiclEg36tWrh6NHj+LMmTPF3g9m69atAABnZ2ctJiMiQ6NaldS0adMirwkJCQHwpAjr7e1d4LwQAiNHjsSnn36KM2fOaCMmEekQ98ggIiIi0nP9+vXDzp07MW7cOPzyyy8wMzMr9LrHjx/js88+w/z589GzZ0/8+++/6nNTp07FDz/8ACEEFi1ahNGjR+sqvl6KjY1F48aNIYQociUAEZGu/fTTT/jmm29QpUoVJCQkqF8vitoj49ChQ+jcuTPy8vL42iCTx48f486dO6hUqZK6zReRPjAzM0NOTg5OnjxZaDEjMTERtWvXhhACffv2VRdVnxcWFgY3NzeUL18eGRkZWk5NRNrEVzkiIiIiPbZy5UoEBQWhV69e+PPPP4ssYgBAmTJl8Pfff6NXr17Ys2cPlixZoj733XffoWPHjpAkCZs3b9ZFdCIi0rGxY8fC1tYWqamp8PDwUK9Qel5ubi6WLl2KPn36IC8vD9WrV8fw4cN1G1bP3b9/H//++y/+/fdf3L9/v8D5W7duYeDAgShXrhyqVq0KGxsbTJ48GY8fP5YhLZHmqfYay87OLvT88ePH1d+7uLgU+TjW1tYAgKysLM2FIyJZsJBBREREpMdWrFgBIQQ+/PDDYo8ZM2YMJEnCqlWr8h1X3aTi0nwiIv1Urlw5rF+/HsbGxti1axeqV6+er8XUF198ge7du8POzg4fffQRMjMzUaZMGQQGBsLExETG5Ppn06ZN6NOnDz766CNYWFjkO5eXl4d3330XW7duRU5ODiRJQmZmJubOnVtoex2i0ki1wfelS5cKPX/kyBH19y1atCjycTIzMwHghZN5iKh0YHNjIiLSC+np6VizZg0OHTqEhIQEZGZmvrRdixAC8fHxOkpIJI/z588DAN56661ij1Fde+HChXzH3377bQAocoYuUWmUnp6Oo0ePFvu1AwCmTZumg2RE8ujSpQsOHjwIHx8fJCUlYffu3eqZ0bt27QIAqDpUV69eHYGBgWjZsqVsefXVnj17AADvvfdegZZR69evx4kTJyCEQLNmzdCxY0eEhobi5MmT2Lp1K3bv3o2ePXvKEZtIY5o2bYobN25g06ZNGDJkSL5zkiRh+/btAABjY2O0a9euyMdJSkoCAFSuXFl7YYlIJ1jIICKiUm/Dhg348MMPce/ePQBPP1y/jOpDOZE+e/ToEQDg6tWrxd6I9erVqwBQoD2Farbt8zNDiUqjtLQ0fPrpp9i4cSNyc3NfaSwLGaTv2rVrh7i4OKxbtw7bt29HVFQU0tLSoFQqUaFCBTg7O6Nfv34YNmwYTE1N5Y6rl2JiYiCEQNu2bQuc8/f3BwA0b94cR44cgbGxMXJycuDq6orIyEisWrWKhQwq9fr164ddu3Zh27ZtWL16NYYOHao+N3v2bCQmJkIIga5du8LS0rLIxzl69CgAoH79+lrPTETaxUIGERGVahEREfD29kZeXh4kSULVqlXh7OwMW1tbbnhIBKBOnTqIiYnBsmXL0Ldv32KNWbp0qXrss65fvw4AqFSpkmZDEulYRkYG2rdvj/j4+GIXv4kMjbGxMXx8fODj4yN3FIOUlpYGAKhVq1a+4zk5OQgLC4MQAmPHjoWx8ZPbOiYmJvjoo49w/PjxfHsHEJVWQ4cOxY8//oirV69i+PDhmDdvHurWrYvz58/na3M6adKkIh9DkiRs3boVQgi0bt1aF7GJSItYyCAiolLtl19+gVKphLm5OZYuXcq+wETP8fDwQHR0NIKCgvDZZ5/hp59+KrKPeU5ODr766isEBQVBCIFBgwblOx8eHg4AqFu3rtZzE2nTzz//jMuXLwMAunfvjkmTJqF58+awtbXlaj0iKhFUbRyfX/ESGRmJhw8fQghRYNVFvXr1AAApKSm6CUmkRRYWFli3bh169uyJzMxMREVFISoqCsDTFfgjR45Ely5dinyMf//9F9euXVOv3CCi0o2FDCIiKtWOHDkCIQS++uorFjGICvHZZ59h9erVuHz5MubMmYMNGzZg0KBBaN68uXplxc2bN3HixAls2LBB3VaqTp06mDx5svpxlEol1q5dCyEEunfvLsvfhUhTtm3bBiEEevfure6xTURUklhYWCAzM1O9MkMlLCwMwJNJBc/3/Dc3N9dZPiJdaNOmDaKiojBlyhT8+++/ePjwIQCgZs2aGD9+PD799NMXjv/uu+8AAFWqVOGKDCI9wEIGERGVanfu3AEA9OjRQ94gRCWUubk5Dh48iN69eyM6Ohr//fcf5syZU+i1qtltTk5O2LlzZ74bIlevXsWIESMAPFnlQVSaJScnAwDGjh0rcxKiku/evXvIzMyEUql86bU1atTQQSLDUKdOHZw+fRohISH5JhBs2bIFQgh06NChwJibN28CAOzs7HSWk0jbHB0dsWHDBuTl5eHmzZswNTWFjY1NscYeOHAAANQt2IiodOMzmYiISjV7e3skJyezFQjRC7z11ls4ceIE5s+fj8WLF+PChQuFXlevXj2MGTMG48aNK9B+qmbNmpg+fbou4uqFWbNmvfD8szNsX3atCjeY1hxLS0s8fvy4wGxmInpi3759WLBgAQ4fPqxucfQyQgjk5uZqOZnh6NatG06dOoUFCxbA1dUVrq6uWLlyJSIjIyGEKHTfq7NnzwIAqlatquu4RFqnUChe+XW7bNmyWkpDRHIQEne3IyKiUmz06NFYsWIF5s+fj48++kjuOESlwvXr1xETE4OMjAwAgI2NDRo1aoRq1arJnEx/KBQKjRdYizMbmoqnS5cuCAkJwebNm9G/f3+54xCVKBMmTMD8+fMBPF2pVxxCCP6e0qAbN27g7bffRmZmZr7jkiShYcOGiI6OLvA606lTJ4SFheHjjz/GvHnzdBmXiIhI61jIICKiUu3ixYto1qwZ7O3tcfr0aVhaWsodiYgICoVCo4/HG4SaFRgYCC8vL7i7u2Pjxo1yxyEqMdauXQsfHx8AgJmZGQYMGIDmzZvD1ta2WL/Xhg0bpu2IBuXQoUPw8vLCjRs31Mdq166NoKAgNGjQIN+18fHxqF+/PiRJwqZNmzBgwAAdpyUiItIuFjKIiKjU27p1K7y9vdG4cWOsWLECjRo1kjsSERm40NBQjT9mx44dNf6Yhmzo0KFYu3YtfvjhB3z11VdyxyEqETp27IhDhw6hevXqOHjwIOrUqSN3JIOXnZ2N8PBwpKSkwN7eHu3bty+03//hw4fV+wF8/vnnsLCw0HVUIiIirWIhg4iISrWRI0cCeNIT+OTJkxBCoHHjxmjQoMFLP8AJIbB8+XJdxCQqEfLy8hAcHIyjR48iJSUFDx48wA8//AB7e3v1NdnZ2cjNzYWRkRHKlCkjY1oi7QkLC0NeXh6+/fZbHD16FM2bN4e3t3exXjsAFLrJLpE+sLGxwb1797B06VL1eywiIiKikoCFDCIiKtWe70MvSVKx+tKrrmOrFjIUQUFBmDBhApKSkvIdj46ORsOGDdV/XrBgAcaPHw9LS0tcv36dmySSXnqTPUy4oTHpM0tLSzx8+BBRUVFwdnaWOw4RERGRWsH1iERERKVIjRo1NL6hLpG+Wbp0KT766CP1pq0VK1bErVu3Cn3ufPDBB/j2229x9+5dbNmyRd0rnUjfcD4XUUEODg44f/487t+/L3cUek58fHy+FZWffPIJKlasKHcsIiIinWEhg4iISrXExES5IxCVaHFxcRg7diwAoHPnzpg3bx4aNGhQ5KatpqamGDhwIJYvX469e/eykEF6KTg4WO4IRCWSu7s7fvjhBxw4cACurq5yxyEAJ0+exP/+9z+Eh4fnO+7h4ZGvkDF//nzMnDkT5cuXx7lz52BiYqLrqERERFrF1lJEREREeuyTTz7BokWL4OTkhKioKJiamgJ42lrn+dZSAODv74/hw4ejUaNGiI6OliM2ERHJ4O7du3jnnXeQkZGBY8eOoUGDBnJHMmhBQUEYNGgQsrOz860iK+z1OzMzE1WrVsWDBw+wceNGvPfee3JEJiIi0prCp+IRERERkV44ePAghBD43//+py5ivEzdunUBAP/99582oxERUQlTvnx57NmzB5UrV0bbtm2xYMECZGRkyB3LIN24cQODBw/G48eP0bBhQ+zatQuZmZlFXm9lZYV+/foBAHbt2qWrmERERDrD1lJEREREeuzq1asAgKZNmxZ7jGqD7wcPHmglExERlUy1a9cG8OT3/507dzB+/HhMmDABFStWhIWFxQvHCiEQHx+vi5gGYc6cOcjKykLNmjVx6NAhWFtbv3SMm5sb/vnnH5w4cUL7AYmIiHSMhQwiItIrmZmZuHLlCjIzM6FUKl96fYcOHXSQikg+qg29X6UokZ6eDuDJzFyi0m7WrFkaf8xp06Zp/DGJSoLn9x6TJAmSJCEtLe2lY1WvN6QZu3fvhhACkydPLlYRA4C6FdiVK1e0mIyIiEgeLGQQEZFeWLp0KRYsWIDo6GgUd/snIQRyc3O1nIxIXtWqVUNcXBwSEhKKvXHr4cOHATydmUtUms2YMUPjN1hZyCB9NWzYMLkj0P9LSkoCALRs2bLYY8qVKwcAuH//vlYyERERyYmFDCIiKtWUSiUGDhyIHTt2AECxixhEhsLNzQ2XLl3CqlWrinWD6u7du1i0aBGEEOjcubMOEhJpnyZfGzjrnPTZypUr5Y5A/0812SYvL6/YY+7evQsAsLS01EomIiIiObGQQUREpdqiRYuwfft2AEDlypUxYsQING/eHLa2tlAoFDKnI5LfmDFjsHTpUoSGhsLPzw/Dhw8v8tr09HR4eHggJSUFJiYm+Oijj3QXlEhLgoOD5Y5ARPTKqlSpgsTERCQkJKB169bFGnP8+HEAQI0aNbQZjYiISBYsZBARUanm7+8PAGjYsCEOHToEGxsbmRMRlSzOzs6YOHEi5s6di1GjRmHXrl0YOHCg+vyRI0dw+vRphIeHY+3atbh37x6EEJg6dSpq1qwpY3IizejYsaPcEYiIXpmrqyuuXLmCDRs2wNvb+6XXZ2dnY/HixRBCwM3NTfsBiYiIdExI7MFBRESlWLly5ZCVlYW1a9fi/ffflzsOUYkkSRLGjRuHhQsXvrAtjupt4f/+9z/88ccfuopHREREzwkJCUHnzp0hhMDu3bvRrVs3AIBCoYAQAtHR0WjYsCGAJ0UMX19fBAYGQqFQ4MyZM2jUqJGc8YmIiDSOKzKIiEgv1K9fX+4IRCWWEALz58/HgAED8PPPPyM0NLRAz20hBNq0aYNvv/0W7777rkxJiYioJMnIyMCZM2dw69YtPHz48KX7zfj6+uoomf5zc3PD+++/j/Xr16Nv376YOHFivhWViYmJuHPnDsLDw7FkyRIkJCRACIGPPvqIRQwiItJLXJFBRESlWvPmzXH69Gns27ePGxOTwXN2dsawYcPg7e0NOzu7Iq/LzMzEqVOnkJaWBqVSiQoVKuCdd95BxYoVdZiWiIhKqpCQEEyfPh2HDx8u9hghhHqDatKMx48fY+DAgfj333+LtaLS3d0d69evh5GRka4iEhER6QwLGUREVKr99ttv+PLLL9kKhwhP200YGRmhW7duGDZsGPr3748yZcrIHY2IiEqJhQsXYvz48ZAk6aUrMJ4lhIBSqdRiMsO1dOlS/Prrr4iPjy/0/FtvvYUpU6bgo48+0nEyIiIi3WEhg4iISrXHjx+jdevWuHDhAvbu3QtXV1e5IxHJxtzcHI8fPwYA9czNcuXKYdCgQRg6dCifH0RE9ELnz59HkyZNkJeXh8aNG2PWrFkwMTFB7969IYTA5cuXcfv2bURFRWHp0qU4efIk2rdvj8WLF8PCwgI1a9aU+6+g186dO4eoqKh8KyqdnZ3RrFmzfCs2Tpw4gebNm8uYlIiISPNYyCAiolIvLS0N7u7uiIqKwoQJE+Dt7Y0GDRrAzMxM7mhEOnXv3j1s3LgRq1evRlhYmHomrermhoODA4YOHQofHx/UrVtXzqhERFQCffLJJ1i0aBEqVaqEy5cvw8rKCrGxsWjcuHGBFReSJOGrr77Cb7/9hs6dO2P//v0yJicAOHLkCL777jvs27ePbb6IiEjvsJBBRESl2rM9gCVJemH/4OexlzPps+TkZKxZswZr1qzBhQsX1MdVz5FWrVph2LBheP/992FtbS1TSiIiKkkaNWqECxcuYNasWfjmm28AoMhChkrXrl0RHByMpUuXYuTIkbqOTAAOHDiA77//HmFhYepjbPNFRET6hoUMIiIq1RQKxWuPZS9nMhQnTpzA6tWrsW7dOqSlpQF4WtAwNTVF79694evri969e3ODUCIiA1a+fHncv38fQUFBePfddwE8aWfk5OQEIQQePXoEExOTfGMCAwPh5eUFNzc3HDx4UI7YekOSJGzZsgX79+/Hf//9BxMTEzg4OMDDwwNt27YtcH1ISAimTJmCiIgI9XgA6N69O3bv3q3T7ERERNrGQgYREZVqM2fOfKPx06dP11ASopJPqVRiz549WL16NbZv346HDx8CeFrUqFChAgYPHoyhQ4fCxcVFzqhERCSDMmXKIDc3FydPnkTTpk0BAElJSahVqxaEELhx4wbs7OzyjTl58iRcXFxgZ2eHlJQUOWLrhaSkJPTv3x/R0dGFnh80aBACAgJgZGSE9PR0fPDBB9i+fTuAp6uS+/Xrh2+++Yav4UREpJdYyCAiIiIyQJmZmer9NEJDQwvsp9GgQQP4+vriyy+/lDMmERHpkL29PdLS0nDo0CH1CoAHDx7AysoKABAaGor27dvnG7N371707NkTpqamePTokc4z64Ps7Gw0b94csbGxRV4jhMDkyZMxfvx4dOzYEUlJSZAkCUZGRvD09MSUKVPQqFEjHaYmIiLSrdfvx0FERFSKnTp1Cp9++qncMYhkY2VlhREjRuDgwYNITEzEDz/8gLfffhuSJEGSJJw/fx5TpkyROyYREelQgwYNAABxcXHqYxYWFnB0dAQA9QqAZ23ZsgUAUKlSJR0k1E8BAQGIjY2FEAIODg5YtmwZIiIicOrUKaxduxbOzs6QJAkLFy6Et7c3EhMTIUkSBg4ciHPnziEgIIBFDCIi0nssZBARkcG4ceMGfvvtNzRp0gQuLi7466+/5I5EVCJUr14dX3zxBX755Rc0atRIvSqDiIgMS/v27SFJEg4dOpTvuLu7OyRJwl9//YWVK1ciKysLaWlp+PXXX7Fs2TIIIdC5c2eZUpd+mzdvBgC89dZbOHv2LEaOHIkWLVqgadOm8PLyQmRkJNq2bYusrCyEh4fDyMgIfn5+2LBhg7rIREREpO/YWoqIiPTaw4cPsXnzZvj7++PgwYPIy8sD8LSXMDf7JkMXGRmJ1atXY/369bh16xaAp5uFWllZ4e7du3LGIyIiHYqIiECbNm1ga2uLq1evwszMDACQnp6O+vXrIyMjo8AYSZJgbm6OqKgovP3227qOrBdq1KiBa9eu4c8//8S4ceMKvebgwYPo2rUrhBAYNmwYVqxYoeOURERE8jKWOwAREZE2BAcHw9/fH5s3b8b9+/cBPL05a29vj/feew8DBw6UMyKRbJKSkrBmzRqsWbMGly5dAvD0+aFQKNC5c2f4+vryOUJEZGBatWqFlStXIjc3FxkZGbC3twcAVKhQAXv27IGnpyeuXLmSb4ydnR38/f1ZxHgD6enpAAAnJ6cir2nSpIn6ew8PD61nIiIiKmm4IoOIiPTGhQsX4O/vj4CAAFy9ehXA05uzb731FgYOHAgPDw+0bduWrXPI4Ny9exeBgYFYvXo1wsPD1cdVz5GGDRti6NCh8PHxQbVq1eSKSUREJVhOTg4OHjyI2NhY5ObmwtHRET169ICFhYXc0Uo1hUIBIQSio6PRsGHDl1536tSpfIUNIiIiQ8AVGUREVKqlp6fjn3/+gb+/P06cOAHg6Y1Za2tr3LlzB0IIzJ49G56ennJGJdK53Nxc7Ny5E6tXr8bOnTuRnZ0N4OlzpFKlSvDy8oKvry+aN28uZ1QiIioFTExM0KNHD/To0UPuKAbN2Ji3coiIyPDw1Y+IiEqdnJwc7NixA/7+/ti9ezdycnLUN2ZNTU3Rq1cv+Pj4oHfv3jA3N5c5LZHuHT16FKtXr8aGDRtw+/ZtAMj3HOnbty98fX3x7rvv8mYIERERERERlXj85EpERKXGsWPH4O/vj8DAQPVmk6pNu9u1awcfHx94enrCxsZG5qRE8pgxYwYCAgKQkJAA4GnxAgBat24NX19feHl5wdraWqaEREREVJQFCxbAzs5OI9dNmzZNU7GIiIhKBO6RQUREpYaqL7Dqpat+/frw8fHBkCFD4ODg8MIx//zzD1tLkd57/jni4OAAHx8f+Pr6om7dujKnIyKi0iQ9PR1Hjx5FQkICMjMzoVQqXzqGN89fj+r1W5OK8/MiIiIqTbgig4iISh0rKyv89ddfGDZsmNxRiEocKysreHh4wNfXFx06dJA7DhERlTIpKSmYNGkSNm3ahNzc3Fcay0LG69PkHFNNF0WIiIhKAhYyiIioVJEkCffv38fIkSPx559/wsfHB4MHD4a9vb3c0Yhkt3btWgwYMABmZmZyRyEiolLo5s2baNu2LZKSkjR6Y51eLDg4WO4IREREJR5bSxERUakRFhYGPz8/bNq0CZmZmQCezDhTKBRwc3PD0KFD4e7uDktLS/UYtpYiIiIiKp5PPvkEixYtAgAMGjQIH3/8MZo2bQpra2vO8iciIiJZsZBBRESlzqNHj7Blyxb4+/tj//79UCqV6g/X5ubm6Nu3L4YOHYoePXrAxMSEhQwyeA8ePAAAWFhYFHr+77//RmBgIG7duoVatWrh448/Rt++fXUZkYiISoAaNWrg2rVrGDp0KPz8/OSOQ0RERKTGQgYREZVqKSkpWLNmDdasWYOzZ88CeNoXuEKFCrh16xYLGWTQduzYgQEDBsDS0hJXr16FlZVVvvMjR47EqlWrADxp3aZ6/nz//ff4+uuvdZ6XiIjkY25ujuzsbAQHB3OfJSIiIipRFHIHICIiehNVqlTBZ599htOnT+PUqVP43//+Bzs7O0iSpC5iAMCkSZMwceJEHDp0SObERLq1Z88eSJKEfv36FShiHD58WD3j1sLCAs7OzjAzM4MkSZg2bRpiYmJkSExERHKpWrUqAKBs2bIyJyEiIiLKj4UMIiLSG02bNsUff/yBq1evIigoCJ6enihTpgwkScL169cxb948uLm5wd7eHp988gkOHDggd2QirTt27BiEEOjUqVOBc0uWLAHw5MbV+fPnceLECVy4cAHVq1dHXl4eFi9erOu4REQkI9UqjOjoaJmTEBEREeXH1lJERKTX7t27h/Xr12P16tUIDw+H6mVPCAEhBHJzc2VOSKRdqn7nYWFhaNeuXb5zdnZ2SE9Px08//YQvvvhCfXz27Nn44osv4OTkpG7ZRkRE+i82NhbNmzeHo6MjIiMjYWZmJnckIiIiIgBckUFERHquXLlyGD16NMLCwhAfH4/p06ejTp06kCQJrOWTIbh58yYAFGgrFRsbi1u3bgEA+vfvn++ci4sLACApKUkHCYmIqKRo1KgRVq5ciYsXL6J79+64dOmS3JGIiIiIAADGcgcgIiLSFQcHB0yfPh3Tp09HeHg4Vq9eLXckIq0zMjICANy+fTvf8cOHDwMAKlWqhPr16+c7Z2NjAwB49OiRDhISEVFJMnjwYDg6OqJ3795o2LAhmjRpgnr16sHCwuKF44QQWL58uY5SEhERkaFhIYOIiAxSu3btCrTZIdJH1apVw+XLl3H69Gm4ubmpj+/cuRNCCLi6uhYYc/fuXQBAxYoVdRWTiIhKiEuXLmHSpEnqVXtnzpzBmTNnXjhGkiQWMoiIiEirWMggIiIi0mOurq6Ii4vDvHnz4OPjg4oVKyIyMhK7d+8GAPTo0aPAmPPnzwMAqlSpotOsREQkr+TkZHTo0AE3b95Ut+C0srKCtbU1FAp2piYiIiL5sJBBREREpMc++eQT+Pn54cqVK6hduzbq1auHc+fOITc3F7a2tnj//fcLjDl48CCEEGjYsKEMiYmISC6zZs1CWloaFAoFJk+ejE8++QQODg5yxyIiIiLiZt9ERERE+qxZs2b47bffIITA/fv3cfLkSTx69AgmJiZYunRpgU3A7969i507dwJAvlZURESk/w4cOAAhBCZOnIhff/2VRQwiIiIqMbgig4iIiEjPffrpp+jatSs2btyIlJQU2NvbY/DgwQU2+QaAkJAQtGjRAgDQp08fXUclIiIZpaamAgAGDhwocxIiIiKi/ISkanxJRERERERERAarTp06SExMREREBFxcXOSOQ0RERKTG1lJEREREREREhG7dugEAIiMjZU5CRERElB9XZBARERERERERLl++jGbNmsHW1hYnT56Era2t3JGIiIiIALCQQURERKTXwsLC3mh8hw4dNJSEiIhKgwMHDsDT0xN2dnb466+/1Ks0iIiIiOTEQgYRERGRHlMoFBBCvNZYIQRyc3M1nIiIiEqqzp07AwCuXbuGuLg4CCFgbW0NR0dHWFhYvHCsEAIHDhzQRUwiIiIyQCxkEBEREekxheL1t0QTQkCpVGowDRERlWTPFr+Le6tACAFJkviaQURERFplLHcAIiIiItKe4ODgl16TlZWFS5cuYd26dTh+/DjatWuHmTNnwsjISAcJiYiopOjQocNrr+IjIiIi0iauyCAiIiIitd9++w1ffvklvL29sWbNGrnjEBEREREREbGQQURERET5eXh4YMuWLQgICICXl5fccYiISEeSk5MBAJaWlrC1tZU5DREREdFTr980mYiIiIj0kq+vLyRJwpIlS+SOQkREOuTg4IBatWph3bp1ckchIiIiyoeFDCIiIiLKp0aNGgCA6OhomZMQEZEumZubAwBatGghcxIiIiKi/FjIICIiIqJ8UlNTATzZBJyIiAxHtWrVAABKpVLmJERERET5sZBBRERERPnMnz8fwNOVGUREZBi6d+8OADh8+LDMSYiIiIjyYyGDiIiIiJCRkYF9+/ahV69eCAoKghAC7u7ucsciIiIdmjhxIszNzTF79mxcu3ZN7jhEREREakKSJEnuEERERESkHUZGRq88RpIk1KtXDxEREShfvrwWUhERUUm1fft2+Pj4oHz58vjll1/g4eEBU1NTuWMRERGRgWMhg4iIiEiPKRSvtgDX2NgYgwYNwpw5c2BnZ6elVEREVBJ17twZAJCUlIQrV65ACAFTU1M4OjrCxsbmhcVxIQQOHDigq6hERERkYFjIICIiItJjM2fOfOk1CoUCVlZWqFWrFtq2bYtKlSrpIBkREZU0CoUCQggAT1bnFYcQApIkQQjBTcKJiIhIa1jIICIiIiIiIiK4ubmpCxmvIzg4WINpiIiIiJ5iIYOIiIiIiIiIiIiIiEqsV2uaTEREREREREREREREpEPGcgcgIiIiIt1JTU1FSEgIYmJicPv2bQCAra0tnJyc4ObmhsqVK8uckIiIiIiIiCg/FjKIiIiIDMCNGzcwadIkbN68Gbm5uYVeY2xsjIEDB+L333+Hvb29jhMSEVFJdPXqVaSkpODBgwdo0aIFzM3N5Y5EREREBoh7ZBARERHpuTNnzqBr1664ffs2XvbWTwiBChUq4MCBA2jcuLGOEhIRUUmSmZmJX3/9FX5+frh+/br6eHR0NBo2bKj+87p167B582aUL18eS5culSMqERERGQgWMoiIiIj0WFZWFurXr6++EdW1a1eMHj0arVq1QpUqVQAAKSkpOH78OJYtW4a9e/cCAN566y1cuHABFhYWsmUnIiLdi4uLQ69evZCQkJCv+C2EKFDISExMRN26dSFJEkJDQ9G+fXs5IhMREZEB4GbfRERERHps3rx5uH79OhQKBZYuXYq9e/di0KBBqFGjBkxNTWFqaooaNWrAw8MDu3fvxrJlyyCEwLVr1zB//ny54xMRkQ49evQIvXv3Rnx8PCwsLPDFF18gKCioyOsdHBzQqVMnAMD27dt1FZOIiIgMEAsZRERERHps27ZtEEJg+PDhGDVq1EuvHzlyJEaMGAFJkrBlyxYdJCQiopJi4cKFuHz5MsqWLYtDhw7h559/Rq9evV445t1334UkSTh69KiOUhIREZEhYiGDiIiISI9dunQJAODl5VXsMYMHD843loiIDMPmzZshhMDEiRPxzjvvFGtM06ZNATxpSUVERESkLSxkEBEREemx+/fvAwBsbW2LPcbGxgbAk/01iIjIcJw/fx4A0L1792KPqVChAgDgzp072ohEREREBICFDCIiIiK9VqlSJQBPb04Vx4ULFwAAFStW1EomIiIqmVTFb0tLy2KPefz4MQDAxMREK5mIiIiIABYyiIiIiPRa69atIUkS/vjjD+Tm5r70+tzcXPzxxx8QQqB169Y6SEhERCWFanVFYmJiscfExsYCAKpUqaKNSEREREQAWMggIiIi0mu+vr4AgNOnT6N37964fv16kddev34dffv2xcmTJwEAw4cP10VEIiIqIZo1awYACAsLK/YYf39/CCHQpk0bbcUiIiIigpAkSZI7BBERERFpj7u7O7Zu3QohBExMTNC9e3e0atUKdnZ2EEIgNTUVERER2LdvH7KzsyFJEtzd3bFx40a5oxMRkQ6tWrUKI0aMgJmZGS5cuIAaNWoAABQKBYQQiI6ORsOGDdXXz507F5MmTYIQAkFBQXj33Xflik5ERER6joUMIiIiIj33+PFj+Pr6YsOGDQAAIUSh16neFg4aNAj+/v4oU6aMzjISEZH88vLy0KxZM5w9exYODg6YP38+evbsCSMjIwghEBMTgwYNGiAqKgpz587FunXrAACurq4ICQmRNzwRERHpNRYyiIiIiAzEzp07sWDBAoSGhuLBgwf5zllYWKBjx44YO3YsevXqJVNCIiKSW3JyMtq3b4+rV69CCAELCwv1a0bFihWRmZmp3uBbkiTUqVMH4eHhsLOzkzM2ERER6TkWMoiIiIgMjFKpREJCAm7fvg0AsLW1Re3atWFkZCRzMiIiKglu376N8ePHIzAwEEqlstBrhBAYNGgQFi5cCBsbGx0nJCIiIkPDQgYRERERERERFZCUlISdO3ciKioKaWlpUCqVqFChApydndG3b1/Uq1dP7ohERERkIFjIICIiIiIiIjJgO3fuxO7du5GUlASlUomqVauiU6dOGDRoEExMTOSOR0RERMRCBhEREZGhuHv3LjZu3IijR48iJSUFDx48wMqVK1GzZk31NdevX8edO3dgZmaG2rVry5iWiIi0LTU1FQMGDMDx48cLPe/g4ICtW7eicePGOk5GRERElJ+x3AGIiIiISPvmzZuHb775Bvfv3wfwZINWIQSysrLyXRcSEgIfHx+YmZnh6tWrsLW1lSMuERFpmVKpRL9+/RAZGVnkNVeuXEGPHj1w9uxZVKxYUYfpiIiIiPJTyB2AiIiIiLRr+vTpmDhxIjIzM2FqaormzZsXea2XlxeqVKmCx48fY9OmTTpMSUREuhQYGIjIyEgIIVC3bl0sX74c0dHRuHDhAjZs2IDWrVsDeLJq4/fff5c5LRERERk6FjKIiIiI9NiJEyfw/fffAwB8fHyQkpJSZAsRAFAoFBg0aBAkScK+fft0FZOIiHQsMDAQwJP2UcePH8eIESPQqFEj1KtXDwMHDsShQ4fQsWNHSJKEDRs2yJyWiIiIDB0LGURERER6bN68eZAkCW3atIG/vz/Kly//0jFt2rQBAERHR2s7HhERyeTUqVMQQmDy5MmwtrYucN7IyAgzZ84E8KTFVGZmpo4TEhERET3FQgYRERGRHgsLC4MQAuPGjSv2GAcHBwDAtWvXtJSKiIjkdvPmTQCAi4tLkdc8e+7WrVtaz0RERERUFBYyiIiIiPTYjRs3AAD169cv9hgzMzMAwOPHj7WSiYiI5Pfw4UMAgKWlZZHXWFhYqL9/9OiR1jMRERERFYWFDCIiIiI9ZmpqCgC4c+dOscekpqYCQKGtRoiIyDBJkiR3BCIiIjJgLGQQERER6bEaNWoAAOLi4oo95uDBgwBebRUHERERERERkbYYyx2AiIiIiLSnS5cuiImJwaJFi/Dhhx++9Ppr165hyZIlEEKge/fuOkhIRERyWrBgAezs7DRy3bRp0zQVi4iIiCgfIXF9KBEREZHeio+PR8OGDZGbm4sZM2Zg6tSpAACFQgEhBKKjo9GwYUMAwMWLF+Hh4YHY2FiULVsWCQkJqFSpkpzxiYhIS1SvA5qkVCo1+nhEREREKlyRQURERKTH6tSpgx9++AFffPEFZsyYgZ07d8Ld3V19fsOGDTAxMUF4eDj27t2LvLw8CCEwd+5cFjGIiPScJuc1arooQkRERPQsrsggIiIiMgC//fYbvv32W+Tk5BR5s0mSJBgZGWH27NmYOHGijhMSEZEuhYaGavwxO3bsqPHHJCIiIgJYyCAiIiIyGOfPn8fs2bMRFBSEmzdv5jtXvnx59OrVC19//TWcnJxkSkhERERERERUEAsZRERERAYoOTkZaWlpUCqVqFChAmrXrg2FQiF3LCIiIiIiIqICWMggIiIiIiIiIiIiIqISi9PuiIiIiIiIiIiIiIioxDKWOwARERERac/du3fx559/AgBGjx4Ne3v7F15/48YNLF26FAAwefJklC1bVusZiYiIiIiIiF6EraWIiIiI9NiCBQswbtw4ODo64uLFiy+9XpIkNGjQAJcvX8aSJUswatQoHaQkIiIiIiIiKhpbSxERERHpsV27dkEIAU9Pz2JdL4SAl5cXJEnCjh07tJyOiIiIiIiI6OVYyCAiIiLSY6dPnwYAtG3btthj2rRpk28sERERERERkZxYyCAiIiLSY2lpaQDw0r0xnlWlShUAQGpqqlYyEREREREREb0KFjKIiIiI9JiZmRkA4MGDB8Ueo7rWyMhIK5mIiIiIiIiIXgULGURERER6TLUSIyoqqthjVNeqVmYQERERERERyYmFDCIiIiI95urqCkmSsGDBAuTk5Lz0+pycHCxYsABCCLRv314HCYmIiIiIiIhejIUMIiIiIj02YsQIAEBcXBy8vb1f2GLqwYMHGDx4MC5dupRvLBEREREREZGchCRJktwhiIiIiEh7vL29sW7dOggh8NZbb2H06NFwdXVVt526ceMGwsLCsGzZMly9ehUA4OHhgfXr18sZm4iIiIiIiAgACxlEREREeu/Ro0fo168f9u/fDyFEkdep3hZ269YN27ZtU28UTkRERERERCQntpYiIiIi0nNmZmbYs2cP5s6di2rVqkGSpEK/qlevjr/++gu7d+9mEYOIiIiIiIhKDK7IICIiIjIgkiTh9OnTOHXqFG7dugUAqFixIpo1a4amTZu+cMUGERERERERkRxYyCAiIiIiIiIiIiIiohKLraWIiIiIiIiIiIiIiKjEYiGDiIiIiIiIiIiIiIhKLGO5AxARERGRbqj2xzhz5gxu3bqFhw8f4mVdRqdNm6ajdERERERERESF4x4ZRERERAZg1apVmDlzJpKSkl5pnFKp1FIiIiIiIiIiouLhigwiIiIiPffNN9/g559/funqCwAQQhTrOiIiIiIiIiJd4R4ZRERERHosIiICP/30EwCgW7duOH36NE6ePAngSdFCqVTi5s2b2LVrF/r16wdJktC+fXvcuHEDeXl5ckYnIiIiIiIiAsDWUkRERER6bfjw4fD394eDgwMuXboEY2NjxMbGonHjxupCxrMWLlyIsWPHomnTpoiIiICpqalMyYmIiIiIiIie4IoMIiIiIj125MgRCCEwYcIEGBu/vKvoxx9/jIEDB+Ls2bNYsGCBDhISERERERERvRgLGURERER67MaNGwCARo0aqY8pFE/fAubk5BQYM3ToUEiShPXr12s/IBEREREREdFLsJBBREREpMdUhQo7Ozv1MUtLS/X3N2/eLDDmrbfeAgBcvnxZy+mIiIiIiIiIXo6FDCIiIiI9VqlSJQDAvXv31McqV64MIyMjAMD58+cLjFGt4sjMzNRBQiIiIiIiIqIXYyGDiIiISI+pWkpduHBBfczU1FR9vLD2UatXrwYAVK1aVQcJiYiIiIiIiF6MhQwiIiIiPebq6gpJkhAcHJzv+Pvvvw9JkrBixQpMnz4dsbGxOH78OD755BMEBgZCCIF3331XptRERERERERETwlJkiS5QxARERGRdsTGxqJx48awtLTE1atXUa5cOQDAgwcP4OTkhMTERAgh8o2RJAm2trY4ffq0er8MIiIiIiIiIrlwRQYRERGRHmvUqBGCg4OxZcsW5Obmqo9bWFggODgY7dq1gyRJ+b6cnJxw4MABFjGIiIiIiIioROCKDCIiIiIDd/HiRcTGxiI3NxeOjo5wdnaWOxIRERERERGRGgsZRERERERERERERERUYrG1FBERERERERERERERlVjGcgcgIiIiIt3Jzc3FyZMnER0djdu3bwMAbG1t4eTkhGbNmsHExETmhERERERERET5sZBBREREZACysrLw3XffYfny5eoCxvNsbGwwatQofPvtt7CystJxQiIiIiIiIqLCcY8MIiIiIj138eJF9OzZE8nJyXjZWz8hBKpXr449e/agfv36OkpIREREREREVDQWMoiIiIj02N27d9GoUSPcuHEDkiTByckJw4YNQ8uWLVG5cmUAQGpqKiIjI7Fq1SpER0cDAKpVq4aYmBiUL19ezvhERERERERELGQQERER6bMpU6bg559/hhACs2bNwpQpUyCEKPRaSZLw008/4dtvv4UQAl9++SV+/PFHHScmIiIiIiIiyo+FDCIiIiI99vbbb+PSpUvw9PTEP//8U6wxgwcPxvr161G/fn2cP39eywmJiIiIiIiIXkwhdwAiIiIi0p6kpCQAwPDhw4s9RnWtaiwRERERERGRnFjIICIiItJjVlZWAAA7O7tij1Fda2lpqZVMRERERERERK+ChQwiIiIiPda4cWMAQFxcXLHHqK5VjSUiIiIiIiKSEwsZRERERHpszJgxkCQJc+fORV5e3kuvz8vLw5w5cyCEwIcffqiDhEREREREREQvxkIGERERkR4bNGgQRowYgWPHjmHAgAFISUkp8trU1FS4u7sjIiICw4cPx/vvv6/DpERERERERESFE5IkSXKHICIiIqI34+/v/8Lz8+fPR2RkJMzMzNC9e3e0aNECdnZ2EEIgNTUVkZGR2Lt3Lx4/fgwXFxeMHTsWAODr66uL+ERERERERERFYiGDiIiISA8oFAoIIV56nSRJRV73/DkhBHJzczWWkYiIiIiIiOh1GMsdgIiIiIg0o7jzU150Hee4EBERERERUUnDQgYRERGRHrhy5YrcEYiIiIiIiIi0gq2liIiIiIiIiIiIiIioxOKKDCIiIiI9FhYWBgCwt7eHo6OjzGmIiIiIiIiIXp1C7gBEREREpD1ubm7o1KkTwsPD5Y5CRERERERE9FpYyCAiIiLSY5aWlgCAxo0by5yEiIiIiIiI6PWwkEFERESkx2rUqAEAePDggcxJiIiIiIiIiF4PCxlEREREeqx3794AgP3798uchIiIiIiIiOj1CEmSJLlDEBEREZF2pKSkoHHjxsjOzkZ4eDicnJzkjkRERERERET0Srgig4iIiEiPValSBUFBQbCyskK7du3w448/IjExUe5YRERERERERMXGFRlEREREeqx27doAgPv37+PWrVsQQgB4sgm4tbU1jIyMihwrhEB8fLxOchIREREREREVhYUMIiIiIj2mULz+AlwhBJRKpQbTEBEREREREb06Y7kDEBEREZH2DBs2TO4IRERERERERG+EKzKIiIiIiIiIiIiIiKjE4mbfRERERERERERERERUYrGQQURERERERPR/7d1tkJVl/Qfw7wFEngIUXJBG0GxkMHzAlBBHQyQIUSDxYRwsSM3EtAc1HZMczSInx1FTcQgfwHJf6DS1guETipkuoihK2kCkpo7imooYD8bC/l8we/5suwvLAu1x+XxmzszZ+76u6/6d+xWc77nuHwAAJUuPDACA3ci6deuyePHirFy5MmvXrs348ePTtWvXli4LAAAAGqVHBgDAbuCtt97KT37yk9x///3ZsGFD8fjSpUtz8MEHF/++8847M2PGjHTr1i2PPPJICoVCS5QLAAAARYIMAIBW7tlnn82YMWPy0UcfZct/+hUKhXpBRlVVVfr27ZsNGzbkT3/6U0aNGtUSJQMAAECRHhkAAK3YqlWrMm7cuHz44Yfp3bt3pk+fnqVLlzY6vqysLKNHj06SPPjgg/+rMgEAAKBRemQAALRiv/71r1NVVZWePXumsrIyffv23eacESNGpKKiIosWLfofVAgAAABbZ0cGAEArNmfOnBQKhVx88cVNCjGS5Etf+lKS5B//+MeuLA0AAACaRJABANCKrVixIkly3HHHNXnOXnvtlSRZvXr1LqkJAAAAtocgAwCgFVu/fn2SZI899mjynDVr1iRJOnbsuEtqAgAAgO0hyAAAaMXKysqSJK+//nqT5yxZsiRJ0qdPn11REgAAAGwXQQYAQCv2la98JUkyb968Jo2vqanJzJkzUygUcuyxx+7K0gAAAKBJBBkAAK3YxIkTU1NTk3vvvbe402JrLrnkkrz00ktJkkmTJu3i6gAAAGDbBBkAAK3YuHHjcvzxx6e6ujonnHBCbr/99lRVVRXPV1dX55133sn999+fY489NjfffHMKhUJOOeWUDB06tAUrBwAAgM0KNTU1NS1dBAAAu86qVatywgkn5MUXX0yhUNjq2JqamgwZMiSPPvpoOnfu/D+qEAAAABpnRwYAQCvXvXv3VFZW5oorrkjXrl1TU1PT4Ktjx4657LLLsmDBAiEGAAAAJcOODACA3ciaNWvy5JNP5vnnn09VVVU2btyYHj16ZNCgQRkxYkS6devW0iUCAABAHYIMAAAAAACgZHm0FAAAAAAAULLatXQBAADsHG+++eZOX7Nv3747fU0AAADYHh4tBQDQSrRp0yaFQmGnrVcoFFJdXb3T1gMAAIDmsCMDAKAV8RsVAAAAWhtBBgBAKzFp0qStnl+1alUqKipSKBTyrW99639UFQAAAOwYj5YCANhNvPLKKznkkENSKBSycePGli4HAAAAmqRNSxcAAAAAAADQGEEGAAAAAABQsgQZAAAAAABAyRJkAAAAAAAAJUuQAQAAAAAAlCxBBgAAAAAAULIEGQAAAAAAQMkSZAAAAAAAACWrXUsXAADAzvGzn/1sq+erqqqaPLbWVVddtUM1AQAAwI4q1NTU1LR0EQAA7Lg2bdqkUCjs1DU3bty4U9cDAACA7WVHBgBAK7Izf6Oys0MRAAAAaA5BBgBAK/HEE0+0dAkAAACw03m0FAAAAAAAULLatHQBAAAAAAAAjRFkAAAAAAAAJUuQAQAAAAAAlCxBBgAAAAAAULIEGQAAAAAAQMkSZAAAAAAAACVLkAEAAAAAAJQsQQYAAAAAAFCyBBkAAAAAAEDJEmQAAAB8RixYsCCFQiGFQiELFiyod37y5MkpFArZf//9/+e1tZRhw4alUChk2LBhLV0KAAC7iCADAABolbb80v+/X506dUq/fv0yfvz4lJeXp7q6uqXLBQAAGiHIAAAAdjvr1q3Lm2++mYqKikycODFDhw7NypUrW7qskrY77vYAAKA0CDIAAIBWb8qUKVm6dGnxVVlZmVtuuaX4pfxzzz2XcePGpaampmUL3UGzZs1KTU1N3njjjZYuBQAAdpp2LV0AAADArlZWVpaBAwfWOTZkyJBMnDgxgwcPzooVK7Jo0aLMnTs3J598cgtVCQAANMSODAAAYLe111575Yorrij+/dBDD7VgNQAAQEMEGQAAwG5t8ODBxff//Oc/k9RtFL5gwYJs2rQpd911V44//vj06tUrbdq0yeTJk+ut9cILL+T8889P//7906VLl3Tu3Dn9+/fPlClTsnz58m3Wsm7dukybNi2HHXZYOnfunB49euSYY47JzJkzs2nTpm3Ob2ofi08++SQ33HBDhg8fnt69e6d9+/bp2rVrBg0alIsuuihPP/10cezVV1+dQqGQ2bNnF+9RQw3UG7J+/frceuutOeGEE4rXKSsry4gRI3LnnXc2qcn6woULc9ppp6V3797p0KFDDjjggJx33nlZtmzZNucCANA6eLQUAACwW9tjjz2K7zdu3Fjv/Pr16zNq1Kg89thjja6xadOmXHrppbnpppvq9dlYvnx5li9fnjvuuCO33XZbzjvvvAbXWLlyZYYPH56//e1vxWNr167NM888k2eeeSa///3vc/HFF2/vx6vnsccey5lnnpl//etfdY5v2LAhS5YsyZIlS3LrrbfucL+Ql156KePGjSuGQ7Xef//9zJ8/P/Pnz8+MGTMyZ86c9OrVq8E1brzxxlx66aV1Qpw33ngjM2fOTHl5ee67774dqhEAgM8GQQYAALBbW7p0afF9nz596p2//PLL8/LLL2fs2LGZPHly+vXrl/feey+rV68ujrnooosyffr0JMlxxx2XyZMn5wtf+EI6deqUl156KTfddFNeeeWVfPe7303v3r0zduzYOteorq7OSSedVAwxRo4cmSlTpmS//fbLm2++menTp+fhhx/Ohx9+uEOf9Yknnsjo0aNTXV2dtm3b5pvf/GbGjRuXvn37Zv369Xn11Vczb968zJkzpzjnggsuyKmnnpqpU6emoqIiffr0ycMPP7zV66xYsSJf/epX8/HHH6dr16753ve+l8GDB2e//fbLBx98kAceeCAzZswoNll/6qmn6gRKSfKHP/yhGNx069Ytl19+eYYNG5Ykefzxx/OrX/0qEydOzD777LND9wQAgNInyAAAAHZb1dXVueGGG4p/135RvqWXX345U6dOzbXXXtvgGo8++mgxxLjjjjtyzjnn1Dl/1FFH5ayzzsqYMWPy+OOP5/vf/35OPPHEtGv3//8dmzFjRhYvXpwkOe+88zJjxoziuS9/+cv5xje+kXPOOSd33XVXsz/r+vXrc9ZZZ6W6ujqdOnXKgw8+WO/zDh06NOeee27eeuut4rGysrKUlZWle/fuSTbvYPnvxun/bdKkSfn4448zaNCgPPLII+nZs2ed8yNHjsxJJ52UMWPG5Nlnn82sWbPyne98p3j+P//5Ty688MIkm0OMysrKDBgwoHj+6KOPzrhx43LMMcfk73//e3NuBwAAnyF6ZAAAALudNWvW5Mknn8zXvva1LFy4MEnSr1+/nH766fXGHnTQQbn66qsbXeu6665LkkyYMKFeiFGrQ4cOufXWW5Ns7jHxxBNP1DlfG4T06tUrN954Y4Nr3HzzzTu0++Cee+7JO++8kySZNm1ag6FNrf3226/Z13nqqafyzDPPJElmz55dL8So9fWvfz2nnnpqkmTWrFl1zlVUVBRr/elPf1onxKg1cODAXHnllc2uEwCAzw5BBgAA0Opdc801dRpTd+nSJcOGDcuCBQuSbN518Mc//jF77rlnvblnnHFG2rZt2+C6q1evLq5R+6V8YwYMGFD8Ur+ysrJ4/N13382rr76aJDn99NPTqVOnBud36dKlwaClqebOnZsk6dy5c53dDzvbAw88kCTp379/DjnkkK2OPe6445Ikzz33XJ3G37X9SAqFQiZNmtTo/G9/+9uNNhoHAKD1EGQAAAC7rQMOOCA//vGPs3Tp0hx++OENjjn00EMbnf/iiy8WG1GfeeaZdcKShl61DbZXrlxZXGPLHh1HHXXUVusdPHhwUz9ag7Ummx9V1VhYsjM8//zzSZJly5Zt837UPj5qw4YNdfp/1N6TAw44oNEdHUmyzz77ZP/9999lnwUAgNKgRwYAANDqTZkyJRdccEGSzb/y79ChQ3r27Jlu3bptc+5ee+3V6Lmqqqpm1bN27dri+y2/wC8rK9vqvF69ejXrekmKIcq+++7b7DWaYmfek23dj2TzPXn99debdU0AAD4bBBkAAECrV1ZWts0G1Y1p7LFSSbJx48bi+xkzZmTo0KFNWrOxcKQ1PCap9p4cdthh+d3vftfkeZ///OfrHWsN9wMAgB0nyAAAAGimHj16FN936tSpWWHJlqHGe++9t9Wx2zq/NT179szbb7+dd999t9lrNEXtPfn3v//d7PCo9p405fPuyD0BAOCzQY8MAACAZjr88MOLuwaefvrpZq2xZUPs5557bqtjt3V+a4444ogkm3tYbPkYp6Zq6u6IQYMGJUlee+21Or1AtkftPXn99dfzwQcfNDru/fffzxtvvNGsawAA8NkhyAAAAGimffbZJ0OGDEmSlJeX5/3339/uNfr06ZMBAwYkSe6///6sW7euwXFr1qzJfffd1+xaTz755CSbe1H85je/2e75HTp0SJJ8+umnWx03duzYJElNTU1uvvnm7b5OkowYMaK4xj333NPouFmzZqWmpqZZ1wAA4LNDkAEAALADpk6dmiRZvXp1Tj311KxatarRsZ9++mluu+22rF+/vs7xKVOmJElWrlyZSy65pMG5P/rRj5rdSDtJzjrrrGIfiiuvvDJPPvlko2Pffvvtesdqm4RXVVXlk08+aXTuyJEjM3jw4CTJ9ddfv83wZenSpZkzZ06dY+PHjy9e79prr82yZcvqzXv11Vfzi1/8YqtrAwDQOggyAAAAdsCJJ56YH/zgB0mSP//5zxkwYECuueaazJ8/P0uWLMnTTz+d2bNn59xzz82+++6bCy+8MNXV1XXWmDJlSvGRTLfffntGjx6dioqKvPDCC6moqMioUaMyc+bMHHnkkc2us0OHDvntb3+bdu3aZe3atRkxYkTOPvvsPPDAA3nhhRdSWVmZu+++O6eddloOPPDAevNrG5lv2rQp559/fhYuXJgVK1YUX1sqLy/P3nvvnY0bN+aMM87I2LFjc++992bRokVZvHhx5s2bl2nTpuXoo4/OoYceWi9Uad++fW655ZYkyUcffZQhQ4bkuuuuy8KFC1NZWZlf/vKXxXq++MUvNvueAADw2aDZNwAAwA668cYbs/fee+faa6/NypUrc/XVVzc6tnPnzmnbtm2dY+3atcvcuXMzfPjwLFu2LA899FAeeuihOmNGjhyZSy65JKNGjWp2nccff3zmzp2bM888Mx999FHuvvvu3H333U2aO3z48AwZMiQLFy5MeXl5ysvL65zf8hFPBx54YCorKzNhwoT89a9/zZw5c+rtuthS165d6x2bMGFCrr/++lx22WVZtWpVrrjiijrnO3XqlPvuuy/XX399vSAFAIDWxY4MAACAHVQoFHLVVVdl+fLlueyyy3LkkUdm7733Ttu2bfO5z30uBx98cCZOnJjZs2fn3XffTceOHeut0adPn7z44ov5+c9/noEDB6Zjx47p3r17hgwZkunTp2fevHlp3779Dtc6atSovPbaa5k2bVqGDh2aHj16pG3btunatWuOOOKI/PCHP8yiRYvqzWvTpk0eeeSRTJ06NYcddli6dOmy1QbgBx10UJYsWZLy8vJMmDAhffv2TceOHdO+ffvsu+++GTZsWKZOnZrFixfnqquuanCNSy+9NH/5y19yyimnpKysLHvuuWf69euXs88+O88//3zGjBmzw/cDAIDSV6jRGQ0AAAAAAChRdmQAAAAAAAAlS5ABAAAAAACULEEGAAAAAABQsgQZAAAAAABAyRJkAAAAAAAAJUuQAQAAAAAAlCxBBgAAAAAAULIEGQAAAAAAQMkSZAAAAAAAACVLkAEAAAAAAJQsQQYAAAAAAFCyBBkAAAAAAEDJEmQAAAAAAAAlS5ABAAAAAACULEEGAAAAAABQsgQZAAAAAABAyRJkAAAAAAAAJUuQAQAAAAAAlCxBBgAAAAAAULIEGQAAAAAAQMkSZAAAAAAAACVLkAEAAAAAAJQsQQYAAAAAAFCyBBkAAAAAAEDJ+j+CtlzI2i5lCAAAAABJRU5ErkJggg=="},"metadata":{"image/png":{"width":793,"height":689}}},{"name":"stdout","text":"----------------------------------------------------------------\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.80      0.71      0.75       637\n           1       0.79      0.68      0.73       703\n           2       0.75      0.65      0.70       722\n           3       0.64      0.66      0.65       443\n           4       0.80      0.87      0.83       450\n           5       0.53      0.75      0.62       293\n           6       0.64      0.71      0.67       455\n           7       0.82      0.85      0.84       600\n           8       0.54      0.64      0.59       400\n           9       0.85      0.76      0.80       697\n\n    accuracy                           0.73      5400\n   macro avg       0.71      0.73      0.72      5400\nweighted avg       0.74      0.73      0.73      5400\n\n","output_type":"stream"}]},{"cell_type":"code","source":"print(\"Size of fp16 model:\",end='')\nprint_model_size(model_fp16)\nprint(\"Size of PTQ model:\",end='')\nprint_model_size(model_quantized_static)\nprint(\"Size of QAT model:\",end='')\nprint_model_size(model_quantized_trained)","metadata":{"execution":{"iopub.status.busy":"2024-04-09T11:19:34.670967Z","iopub.execute_input":"2024-04-09T11:19:34.671363Z","iopub.status.idle":"2024-04-09T11:19:35.314575Z","shell.execute_reply.started":"2024-04-09T11:19:34.671331Z","shell.execute_reply":"2024-04-09T11:19:35.313590Z"},"trusted":true},"execution_count":105,"outputs":[{"name":"stdout","text":"Size of fp16 model:55.71 MB\nSize of PTQ model:28.88 MB\nSize of QAT model:28.88 MB\n","output_type":"stream"}]}]}